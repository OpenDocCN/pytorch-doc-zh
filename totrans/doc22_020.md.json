["```py\nimport torch\nfrom torch import nn\n\nclass MyLinear(nn.Module):\n  def __init__(self, in_features, out_features):\n    super().__init__()\n    self.weight = nn.Parameter(torch.randn(in_features, out_features))\n    self.bias = nn.Parameter(torch.randn(out_features))\n\n  def forward(self, input):\n    return (input @ self.weight) + self.bias \n```", "```py\nm = MyLinear(4, 3)\nsample_input = torch.randn(4)\nm(sample_input)\n: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>) \n```", "```py\nfor parameter in m.named_parameters():\n  print(parameter)\n: ('weight', Parameter containing:\ntensor([[ 1.0597,  1.1796,  0.8247],\n        [-0.5080, -1.2635, -1.1045],\n        [ 0.0593,  0.2469, -1.4299],\n        [-0.4926, -0.5457,  0.4793]], requires_grad=True))\n('bias', Parameter containing:\ntensor([ 0.3634,  0.2015, -0.8525], requires_grad=True)) \n```", "```py\nnet = nn.Sequential(\n  MyLinear(4, 3),\n  nn.ReLU(),\n  MyLinear(3, 1)\n)\n\nsample_input = torch.randn(4)\nnet(sample_input)\n: tensor([-0.6749], grad_fn=<AddBackward0>) \n```", "```py\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l0 = MyLinear(4, 3)\n    self.l1 = MyLinear(3, 1)\n  def forward(self, x):\n    x = self.l0(x)\n    x = F.relu(x)\n    x = self.l1(x)\n    return x \n```", "```py\nnet = Net()\nfor child in net.named_children():\n  print(child)\n: ('l0', MyLinear())\n('l1', MyLinear()) \n```", "```py\nclass BigNet(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l1 = MyLinear(5, 4)\n    self.net = Net()\n  def forward(self, x):\n    return self.net(self.l1(x))\n\nbig_net = BigNet()\nfor module in big_net.named_modules():\n  print(module)\n: ('', BigNet(\n  (l1): MyLinear()\n  (net): Net(\n    (l0): MyLinear()\n    (l1): MyLinear()\n  )\n))\n('l1', MyLinear())\n('net', Net(\n  (l0): MyLinear()\n  (l1): MyLinear()\n))\n('net.l0', MyLinear())\n('net.l1', MyLinear()) \n```", "```py\nclass DynamicNet(nn.Module):\n  def __init__(self, num_layers):\n    super().__init__()\n    self.linears = nn.ModuleList(\n      [MyLinear(4, 4) for _ in range(num_layers)])\n    self.activations = nn.ModuleDict({\n      'relu': nn.ReLU(),\n      'lrelu': nn.LeakyReLU()\n    })\n    self.final = MyLinear(4, 1)\n  def forward(self, x, act):\n    for linear in self.linears:\n      x = linear(x)\n    x = self.activations[act](x)\n    x = self.final(x)\n    return x\n\ndynamic_net = DynamicNet(3)\nsample_input = torch.randn(4)\noutput = dynamic_net(sample_input, 'relu') \n```", "```py\nfor parameter in dynamic_net.named_parameters():\n  print(parameter)\n: ('linears.0.weight', Parameter containing:\ntensor([[-1.2051,  0.7601,  1.1065,  0.1963],\n        [ 3.0592,  0.4354,  1.6598,  0.9828],\n        [-0.4446,  0.4628,  0.8774,  1.6848],\n        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\n('linears.0.bias', Parameter containing:\ntensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\n('linears.1.weight', Parameter containing:\ntensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\n        [-0.0550,  1.5317,  1.1064, -0.5562],\n        [-0.4028, -0.6942,  1.5793, -1.0140],\n        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\n('linears.1.bias', Parameter containing:\ntensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\n('linears.2.weight', Parameter containing:\ntensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n        [-0.3526,  0.8756, -1.5847, -0.6016],\n        [-0.3269, -0.1608,  0.2897, -2.0829],\n        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\n('linears.2.bias', Parameter containing:\ntensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\n('final.weight', Parameter containing:\ntensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))\n('final.bias', Parameter containing:\ntensor([0.3381], requires_grad=True)) \n```", "```py\n# Move all parameters to a CUDA device\ndynamic_net.to(device='cuda')\n\n# Change precision of all parameters\ndynamic_net.to(dtype=torch.float64)\n\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>) \n```", "```py\n# Define a function to initialize Linear weights.\n# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n@torch.no_grad()\ndef init_weights(m):\n  if isinstance(m, nn.Linear):\n    nn.init.xavier_normal_(m.weight)\n    m.bias.fill_(0.0)\n\n# Apply the function recursively on the module and its submodules.\ndynamic_net.apply(init_weights) \n```", "```py\n# Create the network (from previous section) and optimizer\nnet = Net()\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\n# Run a sample training loop that \"teaches\" the network\n# to output the constant zero function\nfor _ in range(10000):\n  input = torch.randn(4)\n  output = net(input)\n  loss = torch.abs(output)\n  net.zero_grad()\n  loss.backward()\n  optimizer.step()\n\n# After training, switch the module to eval mode to do inference, compute performance metrics, etc.\n# (see discussion below for a description of training and evaluation modes)\n...\nnet.eval()\n... \n```", "```py\nprint(net.l1.weight)\n: Parameter containing:\ntensor([[-0.0013],\n        [ 0.0030],\n        [-0.0008]], requires_grad=True) \n```", "```py\nclass ModalModule(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n  def forward(self, x):\n    if self.training:\n      # Add a constant only in training mode.\n      return x + 1.\n    else:\n      return x\n\nm = ModalModule()\nx = torch.randn(4)\n\nprint('training mode output: {}'.format(m(x)))\n: tensor([1.6614, 1.2669, 1.0617, 1.6213, 0.5481])\n\nm.eval()\nprint('evaluation mode output: {}'.format(m(x)))\n: tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]) \n```", "```py\n# Save the module\ntorch.save(net.state_dict(), 'net.pt')\n\n...\n\n# Load the module later on\nnew_net = Net()\nnew_net.load_state_dict(torch.load('net.pt'))\n: <All keys matched successfully> \n```", "```py\nclass RunningMean(nn.Module):\n  def __init__(self, num_features, momentum=0.9):\n    super().__init__()\n    self.momentum = momentum\n    self.register_buffer('mean', torch.zeros(num_features))\n  def forward(self, x):\n    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n    return self.mean \n```", "```py\nm = RunningMean(4)\nfor _ in range(10):\n  input = torch.randn(4)\n  m(input)\n\nprint(m.state_dict())\n: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))\n\n# Serialized form will contain the 'mean' tensor\ntorch.save(m.state_dict(), 'mean.pt')\n\nm_loaded = RunningMean(4)\nm_loaded.load_state_dict(torch.load('mean.pt'))\nassert(torch.all(m.mean == m_loaded.mean)) \n```", "```py\nself.register_buffer('unserialized_thing', torch.randn(5), persistent=False) \n```", "```py\n# Moves all module parameters and buffers to the specified device / dtype\nm.to(device='cuda', dtype=torch.float64) \n```", "```py\nfor buffer in m.named_buffers():\n  print(buffer) \n```", "```py\nclass StatefulModule(nn.Module):\n  def __init__(self):\n    super().__init__()\n    # Setting a nn.Parameter as an attribute of the module automatically registers the tensor\n    # as a parameter of the module.\n    self.param1 = nn.Parameter(torch.randn(2))\n\n    # Alternative string-based way to register a parameter.\n    self.register_parameter('param2', nn.Parameter(torch.randn(3)))\n\n    # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything\n    # except a parameter. \"None\" entries like this will not be present in the module's state_dict.\n    self.register_parameter('param3', None)\n\n    # Registers a list of parameters.\n    self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])\n\n    # Registers a dictionary of parameters.\n    self.param_dict = nn.ParameterDict({\n      'foo': nn.Parameter(torch.randn(3)),\n      'bar': nn.Parameter(torch.randn(4))\n    })\n\n    # Registers a persistent buffer (one that appears in the module's state_dict).\n    self.register_buffer('buffer1', torch.randn(4), persistent=True)\n\n    # Registers a non-persistent buffer (one that does not appear in the module's state_dict).\n    self.register_buffer('buffer2', torch.randn(5), persistent=False)\n\n    # Reserves the \"buffer3\" attribute as a buffer, preventing it from being set to anything\n    # except a buffer. \"None\" entries like this will not be present in the module's state_dict.\n    self.register_buffer('buffer3', None)\n\n    # Adding a submodule registers its parameters as parameters of the module.\n    self.linear = nn.Linear(2, 3)\n\nm = StatefulModule()\n\n# Save and load state_dict.\ntorch.save(m.state_dict(), 'state.pt')\nm_loaded = StatefulModule()\nm_loaded.load_state_dict(torch.load('state.pt'))\n\n# Note that non-persistent buffer \"buffer2\" and reserved attributes \"param3\" and \"buffer3\" do\n# not appear in the state_dict.\nprint(m_loaded.state_dict())\n: OrderedDict([('param1', tensor([-0.0322,  0.9066])),\n               ('param2', tensor([-0.4472,  0.1409,  0.4852])),\n               ('buffer1', tensor([ 0.6949, -0.1944,  1.2911, -2.1044])),\n               ('param_list.0', tensor([ 0.4202, -0.1953])),\n               ('param_list.1', tensor([ 1.5299, -0.8747])),\n               ('param_list.2', tensor([-1.6289,  1.4898])),\n               ('param_dict.bar', tensor([-0.6434,  1.5187,  0.0346, -0.4077])),\n               ('param_dict.foo', tensor([-0.0845, -1.4324,  0.7022])),\n               ('linear.weight', tensor([[-0.3915, -0.6176],\n                                         [ 0.6062, -0.5992],\n                                         [ 0.4452, -0.2843]])),\n               ('linear.bias', tensor([-0.3710, -0.0795, -0.3947]))]) \n```", "```py\n# Initialize module directly onto GPU.\nm = nn.Linear(5, 3, device='cuda')\n\n# Initialize module with 16-bit floating point parameters.\nm = nn.Linear(5, 3, dtype=torch.half)\n\n# Skip default parameter initialization and perform custom (e.g. orthogonal) initialization.\nm = torch.nn.utils.skip_init(nn.Linear, 5, 3)\nnn.init.orthogonal_(m.weight) \n```", "```py\nm = nn.BatchNorm2d(3, dtype=torch.half)\nprint(m.running_mean)\n: tensor([0., 0., 0.], dtype=torch.float16) \n```", "```py\ntorch.manual_seed(1)\n\ndef forward_pre_hook(m, inputs):\n  # Allows for examination and modification of the input before the forward pass.\n  # Note that inputs are always wrapped in a tuple.\n  input = inputs[0]\n  return input + 1.\n\ndef forward_hook(m, inputs, output):\n  # Allows for examination of inputs / outputs and modification of the outputs\n  # after the forward pass. Note that inputs are always wrapped in a tuple while outputs\n  # are passed as-is.\n\n  # Residual computation a la ResNet.\n  return output + inputs[0]\n\ndef backward_hook(m, grad_inputs, grad_outputs):\n  # Allows for examination of grad_inputs / grad_outputs and modification of\n  # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and\n  # grad_outputs are always wrapped in tuples.\n  new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n  return new_grad_inputs\n\n# Create sample module & input.\nm = nn.Linear(3, 3)\nx = torch.randn(2, 3, requires_grad=True)\n\n# ==== Demonstrate forward hooks. ====\n# Run input through module before and after adding hooks.\nprint('output with no forward hooks: {}'.format(m(x)))\n: output with no forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n                                        [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)\n\n# Note that the modified input results in a different output.\nforward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\nprint('output with forward pre hook: {}'.format(m(x)))\n: output with forward pre hook: tensor([[-0.5752, -0.7421,  0.4942],\n                                        [-0.0736,  0.5461,  0.0838]], grad_fn=<AddmmBackward>)\n\n# Note the modified output.\nforward_hook_handle = m.register_forward_hook(forward_hook)\nprint('output with both forward hooks: {}'.format(m(x)))\n: output with both forward hooks: tensor([[-1.0980,  0.6396,  0.4666],\n                                          [ 0.3634,  0.6538,  1.0256]], grad_fn=<AddBackward0>)\n\n# Remove hooks; note that the output here matches the output before adding hooks.\nforward_pre_hook_handle.remove()\nforward_hook_handle.remove()\nprint('output after removing forward hooks: {}'.format(m(x)))\n: output after removing forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n                                               [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)\n\n# ==== Demonstrate backward hooks. ====\nm(x).sum().backward()\nprint('x.grad with no backwards hook: {}'.format(x.grad))\n: x.grad with no backwards hook: tensor([[ 0.4497, -0.5046,  0.3146],\n                                         [ 0.4497, -0.5046,  0.3146]])\n\n# Clear gradients before running backward pass again.\nm.zero_grad()\nx.grad.zero_()\n\nm.register_full_backward_hook(backward_hook)\nm(x).sum().backward()\nprint('x.grad with backwards hook: {}'.format(x.grad))\n: x.grad with backwards hook: tensor([[42., 42., 42.],\n                                      [42., 42., 42.]]) \n```"]