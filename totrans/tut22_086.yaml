- en: Fusing Convolution and Batch Norm using Custom Function
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义函数融合卷积和批量归一化
- en: 原文：[https://pytorch.org/tutorials/intermediate/custom_function_conv_bn_tutorial.html](https://pytorch.org/tutorials/intermediate/custom_function_conv_bn_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/intermediate/custom_function_conv_bn_tutorial.html](https://pytorch.org/tutorials/intermediate/custom_function_conv_bn_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-intermediate-custom-function-conv-bn-tutorial-py)
    to download the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-intermediate-custom-function-conv-bn-tutorial-py)下载完整示例代码
- en: Fusing adjacent convolution and batch norm layers together is typically an inference-time
    optimization to improve run-time. It is usually achieved by eliminating the batch
    norm layer entirely and updating the weight and bias of the preceding convolution
    [0]. However, this technique is not applicable for training models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 将相邻的卷积和批量归一化层融合在一起通常是一种推理时间的优化，以提高运行时性能。通常通过完全消除批量归一化层并更新前面卷积的权重和偏置来实现[0]。然而，这种技术不适用于训练模型。
- en: In this tutorial, we will show a different technique to fuse the two layers
    that can be applied during training. Rather than improved runtime, the objective
    of this optimization is to reduce memory usage.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将展示一种不同的技术来融合这两个层，可以在训练期间应用。与改进运行时性能不同，这种优化的目标是减少内存使用。
- en: The idea behind this optimization is to see that both convolution and batch
    norm (as well as many other ops) need to save a copy of their input during forward
    for the backward pass. For large batch sizes, these saved inputs are responsible
    for most of your memory usage, so being able to avoid allocating another input
    tensor for every convolution batch norm pair can be a significant reduction.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化的理念是看到卷积和批量归一化（以及许多其他操作）都需要在前向传播期间保存其输入的副本以供反向传播使用。对于大批量大小，这些保存的输入占用了大部分内存，因此能够避免为每个卷积批量归一化对分配另一个输入张量可以显著减少内存使用量。
- en: In this tutorial, we avoid this extra allocation by combining convolution and
    batch norm into a single layer (as a custom function). In the forward of this
    combined layer, we perform normal convolution and batch norm as-is, with the only
    difference being that we will only save the inputs to the convolution. To obtain
    the input of batch norm, which is necessary to backward through it, we recompute
    convolution forward again during the backward pass.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们通过将卷积和批量归一化合并为单个层（作为自定义函数）来避免这种额外的分配。在这个组合层的前向传播中，我们执行正常的卷积和批量归一化，唯一的区别是我们只保存卷积的输入。为了获得批量归一化的输入，这对于反向传播是必要的，我们在反向传播期间再次重新计算卷积的前向传播。
- en: It is important to note that the usage of this optimization is situational.
    Though (by avoiding one buffer saved) we always reduce the memory allocated at
    the end of the forward pass, there are cases when the *peak* memory allocated
    may not actually be reduced. See the final section for more details.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，这种优化的使用是情境性的。虽然（通过避免保存一个缓冲区）我们总是在前向传播结束时减少分配的内存，但在某些情况下，*峰值*内存分配实际上可能并未减少。请查看最后一节以获取更多详细信息。
- en: For simplicity, in this tutorial we hardcode bias=False, stride=1, padding=0,
    dilation=1, and groups=1 for Conv2D. For BatchNorm2D, we hardcode eps=1e-3, momentum=0.1,
    affine=False, and track_running_statistics=False. Another small difference is
    that we add epsilon in the denominator outside of the square root in the computation
    of batch norm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，在本教程中，我们将Conv2D的bias=False，stride=1，padding=0，dilation=1和groups=1硬编码。对于BatchNorm2D，我们将eps=1e-3，momentum=0.1，affine=False和track_running_statistics=False硬编码。另一个小的区别是在计算批量归一化时，在平方根的分母外部添加了epsilon。
- en: '[0] [https://nenadmarkus.com/p/fusing-batchnorm-and-conv/](https://nenadmarkus.com/p/fusing-batchnorm-and-conv/)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[0] [https://nenadmarkus.com/p/fusing-batchnorm-and-conv/](https://nenadmarkus.com/p/fusing-batchnorm-and-conv/)'
- en: Backward Formula Implementation for Convolution
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积的反向传播公式实现
- en: Implementing a custom function requires us to implement the backward ourselves.
    In this case, we need both the backward formulas for Conv2D and BatchNorm2D. Eventually
    we’d chain them together in our unified backward function, but below we first
    implement them as their own custom functions so we can validate their correctness
    individually
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实现自定义函数需要我们自己实现反向传播。在这种情况下，我们需要为Conv2D和BatchNorm2D分别实现反向传播公式。最终，我们会将它们链接在一起形成统一的反向传播函数，但在下面，我们首先将它们实现为各自的自定义函数，以便验证它们的正确性
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When testing with `gradcheck`, it is important to use double precision
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`gradcheck`进行测试时，重要的是使用双精度
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Backward Formula Implementation for Batch Norm
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化的反向传播公式实现
- en: 'Batch Norm has two modes: training and `eval` mode. In training mode the sample
    statistics are a function of the inputs. In `eval` mode, we use the saved running
    statistics, which are not a function of the inputs. This makes non-training mode’s
    backward significantly simpler. Below we implement and test only the training
    mode case.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Batch Norm有两种模式：训练模式和`eval`模式。在训练模式下，样本统计量是输入的函数。在`eval`模式下，我们使用保存的运行统计量，这些统计量不是输入的函数。这使得非训练模式的反向传播显著简化。下面我们只实现和测试训练模式的情况。
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Testing with `gradcheck`
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`gradcheck`进行测试
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Fusing Convolution and BatchNorm
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 融合卷积和批量归一化
- en: Now that the bulk of the work has been done, we can combine them together. Note
    that in (1) we only save a single buffer for backward, but this also means we
    recompute convolution forward in (5). Also see that in (2), (3), (4), and (6),
    it’s the same exact code as the examples above.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在大部分工作已经完成，我们可以将它们组合在一起。请注意，在（1）中我们只保存一个用于反向传播的缓冲区，但这也意味着我们在（5）中重新计算卷积的前向传播。还请注意，在（2）、（3）、（4）和（6）中，代码与上面的示例完全相同。
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The next step is to wrap our functional variant in a stateful nn.Module
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将我们的功能变体包装在一个有状态的nn.Module中
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Use `gradcheck` to validate the correctness of our backward formula
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`gradcheck`验证我们的反向传播公式的正确性
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Testing out our new Layer
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试我们的新层
- en: 'Use `FusedConvBN` to train a basic network The code below is after some light
    modifications to the example here: [https://github.com/pytorch/examples/tree/master/mnist](https://github.com/pytorch/examples/tree/master/mnist)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`FusedConvBN`来训练一个基本网络。下面的代码经过了对这里示例的一些轻微修改：[https://github.com/pytorch/examples/tree/master/mnist](https://github.com/pytorch/examples/tree/master/mnist)
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: A Comparison of Memory Usage
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存使用比较
- en: 'If CUDA is enabled, print out memory usage for both fused=True and fused=False
    For an example run on NVIDIA GeForce RTX 3070, NVIDIA CUDA® Deep Neural Network
    library (cuDNN) 8.0.5: fused peak memory: 1.56GB, unfused peak memory: 2.68GB'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用了CUDA，则打印出融合为True和融合为False的内存使用情况。例如，在NVIDIA GeForce RTX 3070上运行，NVIDIA
    CUDA®深度神经网络库（cuDNN）8.0.5：融合峰值内存：1.56GB，未融合峰值内存：2.68GB
- en: It is important to note that the *peak* memory usage for this model may vary
    depending the specific cuDNN convolution algorithm used. For shallower models,
    it may be possible for the peak memory allocated of the fused model to exceed
    that of the unfused model! This is because the memory allocated to compute certain
    cuDNN convolution algorithms can be high enough to “hide” the typical peak you
    would expect to be near the start of the backward pass.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，对于这个模型，*峰值*内存使用量可能会因使用的特定cuDNN卷积算法而异。对于较浅的模型，融合模型的峰值内存分配可能会超过未融合模型！这是因为为计算某些cuDNN卷积算法分配的内存可能足够高，以至于“隐藏”您期望在反向传递开始附近的典型峰值。
- en: For this reason, we also record and display the memory allocated at the end
    of the forward pass as an approximation, and to demonstrate that we indeed allocate
    one fewer buffer per fused `conv-bn` pair.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们还记录并显示在前向传递结束时分配的内存，以便近似，并展示我们确实为每个融合的`conv-bn`对分配了一个更少的缓冲区。
- en: '[PRE12]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Total running time of the script:** ( 0 minutes 37.014 seconds)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（0分钟37.014秒）'
- en: '[`Download Python source code: custom_function_conv_bn_tutorial.py`](../_downloads/187aea79daf1552dd05cdde1f4b4e34d/custom_function_conv_bn_tutorial.py)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：custom_function_conv_bn_tutorial.py`](../_downloads/187aea79daf1552dd05cdde1f4b4e34d/custom_function_conv_bn_tutorial.py)'
- en: '[`Download Jupyter notebook: custom_function_conv_bn_tutorial.ipynb`](../_downloads/e42651bf8aa9a118fc1867c909799393/custom_function_conv_bn_tutorial.ipynb)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：custom_function_conv_bn_tutorial.ipynb`](../_downloads/e42651bf8aa9a118fc1867c909799393/custom_function_conv_bn_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)'
