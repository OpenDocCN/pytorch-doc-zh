- en: Parametrizations Tutorial
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/intermediate/parametrizations.html](https://pytorch.org/tutorials/intermediate/parametrizations.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-intermediate-parametrizations-py) to download
    the full example code
  prefs: []
  type: TYPE_NORMAL
- en: '**Author**: [Mario Lezcano](https://github.com/lezcano)'
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing deep-learning models is a surprisingly challenging task. Classical
    techniques such as penalty methods often fall short when applied on deep models
    due to the complexity of the function being optimized. This is particularly problematic
    when working with ill-conditioned models. Examples of these are RNNs trained on
    long sequences and GANs. A number of techniques have been proposed in recent years
    to regularize these models and improve their convergence. On recurrent models,
    it has been proposed to control the singular values of the recurrent kernel for
    the RNN to be well-conditioned. This can be achieved, for example, by making the
    recurrent kernel [orthogonal](https://en.wikipedia.org/wiki/Orthogonal_matrix).
    Another way to regularize recurrent models is via “[weight normalization](https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html)”.
    This approach proposes to decouple the learning of the parameters from the learning
    of their norms. To do so, the parameter is divided by its [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm)
    and a separate parameter encoding its norm is learned. A similar regularization
    was proposed for GANs under the name of “[spectral normalization](https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html)”.
    This method controls the Lipschitz constant of the network by dividing its parameters
    by their [spectral norm](https://en.wikipedia.org/wiki/Matrix_norm#Special_cases),
    rather than their Frobenius norm.
  prefs: []
  type: TYPE_NORMAL
- en: 'All these methods have a common pattern: they all transform a parameter in
    an appropriate way before using it. In the first case, they make it orthogonal
    by using a function that maps matrices to orthogonal matrices. In the case of
    weight and spectral normalization, they divide the original parameter by its norm.'
  prefs: []
  type: TYPE_NORMAL
- en: More generally, all these examples use a function to put extra structure on
    the parameters. In other words, they use a function to constrain the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, you will learn how to implement and use this pattern to put
    constraints on your model. Doing so is as easy as writing your own `nn.Module`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Requirements: `torch>=1.9.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing parametrizations by hand
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assume that we want to have a square linear layer with symmetric weights, that
    is, with weights `X` such that `X = Xᵀ`. One way to do so is to copy the upper-triangular
    part of the matrix into its lower-triangular part
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We can then use this idea to implement a linear layer with symmetric weights
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The layer can be then used as a regular linear layer
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation, although correct and self-contained, presents a number
    of problems:'
  prefs: []
  type: TYPE_NORMAL
- en: It reimplements the layer. We had to implement the linear layer as `x @ A`.
    This is not very problematic for a linear layer, but imagine having to reimplement
    a CNN or a Transformer…
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It does not separate the layer and the parametrization. If the parametrization
    were more difficult, we would have to rewrite its code for each layer that we
    want to use it in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It recomputes the parametrization every time we use the layer. If we use the
    layer several times during the forward pass, (imagine the recurrent kernel of
    an RNN), it would compute the same `A` every time that the layer is called.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduction to parametrizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parametrizations can solve all these problems as well as others.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by reimplementing the code above using `torch.nn.utils.parametrize`.
    The only thing that we have to do is to write the parametrization as a regular
    `nn.Module`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is all we need to do. Once we have this, we can transform any regular layer
    into a symmetric layer by doing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, the matrix of the linear layer is symmetric
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can do the same thing with any other layer. For example, we can create a
    CNN with [skew-symmetric](https://en.wikipedia.org/wiki/Skew-symmetric_matrix)
    kernels. We use a similar parametrization, copying the upper-triangular part with
    signs reversed into the lower-triangular part
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Inspecting a parametrized module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When a module is parametrized, we find that the module has changed in three
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model.weight` is now a property'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It has a new `module.parametrizations` attribute
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The unparametrized weight has been moved to `module.parametrizations.weight.original`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After parametrizing `weight`, `layer.weight` is turned into a [Python property](https://docs.python.org/3/library/functions.html#property).
    This property computes `parametrization(weight)` every time we request `layer.weight`
    just as we did in our implementation of `LinearSymmetric` above.
  prefs: []
  type: TYPE_NORMAL
- en: Registered parametrizations are stored under a `parametrizations` attribute
    within the module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This `parametrizations` attribute is an `nn.ModuleDict`, and it can be accessed
    as such
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Each element of this `nn.ModuleDict` is a `ParametrizationList`, which behaves
    like an `nn.Sequential`. This list will allow us to concatenate parametrizations
    on one weight. Since this is a list, we can access the parametrizations indexing
    it. Here’s where our `Symmetric` parametrization sits
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The other thing that we notice is that, if we print the parameters, we see that
    the parameter `weight` has been moved
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It now sits under `layer.parametrizations.weight.original`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Besides these three small differences, the parametrization is doing exactly
    the same as our manual implementation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parametrizations are first-class citizens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since `layer.parametrizations` is an `nn.ModuleList`, it means that the parametrizations
    are properly registered as submodules of the original module. As such, the same
    rules for registering parameters in a module apply to register a parametrization.
    For example, if a parametrization has parameters, these will be moved from CPU
    to CUDA when calling `model = model.cuda()`.
  prefs: []
  type: TYPE_NORMAL
- en: Caching the value of a parametrization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parametrizations come with an inbuilt caching system via the context manager
    `parametrize.cached()`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Concatenating parametrizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concatenating two parametrizations is as easy as registering them on the same
    tensor. We may use this to create more complex parametrizations from simpler ones.
    For example, the [Cayley map](https://en.wikipedia.org/wiki/Cayley_transform#Matrix_map)
    maps the skew-symmetric matrices to the orthogonal matrices of positive determinant.
    We can concatenate `Skew` and a parametrization that implements the Cayley map
    to get a layer with orthogonal weights
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This may also be used to prune a parametrized module, or to reuse parametrizations.
    For example, the matrix exponential maps the symmetric matrices to the Symmetric
    Positive Definite (SPD) matrices But the matrix exponential also maps the skew-symmetric
    matrices to the orthogonal matrices. Using these two facts, we may reuse the parametrizations
    before to our advantage
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Initializing parametrizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parametrizations come with a mechanism to initialize them. If we implement a
    method `right_inverse` with signature
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: it will be used when assigning to the parametrized tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s upgrade our implementation of the `Skew` class to support this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We may now initialize a layer that is parametrized with `Skew`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This `right_inverse` works as expected when we concatenate parametrizations.
    To see this, let’s upgrade the Cayley parametrization to also support being initialized
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This initialization step can be written more succinctly as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The name of this method comes from the fact that we would often expect that
    `forward(right_inverse(X)) == X`. This is a direct way of rewriting that the forward
    after the initialization with value `X` should return the value `X`. This constraint
    is not strongly enforced in practice. In fact, at times, it might be of interest
    to relax this relation. For example, consider the following implementation of
    a randomized pruning method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In this case, it is not true that for every matrix A `forward(right_inverse(A))
    == A`. This is only true when the matrix `A` has zeros in the same positions as
    the mask. Even then, if we assign a tensor to a pruned parameter, it will comes
    as no surprise that tensor will be, in fact, pruned
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Removing parametrizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We may remove all the parametrizations from a parameter or a buffer in a module
    by using `parametrize.remove_parametrizations()`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: When removing a parametrization, we may choose to leave the original parameter
    (i.e. that in `layer.parametriations.weight.original`) rather than its parametrized
    version by setting the flag `leave_parametrized=False`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Total running time of the script:** ( 0 minutes 0.000 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: parametrizations.py`](../_downloads/621174a140b9f76910c50ed4afb0e621/parametrizations.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: parametrizations.ipynb`](../_downloads/c9153ca254003481aecc7a760a7b046f/parametrizations.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
