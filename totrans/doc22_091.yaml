- en: torch.utils.mobile_optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/mobile_optimizer.html](https://pytorch.org/docs/stable/mobile_optimizer.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This API is in beta and may change in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Torch mobile supports `torch.utils.mobile_optimizer.optimize_for_mobile` utility
    to run a list of optimization pass with modules in eval mode. The method takes
    the following parameters: a torch.jit.ScriptModule object, a blocklisting optimization
    set, a preserved method list, and a backend.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For CPU Backend, by default, if optimization blocklist is None or empty, `optimize_for_mobile`
    will run the following optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conv2D + BatchNorm fusion** (blocklisting option mobile_optimizer.MobileOptimizerType.CONV_BN_FUSION):
    This optimization pass folds `Conv2d-BatchNorm2d` into `Conv2d` in `forward` method
    of this module and all its submodules. The weight and bias of the `Conv2d` are
    correspondingly updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insert and Fold prepacked ops** (blocklisting option mobile_optimizer.MobileOptimizerType.INSERT_FOLD_PREPACK_OPS):
    This optimization pass rewrites the graph to replace 2D convolutions and linear
    ops with their prepacked counterparts. Prepacked ops are stateful ops in that,
    they require some state to be created, such as weight prepacking and use this
    state, i.e. prepacked weights, during op execution. XNNPACK is one such backend
    that provides prepacked ops, with kernels optimized for mobile platforms (such
    as ARM CPUs). Prepacking of weight enables efficient memory access and thus faster
    kernel execution. At the moment `optimize_for_mobile` pass rewrites the graph
    to replace `Conv2D/Linear` with 1) op that pre-packs weight for XNNPACK conv2d/linear
    ops and 2) op that takes pre-packed weight and activation as input and generates
    output activations. Since 1 needs to be done only once, we fold the weight pre-packing
    such that it is done only once at model load time. This pass of the `optimize_for_mobile`
    does 1 and 2 and then folds, i.e. removes, weight pre-packing ops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReLU/Hardtanh fusion**: XNNPACK ops support fusion of clamping. That is clamping
    of output activation is done as part of the kernel, including for 2D convolution
    and linear op kernels. Thus clamping effectively comes for free. Thus any op that
    can be expressed as clamping op, such as `ReLU` or `hardtanh`, can be fused with
    previous `Conv2D` or `linear` op in XNNPACK. This pass rewrites graph by finding
    `ReLU/hardtanh` ops that follow XNNPACK `Conv2D/linear` ops, written by the previous
    pass, and fuses them together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout removal** (blocklisting option mobile_optimizer.MobileOptimizerType.REMOVE_DROPOUT):
    This optimization pass removes `dropout` and `dropout_` nodes from this module
    when training is false.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conv packed params hoisting** (blocklisting option mobile_optimizer.MobileOptimizerType.HOIST_CONV_PACKED_PARAMS):
    This optimization pass moves convolution packed params to the root module, so
    that the convolution structs can be deleted. This decreases model size without
    impacting numerics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Add/ReLU fusion** (blocklisting option mobile_optimizer.MobileOptimizerType.FUSE_ADD_RELU):
    This pass finds instances of `relu` ops that follow `add` ops and fuses them into
    a single `add_relu`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'for Vulkan Backend, by default, if optimization blocklist is None or empty,
    `optimize_for_mobile` will run the following optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automatic GPU Transfer** (blocklisting option mobile_optimizer.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER):
    This optimization pass rewrites the graph so that moving input and output data
    to and from the GPU becomes part of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimize_for_mobile` will also invoke freeze_module pass which only preserves
    `forward` method. If you have other method to that needed to be preserved, add
    them into the preserved method list and pass into the method.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Optimize a torch script module for mobile deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**script_module** ([*ScriptModule*](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit._script.ScriptModule")) – An instance of torch script module with type
    of ScriptModule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optimization_blocklist** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Set*](https://docs.python.org/3/library/typing.html#typing.Set
    "(in Python v3.12)")*[**_MobileOptimizerType**]**]*) – A set with type of MobileOptimizerType.
    When set is not passed, optimization method will run all the optimizer pass; otherwise,
    optimizer method will run the optimization pass that is not included inside optimization_blocklist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**preserved_methods** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*List*](https://docs.python.org/3/library/typing.html#typing.List
    "(in Python v3.12)")*]*) – A list of methods that needed to be preserved when
    freeze_module pass is invoked'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**backend** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in
    Python v3.12)")) – Device type to use for running the result model (‘CPU’(default),
    ‘Vulkan’ or ‘Metal’).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A new optimized torch script module
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '*RecursiveScriptModule*'
  prefs: []
  type: TYPE_NORMAL
