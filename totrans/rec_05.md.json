["```py\ntorchrec.distributed.planner.constants.kernel_bw_lookup(compute_device: str, compute_kernel: str, hbm_mem_bw: float, ddr_mem_bw: float, caching_ratio: Optional[float] = None, prefetch_pipeline: bool = False) \u2192 Optional[float]\u00b6\n```", "```py\nclass torchrec.distributed.planner.enumerators.EmbeddingEnumerator(topology: Topology, batch_size: int, constraints: Optional[Dict[str, ParameterConstraints]] = None, estimator: Optional[Union[ShardEstimator, List[ShardEstimator]]] = None)\u00b6\n```", "```py\nenumerate(module: Module, sharders: List[ModuleSharder[Module]]) \u2192 List[ShardingOption]\u00b6\n```", "```py\npopulate_estimates(sharding_options: List[ShardingOption]) \u2192 None\u00b6\n```", "```py\ntorchrec.distributed.planner.enumerators.get_partition_by_type(sharding_type: str) \u2192 str\u00b6\n```", "```py\nclass torchrec.distributed.planner.partitioners.GreedyPerfPartitioner(sort_by: SortBy = SortBy.STORAGE, balance_modules: bool = False)\u00b6\n```", "```py\npartition(proposal: List[ShardingOption], storage_constraint: Topology) \u2192 List[ShardingOption]\u00b6\n```", "```py\nsharding_options = [\n        ShardingOption(partition_by=\"uniform\",\n                shards=[\n                    Shards(storage=1, perf=1),\n                    Shards(storage=1, perf=1),\n                ]),\n        ShardingOption(partition_by=\"uniform\",\n                shards=[\n                    Shards(storage=2, perf=2),\n                    Shards(storage=2, perf=2),\n                ]),\n        ShardingOption(partition_by=\"device\",\n                shards=[\n                    Shards(storage=3, perf=3),\n                    Shards(storage=3, perf=3),\n                ])\n        ShardingOption(partition_by=\"device\",\n                shards=[\n                    Shards(storage=4, perf=4),\n                    Shards(storage=4, perf=4),\n                ]),\n    ]\ntopology = Topology(world_size=2)\n\n# First [sharding_options[0] and sharding_options[1]] will be placed on the\n# topology with the uniform strategy, resulting in\n\ntopology.devices[0].perf.total = (1,2)\ntopology.devices[1].perf.total = (1,2)\n\n# Finally sharding_options[2] and sharding_options[3]] will be placed on the\n# topology with the device strategy (see docstring of `partition_by_device` for\n# more details).\n\ntopology.devices[0].perf.total = (1,2) + (3,4)\ntopology.devices[1].perf.total = (1,2) + (3,4)\n\n# The topology updates are done after the end of all the placements (the other\n# in the example is just for clarity). \n```", "```py\nclass torchrec.distributed.planner.partitioners.MemoryBalancedPartitioner(max_search_count: int = 10, tolerance: float = 0.02, balance_modules: bool = False)\u00b6\n```", "```py\npartition(proposal: List[ShardingOption], storage_constraint: Topology) \u2192 List[ShardingOption]\u00b6\n```", "```py\nclass torchrec.distributed.planner.partitioners.OrderedDeviceHardware(device: torchrec.distributed.planner.types.DeviceHardware, local_world_size: int)\u00b6\n```", "```py\ndevice: DeviceHardware\u00b6\n```", "```py\nlocal_world_size: int\u00b6\n```", "```py\nclass torchrec.distributed.planner.partitioners.ShardingOptionGroup(sharding_options: List[torchrec.distributed.planner.types.ShardingOption], storage_sum: torchrec.distributed.planner.types.Storage, perf_sum: float, param_count: int)\u00b6\n```", "```py\nparam_count: int\u00b6\n```", "```py\nperf_sum: float\u00b6\n```", "```py\nsharding_options: List[ShardingOption]\u00b6\n```", "```py\nstorage_sum: Storage\u00b6\n```", "```py\nclass torchrec.distributed.planner.partitioners.SortBy(value)\u00b6\n```", "```py\nPERF = 'perf'\u00b6\n```", "```py\nSTORAGE = 'storage'\u00b6\n```", "```py\ntorchrec.distributed.planner.partitioners.set_hbm_per_device(storage_constraint: Topology, hbm_per_device: int) \u2192 None\u00b6\n```", "```py\nclass torchrec.distributed.planner.perf_models.NoopPerfModel(topology: Topology)\u00b6\n```", "```py\nrate(plan: List[ShardingOption]) \u2192 float\u00b6\n```", "```py\nclass torchrec.distributed.planner.planners.EmbeddingShardingPlanner(topology: Optional[Topology] = None, batch_size: Optional[int] = None, enumerator: Optional[Enumerator] = None, storage_reservation: Optional[StorageReservation] = None, proposer: Optional[Union[Proposer, List[Proposer]]] = None, partitioner: Optional[Partitioner] = None, performance_model: Optional[PerfModel] = None, stats: Optional[Union[Stats, List[Stats]]] = None, constraints: Optional[Dict[str, ParameterConstraints]] = None, debug: bool = True)\u00b6\n```", "```py\ncollective_plan(module: Module, sharders: Optional[List[ModuleSharder[Module]]] = None, pg: Optional[ProcessGroup] = None) \u2192 ShardingPlan\u00b6\n```", "```py\nplan(module: Module, sharders: List[ModuleSharder[Module]]) \u2192 ShardingPlan\u00b6\n```", "```py\nclass torchrec.distributed.planner.proposers.EmbeddingOffloadScaleupProposer(use_depth: bool = True)\u00b6\n```", "```py\nstatic allocate_budget(model: Tensor, clfs: Tensor, budget: int, allocation_priority: Tensor) \u2192 Tensor\u00b6\n```", "```py\nstatic build_affine_storage_model(uvm_caching_sharding_options: List[ShardingOption], enumerator: Enumerator) \u2192 Tensor\u00b6\n```", "```py\nstatic clf_to_bytes(model: Tensor, clfs: Union[float, Tensor]) \u2192 Tensor\u00b6\n```", "```py\nfeedback(partitionable: bool, plan: Optional[List[ShardingOption]] = None, perf_rating: Optional[float] = None, storage_constraint: Optional[Topology] = None) \u2192 None\u00b6\n```", "```py\nstatic get_budget(proposal: List[ShardingOption], storage_constraint: Topology) \u2192 int\u00b6\n```", "```py\nstatic get_cacheability(sharding_option: ShardingOption) \u2192 Optional[float]\u00b6\n```", "```py\nstatic get_expected_lookups(sharding_option: ShardingOption) \u2192 Optional[float]\u00b6\n```", "```py\nload(search_space: List[ShardingOption], enumerator: Optional[Enumerator] = None) \u2192 None\u00b6\n```", "```py\nstatic next_plan(starting_proposal: List[ShardingOption], budget: Optional[int], enumerator: Optional[Enumerator]) \u2192 Optional[List[ShardingOption]]\u00b6\n```", "```py\npropose() \u2192 Optional[List[ShardingOption]]\u00b6\n```", "```py\nclass torchrec.distributed.planner.proposers.GreedyProposer(use_depth: bool = True, threshold: Optional[int] = None)\u00b6\n```", "```py\nfeedback(partitionable: bool, plan: Optional[List[ShardingOption]] = None, perf_rating: Optional[float] = None, storage_constraint: Optional[Topology] = None) \u2192 None\u00b6\n```", "```py\nload(search_space: List[ShardingOption], enumerator: Optional[Enumerator] = None) \u2192 None\u00b6\n```", "```py\npropose() \u2192 Optional[List[ShardingOption]]\u00b6\n```", "```py\nclass torchrec.distributed.planner.proposers.GridSearchProposer(max_proposals: int = 10000)\u00b6\n```", "```py\nfeedback(partitionable: bool, plan: Optional[List[ShardingOption]] = None, perf_rating: Optional[float] = None, storage_constraint: Optional[Topology] = None) \u2192 None\u00b6\n```", "```py\nload(search_space: List[ShardingOption], enumerator: Optional[Enumerator] = None) \u2192 None\u00b6\n```", "```py\npropose() \u2192 Optional[List[ShardingOption]]\u00b6\n```", "```py\nclass torchrec.distributed.planner.proposers.UniformProposer(use_depth: bool = True)\u00b6\n```", "```py\nfeedback(partitionable: bool, plan: Optional[List[ShardingOption]] = None, perf_rating: Optional[float] = None, storage_constraint: Optional[Topology] = None) \u2192 None\u00b6\n```", "```py\nload(search_space: List[ShardingOption], enumerator: Optional[Enumerator] = None) \u2192 None\u00b6\n```", "```py\npropose() \u2192 Optional[List[ShardingOption]]\u00b6\n```", "```py\ntorchrec.distributed.planner.proposers.proposers_to_proposals_list(proposers_list: List[Proposer], search_space: List[ShardingOption]) \u2192 List[List[ShardingOption]]\u00b6\n```", "```py\nclass torchrec.distributed.planner.shard_estimators.EmbeddingOffloadStats(cacheability: float, expected_lookups: int, mrc_hist_counts: Tensor, height: int)\u00b6\n```", "```py\nproperty cacheability: float\u00b6\n```", "```py\nstatic estimate_cache_miss_rate(cache_sizes: Tensor, hist: Tensor, bins: Tensor) \u2192 Tensor\u00b6\n```", "```py\nproperty expected_lookups: int\u00b6\n```", "```py\nexpected_miss_rate(clf: float) \u2192 float\u00b6\n```", "```py\nclass torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator(topology: Topology, constraints: Optional[Dict[str, ParameterConstraints]] = None, is_inference: bool = False)\u00b6\n```", "```py\nestimate(sharding_options: List[ShardingOption], sharder_map: Optional[Dict[str, ModuleSharder[Module]]] = None) \u2192 None\u00b6\n```", "```py\nclass torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator(topology: Topology, constraints: Optional[Dict[str, ParameterConstraints]] = None)\u00b6\n```", "```py\nestimate(sharding_options: List[ShardingOption], sharder_map: Optional[Dict[str, ModuleSharder[Module]]] = None) \u2192 None\u00b6\n```", "```py\ntorchrec.distributed.planner.shard_estimators.calculate_shard_storages(sharder: ModuleSharder[Module], sharding_type: str, tensor: Tensor, compute_device: str, compute_kernel: str, shard_sizes: List[List[int]], batch_sizes: List[int], world_size: int, local_world_size: int, input_lengths: List[float], num_poolings: List[float], caching_ratio: float, is_pooled: bool) \u2192 List[Storage]\u00b6\n```", "```py\ntorchrec.distributed.planner.shard_estimators.perf_func_emb_wall_time(shard_sizes: List[List[int]], compute_kernel: str, compute_device: str, sharding_type: str, batch_sizes: List[int], world_size: int, local_world_size: int, input_lengths: List[float], input_data_type_size: float, table_data_type_size: float, fwd_a2a_comm_data_type_size: float, bwd_a2a_comm_data_type_size: float, fwd_sr_comm_data_type_size: float, bwd_sr_comm_data_type_size: float, num_poolings: List[float], hbm_mem_bw: float, ddr_mem_bw: float, intra_host_bw: float, inter_host_bw: float, bwd_compute_multiplier: float, is_pooled: bool, is_weighted: bool = False, caching_ratio: Optional[float] = None, is_inference: bool = False, prefetch_pipeline: bool = False, expected_cache_fetches: float = 0) \u2192 List[Perf]\u00b6\n```", "```py\nclass torchrec.distributed.planner.stats.EmbeddingStats\u00b6\n```", "```py\nlog(sharding_plan: ShardingPlan, topology: Topology, batch_size: int, storage_reservation: StorageReservation, num_proposals: int, num_plans: int, run_time: float, best_plan: List[ShardingOption], constraints: Optional[Dict[str, ParameterConstraints]] = None, sharders: Optional[List[ModuleSharder[Module]]] = None, debug: bool = True) \u2192 None\u00b6\n```", "```py\nclass torchrec.distributed.planner.stats.NoopEmbeddingStats\u00b6\n```", "```py\nlog(sharding_plan: ShardingPlan, topology: Topology, batch_size: int, storage_reservation: StorageReservation, num_proposals: int, num_plans: int, run_time: float, best_plan: List[ShardingOption], constraints: Optional[Dict[str, ParameterConstraints]] = None, sharders: Optional[List[ModuleSharder[Module]]] = None, debug: bool = True) \u2192 None\u00b6\n```", "```py\ntorchrec.distributed.planner.stats.round_to_one_sigfig(x: float) \u2192 str\u00b6\n```", "```py\nclass torchrec.distributed.planner.storage_reservations.FixedPercentageStorageReservation(percentage: float)\u00b6\n```", "```py\nreserve(topology: Topology, batch_size: int, module: Module, sharders: List[ModuleSharder[Module]], constraints: Optional[Dict[str, ParameterConstraints]] = None) \u2192 Topology\u00b6\n```", "```py\nclass torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation(percentage: float, parameter_multiplier: float = 6.0, dense_tensor_estimate: Optional[int] = None)\u00b6\n```", "```py\nreserve(topology: Topology, batch_size: int, module: Module, sharders: List[ModuleSharder[Module]], constraints: Optional[Dict[str, ParameterConstraints]] = None) \u2192 Topology\u00b6\n```", "```py\nclass torchrec.distributed.planner.storage_reservations.InferenceStorageReservation(percentage: float, dense_tensor_estimate: Optional[int] = None)\u00b6\n```", "```py\nreserve(topology: Topology, batch_size: int, module: Module, sharders: List[ModuleSharder[Module]], constraints: Optional[Dict[str, ParameterConstraints]] = None) \u2192 Topology\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.DeviceHardware(rank: int, storage: Storage, perf: Perf)\u00b6\n```", "```py\nperf: Perf\u00b6\n```", "```py\nrank: int\u00b6\n```", "```py\nstorage: Storage\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.Enumerator(topology: Topology, batch_size: int = 512, constraints: Optional[Dict[str, ParameterConstraints]] = None, estimator: Optional[Union[ShardEstimator, List[ShardEstimator]]] = None)\u00b6\n```", "```py\nabstract enumerate(module: Module, sharders: List[ModuleSharder[Module]]) \u2192 List[ShardingOption]\u00b6\n```", "```py\nabstract populate_estimates(sharding_options: List[ShardingOption]) \u2192 None\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.ParameterConstraints(sharding_types: ~typing.Optional[~typing.List[str]] = None, compute_kernels: ~typing.Optional[~typing.List[str]] = None, min_partition: ~typing.Optional[int] = None, pooling_factors: ~typing.List[float] = <factory>, num_poolings: ~typing.Optional[~typing.List[float]] = None, batch_sizes: ~typing.Optional[~typing.List[int]] = None, is_weighted: bool = False, cache_params: ~typing.Optional[~torchrec.distributed.types.CacheParams] = None, enforce_hbm: ~typing.Optional[bool] = None, stochastic_rounding: ~typing.Optional[bool] = None, bounds_check_mode: ~typing.Optional[~fbgemm_gpu.split_table_batched_embeddings_ops_common.BoundsCheckMode] = None)\u00b6\n```", "```py\nbatch_sizes: Optional[List[int]] = None\u00b6\n```", "```py\nbounds_check_mode: Optional[BoundsCheckMode] = None\u00b6\n```", "```py\ncache_params: Optional[CacheParams] = None\u00b6\n```", "```py\ncompute_kernels: Optional[List[str]] = None\u00b6\n```", "```py\nenforce_hbm: Optional[bool] = None\u00b6\n```", "```py\nis_weighted: bool = False\u00b6\n```", "```py\nmin_partition: Optional[int] = None\u00b6\n```", "```py\nnum_poolings: Optional[List[float]] = None\u00b6\n```", "```py\npooling_factors: List[float]\u00b6\n```", "```py\nsharding_types: Optional[List[str]] = None\u00b6\n```", "```py\nstochastic_rounding: Optional[bool] = None\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.PartitionByType(value)\u00b6\n```", "```py\nDEVICE = 'device'\u00b6\n```", "```py\nHOST = 'host'\u00b6\n```", "```py\nUNIFORM = 'uniform'\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.Partitioner\u00b6\n```", "```py\nabstract partition(proposal: List[ShardingOption], storage_constraint: Topology) \u2192 List[ShardingOption]\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.Perf(fwd_compute: float, fwd_comms: float, bwd_compute: float, bwd_comms: float, prefetch_compute: float = 0.0)\u00b6\n```", "```py\nbwd_comms: float\u00b6\n```", "```py\nbwd_compute: float\u00b6\n```", "```py\nfwd_comms: float\u00b6\n```", "```py\nfwd_compute: float\u00b6\n```", "```py\nprefetch_compute: float = 0.0\u00b6\n```", "```py\nproperty total: float\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.PerfModel\u00b6\n```", "```py\nabstract rate(plan: List[ShardingOption]) \u2192 float\u00b6\n```", "```py\nexception torchrec.distributed.planner.types.PlannerError(message: str, error_type: PlannerErrorType = PlannerErrorType.OTHER)\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.PlannerErrorType(value)\u00b6\n```", "```py\nINSUFFICIENT_STORAGE = 'insufficient_storage'\u00b6\n```", "```py\nOTHER = 'other'\u00b6\n```", "```py\nPARTITION = 'partition'\u00b6\n```", "```py\nSTRICT_CONSTRAINTS = 'strict_constraints'\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.Proposer\u00b6\n```", "```py\nabstract feedback(partitionable: bool, plan: Optional[List[ShardingOption]] = None, perf_rating: Optional[float] = None, storage_constraint: Optional[Topology] = None) \u2192 None\u00b6\n```", "```py\nabstract load(search_space: List[ShardingOption], enumerator: Optional[Enumerator] = None) \u2192 None\u00b6\n```", "```py\nabstract propose() \u2192 Optional[List[ShardingOption]]\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.Shard(size: List[int], offset: List[int], storage: Optional[Storage] = None, perf: Optional[Perf] = None, rank: Optional[int] = None)\u00b6\n```", "```py\noffset: List[int]\u00b6\n```", "```py\nperf: Optional[Perf] = None\u00b6\n```", "```py\nrank: Optional[int] = None\u00b6\n```", "```py\nsize: List[int]\u00b6\n```", "```py\nstorage: Optional[Storage] = None\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.ShardEstimator(topology: Topology, constraints: Optional[Dict[str, ParameterConstraints]] = None)\u00b6\n```", "```py\nabstract estimate(sharding_options: List[ShardingOption], sharder_map: Optional[Dict[str, ModuleSharder[Module]]] = None) \u2192 None\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.ShardingOption(name: str, tensor: Tensor, module: Tuple[str, Module], input_lengths: List[float], batch_size: int, sharding_type: str, partition_by: str, compute_kernel: str, shards: List[Shard], cache_params: Optional[CacheParams] = None, enforce_hbm: Optional[bool] = None, stochastic_rounding: Optional[bool] = None, bounds_check_mode: Optional[BoundsCheckMode] = None, dependency: Optional[str] = None, is_pooled: Optional[bool] = None)\u00b6\n```", "```py\nproperty cache_load_factor: Optional[float]\u00b6\n```", "```py\nproperty fqn: str\u00b6\n```", "```py\nproperty is_pooled: bool\u00b6\n```", "```py\nproperty module: Tuple[str, Module]\u00b6\n```", "```py\nstatic module_pooled(module: Module, sharding_option_name: str) \u2192 bool\u00b6\n```", "```py\nproperty num_inputs: int\u00b6\n```", "```py\nproperty num_shards: int\u00b6\n```", "```py\nproperty path: str\u00b6\n```", "```py\nproperty tensor: Tensor\u00b6\n```", "```py\nproperty total_perf: float\u00b6\n```", "```py\nproperty total_storage: Storage\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.Stats\u00b6\n```", "```py\nabstract log(sharding_plan: ShardingPlan, topology: Topology, batch_size: int, storage_reservation: StorageReservation, num_proposals: int, num_plans: int, run_time: float, best_plan: List[ShardingOption], constraints: Optional[Dict[str, ParameterConstraints]] = None, sharders: Optional[List[ModuleSharder[Module]]] = None, debug: bool = False) \u2192 None\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.Storage(hbm: int, ddr: int)\u00b6\n```", "```py\nddr: int\u00b6\n```", "```py\nfits_in(other: Storage) \u2192 bool\u00b6\n```", "```py\nhbm: int\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.StorageReservation\u00b6\n```", "```py\nabstract reserve(topology: Topology, batch_size: int, module: Module, sharders: List[ModuleSharder[Module]], constraints: Optional[Dict[str, ParameterConstraints]] = None) \u2192 Topology\u00b6\n```", "```py\nclass torchrec.distributed.planner.types.Topology(world_size: int, compute_device: str, hbm_cap: Optional[int] = None, ddr_cap: Optional[int] = None, local_world_size: Optional[int] = None, hbm_mem_bw: float = 963146416.128, ddr_mem_bw: float = 54760833.024, intra_host_bw: float = 644245094.4, inter_host_bw: float = 13421772.8, bwd_compute_multiplier: float = 2)\u00b6\n```", "```py\nproperty bwd_compute_multiplier: float\u00b6\n```", "```py\nproperty compute_device: str\u00b6\n```", "```py\nproperty ddr_mem_bw: float\u00b6\n```", "```py\nproperty devices: List[DeviceHardware]\u00b6\n```", "```py\nproperty hbm_mem_bw: float\u00b6\n```", "```py\nproperty inter_host_bw: float\u00b6\n```", "```py\nproperty intra_host_bw: float\u00b6\n```", "```py\nproperty local_world_size: int\u00b6\n```", "```py\nproperty world_size: int\u00b6\n```", "```py\nclass torchrec.distributed.planner.utils.BinarySearchPredicate(A: int, B: int, tolerance: int)\u00b6\n```", "```py\nnext(prior_result: bool) \u2192 Optional[int]\u00b6\n```", "```py\ntorchrec.distributed.planner.utils.bytes_to_gb(num_bytes: int) \u2192 float\u00b6\n```", "```py\ntorchrec.distributed.planner.utils.bytes_to_mb(num_bytes: Union[float, int]) \u2192 float\u00b6\n```", "```py\ntorchrec.distributed.planner.utils.gb_to_bytes(gb: float) \u2192 int\u00b6\n```", "```py\ntorchrec.distributed.planner.utils.placement(compute_device: str, rank: int, local_size: int) \u2192 str\u00b6\n```", "```py\ntorchrec.distributed.planner.utils.prod(iterable: Iterable[int]) \u2192 int\u00b6\n```", "```py\ntorchrec.distributed.planner.utils.reset_shard_rank(proposal: List[ShardingOption]) \u2192 None\u00b6\n```", "```py\ntorchrec.distributed.planner.utils.sharder_name(t: Type[Any]) \u2192 str\u00b6\n```", "```py\ntorchrec.distributed.planner.utils.storage_repr_in_gb(storage: Optional[Storage]) \u2192 str\u00b6\n```"]