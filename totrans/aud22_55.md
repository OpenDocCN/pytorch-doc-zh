# torchaudio.pipelines

> 原文：[https://pytorch.org/audio/stable/pipelines.html](https://pytorch.org/audio/stable/pipelines.html)

`torchaudio.pipelines`模块将预训练模型与支持函数和元数据打包成简单的API，以执行特定任务。

当使用预训练模型执行任务时，除了使用预训练权重实例化模型外，客户端代码还需要以与训练期间相同的方式构建特征提取和后处理流水线。这需要将训练期间使用的信息传递过去，比如变换的类型和参数（例如，采样率和FFT频率数量）。

为了将这些信息与预训练模型绑定并轻松访问，`torchaudio.pipelines`模块使用Bundle类的概念，该类定义了一组API来实例化流水线和流水线的接口。

以下图示说明了这一点。

![https://download.pytorch.org/torchaudio/doc-assets/pipelines-intro.png](../Images/7dc27a33a67f5b02c554368a2500bcb8.png)

预训练模型和相关流水线被表示为`Bundle`的实例。相同`Bundle`的不同实例共享接口，但它们的实现不受限于相同类型。例如，[`SourceSeparationBundle`](generated/torchaudio.pipelines.SourceSeparationBundle.html#torchaudio.pipelines.SourceSeparationBundle "torchaudio.pipelines.SourceSeparationBundle")定义了执行源分离的接口，但其实例[`CONVTASNET_BASE_LIBRI2MIX`](generated/torchaudio.pipelines.CONVTASNET_BASE_LIBRI2MIX.html#torchaudio.pipelines.CONVTASNET_BASE_LIBRI2MIX "torchaudio.pipelines.CONVTASNET_BASE_LIBRI2MIX")实例化了一个[`ConvTasNet`](generated/torchaudio.models.ConvTasNet.html#torchaudio.models.ConvTasNet "torchaudio.models.ConvTasNet")模型，而[`HDEMUCS_HIGH_MUSDB`](generated/torchaudio.pipelines.HDEMUCS_HIGH_MUSDB.html#torchaudio.pipelines.HDEMUCS_HIGH_MUSDB "torchaudio.pipelines.HDEMUCS_HIGH_MUSDB")实例化了一个[`HDemucs`](generated/torchaudio.models.HDemucs.html#torchaudio.models.HDemucs "torchaudio.models.HDemucs")模型。尽管如此，因为它们共享相同的接口，使用方式是相同的。

注意

在底层，`Bundle`的实现使用了来自其他`torchaudio`模块的组件，比如[`torchaudio.models`](models.html#module-torchaudio.models "torchaudio.models")和[`torchaudio.transforms`](transforms.html#module-torchaudio.transforms "torchaudio.transforms")，甚至第三方库如[SentencPiece](https://github.com/google/sentencepiece)和[DeepPhonemizer](https://github.com/as-ideas/DeepPhonemizer)。但这些实现细节对库用户是抽象的。

## RNN-T流式/非流式ASR[](#rnn-t-streaming-non-streaming-asr "Permalink to this heading")

### 接口[](#interface "Permalink to this heading")

`RNNTBundle`定义了ASR流水线，包括三个步骤：特征提取、推理和去标记化。

![https://download.pytorch.org/torchaudio/doc-assets/pipelines-rnntbundle.png](../Images/d53f88ebd8f526f56982a4de4848dcaf.png)

| [`RNNTBundle`](generated/torchaudio.pipelines.RNNTBundle.html#torchaudio.pipelines.RNNTBundle "torchaudio.pipelines.RNNTBundle") | 用于执行自动语音识别（ASR，语音转文本）推理的RNN-T模型的组件捆绑数据类。 |
| --- | --- |
| [`RNNTBundle.FeatureExtractor`](generated/torchaudio.pipelines.RNNTBundle.FeatureExtractor.html#torchaudio.pipelines.RNNTBundle.FeatureExtractor "torchaudio.pipelines.RNNTBundle.FeatureExtractor") | RNN-T流水线中特征提取部分的接口 |
| [`RNNTBundle.TokenProcessor`](generated/torchaudio.pipelines.RNNTBundle.TokenProcessor.html#torchaudio.pipelines.RNNTBundle.TokenProcessor "torchaudio.pipelines.RNNTBundle.TokenProcessor") | RNN-T流水线中标记处理器部分的接口 |

使用`RNNTBundle`的教程

![在线 ASR 与 Emformer RNN-T](../Images/200081d049505bef5c1ce8e3c321134d.png)

[在线 ASR 与 Emformer RNN-T](tutorials/online_asr_tutorial.html#sphx-glr-tutorials-online-asr-tutorial-py)

在线 ASR 与 Emformer RNN-T![设备 ASR 与 Emformer RNN-T](../Images/62ca7f96e6d3a3011aa85c2a9228f03f.png)

[设备 ASR 与 Emformer RNN-T](tutorials/device_asr.html#sphx-glr-tutorials-device-asr-py)

设备 ASR 与 Emformer RNN-T

### 预训练模型[](#pretrained-models "跳转到此标题")

| [`EMFORMER_RNNT_BASE_LIBRISPEECH`](generated/torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH.html#torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH "torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH") | 基于 Emformer-RNNT 的 ASR 流水线，在 *LibriSpeech* 数据集上进行预训练[[Panayotov *et al.*, 2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")], 能够执行流式和非流式推理。 |
| --- | --- |

## wav2vec 2.0 / HuBERT / WavLM - SSL[](#wav2vec-2-0-hubert-wavlm-ssl "跳转到此标题")

### 界面[](#id2 "跳转到此标题")

`Wav2Vec2Bundle` 实例化生成声学特征的模型，可用于下游推理和微调。

![https://download.pytorch.org/torchaudio/doc-assets/pipelines-wav2vec2bundle.png](../Images/7a92fa41c1718aa05693226b9462514d.png)

| [`Wav2Vec2Bundle`](generated/torchaudio.pipelines.Wav2Vec2Bundle.html#torchaudio.pipelines.Wav2Vec2Bundle "torchaudio.pipelines.Wav2Vec2Bundle") | 数据类，捆绑相关信息以使用预训练的 [`Wav2Vec2Model`](generated/torchaudio.models.Wav2Vec2Model.html#torchaudio.models.Wav2Vec2Model "torchaudio.models.Wav2Vec2Model")。 |
| --- | --- |

### 预训练模型[](#id3 "跳转到此标题")

| [`WAV2VEC2_BASE`](generated/torchaudio.pipelines.WAV2VEC2_BASE.html#torchaudio.pipelines.WAV2VEC2_BASE "torchaudio.pipelines.WAV2VEC2_BASE") | Wav2vec 2.0 模型（“基础”架构），在 *LibriSpeech* 数据集的 960 小时未标记音频上进行预训练[[Panayotov *et al.*, 2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")]（"train-clean-100"、"train-clean-360" 和 "train-other-500" 的组合），未进行微调。 |
| --- | --- |
| [`WAV2VEC2_LARGE`](generated/torchaudio.pipelines.WAV2VEC2_LARGE.html#torchaudio.pipelines.WAV2VEC2_LARGE "torchaudio.pipelines.WAV2VEC2_LARGE") | Wav2vec 2.0 模型（“大”架构），在 *LibriSpeech* 数据集的 960 小时未标记音频上进行预训练[[Panayotov *et al.*, 2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")]（"train-clean-100"、"train-clean-360" 和 "train-other-500" 的组合），未进行微调。 |
| [`WAV2VEC2_LARGE_LV60K`](generated/torchaudio.pipelines.WAV2VEC2_LARGE_LV60K.html#torchaudio.pipelines.WAV2VEC2_LARGE_LV60K "torchaudio.pipelines.WAV2VEC2_LARGE_LV60K") | Wav2vec 2.0 模型（“large-lv60k”架构），在 *Libri-Light* 数据集的 60,000 小时未标记音频上进行预训练，未进行微调。 |
| [`WAV2VEC2_XLSR53`](generated/torchaudio.pipelines.WAV2VEC2_XLSR53.html#torchaudio.pipelines.WAV2VEC2_XLSR53 "torchaudio.pipelines.WAV2VEC2_XLSR53") | Wav2vec 2.0 模型（“基础”架构），在多个数据集的 56,000 小时未标记音频上进行预训练（*多语言 LibriSpeech*，*CommonVoice* 和 *BABEL*），未进行微调。 |
| [`WAV2VEC2_XLSR_300M`](generated/torchaudio.pipelines.WAV2VEC2_XLSR_300M.html#torchaudio.pipelines.WAV2VEC2_XLSR_300M "torchaudio.pipelines.WAV2VEC2_XLSR_300M") | XLS-R 模型，具有 3 亿个参数，在多个数据集的 436,000 小时未标记音频上进行预训练（*多语言 LibriSpeech*，*CommonVoice*，*VoxLingua107*，*BABEL* 和 *VoxPopuli*）涵盖 128 种语言，未进行微调。 |
| [`WAV2VEC2_XLSR_1B`](generated/torchaudio.pipelines.WAV2VEC2_XLSR_1B.html#torchaudio.pipelines.WAV2VEC2_XLSR_1B "torchaudio.pipelines.WAV2VEC2_XLSR_1B") | XLS-R 模型，具有 10 亿个参数，在多个数据集的 436,000 小时未标记音频上进行了预训练（*多语言 LibriSpeech*，*CommonVoice*，*VoxLingua107*，*BABEL* 和 *VoxPopuli*）共 128 种语言，未进行微调。 |
| [`WAV2VEC2_XLSR_2B`](generated/torchaudio.pipelines.WAV2VEC2_XLSR_2B.html#torchaudio.pipelines.WAV2VEC2_XLSR_2B "torchaudio.pipelines.WAV2VEC2_XLSR_2B") | XLS-R 模型，具有 20 亿个参数，在多个数据集的 436,000 小时未标记音频上进行了预训练（*多语言 LibriSpeech*，*CommonVoice*，*VoxLingua107*，*BABEL* 和 *VoxPopuli*）共 128 种语言，未进行微调。 |
| [`HUBERT_BASE`](generated/torchaudio.pipelines.HUBERT_BASE.html#torchaudio.pipelines.HUBERT_BASE "torchaudio.pipelines.HUBERT_BASE") | HuBERT模型（“基础”架构），在*LibriSpeech*数据集的960小时未标记音频上进行预训练[[Panayotov等人，2015年](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210. 2015. doi:10.1109/ICASSP.2015.7178964.")]（包括“train-clean-100”，“train-clean-360”和“train-other-500”），未进行微调。 |
| [`HUBERT_LARGE`](generated/torchaudio.pipelines.HUBERT_LARGE.html#torchaudio.pipelines.HUBERT_LARGE "torchaudio.pipelines.HUBERT_LARGE") | HuBERT模型（“大”架构），在*Libri-Light*数据集的60,000小时未标记音频上进行预训练[[Kahn等人，2020年](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673. 2020. \url https://github.com/facebookresearch/libri-light.")]，未进行微调。 |
| [`HUBERT_XLARGE`](generated/torchaudio.pipelines.HUBERT_XLARGE.html#torchaudio.pipelines.HUBERT_XLARGE "torchaudio.pipelines.HUBERT_XLARGE") | HuBERT模型（“超大”架构），在*Libri-Light*数据集的60,000小时未标记音频上进行预训练[[Kahn等人，2020年](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673. 2020. \url https://github.com/facebookresearch/libri-light.")]，未进行微调。 |
| [`WAVLM_BASE`](generated/torchaudio.pipelines.WAVLM_BASE.html#torchaudio.pipelines.WAVLM_BASE "torchaudio.pipelines.WAVLM_BASE") | WavLM基础模型（“基础”架构），在*LibriSpeech*数据集的960小时未标记音频上进行预训练[[Panayotov等人，2015年](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210. 2015. doi:10.1109/ICASSP.2015.7178964.")]，未进行微调。 |
| [`WAVLM_BASE_PLUS`](generated/torchaudio.pipelines.WAVLM_BASE_PLUS.html#torchaudio.pipelines.WAVLM_BASE_PLUS "torchaudio.pipelines.WAVLM_BASE_PLUS") | WavLM 基础+ 模型（"base" 架构），在 60,000 小时的 Libri-Light 数据集上进行了预训练[[Kahn *et al.*, 2020](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673\. 2020\. \url https://github.com/facebookresearch/libri-light.")]，10,000 小时的 GigaSpeech[[Chen *et al.*, 2021](references.html#id56 "Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Yujun Wang, Zhao You, and Zhiyong Yan. Gigaspeech: an evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. In Proc. Interspeech 2021\. 2021.")]，以及 24,000 小时的 *VoxPopuli*[[Wang *et al.*, 2021](references.html#id5 "Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. CoRR, 2021\. URL: https://arxiv.org/abs/2101.00390, arXiv:2101.00390.")]，未进行微调。 |
| [`WAVLM_LARGE`](generated/torchaudio.pipelines.WAVLM_LARGE.html#torchaudio.pipelines.WAVLM_LARGE "torchaudio.pipelines.WAVLM_LARGE") | WavLM 大型模型（"large" 架构），在 60,000 小时的 Libri-Light 数据集上进行了预训练[[Kahn *et al.*, 2020](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673\. 2020\. \url https://github.com/facebookresearch/libri-light.")]，10,000 小时的 GigaSpeech[[Chen *et al.*, 2021](references.html#id56 "Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Yujun Wang, Zhao You, and Zhiyong Yan. Gigaspeech: an evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. In Proc. Interspeech 2021\. 2021.")]，以及 24,000 小时的 *VoxPopuli*[[Wang *et al.*, 2021](references.html#id5 "Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. CoRR, 2021\. URL: https://arxiv.org/abs/2101.00390, arXiv:2101.00390.")]，未进行微调。 |

## wav2vec 2.0 / HuBERT - 微调 ASR[](#wav2vec-2-0-hubert-fine-tuned-asr "Permalink to this heading")

### Interface[](#id35 "Permalink to this heading")

`Wav2Vec2ASRBundle` 实例化了生成预定义标签上的概率分布的模型，可用于 ASR。

![https://download.pytorch.org/torchaudio/doc-assets/pipelines-wav2vec2asrbundle.png](../Images/5f9b45dac675bb2cb840209162a85158.png)

| [`Wav2Vec2ASRBundle`](generated/torchaudio.pipelines.Wav2Vec2ASRBundle.html#torchaudio.pipelines.Wav2Vec2ASRBundle "torchaudio.pipelines.Wav2Vec2ASRBundle") | 数据类，捆绑了与预训练的 [`Wav2Vec2Model`](generated/torchaudio.models.Wav2Vec2Model.html#torchaudio.models.Wav2Vec2Model "torchaudio.models.Wav2Vec2Model") 相关的信息。 |
| --- | --- |

使用 `Wav2Vec2ASRBundle` 的教程

![使用 Wav2Vec2 进行语音识别](../Images/a6aefab61852740b8a11d3cfd1ac6866.png)

[使用 Wav2Vec2 进行语音识别](tutorials/speech_recognition_pipeline_tutorial.html#sphx-glr-tutorials-speech-recognition-pipeline-tutorial-py)

使用 Wav2Vec2 进行语音识别![CTC 解码器进行 ASR 推断](../Images/260e63239576cae8ee00cfcba8e4889e.png)

[CTC 解码器进行 ASR 推断](tutorials/asr_inference_with_ctc_decoder_tutorial.html#sphx-glr-tutorials-asr-inference-with-ctc-decoder-tutorial-py)

CTC 解码器进行 ASR 推断![使用 Wav2Vec2 进行强制对齐](../Images/6658c9fe256ea584e84432cc92cd4db9.png)

[使用 Wav2Vec2 进行强制对齐](tutorials/forced_alignment_tutorial.html#sphx-glr-tutorials-forced-alignment-tutorial-py)

使用 Wav2Vec2 进行强制对齐

### 预训练模型[](#id36 "跳转到此标题的永久链接")

| [`WAV2VEC2_ASR_BASE_10M`](generated/torchaudio.pipelines.WAV2VEC2_ASR_BASE_10M.html#torchaudio.pipelines.WAV2VEC2_ASR_BASE_10M "torchaudio.pipelines.WAV2VEC2_ASR_BASE_10M") | Wav2vec 2.0 模型（带有额外线性模块的“基础”架构），在 *LibriSpeech* 数据集的 960 小时未标记音频上进行预训练[[Panayotov *et al.*, 2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")]（由 "train-clean-100"、"train-clean-360" 和 "train-other-500" 组成），并在 *Libri-Light* 数据集的 10 分钟转录音频上进行了 ASR 微调[[Kahn *et al.*, 2020](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673\. 2020\. \url https://github.com/facebookresearch/libri-light.")]（"train-10min" 子集）。 |
| --- | --- |
| [`WAV2VEC2_ASR_BASE_100H`](generated/torchaudio.pipelines.WAV2VEC2_ASR_BASE_100H.html#torchaudio.pipelines.WAV2VEC2_ASR_BASE_100H "torchaudio.pipelines.WAV2VEC2_ASR_BASE_100H") | Wav2vec 2.0 模型（带有额外线性模块的“基础”架构），在 *LibriSpeech* 数据集的 960 小时未标记音频上进行预训练[[Panayotov *et al.*, 2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")]（由 "train-clean-100"、"train-clean-360" 和 "train-other-500" 组成），并在 "train-clean-100" 子集的 100 小时转录音频上进行了 ASR 微调。 |
| [`WAV2VEC2_ASR_BASE_960H`](generated/torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H.html#torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H "torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H") | Wav2vec 2.0 模型（"base" 架构，带有额外的线性模块），在 *LibriSpeech* 数据集的 960 小时未标记音频上进行预训练[[Panayotov *et al.*, 2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")]（"train-clean-100"、"train-clean-360" 和 "train-other-500" 的组合），并在相同音频上与相应的转录进行了 ASR 微调。 |
| [`WAV2VEC2_ASR_LARGE_10M`](generated/torchaudio.pipelines.WAV2VEC2_ASR_LARGE_10M.html#torchaudio.pipelines.WAV2VEC2_ASR_LARGE_10M "torchaudio.pipelines.WAV2VEC2_ASR_LARGE_10M") | Wav2vec 2.0 模型（"large" 架构，带有额外的线性模块），在 *LibriSpeech* 数据集的 960 小时未标记音频上进行预训练[[Panayotov *et al.*, 2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")]（"train-clean-100"、"train-clean-360" 和 "train-other-500" 的组合），并在 *Libri-Light* 数据集的 10 分钟转录音频上进行了 ASR 微调[[Kahn *et al.*, 2020](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673\. 2020\. \url https://github.com/facebookresearch/libri-light.")]（"train-10min" 子集）。 |
| [`WAV2VEC2_ASR_LARGE_100H`](generated/torchaudio.pipelines.WAV2VEC2_ASR_LARGE_100H.html#torchaudio.pipelines.WAV2VEC2_ASR_LARGE_100H "torchaudio.pipelines.WAV2VEC2_ASR_LARGE_100H") | Wav2vec 2.0 模型（"large" 架构，带有额外的线性模块），在 *LibriSpeech* 数据集的 960 小时未标记音频上进行预训练[[Panayotov *et al.*, 2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")]（"train-clean-100"、"train-clean-360" 和 "train-other-500" 的组合），并在相同数据集的 100 小时转录音频上进行了 ASR 微调（"train-clean-100" 子集）。 |
| [`WAV2VEC2_ASR_LARGE_960H`](generated/torchaudio.pipelines.WAV2VEC2_ASR_LARGE_960H.html#torchaudio.pipelines.WAV2VEC2_ASR_LARGE_960H "torchaudio.pipelines.WAV2VEC2_ASR_LARGE_960H") | Wav2vec 2.0 模型（"large" 架构，带有额外的线性模块），在 *LibriSpeech* 数据集的 960 小时未标记音频上进行预训练[[Panayotov *et al.*, 2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")]（"train-clean-100"、"train-clean-360" 和 "train-other-500" 的组合），并在相同音频上与相应的转录进行了 ASR 微调。 |
| [`WAV2VEC2_ASR_LARGE_LV60K_10M`](generated/torchaudio.pipelines.WAV2VEC2_ASR_LARGE_LV60K_10M.html#torchaudio.pipelines.WAV2VEC2_ASR_LARGE_LV60K_10M "torchaudio.pipelines.WAV2VEC2_ASR_LARGE_LV60K_10M") | Wav2vec 2.0 模型（"large-lv60k" 架构，带有额外的线性模块），在 *Libri-Light* 数据集的 60,000 小时未标记音频上进行预训练[[Kahn 等人，2020](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673\. 2020\. \url https://github.com/facebookresearch/libri-light.")]，并在相同数据集的经过转录的音频上进行了 ASR 的微调（"train-10min" 子集）。 |
| [`WAV2VEC2_ASR_LARGE_LV60K_100H`](generated/torchaudio.pipelines.WAV2VEC2_ASR_LARGE_LV60K_100H.html#torchaudio.pipelines.WAV2VEC2_ASR_LARGE_LV60K_100H "torchaudio.pipelines.WAV2VEC2_ASR_LARGE_LV60K_100H") | Wav2vec 2.0 模型（"large-lv60k" 架构，带有额外的线性模块），在 *Libri-Light* 数据集的 60,000 小时未标记音频上进行预训练[[Kahn 等人，2020](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673\. 2020\. \url https://github.com/facebookresearch/libri-light.")]，并在 *LibriSpeech* 数据集的经过转录的音频上进行了 ASR 的微调，微调时长为 100 小时[[Panayotov 等人，2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")]（"train-clean-100" 子集）。 |
| [`WAV2VEC2_ASR_LARGE_LV60K_960H`](generated/torchaudio.pipelines.WAV2VEC2_ASR_LARGE_LV60K_960H.html#torchaudio.pipelines.WAV2VEC2_ASR_LARGE_LV60K_960H "torchaudio.pipelines.WAV2VEC2_ASR_LARGE_LV60K_960H") | Wav2vec 2.0 模型（"large-lv60k" 架构，带有额外的线性模块），在 *Libri-Light* 数据集的 60,000 小时未标记音频上进行预训练[[Kahn 等人，2020](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673\. 2020\. \url https://github.com/facebookresearch/libri-light.")] 数据集，并在 *LibriSpeech* 数据集的经过转录的音频上进行了 ASR 的微调，微调时长为 960 小时[[Panayotov 等人，2015](references.html#id13 "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 5206-5210\. 2015\. doi:10.1109/ICASSP.2015.7178964.")]（"train-clean-100"、"train-clean-360" 和 "train-other-500" 的组合）。 |
| [`VOXPOPULI_ASR_BASE_10K_DE`](generated/torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_DE.html#torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_DE "torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_DE") | wav2vec 2.0 模型（“基础”架构），在 *VoxPopuli* 数据集的 10k 小时未标记音频上进行预训练[[Wang 等人，2021](references.html#id5 "Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, 和 Emmanuel Dupoux. Voxpopuli: 用于表示学习、半监督学习和解释的大规模多语言语音语料库。CoRR，2021。URL: https://arxiv.org/abs/2101.00390, arXiv:2101.00390。")]（由 23 种语言组成的“10k”子集），并在来自“de”子集的 282 小时转录音频上进行了 ASR 微调。 |
| [`VOXPOPULI_ASR_BASE_10K_EN`](generated/torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_EN.html#torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_EN "torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_EN") | wav2vec 2.0 模型（“基础”架构），在 *VoxPopuli* 数据集的 10k 小时未标记音频上进行预训练[[Wang 等人，2021](references.html#id5 "Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, 和 Emmanuel Dupoux. Voxpopuli: 用于表示学习、半监督学习和解释的大规模多语言语音语料库。CoRR，2021。URL: https://arxiv.org/abs/2101.00390, arXiv:2101.00390。")]（由 23 种语言组成的“10k”子集），并在来自“en”子集的 543 小时转录音频上进行了 ASR 微调。 |
| [`VOXPOPULI_ASR_BASE_10K_ES`](generated/torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_ES.html#torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_ES "torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_ES") | wav2vec 2.0 模型（“基础”架构），在 *VoxPopuli* 数据集的 10k 小时未标记音频上进行预训练[[Wang 等人，2021](references.html#id5 "Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, 和 Emmanuel Dupoux. Voxpopuli: 用于表示学习、半监督学习和解释的大规模多语言语音语料库。CoRR，2021。URL: https://arxiv.org/abs/2101.00390, arXiv:2101.00390。")]（由 23 种语言组成的“10k”子集），并在来自“es”子集的 166 小时转录音频上进行了 ASR 微调。 |
| [`VOXPOPULI_ASR_BASE_10K_FR`](generated/torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_FR.html#torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_FR "torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_FR") | wav2vec 2.0 模型（“基础”架构），在 *VoxPopuli* 数据集的 10k 小时未标记音频上进行预训练[[Wang 等人，2021](references.html#id5 "Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, 和 Emmanuel Dupoux. Voxpopuli: 用于表示学习、半监督学习和解释的大规模多语言语音语料库。CoRR，2021。URL: https://arxiv.org/abs/2101.00390, arXiv:2101.00390。")]（由 23 种语言组成的“10k”子集），并在来自“fr”子集的 211 小时转录音频上进行了 ASR 微调。 |
| [`VOXPOPULI_ASR_BASE_10K_IT`](generated/torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_IT.html#torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_IT "torchaudio.pipelines.VOXPOPULI_ASR_BASE_10K_IT") | wav2vec 2.0 模型（“base” 架构），在 *VoxPopuli* 数据集的 10,000 小时未标记音频上进行预训练[[Wang *et al.*, 2021](references.html#id5 "Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. CoRR, 2021\. URL: https://arxiv.org/abs/2101.00390, arXiv:2101.00390.")]（由 23 种语言组成的“10k”子集），并在来自“it”子集的 91 小时转录音频上进行了 ASR 微调。 |
| [`HUBERT_ASR_LARGE`](generated/torchaudio.pipelines.HUBERT_ASR_LARGE.html#torchaudio.pipelines.HUBERT_ASR_LARGE "torchaudio.pipelines.HUBERT_ASR_LARGE") | HuBERT 模型（“large” 架构），在 *Libri-Light* 数据集的 60,000 小时未标记音频上进行预训练[[Kahn *et al.*, 2020](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673\. 2020\. \url https://github.com/facebookresearch/libri-light.")], 并在来自 *LibriSpeech* 数据集的 960 小时转录音频上进行了 ASR 微调（由 "train-clean-100", "train-clean-360", 和 "train-other-500" 组成）。 |
| [`HUBERT_ASR_XLARGE`](generated/torchaudio.pipelines.HUBERT_ASR_XLARGE.html#torchaudio.pipelines.HUBERT_ASR_XLARGE "torchaudio.pipelines.HUBERT_ASR_XLARGE") | HuBERT 模型（“extra large” 架构），在 *Libri-Light* 数据集的 60,000 小时未标记音频上进行预训练[[Kahn *et al.*, 2020](references.html#id12 "J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. Libri-light: a benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 7669-7673\. 2020\. \url https://github.com/facebookresearch/libri-light.")], 并在来自 *LibriSpeech* 数据集的 960 小时转录音频上进行了 ASR 微调（由 "train-clean-100", "train-clean-360", 和 "train-other-500" 组成）。 |

## wav2vec 2.0 / HuBERT - 强制对齐[](#wav2vec-2-0-hubert-forced-alignment "Permalink to this heading")

### 界面[](#id59 "Permalink to this heading")

`Wav2Vec2FABundle` 包含预训练模型及其相关字典。此外，它支持附加 `star` 标记维度。

![https://download.pytorch.org/torchaudio/doc-assets/pipelines-wav2vec2fabundle.png](../Images/81159a1c90b6bf1cc96789ecb75c13f0.png)

| [`Wav2Vec2FABundle`](generated/torchaudio.pipelines.Wav2Vec2FABundle.html#torchaudio.pipelines.Wav2Vec2FABundle "torchaudio.pipelines.Wav2Vec2FABundle") | 数据类，捆绑了与预训练的[`Wav2Vec2Model`](generated/torchaudio.models.Wav2Vec2Model.html#torchaudio.models.Wav2Vec2Model "torchaudio.models.Wav2Vec2Model")用于强制对齐的相关信息。 |
| --- | --- |
| [`Wav2Vec2FABundle.Tokenizer`](generated/torchaudio.pipelines.Wav2Vec2FABundle.Tokenizer.html#torchaudio.pipelines.Wav2Vec2FABundle.Tokenizer "torchaudio.pipelines.Wav2Vec2FABundle.Tokenizer") | 分词器的接口 |
| [`Wav2Vec2FABundle.Aligner`](generated/torchaudio.pipelines.Wav2Vec2FABundle.Aligner.html#torchaudio.pipelines.Wav2Vec2FABundle.Aligner "torchaudio.pipelines.Wav2Vec2FABundle.Aligner") | 对齐器的接口 |

使用`Wav2Vec2FABundle`的教程

![CTC强制对齐API教程](../Images/644afa8c7cc662a8465d389ef96d587c.png)

[CTC强制对齐API教程](tutorials/ctc_forced_alignment_api_tutorial.html#sphx-glr-tutorials-ctc-forced-alignment-api-tutorial-py)

CTC强制对齐API教程![多语言数据的强制对齐](../Images/ca023cbba331b61f65d37937f8a25beb.png)

[多语言数据的强制对齐](tutorials/forced_alignment_for_multilingual_data_tutorial.html#sphx-glr-tutorials-forced-alignment-for-multilingual-data-tutorial-py)

多语言数据的强制对齐![使用Wav2Vec2进行强制对齐](../Images/6658c9fe256ea584e84432cc92cd4db9.png)

[使用Wav2Vec2进行强制对齐](tutorials/forced_alignment_tutorial.html#sphx-glr-tutorials-forced-alignment-tutorial-py)

使用Wav2Vec2进行强制对齐

### 预训练模型[](#pertrained-models "跳转到此标题的永久链接")

| [`MMS_FA`](generated/torchaudio.pipelines.MMS_FA.html#torchaudio.pipelines.MMS_FA "torchaudio.pipelines.MMS_FA") | 在来自*将语音技术扩展到1000多种语言*的1,130种语言的31,000小时数据上训练[[Pratap等人，2023](references.html#id71 "Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, and Michael Auli. Scaling speech technology to 1,000+ languages. 2023\. arXiv:2305.13516.")] |
| --- | --- |

## Tacotron2文本到语音[](#tacotron2-text-to-speech "跳转到此标题的永久链接")

`Tacotron2TTSBundle`定义了文本到语音流水线，包括三个步骤：分词、频谱图生成和声码器。频谱图生成基于[`Tacotron2`](generated/torchaudio.models.Tacotron2.html#torchaudio.models.Tacotron2 "torchaudio.models.Tacotron2")模型。

![https://download.pytorch.org/torchaudio/doc-assets/pipelines-tacotron2bundle.png](../Images/97c575d1ba15c954a23c68df0d5b0471.png)

`TextProcessor`可以是基于规则的字符分词，也可以是一个神经网络的G2P模型，从输入文本生成音素序列。

同样，`Vocoder`可以是一个没有学习参数的算法，比如Griffin-Lim，也可以是一个基于神经网络的模型，比如Waveglow。

### 接口[](#id61 "跳转到此标题的永久链接")

| [`Tacotron2TTSBundle`](generated/torchaudio.pipelines.Tacotron2TTSBundle.html#torchaudio.pipelines.Tacotron2TTSBundle "torchaudio.pipelines.Tacotron2TTSBundle") | 数据类，捆绑了与预训练的Tacotron2和声码器相关信息。 |
| --- | --- |
| [`Tacotron2TTSBundle.TextProcessor`](generated/torchaudio.pipelines.Tacotron2TTSBundle.TextProcessor.html#torchaudio.pipelines.Tacotron2TTSBundle.TextProcessor "torchaudio.pipelines.Tacotron2TTSBundle.TextProcessor") | Tacotron2TTS流水线文本处理部分的接口 |
| [`Tacotron2TTSBundle.Vocoder`](generated/torchaudio.pipelines.Tacotron2TTSBundle.Vocoder.html#torchaudio.pipelines.Tacotron2TTSBundle.Vocoder "torchaudio.pipelines.Tacotron2TTSBundle.Vocoder") | Tacotron2TTS流水线的声码器部分的接口 |

使用`Tacotron2TTSBundle`的教程

![使用Tacotron2进行文本到语音转换](../Images/5a248f30c367f9fb17d182966714fd7d.png)

[使用Tacotron2进行文本到语音转换](tutorials/tacotron2_pipeline_tutorial.html#sphx-glr-tutorials-tacotron2-pipeline-tutorial-py)

使用Tacotron2进行文本到语音转换

### 预训练模型[](#id62 "跳转到此标题")

| [`TACOTRON2_WAVERNN_PHONE_LJSPEECH`](generated/torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH.html#torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH "torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH") | 基于音素的TTS流水线，使用在*LJSpeech*上训练的[`Tacotron2`](generated/torchaudio.models.Tacotron2.html#torchaudio.models.Tacotron2 "torchaudio.models.Tacotron2")，训练了1,500个时代，并使用在*LJSpeech*的8位深度波形上训练了10,000个时代的[`WaveRNN`](generated/torchaudio.models.WaveRNN.html#torchaudio.models.WaveRNN "torchaudio.models.WaveRNN")声码器。 |
| --- | --- |
| [`TACOTRON2_WAVERNN_CHAR_LJSPEECH`](generated/torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH.html#torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH "torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH") | 基于字符的TTS流水线，使用在*LJSpeech*上训练的[`Tacotron2`](generated/torchaudio.models.Tacotron2.html#torchaudio.models.Tacotron2 "torchaudio.models.Tacotron2")，训练了1,500个时代，并使用在*LJSpeech*的8位深度波形上训练了10,000个时代的[`WaveRNN`](generated/torchaudio.models.WaveRNN.html#torchaudio.models.WaveRNN "torchaudio.models.WaveRNN")声码器。 |
| [`TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH`](generated/torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH.html#torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH "torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH") | 基于音素的TTS流水线，使用在*LJSpeech*上训练的[`Tacotron2`](generated/torchaudio.models.Tacotron2.html#torchaudio.models.Tacotron2 "torchaudio.models.Tacotron2")，训练了1,500个时代，并使用[`GriffinLim`](generated/torchaudio.transforms.GriffinLim.html#torchaudio.transforms.GriffinLim "torchaudio.transforms.GriffinLim")作为声码器。 |
| [`TACOTRON2_GRIFFINLIM_CHAR_LJSPEECH`](generated/torchaudio.pipelines.TACOTRON2_GRIFFINLIM_CHAR_LJSPEECH.html#torchaudio.pipelines.TACOTRON2_GRIFFINLIM_CHAR_LJSPEECH "torchaudio.pipelines.TACOTRON2_GRIFFINLIM_CHAR_LJSPEECH") | 基于字符的TTS流水线，使用在*LJSpeech*上训练的[`Tacotron2`](generated/torchaudio.models.Tacotron2.html#torchaudio.models.Tacotron2 "torchaudio.models.Tacotron2")，训练了1,500个时代，并使用[`GriffinLim`](generated/torchaudio.transforms.GriffinLim.html#torchaudio.transforms.GriffinLim "torchaudio.transforms.GriffinLim")作为声码器。 |

## 声源分离[](#source-separation "跳转到此标题")

### 界面[](#id69 "跳转到此标题")

`SourceSeparationBundle`实例化声源分离模型，该模型接收单声道音频并生成多声道音频。

![https://download.pytorch.org/torchaudio/doc-assets/pipelines-sourceseparationbundle.png](../Images/69b4503224dac9c3e845bd309a996829.png)

| [`SourceSeparationBundle`](generated/torchaudio.pipelines.SourceSeparationBundle.html#torchaudio.pipelines.SourceSeparationBundle "torchaudio.pipelines.SourceSeparationBundle") | 用于执行源分离的组件的数据类。 |
| --- | --- |

使用`SourceSeparationBundle`的教程

![使用混合Demucs进行音乐源分离](../Images/f822c0c06abbbf25ee5b2b2573665977.png)

[使用混合Demucs进行音乐源分离](tutorials/hybrid_demucs_tutorial.html#sphx-glr-tutorials-hybrid-demucs-tutorial-py)

使用混合Demucs进行音乐源分离

### 预训练模型[](#id70 "跳转到此标题")

| [`CONVTASNET_BASE_LIBRI2MIX`](generated/torchaudio.pipelines.CONVTASNET_BASE_LIBRI2MIX.html#torchaudio.pipelines.CONVTASNET_BASE_LIBRI2MIX "torchaudio.pipelines.CONVTASNET_BASE_LIBRI2MIX") | 使用*ConvTasNet*预训练的源分离流水线[[Luo和Mesgarani，2019](references.html#id22 "Yi Luo和Nima Mesgarani。Conv-tasnet: 超越理想的时频幅度掩蔽进行语音分离。IEEE/ACM音频、语音和语言处理交易，27(8):1256–1266，2019年8月。URL: http://dx.doi.org/10.1109/TASLP.2019.2915167, doi:10.1109/taslp.2019.2915167。")]，在*Libri2Mix数据集*上进行训练[[Cosentino等，2020](references.html#id37 "Joris Cosentino, Manuel Pariente, Samuele Cornell, Antoine Deleforge和Emmanuel Vincent。Librimix: 用于通用语音分离的开源数据集。2020年。arXiv:2005.11262。")]. |
| --- | --- |
| [`HDEMUCS_HIGH_MUSDB_PLUS`](generated/torchaudio.pipelines.HDEMUCS_HIGH_MUSDB_PLUS.html#torchaudio.pipelines.HDEMUCS_HIGH_MUSDB_PLUS "torchaudio.pipelines.HDEMUCS_HIGH_MUSDB_PLUS") | 使用*Hybrid Demucs*预训练的音乐源分离流水线[[Défossez, 2021](references.html#id50 "Alexandre Défossez. 混合频谱图和波形源分离。在ISMIR 2021音乐源分离研讨会论文集中。2021年。")]，在MUSDB-HQ的训练集和测试集以及专门为Meta制作的内部数据库中的额外150首歌曲上进行训练。 |
| [`HDEMUCS_HIGH_MUSDB`](generated/torchaudio.pipelines.HDEMUCS_HIGH_MUSDB.html#torchaudio.pipelines.HDEMUCS_HIGH_MUSDB "torchaudio.pipelines.HDEMUCS_HIGH_MUSDB") | 使用*Hybrid Demucs*预训练的音乐源分离流水线[[Défossez, 2021](references.html#id50 "Alexandre Défossez. 混合频谱图和波形源分离。在ISMIR 2021音乐源分离研讨会论文集中。2021年。")]，在MUSDB-HQ的训练集上进行训练[[Rafii等，2019](references.html#id47 "Zafar Rafii, Antoine Liutkus, Fabian-Robert Stöter, Stylianos Ioannis Mimilakis和Rachel Bittner。MUSDB18-HQ - musdb18的未压缩版本。2019年12月。URL: https://doi.org/10.5281/zenodo.3338373, doi:10.5281/zenodo.3338373。")]. |

## Squim目标[](#squim-objective "跳转到此标题")

### 界面[](#id77 "跳转到此标题")

[`SquimObjectiveBundle`](generated/torchaudio.pipelines.SquimObjectiveBundle.html#torchaudio.pipelines.SquimObjectiveBundle "torchaudio.pipelines.SquimObjectiveBundle")定义了语音质量和可懂度测量（SQUIM）流水线，可以根据输入波形预测**客观**度量分数。

| [`SquimObjectiveBundle`](generated/torchaudio.pipelines.SquimObjectiveBundle.html#torchaudio.pipelines.SquimObjectiveBundle "torchaudio.pipelines.SquimObjectiveBundle") | 封装了与预训练[`SquimObjective`](generated/torchaudio.models.SquimObjective.html#torchaudio.models.SquimObjective "torchaudio.models.SquimObjective")模型使用相关信息的数据类。 |
| --- | --- |

### 预训练模型[](#id78 "跳转到此标题")

| [`SQUIM_OBJECTIVE`](generated/torchaudio.pipelines.SQUIM_OBJECTIVE.html#torchaudio.pipelines.SQUIM_OBJECTIVE "torchaudio.pipelines.SQUIM_OBJECTIVE") | 使用[[Kumar等人，2023年](references.html#id69 "Anurag Kumar, Ke Tan, Zhaoheng Ni, Pranay Manocha, Xiaohui Zhang, Ethan Henderson, and Buye Xu. Torchaudio-squim: reference-less speech quality and intelligibility measures in torchaudio. arXiv preprint arXiv:2304.01448, 2023.")]中描述的方法训练的SquimObjective管道，基于*DNS 2020数据集*[[Reddy等人，2020年](references.html#id65 "Chandan KA Reddy, Vishak Gopal, Ross Cutler, Ebrahim Beyrami, Roger Cheng, Harishchandra Dubey, Sergiy Matusevych, Robert Aichner, Ashkan Aazami, Sebastian Braun, and others. The interspeech 2020 deep noise suppression challenge: datasets, subjective testing framework, and challenge results. arXiv preprint arXiv:2005.13981, 2020.")]。 |
| --- | --- |

## Squim Subjective[](#squim-subjective "跳转到此标题的永久链接")

### 接口[](#id81 "跳转到此标题的永久链接")

[`SquimSubjectiveBundle`](generated/torchaudio.pipelines.SquimSubjectiveBundle.html#torchaudio.pipelines.SquimSubjectiveBundle "torchaudio.pipelines.SquimSubjectiveBundle")定义了可以根据输入波形预测**主观**度量分数的语音质量和可懂度测量（SQUIM）管道。

| [`SquimSubjectiveBundle`](generated/torchaudio.pipelines.SquimSubjectiveBundle.html#torchaudio.pipelines.SquimSubjectiveBundle "torchaudio.pipelines.SquimSubjectiveBundle") | 数据类，捆绑了相关信息以使用预训练的[`SquimSubjective`](generated/torchaudio.models.SquimSubjective.html#torchaudio.models.SquimSubjective "torchaudio.models.SquimSubjective")模型。 |
| --- | --- |

### 预训练模型[](#id82 "跳转到此标题的永久链接")

| [`SQUIM_SUBJECTIVE`](generated/torchaudio.pipelines.SQUIM_SUBJECTIVE.html#torchaudio.pipelines.SQUIM_SUBJECTIVE "torchaudio.pipelines.SQUIM_SUBJECTIVE") | 如[[Manocha和Kumar，2022年](references.html#id66 "Pranay Manocha and Anurag Kumar. Speech quality assessment through mos using non-matching references. arXiv preprint arXiv:2206.12285, 2022.")]和[[Kumar等人，2023年](references.html#id69 "Anurag Kumar, Ke Tan, Zhaoheng Ni, Pranay Manocha, Xiaohui Zhang, Ethan Henderson, and Buye Xu. Torchaudio-squim: reference-less speech quality and intelligibility measures in torchaudio. arXiv preprint arXiv:2304.01448, 2023.")]中描述的方法训练的SquimSubjective管道，基于*BVCC*[[Cooper和Yamagishi，2021年](references.html#id67 "Erica Cooper and Junichi Yamagishi. How do voices from past speech synthesis challenges compare today? arXiv preprint arXiv:2105.02373, 2021.")]和*DAPS*[[Mysore，2014年](references.html#id68 "Gautham J Mysore. Can we automatically transform speech recorded on common consumer devices in real-world environments into professional production quality speech?—a dataset, insights, and challenges. IEEE Signal Processing Letters, 22(8):1006–1010, 2014.")]数据集。 |
| --- | --- |
