- en: Extending PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/notes/extending.html](https://pytorch.org/docs/stable/notes/extending.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this note we’ll cover ways of extending [`torch.nn`](../nn.html#module-torch.nn
    "torch.nn"), [`torch.autograd`](../autograd.html#module-torch.autograd "torch.autograd"),
    [`torch`](../torch.html#module-torch "torch"), and writing custom C++ extensions.
  prefs: []
  type: TYPE_NORMAL
- en: '## Extending [`torch.autograd`](../autograd.html#module-torch.autograd "torch.autograd")[](#extending-torch-autograd
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: Adding operations to [`autograd`](../autograd.html#module-torch.autograd "torch.autograd")
    requires implementing a new [`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") subclass for each operation. Recall that Functions
    are what [`autograd`](../autograd.html#module-torch.autograd "torch.autograd")
    uses to encode the operation history and compute gradients.
  prefs: []
  type: TYPE_NORMAL
- en: The first part of this doc is focused on backward mode AD as it is the most
    widely used feature. A section at the end discusses the extensions for forward
    mode AD.
  prefs: []
  type: TYPE_NORMAL
- en: When to use
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, implement a custom function if you want to perform computations
    in your model that are not differentiable or rely on non-PyTorch libraries (e.g.,
    NumPy), but still wish for your operation to chain with other ops and work with
    the autograd engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some situations, custom functions can also be used to improve performance
    and memory usage: If you implemented your forward and backward passes using a
    [C++ extension](https://pytorch.org/tutorials/advanced/cpp_extension.html), you
    can wrap them in [`Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")
    to interface with the autograd engine. If you’d like to reduce the number of buffers
    saved for the backward pass, custom functions can be used to combine ops together.'
  prefs: []
  type: TYPE_NORMAL
- en: When not to use
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you can already write your function in terms of PyTorch’s built-in ops, its
    backward graph is (most likely) already able to be recorded by autograd. In this
    case, you do not need to implement the backward function yourself. Consider using
    a plain old Python function.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to maintain state, i.e., trainable parameters, you should (also)
    use a custom module. See the section below for more information on extending [`torch.nn`](../nn.html#module-torch.nn
    "torch.nn").
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like to alter the gradients during the backward pass or perform a side
    effect, consider registering a [tensor](https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook)
    or [Module](https://pytorch.org/docs/stable/notes/modules.html#module-hooks) hook.
  prefs: []
  type: TYPE_NORMAL
- en: How to use
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Take the following steps: 1\. Subclass [`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") and implement the [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward"), (optional) `setup_context()` and [`backward()`](../generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward") methods. 2\. Call the proper methods on the
    ctx argument. 3\. Declare whether your function supports [double backward](https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html).
    4\. Validate whether your gradients are correct using gradcheck.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:** After subclassing [`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function"), you’ll need to define 3 methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") is the code that performs the operation. It
    can take as many arguments as you want, with some of them being optional, if you
    specify the default values. All kinds of Python objects are accepted here. `Tensor`
    arguments that track history (i.e., with `requires_grad=True`) will be converted
    to ones that don’t track history before the call, and their use will be registered
    in the graph. Note that this logic won’t traverse lists/dicts/any other data structures
    and will only consider tensors that are direct arguments to the call. You can
    return either a single `Tensor` output, or a [`tuple`](https://docs.python.org/3/library/stdtypes.html#tuple
    "(in Python v3.12)") of tensors if there are multiple outputs. Also, please refer
    to the docs of [`Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")
    to find descriptions of useful methods that can be called only from [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward").'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setup_context()` (optional). One can either write a “combined” [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") that accepts a `ctx` object or (as of PyTorch
    2.0) a separate [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") that does not accept `ctx` and a `setup_context()`
    method where the `ctx` modification happens. The [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") should have the compute and `setup_context()`
    should only be responsible for the `ctx` modification (and not have any compute).
    In general the separate [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") and `setup_context()` is closer to how PyTorch
    native operations work and therefore more composable with various PyTorch subsystems.
    See [Combined or separate forward() and setup_context()](#combining-forward-context)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`backward()`](../generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward") (or `vjp()`) defines the gradient formula.
    It will be given as many `Tensor` arguments as there were outputs, with each of
    them representing gradient w.r.t. that output. It is important NEVER to modify
    these in-place. It should return as many tensors as there were inputs, with each
    of them containing the gradient w.r.t. its corresponding input. If your inputs
    didn’t require gradient (`needs_input_grad` is a tuple of booleans indicating
    whether each input needs gradient computation), or were non-`Tensor` objects,
    you can return `python:None`. Also, if you have optional arguments to [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") you can return more gradients than there were
    inputs, as long as they’re all [`None`](https://docs.python.org/3/library/constants.html#None
    "(in Python v3.12)").'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2:** It is your responsibility to use the functions in `ctx` properly
    in order to ensure that the new [`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") works properly with the autograd engine.'
  prefs: []
  type: TYPE_NORMAL
- en: '[`save_for_backward()`](../generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward
    "torch.autograd.function.FunctionCtx.save_for_backward") must be used to save
    any tensors to be used in the backward pass. Non-tensors should be stored directly
    on ctx. If tensors that are neither input nor output are saved for backward your
    [`Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")
    may not support double backward (see step 3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`mark_dirty()`](../generated/torch.autograd.function.FunctionCtx.mark_dirty.html#torch.autograd.function.FunctionCtx.mark_dirty
    "torch.autograd.function.FunctionCtx.mark_dirty") must be used to mark any input
    that is modified inplace by the forward function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`mark_non_differentiable()`](../generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html#torch.autograd.function.FunctionCtx.mark_non_differentiable
    "torch.autograd.function.FunctionCtx.mark_non_differentiable") must be used to
    tell the engine if an output is not differentiable. By default all output tensors
    that are of differentiable type will be set to require gradient. Tensors of non-differentiable
    type (i.e., integral types) are never marked as requiring gradients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`set_materialize_grads()`](../generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html#torch.autograd.function.FunctionCtx.set_materialize_grads
    "torch.autograd.function.FunctionCtx.set_materialize_grads") can be used to tell
    the autograd engine to optimize gradient computations in the cases where the output
    does not depend on the input by not materializing grad tensors given to backward
    function. That is, if set to False, None object in Python or “undefined tensor”
    (tensor x for which x.defined() is False) in C++ will not be converted to a tensor
    filled with zeros prior to calling backward, and so your code will need to handle
    such objects as if they were tensors filled with zeros. The default value of this
    setting is True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 3:** If your [`Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")
    does not support double backward you should explicitly declare this by decorating
    backward with the `once_differentiable()`. With this decorator, attempts to perform
    double backward through your function will produce an error. See our double backward
    tutorial for more information on double backward.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4:** It is recommended that you use [`torch.autograd.gradcheck()`](../autograd.html#module-torch.autograd.gradcheck
    "torch.autograd.gradcheck") to check whether your backward function correctly
    computes gradients of the forward by computing the Jacobian matrix using your
    backward function and comparing the value element-wise with the Jacobian computed
    numerically using finite-differencing.'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Below you can find code for a `Linear` function, with additional comments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to make it easier to use these custom ops, we recommend either aliasing
    them or wrapping them in a function. Wrapping in a function lets us support default
    arguments and keyword arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we give an additional example of a function that is parametrized by non-Tensor
    arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And here, we optimize the above example by calling set_materialize_grads(False):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you need any “intermediate” Tensors computed in [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") to be saved, either they must be returned as
    outputs, or combine `forward` and `setup_context()` (see [Combined or separate
    forward() and setup_context()](#combining-forward-context)). Note that this means
    if you want gradients to flow through those intermediate values, you need to define
    the gradient formula for them (see also [the double backward tutorial](https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html)
    ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Inputs to `backward`, i.e., `grad_output`, can also be tensors that track history.
    So if `backward` is implemented with differentiable operations, (e.g., invocation
    of another custom [`Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")),
    higher order derivatives will work. In this case, the tensors saved with `save_for_backward`
    can also be used in the backward and have gradients flowing back but tensors saved
    in the `ctx` won’t have gradients flowing back for them. If you need gradients
    to flow back for a Tensor saved in the `ctx`, you should make it an output of
    the custom `Function` and save it with `save_for_backward`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You probably want to check if the backward method you implemented actually
    computes the derivatives of your function. It is possible by comparing with numerical
    approximations using small finite differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: See [Numerical gradient checking](../autograd.html#grad-check) for more details
    on finite-difference gradient comparisons. If your function is used in higher
    order derivatives (differentiating the backward pass) you can use the `gradgradcheck`
    function from the same package to check higher order derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: '### Combined or separate [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") and `setup_context()`[](#combined-or-separate-forward-and-setup-context
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main ways to define [`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function"). Either:'
  prefs: []
  type: TYPE_NORMAL
- en: define a [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") that combines the forward compute logic with
    `setup_context()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (as of PyTorch 2.0) define a separate [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") and `setup_context()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We recommend the second option (separate [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") and `setup_context()`) because that is closer
    to how PyTorch native operations are implemented and it composes with [`torch.func`](../func.api.html#module-torch.func
    "torch.func") transforms. However, we plan to support both approaches going forward;
    combining [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") with `setup_context()`: leads to more flexibility
    since you are able to save intermediates without returning them as output.'
  prefs: []
  type: TYPE_NORMAL
- en: Please see the previous section for how to define [`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") with separate [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") and `setup_context()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how to define a [`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") with combined [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") and `setup_context()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]  ### Forward mode AD'
  prefs: []
  type: TYPE_NORMAL
- en: Overriding the forward mode AD formula has a very similar API with some different
    subtleties. You can implement the [`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") function.
  prefs: []
  type: TYPE_NORMAL
- en: It will be given as many `Tensor` arguments as there were inputs, with each
    of them representing gradient w.r.t. that input. It should return as many tensors
    as there were outputs, with each of them containing the gradient w.r.t. its corresponding
    output. The [`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") will be called just after the [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") method, before the `apply()` returns.
  prefs: []
  type: TYPE_NORMAL
- en: '[`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") has a few subtle differences with the [`backward()`](../generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward") function:'
  prefs: []
  type: TYPE_NORMAL
- en: You can use the ctx to pass any data from the [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") to the [`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") function. If that state will not be needed for
    the [`backward()`](../generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward"), you can explicitly free it by doing `del
    ctx.foo` at the end of the [`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation of [`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") must be backward differentiable or explicitly check
    that none of the given forward mode gradient has `requires_grad` set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") function must match the view/inplace behavior of
    [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward"). For example, if the `i` th input is modified
    inplace, then the `i` th gradient must be updated inplace. Similarly, if the `j`
    th output is a view of the `k` th input. Then the returned `j` th output gradient
    must be a view of the given `k` th input gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the user cannot specify which gradient needs to be computed, the [`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") function should always compute gradients for all
    the outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The forward mode gradients do respect the flag set by [`set_materialize_grads()`](../generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html#torch.autograd.function.FunctionCtx.set_materialize_grads
    "torch.autograd.function.FunctionCtx.set_materialize_grads") and you can get None
    input gradients when this is disabled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.func`](../func.api.html#module-torch.func "torch.func") transforms
    and/or [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap "torch.vmap")[](#torch-func-transforms-and-or-torch-vmap
    "Permalink to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please see [Extending torch.func with autograd.Function](extending.func.html#func-autograd-function)
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: Extending [`torch.nn`](../nn.html#module-torch.nn "torch.nn")[](#extending-torch-nn
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`nn`](../nn.html#module-torch.nn "torch.nn") exports two kinds of interfaces
    - modules and their functional versions. You can extend it in both ways, but we
    recommend using modules for all kinds of layers, that hold any parameters or buffers,
    and recommend using a functional form parameter-less operations like activation
    functions, pooling, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding a functional version of an operation is already fully covered in the
    section above.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a [`Module`](../generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")[](#adding-a-module
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since [`nn`](../nn.html#module-torch.nn "torch.nn") heavily utilizes [`autograd`](../autograd.html#module-torch.autograd
    "torch.autograd"), adding a new [`Module`](../generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module") requires implementing a [`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") that performs the operation and can compute the gradient.
    From now on let’s assume that we want to implement a `Linear` module and we have
    the function implemented as in the listing above. There’s very little code required
    to add this. Now, there are two functions that need to be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '`__init__` (*optional*) - takes in arguments such as kernel sizes, numbers
    of features, etc. and initializes parameters and buffers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`forward()`](../generated/torch.nn.Module.html#torch.nn.Module.forward "torch.nn.Module.forward")
    - instantiates a [`Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")
    and uses it to perform the operation. It’s very similar to a functional wrapper
    shown above.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is how a `Linear` module can be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '## Extending [`torch`](../torch.html#module-torch "torch") Python API[](#extending-torch-python-api
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: You can create custom types that emulate `Tensor` by defining a custom class
    with methods that match `Tensor`. But what if you want to be able to pass these
    types to functions like [`torch.add()`](../generated/torch.add.html#torch.add
    "torch.add") in the top-level [`torch`](../torch.html#module-torch "torch") namespace
    that accept `Tensor` operands?
  prefs: []
  type: TYPE_NORMAL
- en: If your custom Python type defines a method named `__torch_function__`, PyTorch
    will invoke your `__torch_function__` implementation when an instance of your
    custom class is passed to a function in the [`torch`](../torch.html#module-torch
    "torch") namespace. This makes it possible to define custom implementations for
    any of the functions in the [`torch`](../torch.html#module-torch "torch") namespace
    which your `__torch_function__` implementation can call, allowing your users to
    make use of your custom type with existing PyTorch workflows that they have already
    written for `Tensor`. This works with “duck” types that are unrelated to `Tensor`
    as well as user-defined subclasses of `Tensor`.
  prefs: []
  type: TYPE_NORMAL
- en: Extending [`torch`](../torch.html#module-torch "torch") with a `Tensor`-like
    type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This functionality is inspired by the NumPy `__array_function__` protocol. See
    [the NumPy documentation](https://numpy.org/doc/stable/user/basics.dispatch.html#basics-dispatch)
    and [NEP-0018](https://numpy.org/neps/nep-0018-array-function-protocol.html) for
    more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this concrete, let’s begin with a simple example that illustrates the
    API dispatch mechanism. We’ll create a custom type that represents a 2D scalar
    tensor, parametrized by the order `N` and value along the diagonal entries, `value`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This first iteration of the design isn’t very useful. The main functionality
    of `ScalarTensor` is to provide a more compact string representation of a scalar
    tensor than in the base tensor class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If we try to use this object with the [`torch`](../torch.html#module-torch
    "torch") API, we will run into issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding a `__torch_function__` implementation to `ScalarTensor` makes it possible
    for the above operation to succeed. Let’s re-do our implementation, this time
    adding a `__torch_function__` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `__torch_function__` method takes four arguments: `func`, a reference to
    the torch API function that is being overridden, `types`, the list of types of
    Tensor-likes that implement `__torch_function__`, `args`, the tuple of arguments
    passed to the function, and `kwargs`, the dict of keyword arguments passed to
    the function. It uses a global dispatch table named `HANDLED_FUNCTIONS` to store
    custom implementations. The keys of this dictionary are functions in the `torch`
    namespace and the values are implementations for `ScalarTensor`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Using a global dispatch table is not a mandated part of the `__torch_function__`
    API, it is just a useful design pattern for structuring your override implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This class definition isn’t quite enough to make `torch.mean` do the right
    thing when we pass it a `ScalarTensor` – we also need to define an implementation
    for `torch.mean` for `ScalarTensor` operands and add the implementation to the
    `HANDLED_FUNCTIONS` dispatch table dictionary. One way of doing this is to define
    a decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'which can be applied to the implementation of our override:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'With this change we can now use `torch.mean` with `ScalarTensor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course `torch.mean` is an example of the simplest kind of function to override
    since it only takes one operand. We can use the same machinery to override a function
    that takes more than one operand, any one of which might be a tensor or tensor-like
    that defines `__torch_function__`, for example for [`torch.add()`](../generated/torch.add.html#torch.add
    "torch.add"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This version has a fast path for when both operands are `ScalarTensor` instances
    and also a slower path which degrades to converting the data to tensors when either
    operand is not a `ScalarTensor`. That makes the override function correctly when
    either operand is a `ScalarTensor` or a regular `Tensor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that our implementation of `add` does not take `alpha` or `out` as keyword
    arguments like [`torch.add()`](../generated/torch.add.html#torch.add "torch.add")
    does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: For speed and flexibility the `__torch_function__` dispatch mechanism does not
    check that the signature of an override function matches the signature of the
    function being overrided in the [`torch`](../torch.html#module-torch "torch")
    API. For some applications ignoring optional arguments would be fine but to ensure
    full compatibility with `Tensor`, user implementations of torch API functions
    should take care to exactly emulate the API of the function that is being overrided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Functions in the [`torch`](../torch.html#module-torch "torch") API that do
    not have explicit overrides will return `NotImplemented` from `__torch_function__`.
    If all operands with `__torch_function__` defined on them return `NotImplemented`,
    PyTorch will raise a `TypeError`. This means that most of the time operations
    that do not have explicit overrides for a type will raise a `TypeError` when an
    instance of such a type is passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In practice this means that if you would like to implement your overrides using
    a `__torch_function__` implementation along these lines, you will need to explicitly
    implement the full [`torch`](../torch.html#module-torch "torch") API or the entire
    subset of the API that you care about for your use case. This may be a tall order
    as the full [`torch`](../torch.html#module-torch "torch") API is quite extensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option is to not return `NotImplemented` for operations that are not
    handled but to instead pass a `Tensor` to the original [`torch`](../torch.html#module-torch
    "torch") function when no override is available. For example, if we change our
    implementation of `__torch_function__` for `ScalarTensor` to the one below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then [`torch.mul()`](../generated/torch.mul.html#torch.mul "torch.mul") will
    work correctly, although the return type will always be a `Tensor` rather than
    a `ScalarTensor`, even if both operands are `ScalarTensor` instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Also see the `MetadataTensor` example below for another variation on this pattern
    but instead always returns a `MetadataTensor` to propagate metadata through operations
    in the [`torch`](../torch.html#module-torch "torch") API.
  prefs: []
  type: TYPE_NORMAL
- en: The `__torch_function__` protocol is designed for full coverage of the API,
    partial coverage may lead to undesirable results, in particular, certain functions
    raising a `TypeError`. This is especially true for subclasses, where all three
    of torch.add, torch.Tensor.__add__ and torch.Tensor.add must be covered, even
    if they return exactly the same result. Failing to do this may also lead to infinite
    recursion. If one requires the implementation of a function from `torch.Tensor`
    subclasses, they must use `super().__torch_function__` inside their implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Subclassing `torch.Tensor`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As of version 1.7.0, methods on `torch.Tensor` and functions in public `torch.*`
    namespaces applied on `torch.Tensor` subclasses will return subclass instances
    instead of `torch.Tensor` instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If multiple subclasses exist, the lowest one in the hierarchy will be chosen
    by default. If there is no unique way to determine such a case, then a `TypeError`
    is raised:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If one wishes to have a global override for all tensor methods, one can use
    `__torch_function__`. Here is an example that logs all function/method calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: However, if one instead wishes to override a method on the Tensor subclass,
    there one can do so either by directly overriding the method (by defining it for
    a subclass), or by using `__torch_function__` and matching with `func`.
  prefs: []
  type: TYPE_NORMAL
- en: One should be careful within `__torch_function__` for subclasses to always call
    `super().__torch_function__(func, ...)` instead of `func` directly, as was the
    case before version 1.7.0\. Failing to do this may cause `func` to recurse back
    into `__torch_function__` and therefore cause infinite recursion.
  prefs: []
  type: TYPE_NORMAL
- en: Extending [`torch`](../torch.html#module-torch "torch") with a `Tensor` wrapper
    type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another useful case is a type that wraps a `Tensor`, either as an attribute
    or via subclassing. Below we implement a special case of this sort of type, a
    `MetadataTensor` that attaches a dictionary of metadata to a `Tensor` that is
    propagated through [`torch`](../torch.html#module-torch "torch") operations. Since
    this is a generic sort of wrapping for the full [`torch`](../torch.html#module-torch
    "torch") API, we do not need to individually implement each override so we can
    make the `__torch_function__` implementation more permissive about what operations
    are allowed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This simple implementation won’t necessarily work with every function in the
    [`torch`](../torch.html#module-torch "torch") API but it is good enough to capture
    most common operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Operations on multiple types that define `__torch_function__`[](#operations-on-multiple-types-that-define-torch-function
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is possible to use the torch API with multiple distinct types that each
    have a `__torch_function__` implementation, but special care must be taken. In
    such a case the rules are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dispatch operation gathers all distinct implementations of `__torch_function__`
    for each operand and calls them in order: subclasses before superclasses, and
    otherwise left to right in the operator expression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any value other than `NotImplemented` is returned, that value is returned
    as the result. Implementations can register that they do not implement an operation
    by returning `NotImplemented`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If all of the `__torch_function__` implementations return `NotImplemented`,
    PyTorch raises a `TypeError`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing Coverage of Overrides for the PyTorch API[](#testing-coverage-of-overrides-for-the-pytorch-api
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One troublesome aspect of implementing `__torch_function__` is that if some
    operations do and others do not have overrides, users will at best see an inconsistent
    experience, or at worst will see errors raised at runtime when they use a function
    that does not have an override. To ease this process, PyTorch provides a developer-facing
    API for ensuring full support for `__torch_function__` overrides. This API is
    private and may be subject to changes without warning in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, to get a listing of all overridable functions, use `torch.overrides._get_overridable_functions`.
    This returns a dictionary whose keys are namespaces in the `PyTorch` Python API
    and whose values are a list of functions in that namespace that can be overridden.
    For example, let’s print the names of the first 5 functions in `torch.nn.functional`
    that can be overridden:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This listing of functions makes it possible to iterate over all overridable
    functions, however in practice this is not enough to write tests for all of these
    functions without laboriously and manually copying the signature of each function
    for each test. To ease this process, the `torch.overrides._get_testing_overrides`
    function returns a dictionary mapping overridable functions in the `PyTorch` API
    to dummy lambda functions that have the same signature as the original function
    but unconditionally return -1\. These functions are most useful to use with `inspect`
    to analyze the function signature of the original `PyTorch` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, `torch.overrides.get_ignored_functions` returns a tuple of functions
    that explicitly cannot be overrided by `__torch_function__`. This list can be
    useful to confirm that a function that isn’t present in the dictionary returned
    by `get_overridable_functions` cannot be overridden.  ## Extending [`torch`](../torch.html#module-torch
    "torch") native API'
  prefs: []
  type: TYPE_NORMAL
- en: While `__torch_function__` allows one to effectively extend PyTorch’s pure Python
    components’ behavior, it does not allow one to extend the parts of PyTorch implemented
    in C++. To that end, a `Tensor` subclass can also define `__torch_dispatch__`
    which will be able to override the behavior at the C++ level.
  prefs: []
  type: TYPE_NORMAL
- en: To effectively use this feature, it is important to know how the native part
    of PyTorch is implemented. The most important component there is what we call
    the “dispatcher” (the best description can be found in this [blog post](http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/)
    even though it is slightly outdated). As hinted by its name, it is responsible
    for calling the right backend function for a specific call of a function. For
    example, when calling `torch.add(a, b)`, the dispatcher will inspect both arguments,
    figure out which “feature” (autograd, autocast, functionalization, etc) and which
    “backend” (CPU, CUDA, MPS, etc) should be used for this specific call and finally
    call all the right kernels. A very common thing done by a kernel is to “redispatch”.
    For example, when running your neural network on GPU with autocast, the first
    call will be the autocast kernel that will handle any potential autocast logic
    and redispatch down. The next feature in line will be autograd that will properly
    create the autograd graph and then redispatch down. Finally, we reach the backend
    kernel for CUDA which will launch the right CUDA kernel and return the final result.
    On the way out, autograd will attach the graph to the output and, finally, autocast
    will have a chance to do any update it needs on exit.
  prefs: []
  type: TYPE_NORMAL
- en: 'One configuration of the dispatcher is the order in which all these feature
    and backend keys are called. The latest list and their order can be found in `DispatchKey.h`
    inside the `DispatchKey` enum. For the purpose of extending torch, the important
    subset of the ordering for this discussion is:'
  prefs: []
  type: TYPE_NORMAL
- en: vmap -> Autocast -> Autograd -> ZeroTensor -> Neg/Conj -> Functionalize -> Python
    -> Backends
  prefs: []
  type: TYPE_NORMAL
- en: The most important key for the purpose of this discussion is `Python` as every
    Tensor subclass with the `__torch_dispatch__` method defined will call into this
    feature. It is from there that the user-defined method is called and where the
    behavior can be overwritten arbitrarily. From there, calling the provided `func`
    again will perform a “redispatch”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some important implications of this implementation are:'
  prefs: []
  type: TYPE_NORMAL
- en: This code runs “below all features”. It is thus only responsible, like a regular
    backend, for generating the output value of each Tensor (and can, and should,
    ignore all advanced features like autograd, autocast, etc).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any high level feature implements a given function without redispatching,
    it will never reach the `Python` key and so the `__torch_dispatch__` callback
    will never be triggered. This happens in particular for CompositeImplicitAutograd
    functions which are evaluated at the Autograd level without redispatching. This
    is because a CompositeImplicitAutograd function specifies its autograd formula
    by implicitly calling other native ops, so at the Autograd level, the function
    is decomposed into its native ops and those are evaluated instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When calling back to Python and when wrapping the results, the same conversions
    are used as the regular PyTorch Python/C++ binding. In particular, some objects
    cannot be represented in Python and need special handling (undefined Tensors for
    example become None).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our native functions are lazily populated as `torch.ops.{namespace}.{func_name}.{overload_name}`
    as callable Python objects to enable easily interacting with them from Python.
    The `func` object given to `__torch_dispatch__` is always an entry from this namespace.
    This namespace can be used to directly call native ops and bypass the usual Python
    API and binding code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a similar way where `__torch_function__` is able to interpose on all of
    torch’s Python API and Tensor methods, `__torch_dispatch__` is able intercepting
    all calls into the aten native API. Note that all methods on Tensors are converted
    into function calls before entering the dispatcher and thus will appear as function
    calls here: `torch.add(a, 2)` and `a + 2` will lead to exactly the same aten call.
    Most of these functions are defined in `native_functions.yaml` which specifies
    the properties of these functions as well as their backend implementation. Their
    implementation alongside specified features are then automatically registered
    via codegen. Some more exotic functions or features are also registered in other
    places in the C++ codebase or in user-defined C++ extensions.'
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to add new native functions using [`torch.library`](../library.html#module-torch.library
    "torch.library"). This Python feature allows defining and/or adding new implementations
    to native functions. This can be used to add missing kernels, replace existing
    ones or define brand new native functions.
  prefs: []
  type: TYPE_NORMAL
- en: You can find many examples of `__torch_dispatch__`-based subclasses in the [subclass
    zoo](https://github.com/albanD/subclass_zoo) repo.
  prefs: []
  type: TYPE_NORMAL
- en: Extending all [`torch`](../torch.html#module-torch "torch") API with Modes[](#extending-all-torch-api-with-modes
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, there are functions that do not take Tensor inputs. This means
    that the subclass approach described above cannot be used to override the behavior
    of all of PyTorch’s functions. Also, if the use case requires to intercept every
    function call, changing every Tensor to be a subclass can be overly intrusive.
  prefs: []
  type: TYPE_NORMAL
- en: To address this use case, we introduced the concept of “Mode”. These exist for
    `__torch_function__` and `__torch_dispatch__` overrides, are created by subclassing
    respectively `torch.overrides.TorchFunctionMode` and `torch.utils._python_dispatch.TorchDispatchMode`,
    and are used as a context manager.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the description of how it interacts with subclasses and other modes,
    whenever the context manager for a mode is entered, every function behaves as
    if there was an extra Tensor argument at the beginning of the argument list with
    the mode as a subclass. This means in particular that all modes handlers will
    be called before any subclass handler and that modes corresponding to the inner
    context manager will always run first.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to note that within a given mode handler, this specific
    mode is disabled and can be re-enabled manually by doing `with self:`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example that shows logging modes of each type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Which prints the following, with extra comments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Writing custom C++ extensions[](#writing-custom-c-extensions "Permalink to
    this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: See this [PyTorch tutorial](https://pytorch.org/tutorials/advanced/cpp_extension.html)
    for a detailed explanation and examples.
  prefs: []
  type: TYPE_NORMAL
- en: Documentations are available at [torch.utils.cpp_extension](../cpp_extension.html).
  prefs: []
  type: TYPE_NORMAL
