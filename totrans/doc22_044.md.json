["```py\ntorch.backends.cpu.get_cpu_capability()\u00b6\n```", "```py\ntorch.backends.cuda.is_built()\u00b6\n```", "```py\ntorch.backends.cuda.matmul.allow_tf32\u00b6\n```", "```py\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction\u00b6\n```", "```py\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction\u00b6\n```", "```py\ntorch.backends.cuda.cufft_plan_cache\u00b6\n```", "```py\ntorch.backends.cuda.cufft_plan_cache.size\u00b6\n```", "```py\ntorch.backends.cuda.cufft_plan_cache.max_size\u00b6\n```", "```py\ntorch.backends.cuda.cufft_plan_cache.clear()\u00b6\n```", "```py\ntorch.backends.cuda.preferred_linalg_library(backend=None)\u00b6\n```", "```py\ntorch.backends.cuda.SDPBackend\u00b6\n```", "```py\ntorch.backends.cuda.SDPAParams\u00b6\n```", "```py\ntorch.backends.cuda.flash_sdp_enabled()\u00b6\n```", "```py\ntorch.backends.cuda.enable_mem_efficient_sdp(enabled)\u00b6\n```", "```py\ntorch.backends.cuda.mem_efficient_sdp_enabled()\u00b6\n```", "```py\ntorch.backends.cuda.enable_flash_sdp(enabled)\u00b6\n```", "```py\ntorch.backends.cuda.math_sdp_enabled()\u00b6\n```", "```py\ntorch.backends.cuda.enable_math_sdp(enabled)\u00b6\n```", "```py\ntorch.backends.cuda.can_use_flash_attention(params, debug=False)\u00b6\n```", "```py\ntorch.backends.cuda.can_use_efficient_attention(params, debug=False)\u00b6\n```", "```py\ntorch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=True)\u00b6\n```", "```py\ntorch.backends.cudnn.version()\u00b6\n```", "```py\ntorch.backends.cudnn.is_available()\u00b6\n```", "```py\ntorch.backends.cudnn.enabled\u00b6\n```", "```py\ntorch.backends.cudnn.allow_tf32\u00b6\n```", "```py\ntorch.backends.cudnn.deterministic\u00b6\n```", "```py\ntorch.backends.cudnn.benchmark\u00b6\n```", "```py\ntorch.backends.cudnn.benchmark_limit\u00b6\n```", "```py\ntorch.backends.mps.is_available()\u00b6\n```", "```py\ntorch.backends.mps.is_built()\u00b6\n```", "```py\ntorch.backends.mkl.is_available()\u00b6\n```", "```py\nclass torch.backends.mkl.verbose(enable)\u00b6\n```", "```py\nimport torch\nmodel(data)\nwith torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):\n    model(data) \n```", "```py\ntorch.backends.mkldnn.is_available()\u00b6\n```", "```py\nclass torch.backends.mkldnn.verbose(level)\u00b6\n```", "```py\nimport torch\nmodel(data)\nwith torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):\n    model(data) \n```", "```py\ntorch.backends.openmp.is_available()\u00b6\n```", "```py\ntorch.backends.opt_einsum.is_available()\u00b6\n```", "```py\ntorch.backends.opt_einsum.get_opt_einsum()\u00b6\n```", "```py\ntorch.backends.opt_einsum.enabled\u00b6\n```", "```py\ntorch.backends.opt_einsum.strategy\u00b6\n```"]