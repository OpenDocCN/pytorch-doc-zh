["```py\nimport torch\n\nN, C, H, W = 10, 3, 32, 32\n[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.empty](https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty \"torch.empty\")(N, C, H, W)\nprint([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").stride())  # Outputs: (3072, 1024, 32, 1) \n```", "```py\n(3072, 1024, 32, 1) \n```", "```py\n[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").to(memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\"))\nprint([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").shape)  # Outputs: (10, 3, 32, 32) as dimensions order preserved\nprint([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").stride())  # Outputs: (3072, 1, 96, 3) \n```", "```py\ntorch.Size([10, 3, 32, 32])\n(3072, 1, 96, 3) \n```", "```py\n[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").to(memory_format=[torch.contiguous_format](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\"))\nprint([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").stride())  # Outputs: (3072, 1024, 32, 1) \n```", "```py\n(3072, 1024, 32, 1) \n```", "```py\n[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").contiguous(memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\"))\nprint([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").stride())  # Outputs: (3072, 1, 96, 3) \n```", "```py\n(3072, 1, 96, 3) \n```", "```py\nprint([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").is_contiguous(memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\")))  # Outputs: True \n```", "```py\nTrue \n```", "```py\n[special_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.empty](https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty \"torch.empty\")(4, 1, 4, 4)\nprint([special_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").is_contiguous(memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\")))  # Outputs: True\nprint([special_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").is_contiguous(memory_format=[torch.contiguous_format](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\")))  # Outputs: True \n```", "```py\nTrue\nTrue \n```", "```py\n[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.empty](https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty \"torch.empty\")(N, C, H, W, memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\"))\nprint([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").stride())  # Outputs: (3072, 1, 96, 3) \n```", "```py\n(3072, 1, 96, 3) \n```", "```py\n[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").clone()\nprint([y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").stride())  # Outputs: (3072, 1, 96, 3) \n```", "```py\n(3072, 1, 96, 3) \n```", "```py\nif [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available \"torch.cuda.is_available\")():\n    [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").cuda()\n    print([y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").stride())  # Outputs: (3072, 1, 96, 3) \n```", "```py\n(3072, 1, 96, 3) \n```", "```py\n[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.empty_like](https://pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like \"torch.empty_like\")([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nprint([y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").stride())  # Outputs: (3072, 1, 96, 3) \n```", "```py\n(3072, 1, 96, 3) \n```", "```py\n[z](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") + [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\nprint([z](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").stride())  # Outputs: (3072, 1, 96, 3) \n```", "```py\n(3072, 1, 96, 3) \n```", "```py\nif [torch.backends.cudnn.is_available](https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.is_available \"torch.backends.cudnn.is_available\")() and [torch.backends.cudnn.version](https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.version \"torch.backends.cudnn.version\")() >= 7603:\n    [model](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\") = [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(8, 4, 3).cuda().half()\n    [model](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\") = [model.to](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to \"torch.nn.Module.to\")(memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\"))  # Module parameters need to be channels last\n\n    input = [torch.randint](https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint \"torch.randint\")(1, 10, (2, 8, 4, 4), dtype=[torch.float32](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"), requires_grad=True)\n    input = input.to(device=\"cuda\", memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\"), dtype=[torch.float16](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n\n    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [model](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(input)\n    print([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").is_contiguous(memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\")))  # Outputs: True \n```", "```py\nTrue \n```", "```py\n# opt_level = O2\n# keep_batchnorm_fp32 = None <class 'NoneType'>\n# loss_scale = None <class 'NoneType'>\n# CUDNN VERSION: 7603\n# => creating model 'resnet50'\n# Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n# Defaults for this optimization level are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Processing user overrides (additional kwargs that are not None)...\n# After processing overrides, optimization options are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Epoch: [0][10/125] Time 0.866 (0.866) Speed 230.949 (230.949) Loss 0.6735125184 (0.6735) Prec@1 61.000 (61.000) Prec@5 100.000 (100.000)\n# Epoch: [0][20/125] Time 0.259 (0.562) Speed 773.481 (355.693) Loss 0.6968704462 (0.6852) Prec@1 55.000 (58.000) Prec@5 100.000 (100.000)\n# Epoch: [0][30/125] Time 0.258 (0.461) Speed 775.089 (433.965) Loss 0.7877287269 (0.7194) Prec@1 51.500 (55.833) Prec@5 100.000 (100.000)\n# Epoch: [0][40/125] Time 0.259 (0.410) Speed 771.710 (487.281) Loss 0.8285319805 (0.7467) Prec@1 48.500 (54.000) Prec@5 100.000 (100.000)\n# Epoch: [0][50/125] Time 0.260 (0.380) Speed 770.090 (525.908) Loss 0.7370464802 (0.7447) Prec@1 56.500 (54.500) Prec@5 100.000 (100.000)\n# Epoch: [0][60/125] Time 0.258 (0.360) Speed 775.623 (555.728) Loss 0.7592862844 (0.7472) Prec@1 51.000 (53.917) Prec@5 100.000 (100.000)\n# Epoch: [0][70/125] Time 0.258 (0.345) Speed 774.746 (579.115) Loss 1.9698858261 (0.9218) Prec@1 49.500 (53.286) Prec@5 100.000 (100.000)\n# Epoch: [0][80/125] Time 0.260 (0.335) Speed 770.324 (597.659) Loss 2.2505953312 (1.0879) Prec@1 50.500 (52.938) Prec@5 100.000 (100.000) \n```", "```py\n# opt_level = O2\n# keep_batchnorm_fp32 = None <class 'NoneType'>\n# loss_scale = None <class 'NoneType'>\n#\n# CUDNN VERSION: 7603\n#\n# => creating model 'resnet50'\n# Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n#\n# Defaults for this optimization level are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Processing user overrides (additional kwargs that are not None)...\n# After processing overrides, optimization options are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n#\n# Epoch: [0][10/125] Time 0.767 (0.767) Speed 260.785 (260.785) Loss 0.7579724789 (0.7580) Prec@1 53.500 (53.500) Prec@5 100.000 (100.000)\n# Epoch: [0][20/125] Time 0.198 (0.482) Speed 1012.135 (414.716) Loss 0.7007197738 (0.7293) Prec@1 49.000 (51.250) Prec@5 100.000 (100.000)\n# Epoch: [0][30/125] Time 0.198 (0.387) Speed 1010.977 (516.198) Loss 0.7113101482 (0.7233) Prec@1 55.500 (52.667) Prec@5 100.000 (100.000)\n# Epoch: [0][40/125] Time 0.197 (0.340) Speed 1013.023 (588.333) Loss 0.8943189979 (0.7661) Prec@1 54.000 (53.000) Prec@5 100.000 (100.000)\n# Epoch: [0][50/125] Time 0.198 (0.312) Speed 1010.541 (641.977) Loss 1.7113249302 (0.9551) Prec@1 51.000 (52.600) Prec@5 100.000 (100.000)\n# Epoch: [0][60/125] Time 0.198 (0.293) Speed 1011.163 (683.574) Loss 5.8537774086 (1.7716) Prec@1 50.500 (52.250) Prec@5 100.000 (100.000)\n# Epoch: [0][70/125] Time 0.198 (0.279) Speed 1011.453 (716.767) Loss 5.7595844269 (2.3413) Prec@1 46.500 (51.429) Prec@5 100.000 (100.000)\n# Epoch: [0][80/125] Time 0.198 (0.269) Speed 1011.827 (743.883) Loss 2.8196096420 (2.4011) Prec@1 47.500 (50.938) Prec@5 100.000 (100.000) \n```", "```py\n# Need to be done once, after model initialization (or load)\n[model](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\") = [model.to](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to \"torch.nn.Module.to\")(memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\"))  # Replace with your model\n\n# Need to be done for every input\ninput = input.to(memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\"))  # Replace with your input\n[output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [model](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(input) \n```", "```py\ndef contains_cl(args):\n    for t in args:\n        if isinstance(t, [torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n            if t.is_contiguous(memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\")) and not t.is_contiguous():\n                return True\n        elif isinstance(t, list) or isinstance(t, tuple):\n            if contains_cl(list(t)):\n                return True\n    return False\n\ndef print_inputs(args, indent=\"\"):\n    for t in args:\n        if isinstance(t, [torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n            print(indent, t.stride(), t.shape, t.device, t.dtype)\n        elif isinstance(t, list) or isinstance(t, tuple):\n            print(indent, type(t))\n            print_inputs(list(t), indent=indent + \"    \")\n        else:\n            print(indent, t)\n\ndef check_wrapper(fn):\n    name = fn.__name__\n\n    def check_cl(*args, **kwargs):\n        was_cl = contains_cl(args)\n        try:\n            result = fn(*args, **kwargs)\n        except Exception as e:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            print(\"-------------------\")\n            raise e\n        failed = False\n        if was_cl:\n            if isinstance(result, [torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n                if result.dim() == 4 and not result.is_contiguous(memory_format=[torch.channels_last](https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format \"torch.memory_format\")):\n                    print(\n                        \"`{}` got channels_last input, but output is not channels_last:\".format(name),\n                        result.shape,\n                        result.stride(),\n                        result.device,\n                        result.dtype,\n                    )\n                    failed = True\n        if failed and True:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            raise Exception(\"Operator `{}` lost channels_last property\".format(name))\n        return result\n\n    return check_cl\n\nold_attrs = dict()\n\ndef attribute(m):\n    old_attrs[m] = dict()\n    for i in dir(m):\n        e = getattr(m, i)\n        exclude_functions = [\"is_cuda\", \"has_names\", \"numel\", \"stride\", \"Tensor\", \"is_contiguous\", \"__class__\"]\n        if i not in exclude_functions and not i.startswith(\"_\") and \"__call__\" in dir(e):\n            try:\n                old_attrs[m][i] = e\n                setattr(m, i, check_wrapper(e))\n            except Exception as e:\n                print(i)\n                print(e)\n\nattribute([torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nattribute([torch.nn.functional](https://pytorch.org/docs/stable/nn.html#module-torch.nn.functional \"torch.nn.functional\"))\nattribute(torch) \n```", "```py\nfor (m, attrs) in old_attrs.items():\n    for (k, v) in attrs.items():\n        setattr(m, k, v) \n```"]