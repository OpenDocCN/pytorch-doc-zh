["```py\nclass torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, permute_embeddings: bool = False, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\nembedding_dims() \u2192 List[int]\u00b6\n```", "```py\nembedding_names() \u2192 List[str]\u00b6\n```", "```py\nuncombined_embedding_dims() \u2192 List[int]\u00b6\n```", "```py\nuncombined_embedding_names() \u2192 List[str]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, permute_embeddings: bool = False, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\ncreate_input_dist(device: Optional[device] = None) \u2192 BaseSparseFeaturesDist[KeyedJaggedTensor]\u00b6\n```", "```py\ncreate_lookup(device: Optional[device] = None, fused_params: Optional[Dict[str, Any]] = None, feature_processor: Optional[BaseGroupedFeatureProcessor] = None) \u2192 BaseEmbeddingLookup\u00b6\n```", "```py\ncreate_output_dist(device: Optional[device] = None) \u2192 BaseEmbeddingDist[EmbeddingShardingContext, Tensor, Tensor]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDist(device: device, world_size: int)\u00b6\n```", "```py\nforward(local_embs: List[Tensor], sharding_ctx: Optional[NullShardingContext] = None) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingDistWithPermute(device: device, world_size: int, embedding_dims: List[int], permute: List[int])\u00b6\n```", "```py\nforward(local_embs: List[Tensor], sharding_ctx: Optional[NullShardingContext] = None) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.cw_sharding.InferCwPooledEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, permute_embeddings: bool = False, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\ncreate_input_dist(device: Optional[device] = None) \u2192 BaseSparseFeaturesDist[KJTList]\u00b6\n```", "```py\ncreate_lookup(device: Optional[device] = None, fused_params: Optional[Dict[str, Any]] = None, feature_processor: Optional[BaseGroupedFeatureProcessor] = None) \u2192 BaseEmbeddingLookup[KJTList, List[Tensor]]\u00b6\n```", "```py\ncreate_output_dist(device: Optional[device] = None) \u2192 BaseEmbeddingDist[NullShardingContext, List[Tensor], Tensor]\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.EmbeddingsAllToOne(device: device, world_size: int, cat_dim: int)\u00b6\n```", "```py\nforward(tensors: List[Tensor]) \u2192 Tensor\u00b6\n```", "```py\nset_device(device_str: str) \u2192 None\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.EmbeddingsAllToOneReduce(device: device, world_size: int)\u00b6\n```", "```py\nforward(tensors: List[Tensor]) \u2192 Tensor\u00b6\n```", "```py\nset_device(device_str: str) \u2192 None\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.KJTAllToAll(pg: ProcessGroup, splits: List[int], stagger: int = 1)\u00b6\n```", "```py\nkeys=['A','B','C']\nsplits=[2,1]\nkjtA2A = KJTAllToAll(pg, splits)\nawaitable = kjtA2A(rank0_input)\n\n# where:\n# rank0_input is KeyedJaggedTensor holding\n\n#         0           1           2\n# 'A'    [A.V0]       None        [A.V1, A.V2]\n# 'B'    None         [B.V0]      [B.V1]\n# 'C'    [C.V0]       [C.V1]      None\n\n# rank1_input is KeyedJaggedTensor holding\n\n#         0           1           2\n# 'A'     [A.V3]      [A.V4]      None\n# 'B'     None        [B.V2]      [B.V3, B.V4]\n# 'C'     [C.V2]      [C.V3]      None\n\nrank0_output = awaitable.wait()\n\n# where:\n# rank0_output is KeyedJaggedTensor holding\n\n#         0           1           2           3           4           5\n# 'A'     [A.V0]      None      [A.V1, A.V2]  [A.V3]      [A.V4]      None\n# 'B'     None        [B.V0]    [B.V1]        None        [B.V2]      [B.V3, B.V4]\n\n# rank1_output is KeyedJaggedTensor holding\n#         0           1           2           3           4           5\n# 'C'     [C.V0]      [C.V1]      None        [C.V2]      [C.V3]      None \n```", "```py\nforward(input: KeyedJaggedTensor) \u2192 Awaitable[KJTAllToAllTensorsAwaitable]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.KJTAllToAllSplitsAwaitable(pg: ProcessGroup, input: KeyedJaggedTensor, splits: List[int], labels: List[str], tensor_splits: List[List[int]], input_tensors: List[Tensor], keys: List[str], device: device, stagger: int)\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable(pg: ProcessGroup, input: KeyedJaggedTensor, splits: List[int], input_splits: List[List[int]], output_splits: List[List[int]], input_tensors: List[Tensor], labels: List[str], keys: List[str], device: device, stagger: int, stride_per_rank: Optional[List[int]])\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.KJTOneToAll(splits: List[int], world_size: int, device: Optional[device] = None)\u00b6\n```", "```py\nforward(kjt: KeyedJaggedTensor) \u2192 KJTList\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.PooledEmbeddingsAllGather(pg: ProcessGroup, codecs: Optional[QuantizedCommCodecs] = None)\u00b6\n```", "```py\ninit_distributed(rank=rank, size=2, backend=\"nccl\")\npg = dist.new_group(backend=\"nccl\")\ninput = torch.randn(2, 2)\nm = PooledEmbeddingsAllGather(pg)\noutput = m(input)\ntensor = output.wait() \n```", "```py\nforward(local_emb: Tensor) \u2192 PooledEmbeddingsAwaitable\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.PooledEmbeddingsAllToAll(pg: ProcessGroup, dim_sum_per_rank: List[int], device: Optional[device] = None, callbacks: Optional[List[Callable[[Tensor], Tensor]]] = None, codecs: Optional[QuantizedCommCodecs] = None)\u00b6\n```", "```py\ndim_sum_per_rank = [2, 1]\na2a = PooledEmbeddingsAllToAll(pg, dim_sum_per_rank, device)\n\nt0 = torch.rand((6, 2))\nt1 = torch.rand((6, 1))\nrank0_output = a2a(t0).wait()\nrank1_output = a2a(t1).wait()\nprint(rank0_output.size())\n    # torch.Size([3, 3])\nprint(rank1_output.size())\n    # torch.Size([3, 3]) \n```", "```py\nproperty callbacks: List[Callable[[Tensor], Tensor]]\u00b6\n```", "```py\nforward(local_embs: Tensor, batch_size_per_rank: Optional[List[int]] = None) \u2192 PooledEmbeddingsAwaitable\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.PooledEmbeddingsAwaitable(tensor_awaitable: Awaitable[Tensor])\u00b6\n```", "```py\nproperty callbacks: List[Callable[[Tensor], Tensor]]\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.PooledEmbeddingsReduceScatter(pg: ProcessGroup, codecs: Optional[QuantizedCommCodecs] = None)\u00b6\n```", "```py\nforward(local_embs: Tensor, input_splits: Optional[List[int]] = None) \u2192 PooledEmbeddingsAwaitable\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.SeqEmbeddingsAllToOne(device: device, world_size: int)\u00b6\n```", "```py\nforward(tensors: List[Tensor]) \u2192 List[Tensor]\u00b6\n```", "```py\nset_device(device_str: str) \u2192 None\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.SequenceEmbeddingsAllToAll(pg: ProcessGroup, features_per_rank: List[int], device: Optional[device] = None, codecs: Optional[QuantizedCommCodecs] = None)\u00b6\n```", "```py\ninit_distributed(rank=rank, size=2, backend=\"nccl\")\npg = dist.new_group(backend=\"nccl\")\nfeatures_per_rank = [4, 4]\nm = SequenceEmbeddingsAllToAll(pg, features_per_rank)\nlocal_embs = torch.rand((6, 2))\nsharding_ctx: SequenceShardingContext\noutput = m(\n    local_embs=local_embs,\n    lengths=sharding_ctx.lengths_after_input_dist,\n    input_splits=sharding_ctx.input_splits,\n    output_splits=sharding_ctx.output_splits,\n    unbucketize_permute_tensor=None,\n)\ntensor = output.wait() \n```", "```py\nforward(local_embs: Tensor, lengths: Tensor, input_splits: List[int], output_splits: List[int], unbucketize_permute_tensor: Optional[Tensor] = None, batch_size_per_rank: Optional[List[int]] = None, sparse_features_recat: Optional[Tensor] = None) \u2192 SequenceEmbeddingsAwaitable\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable(tensor_awaitable: Awaitable[Tensor], unbucketize_permute_tensor: Optional[Tensor], embedding_dim: int)\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.SplitsAllToAllAwaitable(input_tensors: List[Tensor], pg: ProcessGroup)\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsAllToAll(pg: ProcessGroup, emb_dim_per_rank_per_feature: List[List[int]], device: Optional[device] = None, callbacks: Optional[List[Callable[[Tensor], Tensor]]] = None, codecs: Optional[QuantizedCommCodecs] = None)\u00b6\n```", "```py\nkjt_split = [1, 2]\nemb_dim_per_rank_per_feature = [[2], [3, 3]]\na2a = VariableBatchPooledEmbeddingsAllToAll(\n    pg, emb_dim_per_rank_per_feature, device\n)\n\nt0 = torch.rand(6) # 2 * (2 + 1)\nt1 = torch.rand(24) # 3 * (1 + 3) + 3 * (2 + 2)\n#        r0_batch_size   r1_batch_size\n#  f_0:              2               1\n-----------------------------------------\n#  f_1:              1               2\n#  f_2:              3               2\nr0_batch_size_per_rank_per_feature = [[2], [1]]\nr1_batch_size_per_rank_per_feature = [[1, 3], [2, 2]]\nr0_batch_size_per_feature_pre_a2a = [2, 1, 3]\nr1_batch_size_per_feature_pre_a2a = [1, 2, 2]\n\nrank0_output = a2a(\n    t0, r0_batch_size_per_rank_per_feature, r0_batch_size_per_feature_pre_a2a\n).wait()\nrank1_output = a2a(\n    t1, r1_batch_size_per_rank_per_feature, r1_batch_size_per_feature_pre_a2a\n).wait()\n\n# input splits:\n#   r0: [2*2, 1*2]\n#   r1: [1*3 + 3*3, 2*3 + 2*3]\n\n# output splits:\n#   r0: [2*2, 1*3 + 3*3]\n#   r1: [1*2, 2*3 + 2*3]\n\nprint(rank0_output.size())\n    # torch.Size([16])\n    # 2*2 + 1*3 + 3*3\nprint(rank1_output.size())\n    # torch.Size([14])\n    # 1*2 + 2*3 + 2*3 \n```", "```py\nproperty callbacks: List[Callable[[Tensor], Tensor]]\u00b6\n```", "```py\nforward(local_embs: Tensor, batch_size_per_rank_per_feature: List[List[int]], batch_size_per_feature_pre_a2a: List[int]) \u2192 PooledEmbeddingsAwaitable\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.dist_data.VariableBatchPooledEmbeddingsReduceScatter(pg: ProcessGroup, codecs: Optional[QuantizedCommCodecs] = None)\u00b6\n```", "```py\nforward(local_embs: Tensor, batch_size_per_rank_per_feature: List[List[int]], embedding_dims: List[int]) \u2192 PooledEmbeddingsAwaitable\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None)\u00b6\n```", "```py\nembedding_dims() \u2192 List[int]\u00b6\n```", "```py\nembedding_names() \u2192 List[str]\u00b6\n```", "```py\nembedding_names_per_rank() \u2192 List[List[str]]\u00b6\n```", "```py\nembedding_shard_metadata() \u2192 List[Optional[ShardMetadata]]\u00b6\n```", "```py\nembedding_tables() \u2192 List[ShardedEmbeddingTable]\u00b6\n```", "```py\nfeature_names() \u2192 List[str]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingDist\u00b6\n```", "```py\nforward(local_embs: Tensor, sharding_ctx: Optional[EmbeddingShardingContext] = None) \u2192 Awaitable[Tensor]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.dp_sharding.DpPooledEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None)\u00b6\n```", "```py\ncreate_input_dist(device: Optional[device] = None) \u2192 BaseSparseFeaturesDist[KeyedJaggedTensor]\u00b6\n```", "```py\ncreate_lookup(device: Optional[device] = None, fused_params: Optional[Dict[str, Any]] = None, feature_processor: Optional[BaseGroupedFeatureProcessor] = None) \u2192 BaseEmbeddingLookup\u00b6\n```", "```py\ncreate_output_dist(device: Optional[device] = None) \u2192 BaseEmbeddingDist[EmbeddingShardingContext, Tensor, Tensor]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.dp_sharding.DpSparseFeaturesDist\u00b6\n```", "```py\nforward(sparse_features: KeyedJaggedTensor) \u2192 Awaitable[Awaitable[KeyedJaggedTensor]]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, need_pos: bool = False, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\nembedding_dims() \u2192 List[int]\u00b6\n```", "```py\nembedding_names() \u2192 List[str]\u00b6\n```", "```py\nembedding_names_per_rank() \u2192 List[List[str]]\u00b6\n```", "```py\nembedding_shard_metadata() \u2192 List[Optional[ShardMetadata]]\u00b6\n```", "```py\nembedding_tables() \u2192 List[ShardedEmbeddingTable]\u00b6\n```", "```py\nfeature_names() \u2192 List[str]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingDist(device: device, world_size: int)\u00b6\n```", "```py\nforward(local_embs: List[Tensor], sharding_ctx: Optional[NullShardingContext] = None) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.rw_sharding.InferRwPooledEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, need_pos: bool = False, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\ncreate_input_dist(device: Optional[device] = None) \u2192 BaseSparseFeaturesDist[KJTList]\u00b6\n```", "```py\ncreate_lookup(device: Optional[device] = None, fused_params: Optional[Dict[str, Any]] = None, feature_processor: Optional[BaseGroupedFeatureProcessor] = None) \u2192 BaseEmbeddingLookup[KJTList, List[Tensor]]\u00b6\n```", "```py\ncreate_output_dist(device: Optional[device] = None) \u2192 BaseEmbeddingDist[NullShardingContext, List[Tensor], Tensor]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.rw_sharding.InferRwSparseFeaturesDist(world_size: int, num_features: int, feature_hash_sizes: List[int], device: Optional[device] = None, is_sequence: bool = False, has_feature_processor: bool = False, need_pos: bool = False, embedding_shard_metadata: Optional[List[List[int]]] = None)\u00b6\n```", "```py\nforward(sparse_features: KeyedJaggedTensor) \u2192 KJTList\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingDist(pg: ProcessGroup, embedding_dims: List[int], qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\nforward(local_embs: Tensor, sharding_ctx: Optional[EmbeddingShardingContext] = None) \u2192 Awaitable[Tensor]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.rw_sharding.RwPooledEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, need_pos: bool = False, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\ncreate_input_dist(device: Optional[device] = None) \u2192 BaseSparseFeaturesDist[KeyedJaggedTensor]\u00b6\n```", "```py\ncreate_lookup(device: Optional[device] = None, fused_params: Optional[Dict[str, Any]] = None, feature_processor: Optional[BaseGroupedFeatureProcessor] = None) \u2192 BaseEmbeddingLookup\u00b6\n```", "```py\ncreate_output_dist(device: Optional[device] = None) \u2192 BaseEmbeddingDist[EmbeddingShardingContext, Tensor, Tensor]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.rw_sharding.RwSparseFeaturesDist(pg: ProcessGroup, num_features: int, feature_hash_sizes: List[int], device: Optional[device] = None, is_sequence: bool = False, has_feature_processor: bool = False, need_pos: bool = False)\u00b6\n```", "```py\nforward(sparse_features: KeyedJaggedTensor) \u2192 Awaitable[Awaitable[KeyedJaggedTensor]]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\ntorchrec.distributed.sharding.rw_sharding.get_block_sizes_runtime_device(block_sizes: List[int], runtime_device: device, tensor_cache: Dict[str, Tuple[Tensor, List[Tensor]]], embedding_shard_metadata: Optional[List[List[int]]] = None, dtype: dtype = torch.int32) \u2192 Tuple[Tensor, List[Tensor]]\u00b6\n```", "```py\ntorchrec.distributed.sharding.rw_sharding.get_embedding_shard_metadata(grouped_embedding_configs_per_rank: List[List[GroupedEmbeddingConfig]]) \u2192 Tuple[List[List[int]], bool]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\nembedding_dims() \u2192 List[int]\u00b6\n```", "```py\nembedding_names() \u2192 List[str]\u00b6\n```", "```py\nembedding_names_per_rank() \u2192 List[List[str]]\u00b6\n```", "```py\nembedding_shard_metadata() \u2192 List[Optional[ShardMetadata]]\u00b6\n```", "```py\nembedding_tables() \u2192 List[ShardedEmbeddingTable]\u00b6\n```", "```py\nfeature_names() \u2192 List[str]\u00b6\n```", "```py\nfeature_names_per_rank() \u2192 List[List[str]]\u00b6\n```", "```py\nfeatures_per_rank() \u2192 List[int]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.tw_sharding.InferTwEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\ncreate_input_dist(device: Optional[device] = None) \u2192 BaseSparseFeaturesDist[KJTList]\u00b6\n```", "```py\ncreate_lookup(device: Optional[device] = None, fused_params: Optional[Dict[str, Any]] = None, feature_processor: Optional[BaseGroupedFeatureProcessor] = None) \u2192 BaseEmbeddingLookup[KJTList, List[Tensor]]\u00b6\n```", "```py\ncreate_output_dist(device: Optional[device] = None) \u2192 BaseEmbeddingDist[NullShardingContext, List[Tensor], Tensor]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.tw_sharding.InferTwPooledEmbeddingDist(device: device, world_size: int)\u00b6\n```", "```py\nforward(local_embs: List[Tensor], sharding_ctx: Optional[NullShardingContext] = None) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.tw_sharding.InferTwSparseFeaturesDist(features_per_rank: List[int], world_size: int, device: Optional[device] = None)\u00b6\n```", "```py\nforward(sparse_features: KeyedJaggedTensor) \u2192 KJTList\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingDist(pg: ProcessGroup, dim_sum_per_rank: List[int], emb_dim_per_rank_per_feature: List[List[int]], device: Optional[device] = None, callbacks: Optional[List[Callable[[Tensor], Tensor]]] = None, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\nforward(local_embs: Tensor, sharding_ctx: Optional[EmbeddingShardingContext] = None) \u2192 Awaitable[Tensor]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\ncreate_input_dist(device: Optional[device] = None) \u2192 BaseSparseFeaturesDist[KeyedJaggedTensor]\u00b6\n```", "```py\ncreate_lookup(device: Optional[device] = None, fused_params: Optional[Dict[str, Any]] = None, feature_processor: Optional[BaseGroupedFeatureProcessor] = None) \u2192 BaseEmbeddingLookup\u00b6\n```", "```py\ncreate_output_dist(device: Optional[device] = None) \u2192 BaseEmbeddingDist[EmbeddingShardingContext, Tensor, Tensor]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.tw_sharding.TwSparseFeaturesDist(pg: ProcessGroup, features_per_rank: List[int])\u00b6\n```", "```py\nforward(sparse_features: KeyedJaggedTensor) \u2192 Awaitable[Awaitable[KeyedJaggedTensor]]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.twcw_sharding.TwCwPooledEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, permute_embeddings: bool = False, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\nclass torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, need_pos: bool = False, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\nembedding_dims() \u2192 List[int]\u00b6\n```", "```py\nembedding_names() \u2192 List[str]\u00b6\n```", "```py\nembedding_names_per_rank() \u2192 List[List[str]]\u00b6\n```", "```py\nembedding_shard_metadata() \u2192 List[Optional[ShardMetadata]]\u00b6\n```", "```py\nfeature_names() \u2192 List[str]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingDist(rank: int, cross_pg: ProcessGroup, intra_pg: ProcessGroup, dim_sum_per_node: List[int], emb_dim_per_node_per_feature: List[List[int]], device: Optional[device] = None, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\nforward(local_embs: Tensor, sharding_ctx: Optional[EmbeddingShardingContext] = None) \u2192 Awaitable[Tensor]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.distributed.sharding.twrw_sharding.TwRwPooledEmbeddingSharding(sharding_infos: List[EmbeddingShardingInfo], env: ShardingEnv, device: Optional[device] = None, need_pos: bool = False, qcomm_codecs_registry: Optional[Dict[str, QuantizedCommCodecs]] = None)\u00b6\n```", "```py\ncreate_input_dist(device: Optional[device] = None) \u2192 BaseSparseFeaturesDist[KeyedJaggedTensor]\u00b6\n```", "```py\ncreate_lookup(device: Optional[device] = None, fused_params: Optional[Dict[str, Any]] = None, feature_processor: Optional[BaseGroupedFeatureProcessor] = None) \u2192 BaseEmbeddingLookup\u00b6\n```", "```py\ncreate_output_dist(device: Optional[device] = None) \u2192 BaseEmbeddingDist[EmbeddingShardingContext, Tensor, Tensor]\u00b6\n```", "```py\nclass torchrec.distributed.sharding.twrw_sharding.TwRwSparseFeaturesDist(pg: ProcessGroup, local_size: int, features_per_rank: List[int], feature_hash_sizes: List[int], device: Optional[device] = None, has_feature_processor: bool = False, need_pos: bool = False)\u00b6\n```", "```py\n3 features\n2 hosts with 2 devices each\n\nBucketize each feature into 2 buckets\nStaggered shuffle with feature splits [2, 1]\nAlltoAll operation\n\nNOTE: result of staggered shuffle and AlltoAll operation look the same after\nreordering in AlltoAll\n\nResult:\n    host 0 device 0:\n        feature 0 bucket 0\n        feature 1 bucket 0\n\n    host 0 device 1:\n        feature 0 bucket 1\n        feature 1 bucket 1\n\n    host 1 device 0:\n        feature 2 bucket 0\n\n    host 1 device 1:\n        feature 2 bucket 1 \n```", "```py\nforward(sparse_features: KeyedJaggedTensor) \u2192 Awaitable[Awaitable[KeyedJaggedTensor]]\u00b6\n```", "```py\ntraining: bool\u00b6\n```"]