["```py\nTORCH_LIBRARY(myops,  m)  {\n  m.def(\"myadd(Tensor self, Tensor other) -> Tensor\");\n} \n```", "```py\nTensor  myadd_cpu(const  Tensor&  self_,  const  Tensor&  other_)  {\n  TORCH_CHECK(self_.sizes()  ==  other_.sizes());\n  TORCH_INTERNAL_ASSERT(self_.device().type()  ==  DeviceType::CPU);\n  TORCH_INTERNAL_ASSERT(other_.device().type()  ==  DeviceType::CPU);\n  Tensor  self  =  self_.contiguous();\n  Tensor  other  =  other_.contiguous();\n  Tensor  result  =  torch::empty(self.sizes(),  self.options());\n  const  float*  self_ptr  =  self.data_ptr<float>();\n  const  float*  other_ptr  =  other.data_ptr<float>();\n  float*  result_ptr  =  result.data_ptr<float>();\n  for  (int64_t  i  =  0;  i  <  result.numel();  i++)  {\n  result_ptr[i]  =  self_ptr[i]  +  other_ptr[i];\n  }\n  return  result;\n} \n```", "```py\nTORCH_LIBRARY_IMPL(myops,  CPU,  m)  {\n  m.impl(\"myadd\",  myadd_cpu);\n} \n```", "```py\nTORCH_LIBRARY_IMPL(myops,  CUDA,  m)  {\n  m.impl(\"myadd\",  myadd_cuda);\n} \n```", "```py\nTORCH_LIBRARY_IMPL(myops,  Autograd,  m)  {\n  m.impl(op,  autogradNotImplementedFallback());\n} \n```", "```py\nTORCH_LIBRARY_IMPL(myops,  Autograd,  m)  {\n  m.impl(op,  autogradNotImplementedFallback());\n}\nTORCH_LIBRARY_IMPL(myops,  ADInplaceOrView,  m)  {\n  m.impl(op,  autogradNotImplementedInplaceOrViewFallback());\n} \n```", "```py\nTensor  myadd(const  Tensor&  self,  const  Tensor&  other)  {\n  static  auto  op  =  torch::Dispatcher::singleton()\n  .findSchemaOrThrow(\"myops::myadd\",  \"\")\n  .typed<decltype(myadd)>();\n  return  op.call(self,  other);\n} \n```", "```py\nclass  MyAddFunction  :  public  torch::autograd::Function<MyAddFunction>  {\n  public:\n  static  Tensor  forward(\n  AutogradContext  *ctx,  torch::Tensor  self,  torch::Tensor  other)  {\n  at::AutoNonVariableTypeMode  g;\n  return  myadd(self,  other);\n  }\n\n  static  tensor_list  backward(AutogradContext  *ctx,  tensor_list  grad_outputs)  {\n  auto  grad_output  =  grad_outputs[0];\n  return  {grad_output,  grad_output};\n  }\n};\n\nTensor  myadd_autograd(const  Tensor&  self,  const  Tensor&  other)  {\n  return  MyAddFunction::apply(self,  other)[0];\n} \n```", "```py\nTORCH_LIBRARY_IMPL(myops,  Autograd,  m)  {\n  m.impl(\"myadd\",  myadd_autograd);\n} \n```", "```py\nclass  MyAddFunction  :  ...  {\npublic:\n  static  Tensor  forward(\n  AutogradContext  *ctx,  torch::Tensor  self,  torch::Tensor  other)  {\n\n  if  (self.device().type()  ==  DeviceType::CPU)  {\n  return  add_cpu(self,  other);\n  }  else  if  (self.device().type()  ==  DeviceType::CUDA)  {\n  return  add_cuda(self,  other);\n  }  else  {\n  TORCH_CHECK(0,  \"Unsupported device \",  self.device().type());\n  }\n  }\n  ...\n} \n```", "```py\n// Autocast-specific helper functions\n#include  <ATen/autocast_mode.h>\n\nTensor  mymatmul_autocast(const  Tensor&  self,  const  Tensor&  other)  {\n  c10::impl::ExcludeDispatchKeyGuard  no_autocast(c10::DispatchKey::Autocast);\n  return  mymatmul(at::autocast::cached_cast(at::kHalf,  self),\n  at::autocast::cached_cast(at::kHalf,  other));\n}\n\nTORCH_LIBRARY_IMPL(myops,  Autocast,  m)  {\n  m.impl(\"mymatmul\",  mymatmul_autocast);\n} \n```", "```py\n#include  <ATen/autocast_mode.h>\n\nTensor  my_multiple_input_op_autocast(const  Tensor&  t0,  const  Tensor&  t1)  {\n  c10::impl::ExcludeDispatchKeyGuard  no_autocast(c10::DispatchKey::Autocast);\n  // The required at::kHalf argument is an optimistic initial guess.\n  auto  exec_type  =  at::autocast::promote_type(at::kHalf,  t0,  t1);\n  return  my_multiple_input_op(at::autocast::cached_cast(exec_type,  t0),\n  at::autocast::cached_cast(exec_type,  t1));\n} \n```", "```py\nTensor  myadd_autocast(const  Tensor&  self,  const  Tensor&  other)  {\n  c10::impl::ExcludeDispatchKeyGuard  no_autocast(c10::DispatchKey::Autocast);\n  return  myadd(at::autocast::cached_cast(<desired  dtype>,  self),\n  at::autocast::cached_cast(<desired  dtype>,  other));\n}\n\nTORCH_LIBRARY_IMPL(myops,  Autocast,  m)  {\n  m.impl(\"myadd\",  myadd_autocast);\n} \n```"]