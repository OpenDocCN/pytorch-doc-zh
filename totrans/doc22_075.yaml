- en: Distributed RPC Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/rpc.html](https://pytorch.org/docs/stable/rpc.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The distributed RPC framework provides mechanisms for multi-machine model training
    through a set of primitives to allow for remote communication, and a higher-level
    API to automatically differentiate models split across several machines.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: APIs in the RPC package are stable. There are multiple ongoing work items to
    improve performance and error handling, which will ship in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'CUDA support was introduced in PyTorch 1.9 and is still a **beta** feature.
    Not all features of the RPC package are yet compatible with CUDA support and thus
    their use is discouraged. These unsupported features include: RRefs, JIT compatibility,
    dist autograd and dist optimizer, and profiling. These shortcomings will be addressed
    in future releases.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)
    for a brief introduction to all features related to distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: Basics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The distributed RPC framework makes it easy to run functions remotely, supports
    referencing remote objects without copying the real data around, and provides
    autograd and optimizer APIs to transparently run backward and update parameters
    across RPC boundaries. These features can be categorized into four sets of APIs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Remote Procedure Call (RPC)** supports running a function on the specified
    destination worker with the given arguments and getting the return value back
    or creating a reference to the return value. There are three main RPC APIs: [`rpc_sync()`](#torch.distributed.rpc.rpc_sync
    "torch.distributed.rpc.rpc_sync") (synchronous), [`rpc_async()`](#torch.distributed.rpc.rpc_async
    "torch.distributed.rpc.rpc_async") (asynchronous), and [`remote()`](#torch.distributed.rpc.remote
    "torch.distributed.rpc.remote") (asynchronous and returns a reference to the remote
    return value). Use the synchronous API if the user code cannot proceed without
    the return value. Otherwise, use the asynchronous API to get a future, and wait
    on the future when the return value is needed on the caller. The [`remote()`](#torch.distributed.rpc.remote
    "torch.distributed.rpc.remote") API is useful when the requirement is to create
    something remotely but never need to fetch it to the caller. Imagine the case
    that a driver process is setting up a parameter server and a trainer. The driver
    can create an embedding table on the parameter server and then share the reference
    to the embedding table with the trainer, but itself will never use the embedding
    table locally. In this case, [`rpc_sync()`](#torch.distributed.rpc.rpc_sync "torch.distributed.rpc.rpc_sync")
    and [`rpc_async()`](#torch.distributed.rpc.rpc_async "torch.distributed.rpc.rpc_async")
    are no longer appropriate, as they always imply that the return value will be
    returned to the caller immediately or in the future.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Remote Reference (RRef)** serves as a distributed shared pointer to a local
    or remote object. It can be shared with other workers and reference counting will
    be handled transparently. Each RRef only has one owner and the object only lives
    on that owner. Non-owner workers holding RRefs can get copies of the object from
    the owner by explicitly requesting it. This is useful when a worker needs to access
    some data object, but itself is neither the creator (the caller of [`remote()`](#torch.distributed.rpc.remote
    "torch.distributed.rpc.remote")) or the owner of the object. The distributed optimizer,
    as we will discuss below, is one example of such use cases.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Distributed Autograd** stitches together local autograd engines on all the
    workers involved in the forward pass, and automatically reach out to them during
    the backward pass to compute gradients. This is especially helpful if the forward
    pass needs to span multiple machines when conducting, e.g., distributed model
    parallel training, parameter-server training, etc. With this feature, user code
    no longer needs to worry about how to send gradients across RPC boundaries and
    in which order should the local autograd engines be launched, which can become
    quite complicated where there are nested and inter-dependent RPC calls in the
    forward pass.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Distributed Optimizer**’s constructor takes a [`Optimizer()`](optim.html#torch.optim.Optimizer
    "torch.optim.Optimizer") (e.g., [`SGD()`](generated/torch.optim.SGD.html#torch.optim.SGD
    "torch.optim.SGD"), [`Adagrad()`](generated/torch.optim.Adagrad.html#torch.optim.Adagrad
    "torch.optim.Adagrad"), etc.) and a list of parameter RRefs, creates an [`Optimizer()`](optim.html#torch.optim.Optimizer
    "torch.optim.Optimizer") instance on each distinct RRef owner, and updates parameters
    accordingly when running `step()`. When you have distributed forward and backward
    passes, parameters and gradients will be scattered across multiple workers, and
    hence it requires an optimizer on each of the involved workers. Distributed Optimizer
    wraps all those local optimizers into one, and provides a concise constructor
    and `step()` API.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '## RPC'
  prefs: []
  type: TYPE_NORMAL
- en: Before using RPC and distributed autograd primitives, initialization must take
    place. To initialize the RPC framework we need to use [`init_rpc()`](#torch.distributed.rpc.init_rpc
    "torch.distributed.rpc.init_rpc") which would initialize the RPC framework, RRef
    framework and distributed autograd.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Initializes RPC primitives such as the local RPC agent and distributed autograd,
    which immediately makes the current process ready to send and receive RPCs.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in
    Python v3.12)")) – a globally unique name of this node. (e.g., `Trainer3`, `ParameterServer2`,
    `Master`, `Worker1`) Name can only contain number, alphabet, underscore, colon,
    and/or dash, and must be shorter than 128 characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**backend** ([*BackendType*](#torch.distributed.rpc.BackendType "torch.distributed.rpc.BackendType")*,*
    *optional*) – The type of RPC backend implementation. Supported values is `BackendType.TENSORPIPE`
    (the default). See [Backends](#rpc-backends) for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rank** ([*int*](https://docs.python.org/3/library/functions.html#int "(in
    Python v3.12)")) – a globally unique id/rank of this node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – The number of workers in the group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rpc_backend_options** ([*RpcBackendOptions*](#torch.distributed.rpc.RpcBackendOptions
    "torch.distributed.rpc.RpcBackendOptions")*,* *optional*) – The options passed
    to the RpcAgent constructor. It must be an agent-specific subclass of [`RpcBackendOptions`](#torch.distributed.rpc.RpcBackendOptions
    "torch.distributed.rpc.RpcBackendOptions") and contains agent-specific initialization
    configurations. By default, for all agents, it sets the default timeout to 60
    seconds and performs the rendezvous with an underlying process group initialized
    using `init_method = "env://"`, meaning that environment variables `MASTER_ADDR`
    and `MASTER_PORT` need to be set properly. See [Backends](#rpc-backends) for more
    information and find which options are available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following APIs allow users to remotely execute functions as well as create
    references (RRefs) to remote data objects. In these APIs, when passing a `Tensor`
    as an argument or a return value, the destination worker will try to create a
    `Tensor` with the same meta (i.e., shape, stride, etc.). We intentionally disallow
    transmitting CUDA tensors because it might crash if the device lists on source
    and destination workers do not match. In such cases, applications can always explicitly
    move the input tensors to CPU on the caller and move it to the desired devices
    on the callee if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: TorchScript support in RPC is a prototype feature and subject to change. Since
    v1.5.0, `torch.distributed.rpc` supports calling TorchScript functions as RPC
    target functions, and this will help improve parallelism on the callee side as
    executing TorchScript functions does not require GIL.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Make a blocking RPC call to run function `func` on worker `to`. RPC messages
    are sent and received in parallel to execution of Python code. This method is
    thread-safe.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**to** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)") *or* [*WorkerInfo*](#torch.distributed.rpc.WorkerInfo "torch.distributed.rpc.WorkerInfo")
    *or* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python
    v3.12)")) – name/rank/`WorkerInfo` of the destination worker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**func** (*Callable*) – a callable function, such as Python callables, builtin
    operators (e.g. [`add()`](generated/torch.add.html#torch.add "torch.add")) and
    annotated TorchScript functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**args** ([*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple "(in
    Python v3.12)")) – the argument tuple for the `func` invocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in
    Python v3.12)")) – is a dictionary of keyword arguments for the `func` invocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**timeout** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional*) – timeout in seconds to use for this RPC.
    If the RPC does not complete in this amount of time, an exception indicating it
    has timed out will be raised. A value of 0 indicates an infinite timeout, i.e.
    a timeout error will never be raised. If not provided, the default value set during
    initialization or with `_set_rpc_timeout` is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Returns the result of running `func` with `args` and `kwargs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both workers.
    Refer to [`init_process_group()`](distributed.html#torch.distributed.init_process_group
    "torch.distributed.init_process_group") API for more details. For example,
  prefs: []
  type: TYPE_NORMAL
- en: export MASTER_ADDR=localhost export MASTER_PORT=5678
  prefs: []
  type: TYPE_NORMAL
- en: 'Then run the following code in two different processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Below is an example of running a TorchScript function using RPC.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Make a non-blocking RPC call to run function `func` on worker `to`. RPC messages
    are sent and received in parallel to execution of Python code. This method is
    thread-safe. This method will immediately return a [`Future`](futures.html#torch.futures.Future
    "torch.futures.Future") that can be awaited on.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**to** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)") *or* [*WorkerInfo*](#torch.distributed.rpc.WorkerInfo "torch.distributed.rpc.WorkerInfo")
    *or* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python
    v3.12)")) – name/rank/`WorkerInfo` of the destination worker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**func** (*Callable*) – a callable function, such as Python callables, builtin
    operators (e.g. [`add()`](generated/torch.add.html#torch.add "torch.add")) and
    annotated TorchScript functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**args** ([*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple "(in
    Python v3.12)")) – the argument tuple for the `func` invocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in
    Python v3.12)")) – is a dictionary of keyword arguments for the `func` invocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**timeout** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional*) – timeout in seconds to use for this RPC.
    If the RPC does not complete in this amount of time, an exception indicating it
    has timed out will be raised. A value of 0 indicates an infinite timeout, i.e.
    a timeout error will never be raised. If not provided, the default value set during
    initialization or with `_set_rpc_timeout` is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Returns a [`Future`](futures.html#torch.futures.Future "torch.futures.Future")
    object that can be waited on. When completed, the return value of `func` on `args`
    and `kwargs` can be retrieved from the [`Future`](futures.html#torch.futures.Future
    "torch.futures.Future") object.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Using GPU tensors as arguments or return values of `func` is not supported since
    we don’t support sending GPU tensors over the wire. You need to explicitly copy
    GPU tensors to CPU before using them as arguments or return values of `func`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The `rpc_async` API does not copy storages of argument tensors until sending
    them over the wire, which could be done by a different thread depending on the
    RPC backend type. The caller should make sure that the contents of those tensors
    stay intact until the returned [`Future`](futures.html#torch.futures.Future "torch.futures.Future")
    completes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both workers.
    Refer to [`init_process_group()`](distributed.html#torch.distributed.init_process_group
    "torch.distributed.init_process_group") API for more details. For example,
  prefs: []
  type: TYPE_NORMAL
- en: export MASTER_ADDR=localhost export MASTER_PORT=5678
  prefs: []
  type: TYPE_NORMAL
- en: 'Then run the following code in two different processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Below is an example of running a TorchScript function using RPC.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Make a remote call to run `func` on worker `to` and return an `RRef` to the
    result value immediately. Worker `to` will be the owner of the returned `RRef`,
    and the worker calling `remote` is a user. The owner manages the global reference
    count of its `RRef`, and the owner `RRef` is only destructed when globally there
    are no living references to it.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**to** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)") *or* [*WorkerInfo*](#torch.distributed.rpc.WorkerInfo "torch.distributed.rpc.WorkerInfo")
    *or* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python
    v3.12)")) – name/rank/`WorkerInfo` of the destination worker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**func** (*Callable*) – a callable function, such as Python callables, builtin
    operators (e.g. [`add()`](generated/torch.add.html#torch.add "torch.add")) and
    annotated TorchScript functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**args** ([*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple "(in
    Python v3.12)")) – the argument tuple for the `func` invocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in
    Python v3.12)")) – is a dictionary of keyword arguments for the `func` invocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**timeout** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional*) – timeout in seconds for this remote call.
    If the creation of this `RRef` on worker `to` is not successfully processed on
    this worker within this timeout, then the next time there is an attempt to use
    the RRef (such as `to_here()`), a timeout will be raised indicating this failure.
    A value of 0 indicates an infinite timeout, i.e. a timeout error will never be
    raised. If not provided, the default value set during initialization or with `_set_rpc_timeout`
    is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A user `RRef` instance to the result value. Use the blocking API `torch.distributed.rpc.RRef.to_here()`
    to retrieve the result value locally.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The `remote` API does not copy storages of argument tensors until sending them
    over the wire, which could be done by a different thread depending on the RPC
    backend type. The caller should make sure that the contents of those tensors stay
    intact until the returned RRef is confirmed by the owner, which can be checked
    using the `torch.distributed.rpc.RRef.confirmed_by_owner()` API.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Errors such as timeouts for the `remote` API are handled on a best-effort basis.
    This means that when remote calls initiated by `remote` fail, such as with a timeout
    error, we take a best-effort approach to error handling. This means that errors
    are handled and set on the resulting RRef on an asynchronous basis. If the RRef
    has not been used by the application before this handling (such as `to_here` or
    fork call), then future uses of the `RRef` will appropriately raise errors. However,
    it is possible that the user application will use the `RRef` before the errors
    are handled. In this case, errors may not be raised as they have not yet been
    handled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Get [`WorkerInfo`](#torch.distributed.rpc.WorkerInfo "torch.distributed.rpc.WorkerInfo")
    of a given worker name. Use this [`WorkerInfo`](#torch.distributed.rpc.WorkerInfo
    "torch.distributed.rpc.WorkerInfo") to avoid passing an expensive string on every
    invocation.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**worker_name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – the string name of a worker. If `None`, return the the
    id of the current worker. (default `None`)'
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[`WorkerInfo`](#torch.distributed.rpc.WorkerInfo "torch.distributed.rpc.WorkerInfo")
    instance for the given `worker_name` or [`WorkerInfo`](#torch.distributed.rpc.WorkerInfo
    "torch.distributed.rpc.WorkerInfo") of the current worker if `worker_name` is
    `None`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Perform a shutdown of the RPC agent, and then destroy the RPC agent. This stops
    the local agent from accepting outstanding requests, and shuts down the RPC framework
    by terminating all RPC threads. If `graceful=True`, this will block until all
    local and remote RPC processes reach this method and wait for all outstanding
    work to complete. Otherwise, if `graceful=False`, this is a local shutdown, and
    it does not wait for other RPC processes to reach this method.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: For [`Future`](futures.html#torch.futures.Future "torch.futures.Future") objects
    returned by [`rpc_async()`](#torch.distributed.rpc.rpc_async "torch.distributed.rpc.rpc_async"),
    `future.wait()` should not be called after `shutdown()`.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**graceful** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Whether to do a graceful shutdown or not. If True, this
    will 1) wait until there is no pending system messages for `UserRRefs` and delete
    them; 2) block until all local and remote RPC processes have reached this method
    and wait for all outstanding work to complete.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both workers.
    Refer to [`init_process_group()`](distributed.html#torch.distributed.init_process_group
    "torch.distributed.init_process_group") API for more details. For example,
  prefs: []
  type: TYPE_NORMAL
- en: export MASTER_ADDR=localhost export MASTER_PORT=5678
  prefs: []
  type: TYPE_NORMAL
- en: 'Then run the following code in two different processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: A structure that encapsulates information of a worker in the system. Contains
    the name and ID of the worker. This class is not meant to be constructed directly,
    rather, an instance can be retrieved through [`get_worker_info()`](#torch.distributed.rpc.get_worker_info
    "torch.distributed.rpc.get_worker_info") and the result can be passed in to functions
    such as [`rpc_sync()`](#torch.distributed.rpc.rpc_sync "torch.distributed.rpc.rpc_sync"),
    [`rpc_async()`](#torch.distributed.rpc.rpc_async "torch.distributed.rpc.rpc_async"),
    [`remote()`](#torch.distributed.rpc.remote "torch.distributed.rpc.remote") to
    avoid copying a string on every invocation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Globally unique id to identify the worker.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The name of the worker.
  prefs: []
  type: TYPE_NORMAL
- en: The RPC package also provides decorators which allow applications to specify
    how a given function should be treated on the callee side.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: A decorator for a function indicating that the return value of the function
    is guaranteed to be a [`Future`](futures.html#torch.futures.Future "torch.futures.Future")
    object and this function can run asynchronously on the RPC callee. More specifically,
    the callee extracts the [`Future`](futures.html#torch.futures.Future "torch.futures.Future")
    returned by the wrapped function and installs subsequent processing steps as a
    callback to that [`Future`](futures.html#torch.futures.Future "torch.futures.Future").
    The installed callback will read the value from the [`Future`](futures.html#torch.futures.Future
    "torch.futures.Future") when completed and send the value back as the RPC response.
    That also means the returned [`Future`](futures.html#torch.futures.Future "torch.futures.Future")
    only exists on the callee side and is never sent through RPC. This decorator is
    useful when the wrapped function’s (`fn`) execution needs to pause and resume
    due to, e.g., containing [`rpc_async()`](#torch.distributed.rpc.rpc_async "torch.distributed.rpc.rpc_async")
    or waiting for other signals.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To enable asynchronous execution, applications must pass the function object
    returned by this decorator to RPC APIs. If RPC detected attributes installed by
    this decorator, it knows that this function returns a `Future` object and will
    handle that accordingly. However, this does not mean this decorator has to be
    outmost one when defining a function. For example, when combined with `@staticmethod`
    or `@classmethod`, `@rpc.functions.async_execution` needs to be the inner decorator
    to allow the target function be recognized as a static or class function. This
    target function can still execute asynchronously because, when accessed, the static
    or class method preserves attributes installed by `@rpc.functions.async_execution`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: The returned [`Future`](futures.html#torch.futures.Future "torch.futures.Future")
    object can come from [`rpc_async()`](#torch.distributed.rpc.rpc_async "torch.distributed.rpc.rpc_async"),
    [`then()`](futures.html#torch.futures.Future.then "torch.futures.Future.then"),
    or [`Future`](futures.html#torch.futures.Future "torch.futures.Future") constructor.
    The example below shows directly using the [`Future`](futures.html#torch.futures.Future
    "torch.futures.Future") returned by [`then()`](futures.html#torch.futures.Future.then
    "torch.futures.Future.then").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: When combined with TorchScript decorators, this decorator must be the outmost
    one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: When combined with static or class method, this decorator must be the inner
    one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This decorator also works with RRef helpers, i.e., . `torch.distributed.rpc.RRef.rpc_sync()`,
    `torch.distributed.rpc.RRef.rpc_async()`, and `torch.distributed.rpc.RRef.remote()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '### Backends'
  prefs: []
  type: TYPE_NORMAL
- en: The RPC module can leverage different backends to perform the communication
    between the nodes. The backend to be used can be specified in the [`init_rpc()`](#torch.distributed.rpc.init_rpc
    "torch.distributed.rpc.init_rpc") function, by passing a certain value of the
    [`BackendType`](#torch.distributed.rpc.BackendType "torch.distributed.rpc.BackendType")
    enum. Regardless of what backend is used, the rest of the RPC API won’t change.
    Each backend also defines its own subclass of the [`RpcBackendOptions`](#torch.distributed.rpc.RpcBackendOptions
    "torch.distributed.rpc.RpcBackendOptions") class, an instance of which can also
    be passed to [`init_rpc()`](#torch.distributed.rpc.init_rpc "torch.distributed.rpc.init_rpc")
    to configure the backend’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: An enum class of available backends.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch ships with a builtin `BackendType.TENSORPIPE` backend. Additional ones
    can be registered using the `register_backend()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: An abstract structure encapsulating the options passed into the RPC backend.
    An instance of this class can be passed in to [`init_rpc()`](#torch.distributed.rpc.init_rpc
    "torch.distributed.rpc.init_rpc") in order to initialize RPC with specific configurations,
    such as the RPC timeout and `init_method` to be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: URL specifying how to initialize the process group. Default is `env://`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: A float indicating the timeout to use for all RPCs. If an RPC does not complete
    in this timeframe, it will complete with an exception indicating that it has timed
    out.
  prefs: []
  type: TYPE_NORMAL
- en: TensorPipe Backend
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The TensorPipe agent, which is the default, leverages [the TensorPipe library](https://github.com/pytorch/tensorpipe),
    which provides a natively point-to-point communication primitive specifically
    suited for machine learning that fundamentally addresses some of the limitations
    of Gloo. Compared to Gloo, it has the advantage of being asynchronous, which allows
    a large number of transfers to occur simultaneously, each at their own speed,
    without blocking each other. It will only open pipes between pairs of nodes when
    needed, on demand, and when one node fails only its incident pipes will be closed,
    while all other ones will keep working as normal. In addition, it is able to support
    multiple different transports (TCP, of course, but also shared memory, NVLink,
    InfiniBand, …) and can automatically detect their availability and negotiate the
    best transport to use for each pipe.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorPipe backend has been introduced in PyTorch v1.6 and is being actively
    developed. At the moment, it only supports CPU tensors, with GPU support coming
    soon. It comes with a TCP-based transport, just like Gloo. It is also able to
    automatically chunk and multiplex large tensors over multiple sockets and threads
    in order to achieve very high bandwidths. The agent will be able to pick the best
    transport on its own, with no intervention required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The backend options for `TensorPipeAgent`, derived from [`RpcBackendOptions`](#torch.distributed.rpc.RpcBackendOptions
    "torch.distributed.rpc.RpcBackendOptions").
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**num_worker_threads** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* *optional*) – The number of threads in the thread-pool
    used by `TensorPipeAgent` to execute requests (default: 16).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rpc_timeout** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional*) – The default timeout, in seconds, for RPC
    requests (default: 60 seconds). If the RPC has not completed in this timeframe,
    an exception indicating so will be raised. Callers can override this timeout for
    individual RPCs in [`rpc_sync()`](#torch.distributed.rpc.rpc_sync "torch.distributed.rpc.rpc_sync")
    and [`rpc_async()`](#torch.distributed.rpc.rpc_async "torch.distributed.rpc.rpc_async")
    if necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**init_method** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *optional*) – The URL to initialize the distributed store
    used for rendezvous. It takes any value accepted for the same argument of [`init_process_group()`](distributed.html#torch.distributed.init_process_group
    "torch.distributed.init_process_group") (default: `env://`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device_maps** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Dict**]**,* *optional*) – Device placement mappings from
    this worker to the callee. Key is the callee worker name and value the dictionary
    (`Dict` of `int`, `str`, or `torch.device`) that maps this worker’s devices to
    the callee worker’s devices. (default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**devices** (List[int, str, or `torch.device`], optional) – all local CUDA
    devices used by RPC agent. By Default, it will be initialized to all local devices
    from its own `device_maps` and corresponding devices from its peers’ `device_maps`.
    When processing CUDA RPC requests, the agent will properly synchronize CUDA streams
    for all devices in this `List`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The device map locations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: All devices used by the local agent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: URL specifying how to initialize the process group. Default is `env://`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The number of threads in the thread-pool used by `TensorPipeAgent` to execute
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: A float indicating the timeout to use for all RPCs. If an RPC does not complete
    in this timeframe, it will complete with an exception indicating that it has timed
    out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Set device mapping between each RPC caller and callee pair. This function can
    be called multiple times to incrementally add device placement configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**to** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)")) – Callee name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device_map** (*Dict* *of* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* [*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*, or* [*torch.device*](tensor_attributes.html#torch.device
    "torch.device")) – Device placement mappings from this worker to the callee. This
    map must be invertible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Set local devices used by the TensorPipe RPC agent. When processing CUDA RPC
    requests, the TensorPipe RPC agent will properly synchronize CUDA streams for
    all devices in this `List`.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**devices** (*List* *of* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* [*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*, or* [*torch.device*](tensor_attributes.html#torch.device
    "torch.device")) – local devices used by the TensorPipe RPC agent.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The RPC framework does not automatically retry any [`rpc_sync()`](#torch.distributed.rpc.rpc_sync
    "torch.distributed.rpc.rpc_sync"), [`rpc_async()`](#torch.distributed.rpc.rpc_async
    "torch.distributed.rpc.rpc_async") and [`remote()`](#torch.distributed.rpc.remote
    "torch.distributed.rpc.remote") calls. The reason being that there is no way the
    RPC framework can determine whether an operation is idempotent or not and whether
    it is safe to retry. As a result, it is the application’s responsibility to deal
    with failures and retry if necessary. RPC communication is based on TCP and as
    a result failures could happen due to network failures or intermittent network
    connectivity issues. In such scenarios, the application needs to retry appropriately
    with reasonable backoffs to ensure the network isn’t overwhelmed by aggressive
    retries.  ## RRef'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: RRefs are not currently supported when using CUDA tensors
  prefs: []
  type: TYPE_NORMAL
- en: An `RRef` (Remote REFerence) is a reference to a value of some type `T` (e.g.
    `Tensor`) on a remote worker. This handle keeps the referenced remote value alive
    on the owner, but there is no implication that the value will be transferred to
    the local worker in the future. RRefs can be used in multi-machine training by
    holding references to [nn.Modules](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    that exist on other workers, and calling the appropriate functions to retrieve
    or modify their parameters during training. See [Remote Reference Protocol](rpc/rref.html#remote-reference-protocol)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: A class encapsulating a reference to a value of some type on a remote worker.
    This handle will keep the referenced remote value alive on the worker. A `UserRRef`
    will be deleted when 1) no references to it in both the application code and in
    the local RRef context, or 2) the application has called a graceful shutdown.
    Invoking methods on a deleted RRef leads to undefined behaviors. RRef implementation
    only offers best-effort error detection, and applications should not use `UserRRefs`
    after `rpc.shutdown()`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: RRefs can only be serialized and deserialized by the RPC module. Serializing
    and deserializing RRefs without RPC (e.g., Python pickle, torch [`save()`](generated/torch.save.html#torch.save
    "torch.save") / [`load()`](generated/torch.load.html#torch.load "torch.load"),
    JIT [`save()`](generated/torch.jit.save.html#torch.jit.save "torch.jit.save")
    / [`load()`](generated/torch.jit.load.html#torch.jit.load "torch.jit.load"), etc.)
    will lead to errors.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**value** ([*object*](https://docs.python.org/3/library/functions.html#object
    "(in Python v3.12)")) – The value to be wrapped by this RRef.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**type_hint** (*Type**,* *optional*) – Python type that should be passed to
    `TorchScript` compiler as type hint for `value`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: Following examples skip RPC initialization and shutdown code for simplicity.
    Refer to RPC docs for those details.
  prefs: []
  type: TYPE_NORMAL
- en: Create an RRef using rpc.remote
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Create an RRef from a local object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Share an RRef with other workers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Runs the backward pass using the RRef as the root of the backward pass. If `dist_autograd_ctx_id`
    is provided, we perform a distributed backward pass using the provided ctx_id
    starting from the owner of the RRef. In this case, [`get_gradients()`](#torch.distributed.autograd.get_gradients
    "torch.distributed.autograd.get_gradients") should be used to retrieve the gradients.
    If `dist_autograd_ctx_id` is `None`, it is assumed that this is a local autograd
    graph and we only perform a local backward pass. In the local case, the node calling
    this API has to be the owner of the RRef. The value of the RRef is expected to
    be a scalar Tensor.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**dist_autograd_ctx_id** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* *optional*) – The distributed autograd context id for
    which we should retrieve the gradients (default: -1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**retain_graph** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If `False`, the graph used to compute the
    grad will be freed. Note that in nearly all cases setting this option to `True`
    is not needed and often can be worked around in a much more efficient way. Usually,
    you need to set this to `True` to run backward multiple times (default: False).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Returns whether this `RRef` has been confirmed by the owner. `OwnerRRef` always
    returns true, while `UserRRef` only returns true when the owner knowns about this
    `UserRRef`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Returns whether or not the current node is the owner of this `RRef`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: If the current node is the owner, returns a reference to the local value. Otherwise,
    throws an exception.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Returns worker information of the node that owns this `RRef`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Returns worker name of the node that owns this `RRef`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a helper proxy to easily launch a `remote` using the owner of the RRef
    as the destination to run functions on the object referenced by this RRef. More
    specifically, `rref.remote().func_name(*args, **kwargs)` is the same as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**timeout** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional*) – Timeout for `rref.remote()`. If the creation
    of this `RRef` is not successfully completed within the timeout, then the next
    time there is an attempt to use the RRef (such as `to_here`), a timeout will be
    raised. If not provided, the default RPC timeout will be used. Please see `rpc.remote()`
    for specific timeout semantics for `RRef`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a helper proxy to easily launch an `rpc_async` using the owner of the
    RRef as the destination to run functions on the object referenced by this RRef.
    More specifically, `rref.rpc_async().func_name(*args, **kwargs)` is the same as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**timeout** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional*) – Timeout for `rref.rpc_async()`. If the call
    does not complete within this timeframe, an exception indicating so will be raised.
    If this argument is not provided, the default RPC timeout will be used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a helper proxy to easily launch an `rpc_sync` using the owner of the
    RRef as the destination to run functions on the object referenced by this RRef.
    More specifically, `rref.rpc_sync().func_name(*args, **kwargs)` is the same as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**timeout** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional*) – Timeout for `rref.rpc_sync()`. If the call
    does not complete within this timeframe, an exception indicating so will be raised.
    If this argument is not provided, the default RPC timeout will be used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Blocking call that copies the value of the RRef from the owner to the local
    node and returns it. If the current node is the owner, returns a reference to
    the local value.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**timeout** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional*) – Timeout for `to_here`. If the call does
    not complete within this timeframe, an exception indicating so will be raised.
    If this argument is not provided, the default RPC timeout (60s) will be used.'
  prefs: []
  type: TYPE_NORMAL
- en: More Information about RRef
  prefs: []
  type: TYPE_NORMAL
- en: '[Remote Reference Protocol](rpc/rref.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Background](rpc/rref.html#background)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Assumptions](rpc/rref.html#assumptions)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RRef Lifetime](rpc/rref.html#rref-lifetime)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Design Reasoning](rpc/rref.html#design-reasoning)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Implementation](rpc/rref.html#implementation)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Protocol Scenarios](rpc/rref.html#protocol-scenarios)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[User Share RRef with Owner as Return Value](rpc/rref.html#user-share-rref-with-owner-as-return-value)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[User Share RRef with Owner as Argument](rpc/rref.html#user-share-rref-with-owner-as-argument)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Owner Share RRef with User](rpc/rref.html#owner-share-rref-with-user)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[User Share RRef with User](rpc/rref.html#user-share-rref-with-user)  ## RemoteModule[](#remotemodule
    "Permalink to this heading")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: RemoteModule is not currently supported when using CUDA tensors
  prefs: []
  type: TYPE_NORMAL
- en: '`RemoteModule` is an easy way to create an nn.Module remotely on a different
    process. The actual module resides on a remote host, but the local host has a
    handle to this module and invoke this module similar to a regular nn.Module. The
    invocation however incurs RPC calls to the remote end and can be performed asynchronously
    if needed via additional APIs supported by RemoteModule.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: A RemoteModule instance can only be created after RPC initialization.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It creates a user-specified module on a specified remote node. It behaves like
    a regular `nn.Module` except that the `forward` method is executed on the remote
    node. It takes care of autograd recording to ensure the backward pass propagates
    gradients back to the corresponding remote module.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It generates two methods `forward_async` and `forward` based on the signature
    of the `forward` method of `module_cls`. `forward_async` runs asynchronously and
    returns a Future. The arguments of `forward_async` and `forward` are the same
    as the `forward` method of the module returned by the `module_cls`.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For example, if `module_cls` returns an instance of `nn.Linear`, that has `forward`
    method signature: `def forward(input: Tensor) -> Tensor:`, the generated `RemoteModule`
    will have 2 methods with the signatures:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`def forward(input: Tensor) -> Tensor:``def forward_async(input: Tensor) ->
    Future[Tensor]:`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**remote_device** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – Device on the destination worker where we’d like to place
    this module. The format should be “<workername>/<device>”, where the device field
    can be parsed as torch.device type. E.g., “trainer0/cpu”, “trainer0”, “ps0/cuda:0”.
    In addition, the device field can be optional and the default value is “cpu”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**module_cls** ([*nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class for the module to be created remotely. For example,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**args** (*Sequence**,* *optional*) – args to be passed to `module_cls`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (*Dict**,* *optional*) – kwargs to be passed to `module_cls`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A remote module instance which wraps the `Module` created by the user-provided
    `module_cls`, it has a blocking `forward` method and an asynchronous `forward_async`
    method that returns a future of the `forward` call on the user-provided module
    on the remote side.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following code in two different processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Furthermore, a more practical example that is combined with [DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)
    (DDP) can be found in this [tutorial](https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Return an `RRef` (`RRef[nn.Module]`) pointing to the remote module.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '*RRef*[[*Module*](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.modules.module.Module")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Return a list of `RRef` pointing to the remote module’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: This can typically be used in conjunction with [`DistributedOptimizer`](distributed.optim.html#torch.distributed.optim.DistributedOptimizer
    "torch.distributed.optim.DistributedOptimizer").
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**recurse** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – if True, then returns parameters of the remote module
    and all submodules of the remote module. Otherwise, returns only parameters that
    are direct members of the remote module.'
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list of `RRef` (`List[RRef[nn.Parameter]]`) to remote module’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*List*](https://docs.python.org/3/library/typing.html#typing.List "(in Python
    v3.12)")[*RRef*[[*Parameter*](generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter
    "torch.nn.parameter.Parameter")]]'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Autograd Framework[](#distributed-autograd-framework "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Distributed autograd is not currently supported when using CUDA tensors
  prefs: []
  type: TYPE_NORMAL
- en: This module provides an RPC-based distributed autograd framework that can be
    used for applications such as model parallel training. In short, applications
    may send and receive gradient recording tensors over RPC. In the forward pass,
    we record when gradient recording tensors are sent over RPC and during the backward
    pass we use this information to perform a distributed backward pass using RPC.
    For more details see [Distributed Autograd Design](rpc/distributed_autograd.html#distributed-autograd-design).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Kicks off the distributed backward pass using the provided roots. This currently
    implements the [FAST mode algorithm](rpc/distributed_autograd.html#fast-mode-algorithm)
    which assumes all RPC messages sent in the same distributed autograd context across
    workers would be part of the autograd graph during the backward pass.
  prefs: []
  type: TYPE_NORMAL
- en: We use the provided roots to discover the autograd graph and compute appropriate
    dependencies. This method blocks until the entire autograd computation is done.
  prefs: []
  type: TYPE_NORMAL
- en: We accumulate the gradients in the appropriate [`torch.distributed.autograd.context`](#torch.distributed.autograd.context
    "torch.distributed.autograd.context") on each of the nodes. The autograd context
    to be used is looked up given the `context_id` that is passed in when [`torch.distributed.autograd.backward()`](#torch.distributed.autograd.backward
    "torch.distributed.autograd.backward") is called. If there is no valid autograd
    context corresponding to the given ID, we throw an error. You can retrieve the
    accumulated gradients using the [`get_gradients()`](#torch.distributed.autograd.get_gradients
    "torch.distributed.autograd.get_gradients") API.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**context_id** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – The autograd context id for which we should retrieve the
    gradients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**roots** ([*list*](https://docs.python.org/3/library/stdtypes.html#list "(in
    Python v3.12)")) – Tensors which represent the roots of the autograd computation.
    All the tensors should be scalars.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**retain_graph** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If False, the graph used to compute the
    grad will be freed. Note that in nearly all cases setting this option to True
    is not needed and often can be worked around in a much more efficient way. Usually,
    you need to set this to True to run backward multiple times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Context object to wrap forward and backward passes when using distributed autograd.
    The `context_id` generated in the `with` statement is required to uniquely identify
    a distributed backward pass on all workers. Each worker stores metadata associated
    with this `context_id`, which is required to correctly execute a distributed autograd
    pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Retrieves a map from Tensor to the appropriate gradient for that Tensor accumulated
    in the provided context corresponding to the given `context_id` as part of the
    distributed autograd backward pass.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**context_id** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – The autograd context id for which we should retrieve the
    gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A map where the key is the Tensor and the value is the associated gradient for
    that Tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: More Information about RPC Autograd
  prefs: []
  type: TYPE_NORMAL
- en: '[Distributed Autograd Design](rpc/distributed_autograd.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Background](rpc/distributed_autograd.html#background)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Autograd recording during the forward pass](rpc/distributed_autograd.html#autograd-recording-during-the-forward-pass)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Distributed Autograd Context](rpc/distributed_autograd.html#distributed-autograd-context)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Distributed Backward Pass](rpc/distributed_autograd.html#distributed-backward-pass)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Computing dependencies](rpc/distributed_autograd.html#computing-dependencies)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FAST mode algorithm](rpc/distributed_autograd.html#fast-mode-algorithm)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SMART mode algorithm](rpc/distributed_autograd.html#smart-mode-algorithm)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Distributed Optimizer](rpc/distributed_autograd.html#distributed-optimizer)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Simple end to end example](rpc/distributed_autograd.html#simple-end-to-end-example)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed Optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: See the [torch.distributed.optim](https://pytorch.org/docs/main/distributed.optim.html)
    page for documentation on distributed optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Design Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The distributed autograd design note covers the design of the RPC-based distributed
    autograd framework that is useful for applications such as model parallel training.
  prefs: []
  type: TYPE_NORMAL
- en: '[Distributed Autograd Design](rpc/distributed_autograd.html#distributed-autograd-design)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RRef design note covers the design of the [RRef](#rref) (Remote REFerence)
    protocol used to refer to values on remote workers by the framework.
  prefs: []
  type: TYPE_NORMAL
- en: '[Remote Reference Protocol](rpc/rref.html#remote-reference-protocol)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tutorials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The RPC tutorials introduce users to the RPC framework, provide several example
    applications using [torch.distributed.rpc](#distributed-rpc-framework) APIs, and
    demonstrate how to use [the profiler](https://pytorch.org/docs/stable/autograd.html#profiler)
    to profile RPC-based workloads.
  prefs: []
  type: TYPE_NORMAL
- en: '[Getting started with Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Implementing a Parameter Server using Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Combining Distributed DataParallel with Distributed RPC Framework](https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html)
    (covers **RemoteModule** as well)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Profiling RPC-based Workloads](https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Implementing batch RPC processing](https://pytorch.org/tutorials/intermediate/rpc_async_execution.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Distributed Pipeline Parallel](https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
