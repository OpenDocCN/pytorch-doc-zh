# torch.nn.init

> 原文：[https://pytorch.org/docs/stable/nn.init.html](https://pytorch.org/docs/stable/nn.init.html)

警告

该模块中的所有函数都旨在用于初始化神经网络参数，因此它们都在[`torch.no_grad()`](generated/torch.no_grad.html#torch.no_grad "torch.no_grad")模式下运行，并且不会被autograd考虑。

```py
torch.nn.init.calculate_gain(nonlinearity, param=None)¶
```

返回给定非线性函数的推荐增益值。

值如下：

| 非线性 | 增益 |
| --- | --- |
| Linear / Identity | $1$1 |
| Conv{1,2,3}D | $1$1 |
| Sigmoid | $1$1 |
| Tanh | $\frac{5}{3}$35​ |
| ReLU | $\sqrt{2}$2​ |
| Leaky Relu | $\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}$1+negative_slope22​​ |
| SELU | $\frac{3}{4}$43​ |

警告

为了实现[自正则化神经网络](https://papers.nips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html)，应该使用`nonlinearity='linear'`而不是`nonlinearity='selu'`。这样可以使初始权重的方差为`1 / N`，这对于在前向传递中引入稳定的固定点是必要的。相比之下，`SELU`的默认增益牺牲了归一化效果，以获得更稳定的梯度流动在矩形层中。

参数

+   **nonlinearity** - 非线性函数（nn.functional名称）

+   **param** - 非线性函数的可选参数

示例

```py
>>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2 
```

```py
torch.nn.init.uniform_(tensor, a=0.0, b=1.0, generator=None)¶
```

用从均匀分布中抽取的值填充输入张量。

$\mathcal{U}(a, b)$U(a,b).

参数

+   **张量**（[*张量*](tensors.html#torch.Tensor "torch.Tensor")） - 一个n维torch张量

+   **a**（[*浮点数*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12)")） - 均匀分布的下界

+   **b**（[*浮点数*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12)")） - 均匀分布的上界

+   **generator**（[*可选*](https://docs.python.org/3/library/typing.html#typing.Optional "(在Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator "torch._C.Generator")*]*) - torch生成器用于采样（默认值：无）

返回类型

[*张量*](tensors.html#torch.Tensor "torch.Tensor")

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.uniform_(w) 
```

```py
torch.nn.init.normal_(tensor, mean=0.0, std=1.0, generator=None)¶
```

用从正态分布中抽取的值填充输入张量。

$\mathcal{N}(\text{mean}, \text{std}^2)$N(mean,std2).

参数

+   **张量**（[*张量*](tensors.html#torch.Tensor "torch.Tensor")） - 一个n维torch张量

+   **mean**（[*浮点数*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12)")） - 正态分布的均值

+   **std**（[*浮点数*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12)")） - 正态分布的标准差

+   **generator**（[*可选*](https://docs.python.org/3/library/typing.html#typing.Optional "(在Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator "torch._C.Generator")*]*) - torch生成器用于采样（默认值：无）

返回类型

[*张量*](tensors.html#torch.Tensor "torch.Tensor")

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.normal_(w) 
```

```py
torch.nn.init.constant_(tensor, val)¶
```

用值$\text{val}$填充输入张量。

参数

+   **张量**（[*张量*](tensors.html#torch.Tensor "torch.Tensor")） - 一个n维torch张量

+   **val**（[*浮点数*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12)")） - 用于填充张量的值

返回类型

[*张量*](tensors.html#torch.Tensor "torch.Tensor")

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.constant_(w, 0.3) 
```

```py
torch.nn.init.ones_(tensor)¶
```

用标量值1填充输入张量。

参数

**张量**（[*张量*](tensors.html#torch.Tensor "torch.Tensor")） - 一个n维torch张量

返回类型

[*张量*](tensors.html#torch.Tensor "torch.Tensor")

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.ones_(w) 
```

```py
torch.nn.init.zeros_(tensor)¶
```

用标量值0填充输入张量。

参数

**张量**（[*张量*](tensors.html#torch.Tensor "torch.Tensor")） - 一个n维torch张量

返回类型

[*张量*](tensors.html#torch.Tensor "torch.Tensor")

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.zeros_(w) 
```

```py
torch.nn.init.eye_(tensor)¶
```

用单位矩阵填充2维输入张量。

在线性层中保留输入的身份，尽可能保留多个输入。

参数

**tensor** - 一个2维torch.Tensor

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.eye_(w) 
```

```py
torch.nn.init.dirac_(tensor, groups=1)¶
```

使用Dirac delta函数填充{3, 4, 5}维度的输入张量。

在卷积层中保留输入的身份，尽可能保留多个输入通道。如果groups>1，则每组通道都保留身份

参数

+   **tensor** - 一个{3, 4, 5}维度的torch.Tensor

+   **groups**（[*int*](https://docs.python.org/3/library/functions.html#int) *，* 可选）- 卷积层中的组数（默认值：1）

示例

```py
>>> w = torch.empty(3, 16, 5, 5)
>>> nn.init.dirac_(w)
>>> w = torch.empty(3, 24, 5, 5)
>>> nn.init.dirac_(w, 3) 
```

```py
torch.nn.init.xavier_uniform_(tensor, gain=1.0, generator=None)¶
```

使用Xavier均匀分布填充输入张量的值。

该方法在《理解深度前馈神经网络训练困难性》- Glorot, X. & Bengio, Y.（2010）中有描述。生成的张量将从$\mathcal{U}(-a, a)$中采样值，其中

$a = \text{gain} \times \sqrt{\frac{6}{\text{fan\_in} + \text{fan\_out}}}$ 

也称为Glorot初始化。

参数

+   **tensor**（[*Tensor*](tensors.html#torch.Tensor)）- 一个n维torch.Tensor

+   **gain**（[*float*](https://docs.python.org/3/library/functions.html#float)）- 可选的缩放因子

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional) *[*[*Generator*](generated/torch.Generator.html#torch.Generator)*]*) - 用于采样的torch生成器（默认值：无）

返回类型

[*Tensor*](tensors.html#torch.Tensor)

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu')) 
```

```py
torch.nn.init.xavier_normal_(tensor, gain=1.0, generator=None)¶
```

使用Xavier正态分布填充输入张量的值。

该方法在《理解深度前馈神经网络训练困难性》- Glorot, X. & Bengio, Y.（2010）中有描述。生成的张量将从$\mathcal{N}(0, \text{std}^2)$中采样值，其中

$\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\_in} + \text{fan\_out}}}$ 

也称为Glorot初始化。

参数

+   **tensor**（[*Tensor*](tensors.html#torch.Tensor)）- 一个n维torch.Tensor

+   **gain**（[*float*](https://docs.python.org/3/library/functions.html#float)）- 可选的缩放因子

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional) *[*[*Generator*](generated/torch.Generator.html#torch.Generator)*]*) - 用于采样的torch生成器（默认值：无）

返回类型

[*Tensor*](tensors.html#torch.Tensor)

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.xavier_normal_(w) 
```

```py
torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu', generator=None)¶
```

使用Kaiming均匀分布填充输入张量的值。

该方法在《深入研究整流器：超越ImageNet分类的人类级性能》- He, K.等人（2015）中有描述。生成的张量将从$\mathcal{U}(-\text{bound}, \text{bound})$中采样值，其中

$\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\_mode}}}$ 

也称为He初始化。

参数

+   **tensor**（[*Tensor*](tensors.html#torch.Tensor)）- 一个n维torch.Tensor

+   **a**（[*float*](https://docs.python.org/3/library/functions.html#float)）- 此层后使用的整流器的负斜率（仅与'leaky_relu'一起使用）

+   **mode**（[*str*](https://docs.python.org/3/library/stdtypes.html#str)）- 要么'fan_in'（默认），要么'fan_out'。选择'fan_in'保留前向传播中权重方差的幅度。选择'fan_out'保留反向传播中的幅度。

+   **nonlinearity**（[*str*](https://docs.python.org/3/library/stdtypes.html#str)）- 非线性函数（nn.functional名称），建议仅与'relu'或'leaky_relu'一起使用（默认）。

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional "(在Python v3.12中)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator "torch._C.Generator")*]*) - 用于抽样的torch生成器（默认值：无）

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu') 
```

```py
torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu', generator=None)¶
```

用Kaiming正态分布填充输入张量的值。

该方法在Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K.等人（2015）中有描述。生成的张量将具有从$\mathcal{N}(0, \text{std}^2)$N(0,std2)中抽样的值，其中

$\text{std} = \frac{\text{gain}}{\sqrt{\text{fan\_mode}}}$ std=fan_mode​gain​

也称为He初始化。

参数

+   **tensor**（[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")）- 一个n维torch.Tensor

+   **a**（[*float*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12中)")）- 在此层后使用的整流器的负斜率（仅与`'leaky_relu'`一起使用）

+   **mode**（[*str*](https://docs.python.org/3/library/stdtypes.html#str "(在Python v3.12中)")）- `'fan_in'`（默认）或`'fan_out'`。选择`'fan_in'`保留前向传播中权重方差的幅度。选择`'fan_out'`保留反向传播中的幅度。

+   **nonlinearity**（[*str*](https://docs.python.org/3/library/stdtypes.html#str "(在Python v3.12中)")）- 非线性函数（nn.functional名称），建议仅与`'relu'`或`'leaky_relu'`一起使用（默认值）。

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional "(在Python v3.12中)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator "torch._C.Generator")*]*) - 用于抽样的torch生成器（默认值：无）

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu') 
```

```py
torch.nn.init.trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0, generator=None)¶
```

用截断正态分布中抽取的值填充输入张量。

这些值实际上是从正态分布$\mathcal{N}(\text{mean}, \text{std}^2)$N(mean,std2)中抽取的，超出$[a, b]$[a,b]的值会被重新绘制，直到它们在范围内。用于生成随机值的方法在$a \leq \text{mean} \leq b$时效果最佳。

参数

+   **tensor**（[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")）- 一个n维torch.Tensor

+   **mean**（[*float*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12中)")）- 正态分布的均值

+   **std**（[*float*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12中)")）- 正态分布的标准差

+   **a**（[*float*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12中)")）- 最小截断值

+   **b**（[*float*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12中)")）- 最大截断值

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional "(在Python v3.12中)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator "torch._C.Generator")*]*) - 用于抽样的torch生成器（默认值：无）

返回类型

[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.trunc_normal_(w) 
```

```py
torch.nn.init.orthogonal_(tensor, gain=1, generator=None)¶
```

用（半）正交矩阵填充输入张量。

在Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A.等人（2013）中有描述。输入张量必须至少有2个维度，对于超过2个维度的张量，尾部维度会被展平。

参数

+   **tensor** - 一个n维torch.Tensor，其中$n \geq 2$

+   **gain** - 可选的缩放因子

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional "(在Python v3.12中)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator "torch._C.Generator")*]*) - 用于抽样的torch生成器（默认值：无）

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.orthogonal_(w) 
```

```py
torch.nn.init.sparse_(tensor, sparsity, std=0.01, generator=None)¶
```

将2D输入张量填充为稀疏矩阵。

非零元素将从正态分布$\mathcal{N}(0, 0.01)$N(0,0.01)中抽取，如Deep learning via Hessian-free optimization - Martens, J.（2010）中所述。

参数

+   **tensor** - 一个n维torch张量

+   **sparsity** - 每列中要设置为零的元素比例

+   **std** - 用于生成非零值的正态分布的标准差

+   **generator**（[*可选*](https://docs.python.org/3/library/typing.html#typing.Optional "(在Python v3.12中)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator "torch._C.Generator")*]*) - 用于采样的torch生成器（默认值：无）

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.sparse_(w, sparsity=0.1) 
```
