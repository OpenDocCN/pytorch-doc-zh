# torch.nn.init

> 原文：[`pytorch.org/docs/stable/nn.init.html`](https://pytorch.org/docs/stable/nn.init.html)

警告

该模块中的所有函数都旨在用于初始化神经网络参数，因此它们都在`torch.no_grad()`模式下运行，并且不会被 autograd 考虑。

```py
torch.nn.init.calculate_gain(nonlinearity, param=None)
```

返回给定非线性函数的推荐增益值。

值如下：

| 非线性 | 增益 |
| --- | --- |
| Linear / Identity | $1$ |
| Conv{1,2,3}D | $1$ |
| Sigmoid | $1$ |
| Tanh | $\frac{5}{3}$​ |
| ReLU | $\sqrt{2}$​ |
| Leaky Relu | $\sqrt{\frac{2}{1 + \text{negative\_slope}²}}$​ |
| SELU | $\frac{3}{4}$​ |

警告

为了实现[自正则化神经网络](https://papers.nips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html)，应该使用`nonlinearity='linear'`而不是`nonlinearity='selu'`。这样可以使初始权重的方差为`1 / N`，这对于在前向传递中引入稳定的固定点是必要的。相比之下，`SELU`的默认增益牺牲了归一化效果，以获得更稳定的梯度流动在矩形层中。

参数

+   **nonlinearity** - 非线性函数（nn.functional 名称）

+   **param** - 非线性函数的可选参数

示例

```py
>>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2 
```

```py
torch.nn.init.uniform_(tensor, a=0.0, b=1.0, generator=None)
```

用从均匀分布中抽取的值填充输入张量。

$\mathcal{U}(a, b)$.

参数

+   **张量**（*张量*） - 一个 n 维 torch 张量

+   **a**（[*浮点数*](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12)")） - 均匀分布的下界

+   **b**（[*浮点数*](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12)")） - 均匀分布的上界

+   **generator**（[*可选*](https://docs.python.org/3/library/typing.html#typing.Optional "(在 Python v3.12)")**[*Generator**]*) - torch 生成器用于采样（默认值：无）

返回类型

*张量*

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.uniform_(w) 
```

```py
torch.nn.init.normal_(tensor, mean=0.0, std=1.0, generator=None)
```

用从正态分布中抽取的值填充输入张量。

$\mathcal{N}(\text{mean}, \text{std}²)$.

参数

+   **张量**（*张量*） - 一个 n 维 torch 张量

+   **mean**（[*浮点数*](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12)")） - 正态分布的均值

+   **std**（[*浮点数*](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12)")） - 正态分布的标准差

+   **generator**（[*可选*](https://docs.python.org/3/library/typing.html#typing.Optional "(在 Python v3.12)")**[*Generator**]*) - torch 生成器用于采样（默认值：无）

返回类型

*张量*

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.normal_(w) 
```

```py
torch.nn.init.constant_(tensor, val)
```

用值$\text{val}$填充输入张量。

参数

+   **张量**（*张量*） - 一个 n 维 torch 张量

+   **val**（[*浮点数*](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12)")） - 用于填充张量的值

返回类型

*张量*

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.constant_(w, 0.3) 
```

```py
torch.nn.init.ones_(tensor)
```

用标量值 1 填充输入张量。

参数

**张量**（*张量*） - 一个 n 维 torch 张量

返回类型

*张量*

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.ones_(w) 
```

```py
torch.nn.init.zeros_(tensor)
```

用标量值 0 填充输入张量。

参数

**张量**（*张量*） - 一个 n 维 torch 张量

返回类型

*张量*

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.zeros_(w) 
```

```py
torch.nn.init.eye_(tensor)
```

用单位矩阵填充 2 维输入张量。

在线性层中保留输入的身份，尽可能保留多个输入。

参数

**tensor** - 一个 2 维 torch.Tensor

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.eye_(w) 
```

```py
torch.nn.init.dirac_(tensor, groups=1)
```

使用 Dirac delta 函数填充{3, 4, 5}维度的输入张量。

在卷积层中保留输入的身份，尽可能保留多个输入通道。如果 groups>1，则每组通道都保留身份

参数

+   **tensor** - 一个{3, 4, 5}维度的 torch.Tensor

+   **groups**（[*int*](https://docs.python.org/3/library/functions.html#int) *，* 可选）- 卷积层中的组数（默认值：1）

示例

```py
>>> w = torch.empty(3, 16, 5, 5)
>>> nn.init.dirac_(w)
>>> w = torch.empty(3, 24, 5, 5)
>>> nn.init.dirac_(w, 3) 
```

```py
torch.nn.init.xavier_uniform_(tensor, gain=1.0, generator=None)
```

使用 Xavier 均匀分布填充输入张量的值。

该方法在《理解深度前馈神经网络训练困难性》- Glorot, X. & Bengio, Y.（2010）中有描述。生成的张量将从$\mathcal{U}(-a, a)$中采样值，其中

$a = \text{gain} \times \sqrt{\frac{6}{\text{fan\_in} + \text{fan\_out}}}$ 

也称为 Glorot 初始化。

参数

+   **tensor**（*Tensor*）- 一个 n 维 torch.Tensor

+   **gain**（[*float*](https://docs.python.org/3/library/functions.html#float)）- 可选的缩放因子

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional) **[*Generator**]*) - 用于采样的 torch 生成器（默认值：无）

返回类型

*Tensor*

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu')) 
```

```py
torch.nn.init.xavier_normal_(tensor, gain=1.0, generator=None)
```

使用 Xavier 正态分布填充输入张量的值。

该方法在《理解深度前馈神经网络训练困难性》- Glorot, X. & Bengio, Y.（2010）中有描述。生成的张量将从$\mathcal{N}(0, \text{std}²)$中采样值，其中

$\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\_in} + \text{fan\_out}}}$ 

也称为 Glorot 初始化。

参数

+   **tensor**（*Tensor*）- 一个 n 维 torch.Tensor

+   **gain**（[*float*](https://docs.python.org/3/library/functions.html#float)）- 可选的缩放因子

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional) **[*Generator**]*) - 用于采样的 torch 生成器（默认值：无）

返回类型

*Tensor*

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.xavier_normal_(w) 
```

```py
torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu', generator=None)
```

使用 Kaiming 均匀分布填充输入张量的值。

该方法在《深入研究整流器：超越 ImageNet 分类的人类级性能》- He, K.等人（2015）中有描述。生成的张量将从$\mathcal{U}(-\text{bound}, \text{bound})$中采样值，其中

$\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\_mode}}}$ 

也称为 He 初始化。

参数

+   **tensor**（*Tensor*）- 一个 n 维 torch.Tensor

+   **a**（[*float*](https://docs.python.org/3/library/functions.html#float)）- 此层后使用的整流器的负斜率（仅与'leaky_relu'一起使用）

+   **mode**（[*str*](https://docs.python.org/3/library/stdtypes.html#str)）- 要么'fan_in'（默认），要么'fan_out'。选择'fan_in'保留前向传播中权重方差的幅度。选择'fan_out'保留反向传播中的幅度。

+   **nonlinearity**（[*str*](https://docs.python.org/3/library/stdtypes.html#str)）- 非线性函数（nn.functional 名称），建议仅与'relu'或'leaky_relu'一起使用（默认）。

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional "(在 Python v3.12 中)")**[*Generator**]*) - 用于抽样的 torch 生成器（默认值：无）

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu') 
```

```py
torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu', generator=None)
```

用 Kaiming 正态分布填充输入张量的值。

该方法在 Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K.等人（2015）中有描述。生成的张量将具有从$\mathcal{N}(0, \text{std}²)$中抽样的值，其中

$\text{std} = \frac{\text{gain}}{\sqrt{\text{fan\_mode}}}$ std=fan_mode​gain​

也称为 He 初始化。

参数

+   **tensor**（*Tensor*）- 一个 n 维 torch.Tensor

+   **a**（[*float*](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12 中)")）- 在此层后使用的整流器的负斜率（仅与`'leaky_relu'`一起使用）

+   **mode**（[*str*](https://docs.python.org/3/library/stdtypes.html#str "(在 Python v3.12 中)")）- `'fan_in'`（默认）或`'fan_out'`。选择`'fan_in'`保留前向传播中权重方差的幅度。选择`'fan_out'`保留反向传播中的幅度。

+   **nonlinearity**（[*str*](https://docs.python.org/3/library/stdtypes.html#str "(在 Python v3.12 中)")）- 非线性函数（nn.functional 名称），建议仅与`'relu'`或`'leaky_relu'`一起使用（默认值）。

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional "(在 Python v3.12 中)")**[*Generator**]*) - 用于抽样的 torch 生成器（默认值：无）

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu') 
```

```py
torch.nn.init.trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0, generator=None)
```

用截断正态分布中抽取的值填充输入张量。

这些值实际上是从正态分布$\mathcal{N}(\text{mean}, \text{std}²)$中抽取的，超出$[a, b]$的值会被重新绘制，直到它们在范围内。用于生成随机值的方法在$a \leq \text{mean} \leq b$时效果最佳。

参数

+   **tensor**（*Tensor*）- 一个 n 维 torch.Tensor

+   **mean**（[*float*](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12 中)")）- 正态分布的均值

+   **std**（[*float*](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12 中)")）- 正态分布的标准差

+   **a**（[*float*](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12 中)")）- 最小截断值

+   **b**（[*float*](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12 中)")）- 最大截断值

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional "(在 Python v3.12 中)")**[*Generator**]*) - 用于抽样的 torch 生成器（默认值：无）

返回类型

*Tensor*

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.trunc_normal_(w) 
```

```py
torch.nn.init.orthogonal_(tensor, gain=1, generator=None)
```

用（半）正交矩阵填充输入张量。

在 Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A.等人（2013）中有描述。输入张量必须至少有 2 个维度，对于超过 2 个维度的张量，尾部维度会被展平。

参数

+   **tensor** - 一个 n 维 torch.Tensor，其中$n \geq 2$

+   **gain** - 可选的缩放因子

+   **generator**（[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional "(在 Python v3.12 中)")**[*Generator**]*) - 用于抽样的 torch 生成器（默认值：无）

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.orthogonal_(w) 
```

```py
torch.nn.init.sparse_(tensor, sparsity, std=0.01, generator=None)
```

将 2D 输入张量填充为稀疏矩阵。

非零元素将从正态分布$\mathcal{N}(0, 0.01)$中抽取，如 Deep learning via Hessian-free optimization - Martens, J.（2010）中所述。

参数

+   **tensor** - 一个 n 维 torch 张量

+   **sparsity** - 每列中要设置为零的元素比例

+   **std** - 用于生成非零值的正态分布的标准差

+   **generator**（[*可选*](https://docs.python.org/3/library/typing.html#typing.Optional "(在 Python v3.12 中)")**[*Generator**]*) - 用于采样的 torch 生成器（默认值：无）

示例

```py
>>> w = torch.empty(3, 5)
>>> nn.init.sparse_(w, sparsity=0.1) 
```
