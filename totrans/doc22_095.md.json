["```py\n>>> torch.zeros(2, 3, names=('N', 'C'))\ntensor([[0., 0., 0.],\n [0., 0., 0.]], names=('N', 'C')) \n```", "```py\n>>> imgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W'))\n>>> imgs.names\n('N', 'C', 'H', 'W')\n\n>>> renamed_imgs = imgs.rename(H='height', W='width')\n>>> renamed_imgs.names\n('N', 'C', 'height', 'width) \n```", "```py\n>>> imgs = torch.randn(1, 2, 2, 3 , names=(None, 'C', 'H', 'W'))\n>>> imgs.names\n(None, 'C', 'H', 'W') \n```", "```py\n>>> x = torch.randn(3, 3, names=('N', 'C'))\n>>> x.abs().names\n('N', 'C') \n```", "```py\nx = torch.randn(3, names=('X',))\ny = torch.randn(3)\nz = torch.randn(3, names=('Z',)) \n```", "```py\n>>> # x + y  # match('X', None) is True\n>>> # x + z  # match('X', 'Z') is False\n>>> # x + x  # match('X', 'X') is True\n\n>>> x + z\nError when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match. \n```", "```py\n>>> (x + y).names\n('X',)\n>>> (x + x).names\n('X',) \n```", "```py\n# This function is agnostic to the dimension ordering of `input`,\n# as long as it has a `C` dimension somewhere.\ndef scale_channels(input, scale):\n    scale = scale.refine_names('C')\n    return input * scale.align_as(input)\n\n>>> num_channels = 3\n>>> scale = torch.randn(num_channels, names=('C',))\n>>> imgs = torch.rand(3, 3, 3, num_channels, names=('N', 'H', 'W', 'C'))\n>>> more_imgs = torch.rand(3, num_channels, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> videos = torch.randn(3, num_channels, 3, 3, 3, names=('N', 'C', 'H', 'W', 'D')\n\n>>> scale_channels(imgs, scale)\n>>> scale_channels(more_imgs, scale)\n>>> scale_channels(videos, scale) \n```", "```py\n>>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n>>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n# Move the F (dim 5) and E dimension (dim 4) to the front while keeping\n# the rest in the same order\n>>> tensor.permute(5, 4, 0, 1, 2, 3)\n>>> named_tensor.align_to('F', 'E', ...) \n```", "```py\n>>> imgs = torch.randn(32, 3, 128, 128)\n>>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n\n>>> flat_imgs = imgs.view(32, -1)\n>>> named_flat_imgs = named_imgs.flatten(['C', 'H', 'W'], 'features')\n>>> named_flat_imgs.names\n('N', 'features')\n\n>>> unflattened_named_imgs = named_flat_imgs.unflatten('features', [('C', 3), ('H', 128), ('W', 128)])\n>>> unflattened_named_imgs.names\n('N', 'C', 'H', 'W') \n```", "```py\n>>> x = torch.randn(3, names=('D',))\n>>> weight = torch.randn(3, names=('D',), requires_grad=True)\n>>> loss = (x - weight).abs()\n>>> grad_loss = torch.randn(3)\n>>> loss.backward(grad_loss)\n>>> weight.grad  # Unnamed for now. Will be named in the future\ntensor([-1.8107, -0.6357,  0.0783])\n\n>>> weight.grad.zero_()\n>>> grad_loss = grad_loss.refine_names('C')\n>>> loss = (x - weight).abs()\n# Ideally we'd check that the names of loss and grad_loss match but we don't yet.\n>>> loss.backward(grad_loss)\n>>> weight.grad\ntensor([-1.8107, -0.6357,  0.0783]) \n```", "```py\nclass torch.Tensor\n```", "```py\nnames\u00b6\n```", "```py\nrename(*names, **rename_map)\u00b6\n```", "```py\n>>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))\n>>> renamed_imgs = imgs.rename(N='batch', C='channels')\n>>> renamed_imgs.names\n('batch', 'channels', 'H', 'W')\n\n>>> renamed_imgs = imgs.rename(None)\n>>> renamed_imgs.names\n(None, None, None, None)\n\n>>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')\n>>> renamed_imgs.names\n('batch', 'channel', 'height', 'width') \n```", "```py\nrename_(*names, **rename_map)\u00b6\n```", "```py\nrefine_names(*names)\u00b6\n```", "```py\n>>> imgs = torch.randn(32, 3, 128, 128)\n>>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n>>> named_imgs.names\n('N', 'C', 'H', 'W')\n\n>>> tensor = torch.randn(2, 3, 5, 7, 11)\n>>> tensor = tensor.refine_names('A', ..., 'B', 'C')\n>>> tensor.names\n('A', None, None, 'B', 'C') \n```", "```py\nalign_as(other) \u2192 Tensor\u00b6\n```", "```py\n# Example 1: Applying a mask\n>>> mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H')\n>>> imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C'))\n>>> imgs.masked_fill_(mask.align_as(imgs), 0)\n\n# Example 2: Applying a per-channel-scale\n>>> def scale_channels(input, scale):\n>>>    scale = scale.refine_names('C')\n>>>    return input * scale.align_as(input)\n\n>>> num_channels = 3\n>>> scale = torch.randn(num_channels, names=('C',))\n>>> imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C'))\n>>> more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W'))\n>>> videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))\n\n# scale_channels is agnostic to the dimension order of the input\n>>> scale_channels(imgs, scale)\n>>> scale_channels(more_imgs, scale)\n>>> scale_channels(videos, scale) \n```", "```py\nalign_to(*names)\u00b6\n```", "```py\n>>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n>>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n# Move the F and E dims to the front while keeping the rest in order\n>>> named_tensor.align_to('F', 'E', ...) \n```", "```py\nflatten(dims, out_dim) \u2192 Tensor\n```", "```py\n>>> imgs = torch.randn(32, 3, 128, 128, names=('N', 'C', 'H', 'W'))\n>>> flat_imgs = imgs.flatten(['C', 'H', 'W'], 'features')\n>>> flat_imgs.names, flat_imgs.shape\n(('N', 'features'), torch.Size([32, 49152])) \n```"]