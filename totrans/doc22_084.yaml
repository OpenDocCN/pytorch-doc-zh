- en: torch.utils.bottleneck
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/bottleneck.html](https://pytorch.org/docs/stable/bottleneck.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: torch.utils.bottleneck is a tool that can be used as an initial step for debugging
    bottlenecks in your program. It summarizes runs of your script with the Python
    profiler and PyTorch’s autograd profiler.
  prefs: []
  type: TYPE_NORMAL
- en: Run it on the command line with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: where [args] are any number of arguments to script.py, or run `python -m torch.utils.bottleneck
    -h` for more usage instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Because your script will be profiled, please ensure that it exits in a finite
    amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the asynchronous nature of CUDA kernels, when running against CUDA code,
    the cProfile output and CPU-mode autograd profilers may not show correct timings:
    the reported CPU time reports the amount of time used to launch the kernels but
    does not include the time the kernel spent executing on a GPU unless the operation
    does a synchronize. Ops that do synchronize appear to be extremely expensive under
    regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode
    autograd profiler may be helpful.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look
    at, you should first check if your script is CPU-bound (“CPU total time is much
    greater than CUDA total time”). If it is CPU-bound, looking at the results of
    the CPU-mode autograd profiler will help. If on the other hand your script spends
    most of its time executing on the GPU, then it makes sense to start looking for
    responsible CUDA operators in the output of the CUDA-mode autograd profiler.
  prefs: []
  type: TYPE_NORMAL
- en: Of course the reality is much more complicated and your script might not be
    in one of those two extremes depending on the part of the model you’re evaluating.
    If the profiler outputs don’t help, you could try looking at the result of [`torch.autograd.profiler.emit_nvtx()`](autograd.html#torch.autograd.profiler.emit_nvtx
    "torch.autograd.profiler.emit_nvtx") with `nvprof`. However, please take into
    account that the NVTX overhead is very high and often gives a heavily skewed timeline.
    Similarly, `Intel® VTune™ Profiler` helps to analyze performance on Intel platforms
    further with [`torch.autograd.profiler.emit_itt()`](autograd.html#torch.autograd.profiler.emit_itt
    "torch.autograd.profiler.emit_itt").
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: If you are profiling CUDA code, the first profiler that `bottleneck` runs (cProfile)
    will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting.
    This should not matter if your bottlenecks result in code much slower than the
    CUDA startup time.
  prefs: []
  type: TYPE_NORMAL
- en: For more complicated uses of the profilers (like in a multi-GPU case), please
    see [https://docs.python.org/3/library/profile.html](https://docs.python.org/3/library/profile.html)
    or [`torch.autograd.profiler.profile()`](autograd.html#torch.autograd.profiler.profile
    "torch.autograd.profiler.profile") for more information.
  prefs: []
  type: TYPE_NORMAL
