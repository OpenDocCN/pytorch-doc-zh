- en: Per-sample-gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/intermediate/per_sample_grads.html](https://pytorch.org/tutorials/intermediate/per_sample_grads.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-intermediate-per-sample-grads-py) to download
    the full example code
  prefs: []
  type: TYPE_NORMAL
- en: '## What is it?'
  prefs: []
  type: TYPE_NORMAL
- en: Per-sample-gradient computation is computing the gradient for each and every
    sample in a batch of data. It is a useful quantity in differential privacy, meta-learning,
    and optimization research.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial requires PyTorch 2.0.0 or later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s generate a batch of dummy data and pretend that we’re working with an
    MNIST dataset. The dummy images are 28 by 28 and we use a minibatch of size 64.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In regular model training, one would forward the minibatch through the model,
    and then call .backward() to compute gradients. This would generate an ‘average’
    gradient of the entire mini-batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In contrast to the above approach, per-sample-gradient computation is equivalent
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: for each individual sample of the data, perform a forward and a backward pass
    to get an individual (per-sample) gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`sample_grads[0]` is the per-sample-grad for model.conv1.weight. `model.conv1.weight.shape`
    is `[32, 1, 3, 3]`; notice how there is one gradient, per sample, in the batch
    for a total of 64.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Per-sample-grads, *the efficient way*, using function transforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can compute per-sample-gradients efficiently by using function transforms.
  prefs: []
  type: TYPE_NORMAL
- en: The `torch.func` function transform API transforms over functions. Our strategy
    is to define a function that computes the loss and then apply transforms to construct
    a function that computes per-sample-gradients.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the `torch.func.functional_call` function to treat an `nn.Module`
    like a function.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s extract the state from `model` into two dictionaries, parameters
    and buffers. We’ll be detaching them because we won’t use regular PyTorch autograd
    (e.g. Tensor.backward(), torch.autograd.grad).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s define a function to compute the loss of the model given a single
    input rather than a batch of inputs. It is important that this function accepts
    the parameters, the input, and the target, because we will be transforming over
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Note - because the model was originally written to handle batches, we’ll use
    `torch.unsqueeze` to add a batch dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s use the `grad` transform to create a new function that computes the
    gradient with respect to the first argument of `compute_loss` (i.e. the `params`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `ft_compute_grad` function computes the gradient for a single (sample, target)
    pair. We can use `vmap` to get it to compute the gradient over an entire batch
    of samples and targets. Note that `in_dims=(None, None, 0, 0)` because we wish
    to map `ft_compute_grad` over the 0th dimension of the data and targets, and use
    the same `params` and buffers for each.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s used our transformed function to compute per-sample-gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'we can double check that the results using `grad` and `vmap` match the results
    of hand processing each one individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A quick note: there are limitations around what types of functions can be transformed
    by `vmap`. The best functions to transform are ones that are pure functions: a
    function where the outputs are only determined by the inputs, and that have no
    side effects (e.g. mutation). `vmap` is unable to handle mutation of arbitrary
    Python data structures, but it is able to handle many in-place PyTorch operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Curious about how the performance of `vmap` compares?
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently the best results are obtained on newer GPU’s such as the A100 (Ampere)
    where we’ve seen up to 25x speedups on this example, but here are some results
    on our build machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: There are other optimized solutions (like in [https://github.com/pytorch/opacus](https://github.com/pytorch/opacus))
    to computing per-sample-gradients in PyTorch that also perform better than the
    naive method. But it’s cool that composing `vmap` and `grad` give us a nice speedup.
  prefs: []
  type: TYPE_NORMAL
- en: In general, vectorization with `vmap` should be faster than running a function
    in a for-loop and competitive with manual batching. There are some exceptions
    though, like if we haven’t implemented the `vmap` rule for a particular operation
    or if the underlying kernels weren’t optimized for older hardware (GPUs). If you
    see any of these cases, please let us know by opening an issue at on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 0 minutes 10.810 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: per_sample_grads.py`](../_downloads/bb0e78bec4d7a6e9b86b2e285cd06671/per_sample_grads.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: per_sample_grads.ipynb`](../_downloads/df89b8f78d7ed3520a0f632afae4a5b9/per_sample_grads.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
