- en: Per-sample-gradients
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个样本的梯度
- en: 原文：[https://pytorch.org/tutorials/intermediate/per_sample_grads.html](https://pytorch.org/tutorials/intermediate/per_sample_grads.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/tutorials/intermediate/per_sample_grads.html](https://pytorch.org/tutorials/intermediate/per_sample_grads.html)'
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-intermediate-per-sample-grads-py) to download
    the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-intermediate-per-sample-grads-py)下载完整示例代码
- en: '## What is it?'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '## 它是什么？'
- en: Per-sample-gradient computation is computing the gradient for each and every
    sample in a batch of data. It is a useful quantity in differential privacy, meta-learning,
    and optimization research.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本梯度计算是计算批量数据中每个样本的梯度。在差分隐私、元学习和优化研究中，这是一个有用的量。
- en: Note
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This tutorial requires PyTorch 2.0.0 or later.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程需要 PyTorch 2.0.0 或更高版本。
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s generate a batch of dummy data and pretend that we’re working with an
    MNIST dataset. The dummy images are 28 by 28 and we use a minibatch of size 64.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一批虚拟数据，并假装我们正在处理一个 MNIST 数据集。虚拟图像是 28x28，我们使用大小为 64 的小批量。
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In regular model training, one would forward the minibatch through the model,
    and then call .backward() to compute gradients. This would generate an ‘average’
    gradient of the entire mini-batch:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规模型训练中，人们会将小批量数据通过模型前向传播，然后调用 .backward() 来计算梯度。这将生成整个小批量的‘平均’梯度：
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In contrast to the above approach, per-sample-gradient computation is equivalent
    to:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述方法相反，每个样本梯度计算等同于：
- en: for each individual sample of the data, perform a forward and a backward pass
    to get an individual (per-sample) gradient.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于数据的每个单独样本，执行前向和后向传递以获得单个（每个样本）梯度。
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`sample_grads[0]` is the per-sample-grad for model.conv1.weight. `model.conv1.weight.shape`
    is `[32, 1, 3, 3]`; notice how there is one gradient, per sample, in the batch
    for a total of 64.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample_grads[0]` 是模型 `conv1.weight` 的每个样本梯度。`model.conv1.weight.shape` 是 `[32,
    1, 3, 3]`；注意每个样本在批处理中有一个梯度，总共有 64 个。'
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Per-sample-grads, *the efficient way*, using function transforms
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每个样本梯度，*高效的方式*，使用函数转换
- en: We can compute per-sample-gradients efficiently by using function transforms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用函数转换来高效地计算每个样本的梯度。
- en: The `torch.func` function transform API transforms over functions. Our strategy
    is to define a function that computes the loss and then apply transforms to construct
    a function that computes per-sample-gradients.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.func` 函数转换 API 对函数进行转换。我们的策略是定义一个计算损失的函数，然后应用转换来构建一个计算每个样本梯度的函数。'
- en: We’ll use the `torch.func.functional_call` function to treat an `nn.Module`
    like a function.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `torch.func.functional_call` 函数来将 `nn.Module` 视为一个函数。
- en: First, let’s extract the state from `model` into two dictionaries, parameters
    and buffers. We’ll be detaching them because we won’t use regular PyTorch autograd
    (e.g. Tensor.backward(), torch.autograd.grad).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从 `model` 中提取状态到两个字典中，parameters 和 buffers。我们将对它们进行分离，因为我们不会使用常规的 PyTorch
    autograd（例如 Tensor.backward()，torch.autograd.grad）。
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, let’s define a function to compute the loss of the model given a single
    input rather than a batch of inputs. It is important that this function accepts
    the parameters, the input, and the target, because we will be transforming over
    them.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个函数来计算模型给定单个输入而不是一批输入的损失。这个函数接受参数、输入和目标是很重要的，因为我们将对它们进行转换。
- en: Note - because the model was originally written to handle batches, we’ll use
    `torch.unsqueeze` to add a batch dimension.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 - 因为模型最初是为处理批量而编写的，我们将使用 `torch.unsqueeze` 来添加一个批处理维度。
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, let’s use the `grad` transform to create a new function that computes the
    gradient with respect to the first argument of `compute_loss` (i.e. the `params`).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 `grad` 转换来创建一个新函数，该函数计算相对于 `compute_loss` 的第一个参数（即 `params`）的梯度。
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `ft_compute_grad` function computes the gradient for a single (sample, target)
    pair. We can use `vmap` to get it to compute the gradient over an entire batch
    of samples and targets. Note that `in_dims=(None, None, 0, 0)` because we wish
    to map `ft_compute_grad` over the 0th dimension of the data and targets, and use
    the same `params` and buffers for each.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`ft_compute_grad` 函数计算单个（样本，目标）对的梯度。我们可以使用 `vmap` 来让它计算整个批量样本和目标的梯度。注意 `in_dims=(None,
    None, 0, 0)`，因为我们希望将 `ft_compute_grad` 映射到数据和目标的第 0 维，并对每个使用相同的 `params` 和 buffers。'
- en: '[PRE9]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, let’s used our transformed function to compute per-sample-gradients:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用我们转换后的函数来计算每个样本的梯度：
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'we can double check that the results using `grad` and `vmap` match the results
    of hand processing each one individually:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 `grad` 和 `vmap` 来双重检查结果，以确保与手动处理每个结果一致：
- en: '[PRE11]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A quick note: there are limitations around what types of functions can be transformed
    by `vmap`. The best functions to transform are ones that are pure functions: a
    function where the outputs are only determined by the inputs, and that have no
    side effects (e.g. mutation). `vmap` is unable to handle mutation of arbitrary
    Python data structures, but it is able to handle many in-place PyTorch operations.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速说明：关于哪些类型的函数可以被 `vmap` 转换存在一些限制。最适合转换的函数是纯函数：输出仅由输入决定，并且没有副作用（例如突变）。`vmap`
    无法处理任意 Python 数据结构的突变，但它可以处理许多原地 PyTorch 操作。
- en: Performance comparison
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能比较
- en: Curious about how the performance of `vmap` compares?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 想知道 `vmap` 的性能如何？
- en: 'Currently the best results are obtained on newer GPU’s such as the A100 (Ampere)
    where we’ve seen up to 25x speedups on this example, but here are some results
    on our build machines:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最佳结果是在新型 GPU（如 A100（Ampere））上获得的，在这个示例中我们看到了高达 25 倍的加速，但是这里是我们构建机器上的一些结果：
- en: '[PRE12]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: There are other optimized solutions (like in [https://github.com/pytorch/opacus](https://github.com/pytorch/opacus))
    to computing per-sample-gradients in PyTorch that also perform better than the
    naive method. But it’s cool that composing `vmap` and `grad` give us a nice speedup.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中有其他优化的解决方案（例如 [https://github.com/pytorch/opacus](https://github.com/pytorch/opacus)）来计算每个样本的梯度，这些解决方案的性能也比朴素方法更好。但是将
    `vmap` 和 `grad` 组合起来给我们带来了一个很好的加速。
- en: In general, vectorization with `vmap` should be faster than running a function
    in a for-loop and competitive with manual batching. There are some exceptions
    though, like if we haven’t implemented the `vmap` rule for a particular operation
    or if the underlying kernels weren’t optimized for older hardware (GPUs). If you
    see any of these cases, please let us know by opening an issue at on GitHub.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，使用 `vmap` 进行向量化应该比在 for 循环中运行函数更快，并且与手动分批处理相竞争。但也有一些例外情况，比如如果我们没有为特定操作实现
    `vmap` 规则，或者如果底层内核没有针对旧硬件（GPU）进行优化。如果您遇到这些情况，请通过在 GitHub 上开启一个问题来告诉我们。
- en: '**Total running time of the script:** ( 0 minutes 10.810 seconds)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间:** ( 0 分钟 10.810 秒)'
- en: '[`Download Python source code: per_sample_grads.py`](../_downloads/bb0e78bec4d7a6e9b86b2e285cd06671/per_sample_grads.py)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载 Python 源代码: per_sample_grads.py`](../_downloads/bb0e78bec4d7a6e9b86b2e285cd06671/per_sample_grads.py)'
- en: '[`Download Jupyter notebook: per_sample_grads.ipynb`](../_downloads/df89b8f78d7ed3520a0f632afae4a5b9/per_sample_grads.ipynb)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载 Jupyter 笔记本: per_sample_grads.ipynb`](../_downloads/df89b8f78d7ed3520a0f632afae4a5b9/per_sample_grads.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery 生成的画廊](https://sphinx-gallery.github.io)'
