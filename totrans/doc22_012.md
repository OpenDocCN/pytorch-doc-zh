# CUDA 语义

> 原文：[`pytorch.org/docs/stable/notes/cuda.html`](https://pytorch.org/docs/stable/notes/cuda.html)

`torch.cuda`用于设置和运行 CUDA 操作。它跟踪当前选择的 GPU，您分配的所有 CUDA 张量默认将在该设备上创建。所选设备可以通过`torch.cuda.device`上下文管理器更改。

然而，一旦分配了张量，您可以对其进行操作，而不管所选设备如何，结果将始终放置在与张量相同的设备上。

默认情况下不允许跨 GPU 操作，除了`copy_()`和其他具有类似复制功能的方法，如`to()`和`cuda()`。除非启用对等内存访问，否则任何尝试在不同设备上分布的张量上启动操作的尝试都将引发错误。

以下是一个展示示例：

```py
cuda = torch.device('cuda')     # Default CUDA device
cuda0 = torch.device('cuda:0')
cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)

x = torch.tensor([1., 2.], device=cuda0)
# x.device is device(type='cuda', index=0)
y = torch.tensor([1., 2.]).cuda()
# y.device is device(type='cuda', index=0)

with torch.cuda.device(1):
    # allocates a tensor on GPU 1
    a = torch.tensor([1., 2.], device=cuda)

    # transfers a tensor from CPU to GPU 1
    b = torch.tensor([1., 2.]).cuda()
    # a.device and b.device are device(type='cuda', index=1)

    # You can also use ``Tensor.to`` to transfer a tensor:
    b2 = torch.tensor([1., 2.]).to(device=cuda)
    # b.device and b2.device are device(type='cuda', index=1)

    c = a + b
    # c.device is device(type='cuda', index=1)

    z = x + y
    # z.device is device(type='cuda', index=0)

    # even within a context, you can specify the device
    # (or give a GPU index to the .cuda call)
    d = torch.randn(2, device=cuda2)
    e = torch.randn(2).to(cuda2)
    f = torch.randn(2).cuda(cuda2)
    # d.device, e.device, and f.device are all device(type='cuda', index=2) 
```

## Ampere（以及更高版本）设备上的 TensorFloat-32（TF32）[](#tensorfloat-32-tf32-on-ampere-and-later-devices "跳转到此标题的永久链接")

从 PyTorch 1.7 开始，有一个名为 allow_tf32 的新标志。在 PyTorch 1.7 到 PyTorch 1.11 中，默认为 True，在 PyTorch 1.12 及以后为 False。该标志控制 PyTorch 是否允许在 NVIDIA GPU 上使用 TensorFloat32（TF32）张量核心来计算 matmul（矩阵乘法和批量矩阵乘法）和卷积。

TF32 张量核心旨在通过将输入数据四舍五入为 10 位尾数，并使用 FP32 精度累积结果，保持 FP32 动态范围，从而在 torch.float32 张量上实现更好的 matmul 和卷积性能。

matmuls 和卷积是分别控制的，它们对应的标志可以在以下位置访问：

```py
# The flag below controls whether to allow TF32 on matmul. This flag defaults to False
# in PyTorch 1.12 and later.
torch.backends.cuda.matmul.allow_tf32 = True

# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.
torch.backends.cudnn.allow_tf32 = True 
```

matmuls 的精度也可以更广泛地设置（不仅限于 CUDA），通过`set_float_32_matmul_precision()`。请注意，除了 matmuls 和卷积本身，内部使用 matmuls 或卷积的函数和 nn 模块也会受到影响。这些包括 nn.Linear、nn.Conv*、cdist、tensordot、affine grid 和 grid sample、adaptive log softmax、GRU 和 LSTM。

要了解精度和速度的概念，请参见下面的示例代码和基准数据（在 A100 上）：

```py
a_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')
b_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')
ab_full = a_full @ b_full
mean = ab_full.abs().mean()  # 80.7277

a = a_full.float()
b = b_full.float()

# Do matmul at TF32 mode.
torch.backends.cuda.matmul.allow_tf32 = True
ab_tf32 = a @ b  # takes 0.016s on GA100
error = (ab_tf32 - ab_full).abs().max()  # 0.1747
relative_error = error / mean  # 0.0022

# Do matmul with TF32 disabled.
torch.backends.cuda.matmul.allow_tf32 = False
ab_fp32 = a @ b  # takes 0.11s on GA100
error = (ab_fp32 - ab_full).abs().max()  # 0.0031
relative_error = error / mean  # 0.000039 
```

从上面的例子可以看出，启用 TF32 后，在 A100 上速度快约 7 倍，与双精度相比的相对误差大约大 2 个数量级。请注意，TF32 与单精度速度的确切比率取决于硬件生成，例如内存带宽与计算的比率以及 TF32 与 FP32 matmul 吞吐量的比率可能会因世代或模型而异。如果需要完整的 FP32 精度，用户可以通过禁用 TF32 来实现：

```py
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False 
```

要在 C++中关闭 TF32 标志，可以执行以下操作：

```py
at::globalContext().setAllowTF32CuBLAS(false);
at::globalContext().setAllowTF32CuDNN(false); 
```

有关 TF32 的更多信息，请参见：

+   [TensorFloat-32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)

+   [CUDA 11](https://devblogs.nvidia.com/cuda-11-features-revealed/)

+   [Ampere 架构](https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/)  ## FP16 GEMMs 中的降低精度缩减[](#reduced-precision-reduction-in-fp16-gemms "跳转到此标题的永久链接")

fp16 GEMMs 可能使用一些中间降低精度的缩减（例如在 fp16 而不是 fp32 中）。这些选择性的精度降低可以在某些工作负载（特别是具有大 k 维度的工作负载）和 GPU 架构上实现更高的性能，但会牺牲数值精度和可能会发生溢出。

V100 上的一些示例基准数据：

```py
[--------------------------- bench_gemm_transformer --------------------------]
      [  m ,  k  ,  n  ]    |  allow_fp16_reduc=True  |  allow_fp16_reduc=False
1 threads: --------------------------------------------------------------------
      [4096, 4048, 4096]    |           1634.6        |           1639.8
      [4096, 4056, 4096]    |           1670.8        |           1661.9
      [4096, 4080, 4096]    |           1664.2        |           1658.3
      [4096, 4096, 4096]    |           1639.4        |           1651.0
      [4096, 4104, 4096]    |           1677.4        |           1674.9
      [4096, 4128, 4096]    |           1655.7        |           1646.0
      [4096, 4144, 4096]    |           1796.8        |           2519.6
      [4096, 5096, 4096]    |           2094.6        |           3190.0
      [4096, 5104, 4096]    |           2144.0        |           2663.5
      [4096, 5112, 4096]    |           2149.1        |           2766.9
      [4096, 5120, 4096]    |           2142.8        |           2631.0
      [4096, 9728, 4096]    |           3875.1        |           5779.8
      [4096, 16384, 4096]   |           6182.9        |           9656.5
(times in microseconds). 
```

如果需要完整精度降低，用户可以通过以下方式在 fp16 GEMMs 中禁用减少精度降低：

```py
torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False 
```

要在 C++ 中切换减少精度降低标志，可以执行

```py
at::globalContext().setAllowFP16ReductionCuBLAS(false); 
```  ## 减少 BF16 GEMMs 中的精度降低

类似的标志（如上所述）也适用于 BFloat16 GEMMs。请注意，此开关默认设置为 True 用于 BF16，如果您在工作负载中观察到数值不稳定性，可能希望将其设置为 False。

如果不需要减少精度降低，用户可以通过以下方式禁用 bf16 GEMMs 中的减少精度降低：

```py
torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False 
```

要在 C++ 中切换减少精度降低标志，可以执行

```py
at::globalContext().setAllowBF16ReductionCuBLAS(true); 
```

## 异步执行

默认情况下，GPU 操作是异步的。当调用使用 GPU 的函数时，操作会*排队*到特定设备，但不一定会立即执行。这使我们能够并行执行更多计算，包括在 CPU 或其他 GPU 上的操作。

总的来说，异步计算的效果对调用者是不可见的，因为（1）每个设备按照排队的顺序执行操作，（2）PyTorch 在 CPU 和 GPU 之间或两个 GPU 之间复制数据时会自动执行必要的同步。因此，计算将继续进行，就好像每个操作都是同步执行的。

可以通过设置环境变量`CUDA_LAUNCH_BLOCKING=1`来强制同步计算。当 GPU 上发生错误时，这可能会很方便。（使用异步执行时，直到操作实际执行后才报告此类错误，因此堆栈跟踪不显示请求位置。）

异步计算的一个后果是，没有同步的时间测量不准确。为了获得精确的测量结果，应在测量之前调用 `torch.cuda.synchronize()`，或者使用 `torch.cuda.Event` 记录时间如下：

```py
start_event = torch.cuda.Event(enable_timing=True)
end_event = torch.cuda.Event(enable_timing=True)
start_event.record()

# Run some things here

end_event.record()
torch.cuda.synchronize()  # Wait for the events to be recorded!
elapsed_time_ms = start_event.elapsed_time(end_event) 
```

作为例外，一些函数（如`to()` 和 `copy_()`）允许显式的 `non_blocking` 参数，让调用者在不必要时绕过同步。另一个例外是 CUDA 流，下面会解释。

### CUDA 流

[CUDA 流](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams) 是属于特定设备的线性执行序列。通常情况下，您不需要显式创建一个：默认情况下，每个设备使用自己的“默认”流。

每个流内部的操作按创建顺序串行化，但来自不同流的操作可以以任何相对顺序并发执行，除非使用显式同步函数（如`synchronize()` 或 `wait_stream()`）。例如，以下代码是不正确的：

```py
cuda = torch.device('cuda')
s = torch.cuda.Stream()  # Create a new stream.
A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)
with torch.cuda.stream(s):
    # sum() may start execution before normal_() finishes!
    B = torch.sum(A) 
```

当“当前流”是默认流时，PyTorch 在数据移动时会自动执行必要的同步，如上所述。但是，当使用非默认流时，用户有责任确保适当的同步。此示例的修复版本如下：

```py
cuda = torch.device('cuda')
s = torch.cuda.Stream()  # Create a new stream.
A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)
s.wait_stream(torch.cuda.default_stream(cuda))  # NEW!
with torch.cuda.stream(s):
    B = torch.sum(A)
A.record_stream(s)  # NEW! 
```

有两个新的添加。`torch.cuda.Stream.wait_stream()`调用确保`normal_()`执行完成后，我们开始在侧流上运行`sum(A)`。`torch.Tensor.record_stream()`（有关更多详细信息，请参见）确保在`sum(A)`完成之前我们不会释放 A。您还可以在以后的某个时间点手动等待流`torch.cuda.default_stream(cuda).wait_stream(s)`（请注意，立即等待是没有意义的，因为这将阻止流执行与默认流上的其他工作并行运行）。有关何时使用其中一个的更多详细信息，请参阅`torch.Tensor.record_stream()`的文档。

请注意，即使没有读取依赖关系，例如在这个例子中看到的情况下，这种同步也是必要的：

```py
cuda = torch.device('cuda')
s = torch.cuda.Stream()  # Create a new stream.
A = torch.empty((100, 100), device=cuda)
s.wait_stream(torch.cuda.default_stream(cuda))  # STILL REQUIRED!
with torch.cuda.stream(s):
    A.normal_(0.0, 1.0)
    A.record_stream(s) 
```

尽管`s`上的计算不读取`A`的内容，也没有其他对`A`的使用，但仍然需要同步，因为`A`可能对应于由 CUDA 缓存分配器重新分配的内存，其中包含来自旧（已释放）内存的挂起操作。

### 反向传递的流语义[](#stream-semantics-of-backward-passes "跳转到此标题")

每个反向 CUDA 操作都在用于其对应的前向操作的相同流上运行。如果您的前向传递在不同流上并行运行独立操作，这有助于反向传递利用相同的并行性。

关于周围操作的反向调用的流语义与任何其他调用的流语义相同。在反向传递中，即使反向操作在多个流上运行，也会插入内部同步，以确保这一点，如前一段所述。更具体地说，当调用`autograd.backward`, `autograd.grad`, 或 `tensor.backward`，并可选择将 CUDA 张量作为初始梯度（例如，`autograd.backward(..., grad_tensors=initial_grads)`, `autograd.grad(..., grad_outputs=initial_grads)`, 或 `tensor.backward(..., gradient=initial_grad)`)时，

1.  可选择填充初始梯度，

1.  调用反向传递，并

1.  使用梯度

具有与任何一组操作相同的流语义关系：

```py
s = torch.cuda.Stream()

# Safe, grads are used in the same stream context as backward()
with torch.cuda.stream(s):
    loss.backward()
    use grads

# Unsafe
with torch.cuda.stream(s):
    loss.backward()
use grads

# Safe, with synchronization
with torch.cuda.stream(s):
    loss.backward()
torch.cuda.current_stream().wait_stream(s)
use grads

# Safe, populating initial grad and invoking backward are in the same stream context
with torch.cuda.stream(s):
    loss.backward(gradient=torch.ones_like(loss))

# Unsafe, populating initial_grad and invoking backward are in different stream contexts,
# without synchronization
initial_grad = torch.ones_like(loss)
with torch.cuda.stream(s):
    loss.backward(gradient=initial_grad)

# Safe, with synchronization
initial_grad = torch.ones_like(loss)
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    initial_grad.record_stream(s)
    loss.backward(gradient=initial_grad) 
```

#### BC 注意：在默认流上使用梯度[](#bc-note-using-grads-on-the-default-stream "跳转到此标题")

在 PyTorch 的早期版本（1.9 及更早版本）中，自动求导引擎总是将默认流与所有反向操作同步，因此以下模式：

```py
with torch.cuda.stream(s):
    loss.backward()
use grads 
```

只要`使用梯度`发生在默认流上，就是安全的。在当前的 PyTorch 中，该模式不再安全。如果`backward()`和`使用梯度`在不同的流上下文中，您必须同步这些流：

```py
with torch.cuda.stream(s):
    loss.backward()
torch.cuda.current_stream().wait_stream(s)
use grads 
```

即使`使用梯度`在默认流上。## 内存管理[](#memory-management "跳转到此标题")

PyTorch 使用缓存内存分配器加速内存分配。这允许快速内存释放而无需设备同步。但是，分配器管理的未使用内存仍会显示为在 `nvidia-smi` 中使用。您可以使用 `memory_allocated()` 和 `max_memory_allocated()` 监视张量占用的内存，并使用 `memory_reserved()` 和 `max_memory_reserved()` 监视缓存分配器管理的总内存量。调用 `empty_cache()` 释放 PyTorch 中所有**未使用**的缓存内存，以便其他 GPU 应用程序可以使用。但是，张量占用的 GPU 内存不会被释放，因此不能增加供 PyTorch 使用的 GPU 内存量。

为了更好地了解 CUDA 内存如何随时间变化，了解 CUDA 内存使用情况 描述了捕获和可视化内存使用痕迹的工具。

对于更高级的用户，我们通过 `memory_stats()` 提供更全面的内存基准测试。我们还提供通过 `memory_snapshot()` 捕获内存分配器状态的完整快照的功能，这可以帮助您了解代码产生的底层分配模式。

### 环境变量

使用缓存分配器可能会干扰 `cuda-memcheck` 等内存检查工具。要使用 `cuda-memcheck` 调试内存错误，请在环境中设置 `PYTORCH_NO_CUDA_MEMORY_CACHING=1` 以禁用缓存。

缓存分配器的行为可以通过环境变量 `PYTORCH_CUDA_ALLOC_CONF` 进行控制。格式为 `PYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>...` 可用选项：

+   `backend` 允许选择底层分配器的实现。目前，有效选项有 `native`，使用 PyTorch 的原生实现，以及 `cudaMallocAsync`，使用 [CUDA 内置的异步分配器](https://developer.nvidia.com/blog/using-cuda-stream-ordered-memory-allocator-part-1/)。`cudaMallocAsync` 需要 CUDA 11.4 或更新版本。默认值是 `native`。`backend` 适用于进程使用的所有设备，不能针对每个设备指定。

+   `max_split_size_mb` 防止原生分配器分割大于此大小（以 MB 为单位）的块。这可以减少碎片化，并可能使一些边缘工作负载在不耗尽内存的情况下完成。性能成本可能从‘零’到‘可观’不等，取决于分配模式。默认值为无限制，即所有块都可以分割。`memory_stats()` 和 `memory_summary()` 方法对调整很有用。此选项应作为最后的手段用于因‘内存不足’而中止的工作负载，并显示大量非活动分割块。`max_split_size_mb` 仅在 `backend:native` 时有意义。对于 `backend:cudaMallocAsync`，`max_split_size_mb` 将被忽略。

+   `roundup_power2_divisions`有助于将请求的分配大小四舍五入到最接近的 2 的幂次方，并更好地利用块。在原生 CUDACachingAllocator 中，大小会被四舍五入到 512 的倍数块大小，因此对于较小的大小效果很好。然而，对于大型相邻分配来说，这可能效率低下，因为每个分配都会进入不同大小的块，并且这些块的重复使用被最小化。这可能会产生大量未使用的块，并浪费 GPU 内存容量。此选项使分配大小四舍五入到最接近的 2 的幂次方。例如，如果我们需要将大小为 1200 四舍五入，如果分割数为 4，那么大小 1200 位于 1024 和 2048 之间，如果在它们之间进行 4 次分割，值为 1024、1280、1536 和 1792。因此，大小为 1200 的分配将被四舍五入为 1280，作为最接近 2 的幂次方的上限。指定一个值应用于所有分配大小，或指定一个键值对数组，为每个 2 的幂次方间隔单独设置 2 的幂次方分割。例如，为所有小于 256MB 的分配设置 1 个分割，为 256MB 至 512MB 之间的分配设置 2 个分割，为 512MB 至 1GB 之间的分配设置 4 个分割，为任何更大的分配设置 8 个分割，将旋钮值设置为：[256:1,512:2,1024:4,>:8]。`roundup_power2_divisions`仅在`backend:native`时有意义。在`backend:cudaMallocAsync`中，`roundup_power2_divisions`会被忽略。

+   `garbage_collection_threshold`有助于主动回收未使用的 GPU 内存，以避免触发昂贵的同步和回收所有操作（release_cached_blocks），这可能对延迟关键的 GPU 应用（例如服务器）不利。设置此阈值（例如 0.8）后，如果 GPU 内存容量使用超过阈值（即分配给 GPU 应用程序的总内存的 80%），分配器将开始回收 GPU 内存块。该算法更倾向于首先释放旧的和未使用的块，以避免释放正在被活跃重复使用的块。阈值应该在大于 0.0 且小于 1.0 之间。`garbage_collection_threshold`仅在`backend:native`时有意义。在`backend:cudaMallocAsync`中，`garbage_collection_threshold`会被忽略。

+   `expandable_segments`（实验性，默认值：False）如果设置为 True，则此设置指示分配器创建可以稍后扩展的 CUDA 分配，以更好地处理频繁更改分配大小的情况，例如具有不断更改批量大小的作业。通常对于大型（>2MB）分配，分配器调用 cudaMalloc 以获取与用户请求的大小相同的分配。将来，如果这些分配的部分是空闲的，它们可以被重用于其他请求。当程序多次请求完全相同大小的请求或者是该大小的倍数时，这种方法效果很好。许多深度学习模型遵循这种行为。然而，一个常见的例外是当批量大小从一次迭代到下一次略微变化时，例如在批量推理中。当程序最初以批量大小 N 运行时，它将进行适合该大小的分配。如果将来以大小 N - 1 运行，则现有的分配仍然足够大。但是，如果以大小 N + 1 运行，则将不得不进行稍微更大的新分配。并非所有张量的大小都相同。有些可能是(N + 1)*A，而其他可能是(N + 1)*A*B，其中 A 和 B 是模型中的一些非批量维度。因为分配器在足够大时重用现有的分配，一些(N + 1)*A 的分配实际上将适合已经存在的 N*B*A 段中，尽管不完全。随着模型的运行，它将部分填充所有这些段，留下这些段末尾的无法使用的空闲内存片段。在某个时刻，分配器将需要 cudaMalloc 一个新的(N + 1)*A*B 段。如果没有足够的内存，现在没有办法恢复现有段末尾的空闲内存片段。对于 50 层以上的模型，这种模式可能重复 50 多次，创建许多碎片。

    expandable_segments 允许分配器最初创建一个段，然后在需要更多内存时扩展其大小。它尝试创建一个（每个流）随需增长的段，而不是每次分配一个段。现在当 N + 1 的情况发生时，分配将很好地平铺到一个大段中，直到填满。然后请求更多内存并附加到段的末尾。这个过程不会创建太多无法使用的内存碎片，因此更有可能成功找到这些内存。

    pinned_use_cuda_host_register 选项是一个布尔标志，用于确定是否使用 CUDA API 的 cudaHostRegister 函数来分配固定内存，而不是默认的 cudaHostAlloc。当设置为 True 时，内存是使用常规 malloc 分配的，然后在调用 cudaHostRegister 之前将页面映射到内存。这种页面的预映射有助于减少执行 cudaHostRegister 期间的锁定时间。

    当 pinned_use_cuda_host_register 设置为 True 时，pinned_num_register_threads 选项才有效。默认情况下，使用一个线程来映射页面。此选项允许使用更多线程并行化页面映射操作，以减少固定内存的总分配时间。根据基准测试结果，此选项的一个良好值为 8。

注意

一些由 CUDA 内存管理 API 报告的统计数据是特定于`backend:native`的，并且在`backend:cudaMallocAsync`中没有意义。有关详细信息，请参阅每个函数的文档字符串。## 为 CUDA 使用自定义内存分配器

可以将分配器定义为 C/C++中的简单函数，并将它们编译为共享库，下面的代码展示了一个基本的分配器，只是跟踪所有内存操作。

```py
#include  <sys/types.h>
#include  <cuda_runtime_api.h>
#include  <iostream>
// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC
extern  "C"  {
void*  my_malloc(ssize_t  size,  int  device,  cudaStream_t  stream)  {
  void  *ptr;
  cudaMalloc(&ptr,  size);
  std::cout<<"alloc "<<ptr<<size<<std::endl;
  return  ptr;
}

void  my_free(void*  ptr,  ssize_t  size,  int  device,  cudaStream_t  stream)  {
  std::cout<<"free "<<ptr<<  " "<<stream<<std::endl;
  cudaFree(ptr);
}
} 
```

这可以通过`torch.cuda.memory.CUDAPluggableAllocator`在 Python 中使用。用户负责提供与上述签名匹配的.so 文件路径和 alloc/free 函数的名称。

```py
import torch

# Load the allocator
new_alloc = torch.cuda.memory.CUDAPluggableAllocator(
    'alloc.so', 'my_malloc', 'my_free')
# Swap the current allocator
torch.cuda.memory.change_current_allocator(new_alloc)
# This will allocate memory in the device using the new allocator
b = torch.zeros(10, device='cuda') 
```

```py
import torch

# Do an initial memory allocator
b = torch.zeros(10, device='cuda')
# Load the allocator
new_alloc = torch.cuda.memory.CUDAPluggableAllocator(
    'alloc.so', 'my_malloc', 'my_free')
# This will error since the current allocator was already instantiated
torch.cuda.memory.change_current_allocator(new_alloc) 
```

## cuBLAS 工作空间

对于每个 cuBLAS 句柄和 CUDA 流的组合，如果该句柄和流组合执行需要工作空间的 cuBLAS 内核，则将分配一个 cuBLAS 工作空间。为了避免重复分配工作空间，除非调用`torch._C._cuda_clearCublasWorkspaces()`，否则这些工作空间不会被释放。每次分配的工作空间大小可以通过环境变量`CUBLAS_WORKSPACE_CONFIG`指定，格式为`:[SIZE]:[COUNT]`。例如，默认每次分配的工作空间大小为`CUBLAS_WORKSPACE_CONFIG=:4096:2:16:8`，指定总大小为`2 * 4096 + 8 * 16 KiB`。要强制 cuBLAS 避免使用工作空间，请设置`CUBLAS_WORKSPACE_CONFIG=:0:0`。

## cuFFT 计划缓存

对于每个 CUDA 设备，使用 cuFFT 计划的 LRU 缓存来加速重复运行 FFT 方法（例如`torch.fft.fft()`）在具有相同几何形状和相同配置的 CUDA 张量上。因为一些 cuFFT 计划可能分配 GPU 内存，这些缓存具有最大容量。

您可以使用以下 API 来控制和查询当前设备缓存的属性：

+   `torch.backends.cuda.cufft_plan_cache.max_size`给出缓存的容量（在 CUDA 10 及更新版本上默认为 4096，在旧版本的 CUDA 上为 1023）。直接设置此值会修改容量。

+   `torch.backends.cuda.cufft_plan_cache.size`给出当前驻留在缓存中的计划数量。

+   `torch.backends.cuda.cufft_plan_cache.clear()`清除缓存。

要控制和查询非默认设备的计划缓存，可以使用`torch.backends.cuda.cufft_plan_cache`对象与`torch.device`对象或设备索引进行索引，并访问上述属性之一。例如，要设置设备`1`的缓存容量，可以编写`torch.backends.cuda.cufft_plan_cache[1].max_size = 10`。## 即时编译

PyTorch 会在 CUDA 张量上执行一些操作时，如 torch.special.zeta，进行即时编译。这种编译可能会耗时（取决于您的硬件和软件，最多几秒钟），并且对于单个运算符可能会多次发生，因为许多 PyTorch 运算符实际上会从各种内核中选择，每个内核必须根据其输入编译一次。这种编译每个进程只发生一次，或者如果使用内核缓存，则只发生一次。

默认情况下，如果定义了 XDG_CACHE_HOME，则 PyTorch 会在$XDG_CACHE_HOME/torch/kernels 中创建一个内核缓存，如果没有定义则在$HOME/.cache/torch/kernels 中创建（在 Windows 上，内核缓存尚不受支持）。缓存行为可以直接通过两个环境变量进行控制。如果将 USE_PYTORCH_KERNEL_CACHE 设置为 0，则不会使用任何缓存，如果设置了 PYTORCH_KERNEL_CACHE_PATH，则该路径将用作内核缓存的位置，而不是默认位置。

## 最佳实践

### 与设备无关的代码

由于 PyTorch 的结构，您可能需要明确编写与设备无关（CPU 或 GPU）的代码；一个示例可能是创建一个新张量作为循环神经网络的初始隐藏状态。

第一步是确定是否应该使用 GPU。一个常见的模式是使用 Python 的`argparse`模块读取用户参数，并有一个可以用来禁用 CUDA 的标志，结合`is_available()`。在下面的示例中，`args.device`会产生一个`torch.device`对象，可以用来将张量移动到 CPU 或 CUDA。

```py
import argparse
import torch

parser = argparse.ArgumentParser(description='PyTorch Example')
parser.add_argument('--disable-cuda', action='store_true',
                    help='Disable CUDA')
args = parser.parse_args()
args.device = None
if not args.disable_cuda and torch.cuda.is_available():
    args.device = torch.device('cuda')
else:
    args.device = torch.device('cpu') 
```

注意

在给定环境中评估 CUDA 的可用性时（`is_available()`），PyTorch 的默认行为是调用 CUDA Runtime API 方法[cudaGetDeviceCount](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18808e54893cfcaafefeab31a73cc55f)。因为这个调用反过来会初始化 CUDA Driver API（通过[cuInit](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__INITIALIZE.html#group__CUDA__INITIALIZE_1g0a2f1517e1bd8502c7194c3a8c134bc3)），如果尚未初始化，那么运行`is_available()`的进程的后续分叉将因 CUDA 初始化错误而失败。

可以在导入执行`is_available()`的 PyTorch 模块之前（或直接执行之前）在环境中设置`PYTORCH_NVML_BASED_CUDA_CHECK=1`，以便指导`is_available()`尝试基于 NVML 的评估（[nvmlDeviceGetCount_v2](https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_1ga93623b195bff04bbe3490ca33c8a42d)）。如果基于 NVML 的评估成功（即 NVML 发现/初始化不失败），`is_available()`调用将不会影响后续的分叉。

如果 NVML 发现/初始化失败，`is_available()`将回退到标准的 CUDA Runtime API 评估，并且前面提到的分叉约束将适用。

请注意，上述基于 NVML 的 CUDA 可用性评估提供的保证比默认的 CUDA Runtime API 方法更弱（需要 CUDA 初始化成功）。在某些情况下，基于 NVML 的检查可能成功，而后续的 CUDA 初始化失败。

现在我们有了`args.device`，我们可以使用它在所需的设备上创建一个张量。

```py
x = torch.empty((8, 42), device=args.device)
net = Network().to(device=args.device) 
```

这可以在许多情况下用于生成设备无关的代码。以下是在使用数据加载器时的示例：

```py
cuda0 = torch.device('cuda:0')  # CUDA GPU 0
for i, x in enumerate(train_loader):
    x = x.to(cuda0) 
```

在系统上使用多个 GPU 时，可以使用`CUDA_VISIBLE_DEVICES`环境标志来管理 PyTorch 可用的 GPU。如上所述，要手动控制在哪个 GPU 上创建张量的最佳实践是使用`torch.cuda.device`上下文管理器。

```py
print("Outside device is 0")  # On device 0 (default in most scenarios)
with torch.cuda.device(1):
    print("Inside device is 1")  # On device 1
print("Outside device is still 0")  # On device 0 
```

如果您有一个张量，并希望在同一设备上创建相同类型的新张量，则可以使用`torch.Tensor.new_*`方法（参见`torch.Tensor`）。虽然先前提到的`torch.*`工厂函数（Creation Ops）依赖于当前 GPU 上下文和您传递的属性参数，`torch.Tensor.new_*`方法会保留张量的设备和其他属性。

这是在创建模块时的推荐做法，其中在前向传递期间需要在内部创建新张量。

```py
cuda = torch.device('cuda')
x_cpu = torch.empty(2)
x_gpu = torch.empty(2, device=cuda)
x_cpu_long = torch.empty(2, dtype=torch.int64)

y_cpu = x_cpu.new_full([3, 2], fill_value=0.3)
print(y_cpu)

    tensor([[ 0.3000,  0.3000],
            [ 0.3000,  0.3000],
            [ 0.3000,  0.3000]])

y_gpu = x_gpu.new_full([3, 2], fill_value=-5)
print(y_gpu)

    tensor([[-5.0000, -5.0000],
            [-5.0000, -5.0000],
            [-5.0000, -5.0000]], device='cuda:0')

y_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])
print(y_cpu_long)

    tensor([[ 1,  2,  3]]) 
```

如果您想创建一个与另一个张量相同类型和大小的张量，并用 1 或 0 填充它，`ones_like()`或`zeros_like()`提供了方便的辅助函数（还保留了张量的`torch.device`和`torch.dtype`）。

```py
x_cpu = torch.empty(2, 3)
x_gpu = torch.empty(2, 3)

y_cpu = torch.ones_like(x_cpu)
y_gpu = torch.zeros_like(x_gpu) 
```

### 使用固定内存缓冲区[](#use-pinned-memory-buffers "Permalink to this heading")

警告

这是一个高级提示。如果过度使用固定内存，当内存不足时可能会导致严重问题，您应该意识到固定通常是一个昂贵的操作。

主机到 GPU 的拷贝速度在源自固定（锁页）内存时要快得多。CPU 张量和存储提供了一个`pin_memory()`方法，返回一个数据放在固定区域的对象副本。

此外，一旦您固定了一个张量或存储，您可以使用异步 GPU 拷贝。只需在`to()`或`cuda()`调用中传递一个额外的`non_blocking=True`参数。这可以用来重叠数据传输和计算。

您可以通过在其构造函数中传递`pin_memory=True`来使`DataLoader`返回放置在固定内存中的批次。### 使用 nn.parallel.DistributedDataParallel 而不是多进程或 nn.DataParallel[](#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel "Permalink to this heading")

大多数涉及批量输入和多个 GPU 的用例应默认使用`DistributedDataParallel`来利用多个 GPU。

使用 CUDA 模型与`multiprocessing`存在重要注意事项；除非确切满足数据处理要求，否则您的程序可能会出现不正确或未定义的行为。

建议使用`DistributedDataParallel`来进行多 GPU 训练，而不是`DataParallel`，即使只有一个节点。

`DistributedDataParallel`和`DataParallel`之间的区别是：`DistributedDataParallel`使用多进程，其中为每个 GPU 创建一个进程，而`DataParallel`使用多线程。通过使用多进程，每个 GPU 都有其专用进程，这避免了 Python 解释器的 GIL 引起的性能开销。

如果您使用`DistributedDataParallel`，您可以使用 torch.distributed.launch 实用程序来启动您的程序，请参阅第三方后端。## CUDA 图

CUDA 图是 CUDA 流及其依赖流执行的工作记录（主要是内核及其参数）。有关基础 CUDA API 的一般原则和详细信息，请参阅[使用 CUDA 图入门](https://developer.nvidia.com/blog/cuda-graphs/)和 CUDA C 编程指南的[图形部分](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs)。

PyTorch 支持使用[流捕获](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#creating-a-graph-using-stream-capture)构建 CUDA 图，这将使 CUDA 流处于*捕获模式*。发送到捕获流的 CUDA 工作实际上不会在 GPU 上运行。相反，工作将记录在图中。

捕获后，可以*启动*图以多次运行 GPU 工作。每次重播都会使用相同的内核和相同的参数运行相同的内核。对于指针参数，这意味着使用相同的内存地址。通过在每次重播之前用新数据（例如来自新批次）填充输入内存，可以在新数据上重新运行相同的工作。

### 为什么使用 CUDA 图？

重播图牺牲了典型急切执行的动态灵活性，以换取**大大减少的 CPU 开销**。图的参数和内核是固定的，因此图的重播跳过了所有层的参数设置和内核调度，包括 Python、C++和 CUDA 驱动程序开销。在幕后，重播通过单个调用[cudaGraphLaunch](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH_1g1accfe1da0c605a577c22d9751a09597)将整个图的工作提交给 GPU。重播中的内核在 GPU 上执行速度稍快，但省略 CPU 开销是主要好处。

如果您的网络的全部或部分是图形安全的（通常意味着静态形状和静态控制流，但请参阅其他约束），并且您怀疑其运行时至少在某种程度上受到 CPU 限制，则应尝试 CUDA 图。

### PyTorch API

警告

此 API 处于 beta 阶段，可能会在未来版本中更改。

PyTorch 通过原始`torch.cuda.CUDAGraph`类和两个方便的包装器`torch.cuda.graph`和`torch.cuda.make_graphed_callables`公开图形。

`torch.cuda.graph`是一个简单、多功能的上下文管理器，可以在其上下文中捕获 CUDA 工作。在捕获之前，通过运行几个急切迭代来预热要捕获的工作负载。预热必须在侧流上进行。由于图在每次重播时都从相同的内存地址读取和写入，因此在捕获期间必须保持对保存输入和输出数据的张量的长期引用。要在新输入数据上运行图，请将新数据复制到捕获的输入张量中，重播图，然后从捕获的输出张量中读取新输出。示例：

```py
g = torch.cuda.CUDAGraph()

# Placeholder input used for capture
static_input = torch.empty((5,), device="cuda")

# Warmup before capture
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    for _ in range(3):
        static_output = static_input * 2
torch.cuda.current_stream().wait_stream(s)

# Captures the graph
# To allow capture, automatically sets a side stream as the current stream in the context
with torch.cuda.graph(g):
    static_output = static_input * 2

# Fills the graph's input memory with new data to compute on
static_input.copy_(torch.full((5,), 3, device="cuda"))
g.replay()
# static_output holds the results
print(static_output)  # full of 3 * 2 = 6

# Fills the graph's input memory with more data to compute on
static_input.copy_(torch.full((5,), 4, device="cuda"))
g.replay()
print(static_output)  # full of 4 * 2 = 8 
```

查看整个网络捕获，与 torch.cuda.amp 一起使用，以及使用多个流以获取现实和高级模式。

`make_graphed_callables`更为复杂。`make_graphed_callables`接受 Python 函数和`torch.nn.Module`。对于每个传递的函数或模块，它会创建前向传递和反向传递工作的单独图。请参阅部分网络捕获。

#### 约束

如果一组操作是*可捕获*的，则不会违反以下任何约束。

约束条件适用于`torch.cuda.graph`上下文中的所有工作，以及您传递给`torch.cuda.make_graphed_callables()`的任何可调用对象的前向和后向传递中的所有工作。

违反任何这些规则可能会导致运行时错误：

+   捕获必须发生在非默认流上。（只有在使用原始`CUDAGraph.capture_begin`和`CUDAGraph.capture_end`调用时才需要关注。`graph`和`make_graphed_callables()`会为您设置一个侧边流。）

+   禁止与 GPU 同步的操作（例如`.item()`调用）。

+   允许 CUDA RNG 操作，但必须使用默认生成器。例如，明确构造一个新的`torch.Generator`实例，并将其作为`generator`参数传递给 RNG 函数是被禁止的。

违反任何这些规则可能会导致潜在的数值错误或未定义行为：

+   在一个进程中，一次只能进行一次捕获。

+   在进行捕获时，此进程中不得运行任何未捕获的 CUDA 工作（在任何线程上）。

+   不会捕获 CPU 工作。如果捕获的操作包括 CPU 工作，则在重放过程中将省略该工作。

+   每次重放都从同一（虚拟）内存地址读取和写入。

+   禁止动态控制流（基于 CPU 或 GPU 数据）。

+   禁止动态形状。图假定捕获的操作序列中的每个张量在每次重放中都具有相同的大小和布局。

+   允许在捕获中使用多个流，但有限制。

#### 非约束条件

+   一旦捕获，图可以在任何流上重放。

### 整个网络捕获

如果您的整个网络是可捕获的，您可以捕获和重放整个迭代：

```py
N, D_in, H, D_out = 640, 4096, 2048, 1024
model = torch.nn.Sequential(torch.nn.Linear(D_in, H),
                            torch.nn.Dropout(p=0.2),
                            torch.nn.Linear(H, D_out),
                            torch.nn.Dropout(p=0.1)).cuda()
loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

# Placeholders used for capture
static_input = torch.randn(N, D_in, device='cuda')
static_target = torch.randn(N, D_out, device='cuda')

# warmup
# Uses static_input and static_target here for convenience,
# but in a real setting, because the warmup includes optimizer.step()
# you must use a few batches of real data.
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    for i in range(3):
        optimizer.zero_grad(set_to_none=True)
        y_pred = model(static_input)
        loss = loss_fn(y_pred, static_target)
        loss.backward()
        optimizer.step()
torch.cuda.current_stream().wait_stream(s)

# capture
g = torch.cuda.CUDAGraph()
# Sets grads to None before capture, so backward() will create
# .grad attributes with allocations from the graph's private pool
optimizer.zero_grad(set_to_none=True)
with torch.cuda.graph(g):
    static_y_pred = model(static_input)
    static_loss = loss_fn(static_y_pred, static_target)
    static_loss.backward()
    optimizer.step()

real_inputs = [torch.rand_like(static_input) for _ in range(10)]
real_targets = [torch.rand_like(static_target) for _ in range(10)]

for data, target in zip(real_inputs, real_targets):
    # Fills the graph's input memory with new data to compute on
    static_input.copy_(data)
    static_target.copy_(target)
    # replay() includes forward, backward, and step.
    # You don't even need to call optimizer.zero_grad() between iterations
    # because the captured backward refills static .grad tensors in place.
    g.replay()
    # Params have been updated. static_y_pred, static_loss, and .grad
    # attributes hold values from computing on this iteration's data. 
```  ### 部分网络捕获[](#partial-network-capture "Permalink to this heading")

如果您的网络中有一部分不安全可捕获（例如，由于动态控制流、动态形状、CPU 同步或基本的 CPU 端逻辑），您可以急切地运行不安全的部分，并使用`torch.cuda.make_graphed_callables()`仅对可捕获的部分进行图形化处理。

默认情况下，由`make_graphed_callables()`返回的可调用对象是自动求导感知的，并且可以在训练循环中直接替换您传递的函数或`nn.Module`。

`make_graphed_callables()`在内部创建`CUDAGraph`对象，运行预热迭代，并根据需要维护静态输入和输出。因此（与`torch.cuda.graph`不同），您无需手动处理这些。

在以下示例中，数据相关的动态控制流意味着网络不能端到端捕获，但`make_graphed_callables()`让我们能够捕获和运行图安全的部分作为图形:

```py
N, D_in, H, D_out = 640, 4096, 2048, 1024

module1 = torch.nn.Linear(D_in, H).cuda()
module2 = torch.nn.Linear(H, D_out).cuda()
module3 = torch.nn.Linear(H, D_out).cuda()

loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.SGD(chain(module1.parameters(),
                                  module2.parameters(),
                                  module3.parameters()),
                            lr=0.1)

# Sample inputs used for capture
# requires_grad state of sample inputs must match
# requires_grad state of real inputs each callable will see.
x = torch.randn(N, D_in, device='cuda')
h = torch.randn(N, H, device='cuda', requires_grad=True)

module1 = torch.cuda.make_graphed_callables(module1, (x,))
module2 = torch.cuda.make_graphed_callables(module2, (h,))
module3 = torch.cuda.make_graphed_callables(module3, (h,))

real_inputs = [torch.rand_like(x) for _ in range(10)]
real_targets = [torch.randn(N, D_out, device="cuda") for _ in range(10)]

for data, target in zip(real_inputs, real_targets):
    optimizer.zero_grad(set_to_none=True)

    tmp = module1(data)  # forward ops run as a graph

    if tmp.sum().item() > 0:
        tmp = module2(tmp)  # forward ops run as a graph
    else:
        tmp = module3(tmp)  # forward ops run as a graph

    loss = loss_fn(tmp, target)
    # module2's or module3's (whichever was chosen) backward ops,
    # as well as module1's backward ops, run as graphs
    loss.backward()
    optimizer.step() 
```  ### 与 torch.cuda.amp 一起使用[](#usage-with-torch-cuda-amp "跳转到此标题")

对于典型的优化器，`GradScaler.step`会将 CPU 与 GPU 同步，这在捕获过程中是被禁止的。为了避免错误，要么使用部分网络捕获，要么（如果前向、损失和反向是捕获安全的）捕获前向、损失和反向，但不捕获优化器步骤:

```py
# warmup
# In a real setting, use a few batches of real data.
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    for i in range(3):
        optimizer.zero_grad(set_to_none=True)
        with torch.cuda.amp.autocast():
            y_pred = model(static_input)
            loss = loss_fn(y_pred, static_target)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
torch.cuda.current_stream().wait_stream(s)

# capture
g = torch.cuda.CUDAGraph()
optimizer.zero_grad(set_to_none=True)
with torch.cuda.graph(g):
    with torch.cuda.amp.autocast():
        static_y_pred = model(static_input)
        static_loss = loss_fn(static_y_pred, static_target)
    scaler.scale(static_loss).backward()
    # don't capture scaler.step(optimizer) or scaler.update()

real_inputs = [torch.rand_like(static_input) for _ in range(10)]
real_targets = [torch.rand_like(static_target) for _ in range(10)]

for data, target in zip(real_inputs, real_targets):
    static_input.copy_(data)
    static_target.copy_(target)
    g.replay()
    # Runs scaler.step and scaler.update eagerly
    scaler.step(optimizer)
    scaler.update() 
```  ### 与多个流一起使用[](#usage-with-multiple-streams "跳转到此标题")

捕获模式会自动传播到与捕获流同步的任何流。在捕获过程中，您可以通过向不同流发出调用来暴露并行性，但整体流依赖 DAG 必须从初始捕获流开始分支，并在捕获开始后重新加入初始流，然后在捕获结束前重新加入初始流:

```py
with torch.cuda.graph(g):
    # at context manager entrance, torch.cuda.current_stream()
    # is the initial capturing stream

    # INCORRECT (does not branch out from or rejoin initial stream)
    with torch.cuda.stream(s):
        cuda_work()

    # CORRECT:
    # branches out from initial stream
    s.wait_stream(torch.cuda.current_stream())
    with torch.cuda.stream(s):
        cuda_work()
    # rejoins initial stream before capture ends
    torch.cuda.current_stream().wait_stream(s) 
```

注意

为了避免对在 nsight 系统或 nvprof 中查看重播的高级用户造成困惑: 与急切执行不同，图形在捕获中将非平凡的流 DAG 解释为提示，而不是命令。在重播过程中，图形可能会将独立操作重新组织到不同的流中，或以不同的顺序排队（同时尊重您原始 DAG 的整体依赖关系）。

### 使用 DistributedDataParallel[](#usage-with-distributeddataparallel "跳转到此标题")

#### NCCL < 2.9.6

早于 2.9.6 的 NCCL 版本不允许捕获集合。您必须使用部分网络捕获，将所有 reduce 推迟到反向的图形化部分之外。

在使用 DDP 包装网络之前，在可图形化的网络部分上调用`make_graphed_callables()`。

#### NCCL >= 2.9.6

NCCL 版本 2.9.6 或更高版本允许图中的集合。捕获整个反向传播的方法是一个可行的选择，但需要三个设置步骤。

1.  禁用 DDP 的内部异步错误处理:

    ```py
    os.environ["NCCL_ASYNC_ERROR_HANDLING"] = "0"
    torch.distributed.init_process_group(...) 
    ```

1.  在完全反向捕获之前，DDP 必须在侧流上下文中构建:

    ```py
    with torch.cuda.stream(s):
        model = DistributedDataParallel(model) 
    ```

1.  在捕获之前，您的预热必须至少运行 11 次启用 DDP 的急切迭代。

### 图内存管理[](#graph-memory-management "跳转到此标题")

捕获的图形在每次重播时都会作用于相同的虚拟地址。如果 PyTorch 释放内存，后续的重播可能会导致非法内存访问。如果 PyTorch 将内存重新分配给新张量，重播可能会破坏这些张量看到的值。因此，图形使用的虚拟地址必须在重播过程中保留给图形。PyTorch 缓存分配器通过检测捕获正在进行并从图形私有内存池中满足捕获的分配来实现这一点。私有池会一直保持活动，直到其`CUDAGraph`对象和捕获期间创建的所有张量超出范围。

私有池会自动维护。默认情况下，分配器为每个捕获创建一个单独的私有池。如果捕获多个图形，这种保守的方法确保图形重播永远不会破坏彼此的值，但有时会不必要地浪费内存。

#### 跨捕获共享内存[](#sharing-memory-across-captures "跳转到此标题")

为了节省存储在私有池中的内存，`torch.cuda.graph` 和 `torch.cuda.make_graphed_callables()` 可选地允许不同的捕获共享同一个私有池。如果你知道一组图形将始终按照它们被捕获的顺序重播，并且永远不会同时重播，那么共享一个私有池是安全的。

`torch.cuda.graph` 的 `pool` 参数是使用特定私有池的提示，并且可以用于跨图形共享内存，如下所示：

```py
g1 = torch.cuda.CUDAGraph()
g2 = torch.cuda.CUDAGraph()

# (create static inputs for g1 and g2, run warmups of their workloads...)

# Captures g1
with torch.cuda.graph(g1):
    static_out_1 = g1_workload(static_in_1)

# Captures g2, hinting that g2 may share a memory pool with g1
with torch.cuda.graph(g2, pool=g1.pool()):
    static_out_2 = g2_workload(static_in_2)

static_in_1.copy_(real_data_1)
static_in_2.copy_(real_data_2)
g1.replay()
g2.replay() 
```

使用 `torch.cuda.make_graphed_callables()`，如果你想要为多个可调用对象创建图形，并且知道它们将始终按照相同顺序运行（并且永远不会同时运行），请将它们作为元组传递，按照实际工作负载中将要运行的顺序，`make_graphed_callables()` 将使用共享的私有池捕获它们的图形。

如果在实际工作负载中，你的可调用对象将按照偶尔变化的顺序运行，或者将同时运行，那么将它们作为元组传递给单个 `make_graphed_callables()` 调用是不允许的。相反，你必须为每个可调用对象单独调用 `make_graphed_callables()`。
