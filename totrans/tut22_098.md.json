["```py\n# imports\nimport os\nfrom io import open\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F \n```", "```py\nclass LSTMModel([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n  \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super([LSTMModel](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.drop = [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(dropout)\n        self.encoder = [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding \"torch.nn.Embedding\")(ntoken, ninp)\n        self.rnn = [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM \"torch.nn.LSTM\")(ninp, nhid, nlayers, dropout=dropout)\n        self.decoder = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(nhid, ntoken)\n\n        self.init_weights()\n\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        [output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), hidden = self.rnn(emb, hidden)\n        [output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = self.drop([output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n        decoded = self.decoder([output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n        return decoded, hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters())\n        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n                weight.new_zeros(self.nlayers, bsz, self.nhid)) \n```", "```py\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n\n    def tokenize(self, path):\n  \"\"\"Tokenizes a text file.\"\"\"\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, 'r', encoding=\"utf8\") as f:\n            for line in f:\n                words = line.split() + ['<eos>']\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, 'r', encoding=\"utf8\") as f:\n            idss = []\n            for line in f:\n                words = line.split() + ['<eos>']\n                ids = []\n                for word in words:\n                    ids.append(self.dictionary.word2idx[word])\n                idss.append([torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")(ids).type([torch.int64](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")))\n            ids = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")(idss)\n\n        return ids\n\nmodel_data_filepath = 'data/'\n\ncorpus = Corpus(model_data_filepath + 'wikitext-2') \n```", "```py\nntokens = len(corpus.dictionary)\n\nmodel = [LSTMModel](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(\n    ntoken = ntokens,\n    ninp = 512,\n    nhid = 256,\n    nlayers = 5,\n)\n\n[model.load_state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict \"torch.nn.Module.load_state_dict\")(\n    [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html#torch.load \"torch.load\")(\n        model_data_filepath + 'word_language_model_quantize.pth',\n        map_location=[torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")('cpu')\n        )\n    )\n\n[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")()\nprint(model) \n```", "```py\nLSTMModel(\n  (drop): Dropout(p=0.5, inplace=False)\n  (encoder): Embedding(33278, 512)\n  (rnn): LSTM(512, 256, num_layers=5, dropout=0.5)\n  (decoder): Linear(in_features=256, out_features=33278, bias=True)\n) \n```", "```py\n[input_](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randint](https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint \"torch.randint\")(ntokens, (1, 1), dtype=[torch.long](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\nhidden = model.init_hidden(1)\ntemperature = 1.0\nnum_words = 1000\n\nwith open(model_data_filepath + 'out.txt', 'w') as outf:\n    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():  # no tracking history\n        for i in range(num_words):\n            [output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), hidden = model([input_](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), hidden)\n            [word_weights](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").squeeze().div(temperature).exp().cpu()\n            [word_idx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.multinomial](https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial \"torch.multinomial\")([word_weights](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), 1)[0]\n            [input_](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").fill_([word_idx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\n            word = corpus.dictionary.idx2word[[word_idx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")]\n\n            outf.write(str(word.encode('utf-8')) + ('\\n' if i % 20 == 19 else ' '))\n\n            if i % 100 == 0:\n                print('| Generated {}/{} words'.format(i, 1000))\n\nwith open(model_data_filepath + 'out.txt', 'r') as outf:\n    all_output = outf.read()\n    print(all_output) \n```", "```py\n| Generated 0/1000 words\n| Generated 100/1000 words\n| Generated 200/1000 words\n| Generated 300/1000 words\n| Generated 400/1000 words\n| Generated 500/1000 words\n| Generated 600/1000 words\n| Generated 700/1000 words\n| Generated 800/1000 words\n| Generated 900/1000 words\nb'.' b'Ross' b\"'\" b'final' b'focus' b'respects' b'with' b'rice' b'Rajeev' b'implements' b'.' b'<unk>' b'Darwin' b',' b'a' b'comfortably' b',' b'called' b'that' b'it'\nb'is' b'\"' b'significant' b'alive' b'\"' b'from' b'perform' b'@-@' b'hearted' b',' b'can' b'be' b'among' b'what' b'he' b'is' b'a' b'Sixth' b'minister' b'as'\nb'a' b'analysis' b',' b'bathtub' b'for' b'1798' b'and' b'an' b'Nourrit' b'who' b'left' b'the' b'same' b'name' b',' b'which' b'they' b'saw' b'to' b'\"'\nb'let' b'most' b'or' b'me' b'of' b'its' b'all' b'time' b'that' b'might' b'have' b'done' b'on' b'back' b'on' b'their' b'character' b'position' b'.' b'\"'\nb'<eos>' b'The' b'2010' b'Peach' b'Bird' b\"'\" b'Union' b'(' b'1888' b')' b',' b'which' b'could' b'be' b'actively' b'composed' b'in' b'London' b'and' b'in'\nb'1609' b'.' b'The' b'work' b'have' b'October' b',' b'but' b',' b'since' b'the' b'parish' b'of' b'times' b'is' b'hard' b'and' b'severely' b'ignored' b'the'\nb'plums' b',' b'they' b'<unk>' b'or' b'Giuseppe' b'Leo' b'Rodman' b'for' b'the' b'game' b'<unk>' b',' b'and' b'were' b'released' b'and' b'because' b'it' b'apparently'\nb'spent' b'before' b'with' b'those' b'arena' b'to' b'deciding' b'.' b'\"' b'strumming' b'on' b'You' b'then' b'heard' b'enough' b'that' b'we' b'have' b'rhythm' b'channels'\nb'in' b'a' b'video' b'off' b'his' b'complete' b'novel' b'\"' b'.' b'The' b'population' b'of' b'Ceres' b'will' b'be' b'negative' b'for' b'strictly' b'@-@' b'hawk'\nb'to' b'come' b'into' b'Year' b'1' b'.' b'There' b'is' b'a' b'pair' b'of' b'using' b'526' b',' b'O2' b',' b'nose' b',' b'<unk>' b'and'\nb'coalitions' b'with' b'promyelocytic' b'officials' b'were' b'somewhat' b'developing' b'.' b'The' b'work' b'would' b'be' b'tested' b'as' b'a' b'hunt' b'to' b'Castle' b'network' b'including'\nb'possible' b'gear' b'.' b'<eos>' b'<eos>' b'=' b'=' b'Behavior' b'=' b'=' b'<eos>' b'<eos>' b'<unk>' b'Michael' b'David' b'J.' b'M.' b'hilarious' b'(' b'died'\nb'port' b'6' b':' b'12' b'<eos>' b'Ffordd' b'admirable' b'reality' b')' b'<eos>' b'trade' b'classifications' b',' b'without' b'a' b'creator' b';' b'of' b'even' b'@-@'\nb'narial' b'earth' b',' b'building' b'rare' b'sounds' b',' b'Ridgway' b'contents' b',' b'any' b'GAA' b'in' b'air' b',' b'bleeding' b'.' b'<eos>' b'John' b'Leonard'\nb'Rick' b'Smith' b'(' b'Evangeline' b'J.' b'Male' b')' b',' b'who' b'are' b'also' b'known' b'to' b'be' b'generally' b'portrayed' b'as' b'director' b'of' b'the'\nb'Roman' b'origin' b'of' b'Sport' b'@-@' b'class' b'consent' b',' b'a' b'new' b'example' b'of' b'high' b'non' b'@-@' b'Crusader' b'forces' b'could' b'be' b'found'\nb'by' b'<unk>' b'the' b'death' b'of' b'fish' b'highways' b'.' b'<eos>' b'<eos>' b'=' b'=' b'Background' b'=' b'=' b'<eos>' b'<eos>' b'The' b'majority' b'of'\nb'year' b',' b'Superman' b',' b'was' b'also' b'built' b'into' b'alphabet' b'.' b'The' b'NW' b'were' b'written' b'by' b'other' b'astronomers' b'such' b'as' b'<unk>'\nb'Jermaine' b'Farr' b',' b'with' b'respond' b'to' b'power' b'(' b'reorganize' b')' b'.' b'These' b'birds' b'have' b'had' b'hosted' b'North' b'AIDS' b'since' b'vocalization'\nb'.' b'It' b'depicting' b'an' b'Normal' b'female' b'extended' b'after' b',' b'leaving' b'Petrie' b'resembled' b'Taylor' b'issues' b'has' b'significant' b'governmental' b'features' b',' b'called'\nb'it' b',' b'\"' b'Parts' b'as' b'well' b'to' b'kill' b'us' b'from' b'Haifa' b'is' b'an' b'gift' b'off' b'them' b'.' b'\"' b'In' b'a'\nb'review' b'that' b'Downs' b',' b'\"' b'Every' b'blames' b'recent' b'human' b'parallels' b'you' b'is' b'Zeller' b'envisioned' b',' b'you' b'The' b'last' b'an' b'middle'\nb'adult' b'person' b'in' b'ratio' b'of' b'male' b'throwing' b'lists' b'daily' b'letters' b'even' b',' b'attack' b',' b'and' b'inflict' b'you' b'into' b'Lost' b','\nb'but' b'you' b'Rock' b'have' b'access' b'to' b'the' b'Mendip' b'conception' b'who' b\"'re\" b'overthrow' b'what' b'everything' b'in' b'than' b'store' b'particles' b'.' b'\"'\nb'The' b'face' b'recognized' b'Innis' b'was' b'of' b'unrepentant' b'Ulaid' b'.' b'glider' b'rent' b'for' b'Sister' b'Weber' b'are' b'exposing' b'to' b'seek' b'during' b'the'\nb'hear' b'film' b'dislike' b\"'s\" b'staged' b'alignment' b'.' b'Another' b'cloth' b'was' b'only' b'impressed' b'by' b'Lab' b',' b'they' b'also' b'occasionally' b'learnt' b'a'\nb'listener' b'.' b'<eos>' b'As' b'Plunkett' b\"'s\" b'death' b',' b'many' b'images' b'entrusted' b'to' b'join' b'items' b'display' b'models' b'than' b'foot' b'in' b'British'\nb'countries' b'.' b'<unk>' b'indicated' b'is' b'also' b'safe' b'to' b'decide' b'down' b'McFarland' b',' b'even' b'that' b'searching' b'approaches' b'a' b'winds' b'for' b'two'\nb'years' b'of' b'established' b'.' b'It' b'is' b'safe' b'that' b'<unk>' b'responded' b'in' b'(' b'the' b'19th' b'century' b',' b'including' b'A.' b\"'\\xc3\\xa9tat\" b';'\nb'it' b'will' b'be' b'in' b'their' b'longer' b',' b'propel' b'\"' b'<unk>' b'\"' b',' b'which' b'aiding' b'God' b'@-@' b'black' b'overly' b',' b'astronomical'\nb',' b'business' b',' b'<unk>' b',' b'<unk>' b',' b'or' b'grey' b'timeline' b'by' b'dismissal' b'before' b'mutualistic' b',' b'and' b'substrate' b'attention' b'given' b'as'\nb'a' b'certain' b'species' b'of' b'153' b'stages' b'.' b'<unk>' b'in' b'toilet' b'can' b'be' b'found' b'to' b'signs' b'of' b'450' b',' b'compared' b'to'\nb'50' b'%' b'closer' b',' b'while' b'manuscripts' b'may' b'be' b'\"' b'distinguished' b'it' b'\"' b'.' b'Incubation' b'resemble' b'Jordan' b'a' b'extremes' b',' b'Illinois'\nb'concluding' b'much' b'of' b'the' b'player' b\"'s\" b'earlier' b'the' b'<unk>' b'broods' b'policies' b'.' b'<eos>' b'As' b'a' b'year' b',' b'he' b'is' b'found'\nb'to' b'scare' b'taking' b'place' b'upon' b'behind' b'other' b'device' b',' b'including' b'its' b'further' b'sequence' b',' b'which' b'saw' b'him' b'a' b'painting' b'of'\nb'conspiracy' b'that' b'enters' b'<unk>' b'to' b'cook' b'.' b'By' b'this' b'attacks' b',' b'they' b'are' b'shown' b'that' b'<unk>' b'(' b'an' b'one' b'@-@'\nb'year' b')' b',' b'\"' b'vision' b'(' b'still' b'most' b'equivalent' b'mourning' b')' b',' b'a' b'high' b'man' b'or' b'sings' b'large' b'Bruins' b'and'\nb'rifles' b'all' b'by' b'night' b'<unk>' b',' b'not' b'nursing' b'.' b'\"' b'Some' b'authors' b'like' b'H.' b'<unk>' b'<unk>' b'is' b'a' b'pure' b'character'\nb'.' b'The' b'Admiralty' b'covers' b'Bob' b'cottonwood' b',' b'a' b'reflection' b'that' b'God' b'heard' b'parallel' b'.' b'reporters' b'went' b'forward' b'with' b'his' b'unusually'\nb'controversial' b'Fern\\xc3\\xa1ndez' b',' b'back' b'\"' b'that' b'many' b'authors' b\"'re\" b'forbidden' b'between' b'Black' b'Island' b'worker' b'!' b\"'\" b'learns' b'\"' b'(' b'2006'\nb')' b',' b'whose' b'<unk>' b'will' b'be' b'seen' b'as' b'a' b'child' b'.' b'Scully' b'is' b'trouble' b'apart' b'in' b'the' b'nominally' b',' b'and'\nb'only' b'they' b'can' b'not' b'specifically' b'specify' b'after' b'they' b'could' b'be' b'rapidly' b'known' b'.' b'However' b',' b'it' b'may' b'assassinate' b'double' b'in'\nb'other' b'ways' b',' b'even' b'because' b'he' b'provide' b'11' b'shock' b',' b'<unk>' b'the' b'Canary' b'Sun' b'breaker' b'.' b'<unk>' b'even' b'<unk>' b'by'\nb'a' b'variety' b'of' b'other' b'factors' b',' b'which' b'Canterbury' b'doesn' b\"'t\" b'be' b'named' b'as' b'they' b'have' b'the' b'127th' b'mention' b'.' b'flocks'\nb'fail' b'to' b'be' b'Allah' b',' b'depressed' b'peninsula' b',' b'<unk>' b',' b'and' b'@-@' b'head' b'ice' b'<unk>' b',' b'which' b'may' b'be' b'applied'\nb'to' b'both' b'New' b'Zealand' b'.' b'The' b'food' b'and' b'so' b'they' b'can' b'react' b'into' b'Blue' b'or' b'eye' b'itself' b'.' b'They' b'may'\nb'improve' b'their' b'position' b'complimented' b'up' b'or' b'place' b'resulted' b'on' b'all' b'Alfa' b'to' b'keep' b'care' b'of' b'Ceres' b',' b'orbiting' b'or' b'wide'\nb',' b'then' b'by' b'its' b'space' b'.' b'<unk>' b',' b'they' b'were' b'will' b'try' b'the' b'kakapo' b'of' b'unusual' b',' b'<unk>' b'<unk>' b'or'\nb'synthesize' b'Dead' b'(' b'860' b'<unk>' b'<unk>' b')' b'on' b'Activision' b'rather' b'@-@' b'thirds' b'of' b'spotlight' b'its' b'spectrum' b':' b'dying' b',' b'when'\nb'British' b'behaviour' b'was' b'a' b'calculate' b'compound' b'to' b'merge' b',' b'with' b'some' b'chicks' b'to' b'use' b'their' b'bestow' b'.' b'It' b'may' b'indicate' \n```", "```py\nbptt = 25\n[criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\") = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\neval_batch_size = 1\n\n# create test data set\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into ``bsz`` parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the ``bsz`` batches.\n    return data.view(bsz, -1).t().contiguous()\n\n[test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = batchify([corpus.test](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), eval_batch_size)\n\n# Evaluation functions\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target\n\ndef repackage_hidden(h):\n  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n\n  if isinstance(h, [torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n      return h.detach()\n  else:\n      return tuple(repackage_hidden(v) for v in h)\n\ndef evaluate(model_, data_source):\n    # Turn on evaluation mode which disables dropout.\n    model_.eval()\n    total_loss = 0.\n    hidden = model_.init_hidden(eval_batch_size)\n    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            [output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), hidden = model_(data, hidden)\n            hidden = repackage_hidden(hidden)\n            output_flat = [output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").view(-1, ntokens)\n            total_loss += len(data) * [criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")(output_flat, targets).item()\n    return total_loss / (len(data_source) - 1) \n```", "```py\nimport torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {[nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM \"torch.nn.LSTM\"), [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")}, dtype=[torch.qint8](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")\n)\nprint(quantized_model) \n```", "```py\nLSTMModel(\n  (drop): Dropout(p=0.5, inplace=False)\n  (encoder): Embedding(33278, 512)\n  (rnn): DynamicQuantizedLSTM(512, 256, num_layers=5, dropout=0.5)\n  (decoder): DynamicQuantizedLinear(in_features=256, out_features=33278, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n) \n```", "```py\ndef print_size_of_model(model):\n    [torch.save](https://pytorch.org/docs/stable/generated/torch.save.html#torch.save \"torch.save\")([model.state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict \"torch.nn.Module.state_dict\")(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model) \n```", "```py\nSize (MB): 113.944064\nSize (MB): 79.738484 \n```", "```py\n[torch.set_num_threads](https://pytorch.org/docs/stable/generated/torch.set_num_threads.html#torch.set_num_threads \"torch.set_num_threads\")(1)\n\ndef time_model_evaluation(model, [test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n    s = time.time()\n    loss = evaluate(model, [test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    elapsed = time.time() - s\n    print('''loss: {0:.3f}\\nelapsed time (seconds): {1:.1f}'''.format(loss, elapsed))\n\ntime_model_evaluation(model, [test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\ntime_model_evaluation(quantized_model, [test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) \n```", "```py\nloss: 5.167\nelapsed time (seconds): 205.3\nloss: 5.168\nelapsed time (seconds): 116.8 \n```"]