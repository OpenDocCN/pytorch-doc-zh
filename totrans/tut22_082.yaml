- en: Dynamic Parallelism in TorchScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/advanced/torch-script-parallelism.html](https://pytorch.org/tutorials/advanced/torch-script-parallelism.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this tutorial, we introduce the syntax for doing *dynamic inter-op parallelism*
    in TorchScript. This parallelism has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: dynamic - The number of parallel tasks created and their workload can depend
    on the control flow of the program.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: inter-op - The parallelism is concerned with running TorchScript program fragments
    in parallel. This is distinct from *intra-op parallelism*, which is concerned
    with splitting up individual operators and running subsets of the operator’s work
    in parallel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic Syntax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two important APIs for dynamic parallelism are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.jit.fork(fn : Callable[..., T], *args, **kwargs) -> torch.jit.Future[T]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.jit.wait(fut : torch.jit.Future[T]) -> T`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A good way to demonstrate how these work is by way of an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`fork()` takes the callable `fn` and arguments to that callable `args` and
    `kwargs` and creates an asynchronous task for the execution of `fn`. `fn` can
    be a function, method, or Module instance. `fork()` returns a reference to the
    value of the result of this execution, called a `Future`. Because `fork` returns
    immediately after creating the async task, `fn` may not have been executed by
    the time the line of code after the `fork()` call is executed. Thus, `wait()`
    is used to wait for the async task to complete and return the value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These constructs can be used to overlap the execution of statements within
    a function (shown in the worked example section) or be composed with other language
    constructs like loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When we initialized an empty list of Futures, we needed to add an explicit
    type annotation to `futures`. In TorchScript, empty containers default to assuming
    they contain Tensor values, so we annotate the list constructor # as being of
    type `List[torch.jit.Future[torch.Tensor]]`'
  prefs: []
  type: TYPE_NORMAL
- en: This example uses `fork()` to launch 100 instances of the function `foo`, waits
    on the 100 tasks to complete, then sums the results, returning `-100.0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied Example: Ensemble of Bidirectional LSTMs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s try to apply parallelism to a more realistic example and see what sort
    of performance we can get out of it. First, let’s define the baseline model: an
    ensemble of bidirectional LSTM layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: On my machine, this network runs in `2.05` seconds. We can do a lot better!
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing Forward and Backward Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A very simple thing we can do is parallelize the forward and backward layers
    within `BidirectionalRecurrentLSTM`. For this, the structure of the computation
    is static, so we don’t actually even need any loops. Let’s rewrite the `forward`
    method of `BidirectionalRecurrentLSTM` like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `forward()` delegates execution of `cell_f` to another thread,
    while it continues to execute `cell_b`. This causes the execution of both the
    cells to be overlapped with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Running the script again with this simple modification yields a runtime of `1.71`
    seconds for an improvement of `17%`!
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside: Visualizing Parallelism'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re not done optimizing our model but it’s worth introducing the tooling we
    have for visualizing performance. One important tool is the [PyTorch profiler](https://pytorch.org/docs/stable/autograd.html#profiler).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the profiler along with the Chrome trace export functionality to
    visualize the performance of our parallelized model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This snippet of code will write out a file named `parallel.json`. If you navigate
    Google Chrome to `chrome://tracing`, click the `Load` button, and load in that
    JSON file, you should see a timeline like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://i.imgur.com/rm5hdG9.png](../Images/6b495cb0cd4336a2469d9f07696faa3e.png)'
  prefs: []
  type: TYPE_IMG
- en: The horizontal axis of the timeline represents time and the vertical axis represents
    threads of execution. As we can see, we are running two `lstm` instances at a
    time. This is the result of our hard work parallelizing the bidirectional layers!
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing Models in the Ensemble
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may have noticed that there is a further parallelization opportunity in
    our code: we can also run the models contained in `LSTMEnsemble` in parallel with
    each other. The way to do that is simple enough, this is how we should change
    the `forward` method of `LSTMEnsemble`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, if you value brevity, we can use list comprehensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Like described in the intro, we’ve used loops to fork off tasks for each of
    the models in our ensemble. We’ve then used another loop to wait for all of the
    tasks to be completed. This provides even more overlap of computation.
  prefs: []
  type: TYPE_NORMAL
- en: With this small update, the script runs in `1.4` seconds, for a total speedup
    of `32%`! Pretty good for two lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use the Chrome tracer again to see where’s going on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://i.imgur.com/kA0gyQm.png](../Images/ac8752539498c11001a65c1ff470d696.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now see that all `LSTM` instances are being run fully in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we learned about `fork()` and `wait()`, the basic APIs for
    doing dynamic, inter-op parallelism in TorchScript. We saw a few typical usage
    patterns for using these functions to parallelize the execution of functions,
    methods, or `Modules` in TorchScript code. Finally, we worked through an example
    of optimizing a model using this technique and explored the performance measurement
    and visualization tooling available in PyTorch.
  prefs: []
  type: TYPE_NORMAL
