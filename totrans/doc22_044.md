# torch.backends

> 原文：[https://pytorch.org/docs/stable/backends.html](https://pytorch.org/docs/stable/backends.html)

torch.backends控制PyTorch支持的各种后端的行为。

这些后端包括：

+   `torch.backends.cpu`

+   `torch.backends.cuda`

+   `torch.backends.cudnn`

+   `torch.backends.mps`

+   `torch.backends.mkl`

+   `torch.backends.mkldnn`

+   `torch.backends.openmp`

+   `torch.backends.opt_einsum`

+   `torch.backends.xeon`

## torch.backends.cpu

```py
torch.backends.cpu.get_cpu_capability()¶
```

返回CPU能力作为字符串值。

可能的值：- “DEFAULT” - “VSX” - “Z VECTOR” - “NO AVX” - “AVX2” - “AVX512”

返回类型

[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)")  ## torch.backends.cuda

```py
torch.backends.cuda.is_built()¶
```

返回PyTorch是否构建有CUDA支持。

请注意，这并不一定意味着CUDA可用；只是如果在具有工作CUDA驱动程序和设备的机器上运行此PyTorch二进制文件，我们将能够使用它。

```py
torch.backends.cuda.matmul.allow_tf32¶
```

一个控制在安培或更新的GPU上是否可以使用TensorFloat-32张量核心进行矩阵乘法的布尔值。请参阅[Ampere（以及更高版本）设备上的TensorFloat-32（TF32）](notes/cuda.html#tf32-on-ampere)。

```py
torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction¶
```

一个控制是否允许使用减少精度的规约（例如，使用fp16累积类型）与fp16 GEMM一起使用的布尔值。

```py
torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction¶
```

一个控制是否允许使用bf16 GEMM的减少精度规约的布尔值。

```py
torch.backends.cuda.cufft_plan_cache¶
```

`cufft_plan_cache`包含每个CUDA设备的cuFFT计划缓存。通过torch.backends.cuda.cufft_plan_cache[i]查询特定设备i的缓存。

```py
torch.backends.cuda.cufft_plan_cache.size¶
```

一个只读[`int`](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)")，显示cuFFT计划缓存中当前计划的数量。

```py
torch.backends.cuda.cufft_plan_cache.max_size¶
```

一个[`int`](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)")，控制cuFFT计划缓存的容量。

```py
torch.backends.cuda.cufft_plan_cache.clear()¶
```

清除cuFFT计划缓存。

```py
torch.backends.cuda.preferred_linalg_library(backend=None)¶
```

覆盖PyTorch用于在CUDA线性代数操作中选择cuSOLVER和MAGMA之间的启发式。

警告

此标志是实验性的，可能会更改。

当PyTorch运行CUDA线性代数操作时，通常会使用cuSOLVER或MAGMA库，如果两者都可用，则会根据启发式决定使用哪个。此标志（一个[`str`](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)")）允许覆盖这些启发式。

+   如果设置为“cusolver”，则将尽可能使用cuSOLVER。

+   如果设置为“magma”，则将尽可能使用MAGMA。

+   如果设置为“default”（默认），则将使用启发式来在cuSOLVER和MAGMA之间进行选择（如果两者都可用）。

+   当没有输入时，此函数返回当前首选库。

+   用户可以使用环境变量TORCH_LINALG_PREFER_CUSOLVER=1全局设置首选库为cuSOLVER。此标志仅设置首选库的初始值，首选库仍可能在脚本中的后续函数调用中被覆盖。

注意：当首选库为其他库时，如果首选库未实现所调用的操作，则仍然可以使用其他库。如果PyTorch的启发式库选择对您应用程序的输入不正确，则此标志可能会实现更好的性能。

当前支持的linalg运算符：

+   [`torch.linalg.inv()`](generated/torch.linalg.inv.html#torch.linalg.inv "torch.linalg.inv")

+   [`torch.linalg.inv_ex()`](generated/torch.linalg.inv_ex.html#torch.linalg.inv_ex "torch.linalg.inv_ex")

+   [`torch.linalg.cholesky()`](generated/torch.linalg.cholesky.html#torch.linalg.cholesky "torch.linalg.cholesky")

+   [`torch.linalg.cholesky_ex()`](generated/torch.linalg.cholesky_ex.html#torch.linalg.cholesky_ex "torch.linalg.cholesky_ex")

+   [`torch.cholesky_solve()`](generated/torch.cholesky_solve.html#torch.cholesky_solve "torch.cholesky_solve")

+   [`torch.cholesky_inverse()`](generated/torch.cholesky_inverse.html#torch.cholesky_inverse "torch.cholesky_inverse")

+   [`torch.linalg.lu_factor()`](generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor "torch.linalg.lu_factor")

+   [`torch.linalg.lu()`](generated/torch.linalg.lu.html#torch.linalg.lu "torch.linalg.lu")

+   [`torch.linalg.lu_solve()`](generated/torch.linalg.lu_solve.html#torch.linalg.lu_solve "torch.linalg.lu_solve")

+   [`torch.linalg.qr()`](generated/torch.linalg.qr.html#torch.linalg.qr "torch.linalg.qr")

+   [`torch.linalg.eigh()`](generated/torch.linalg.eigh.html#torch.linalg.eigh "torch.linalg.eigh")

+   `torch.linalg.eighvals()`

+   [`torch.linalg.svd()`](generated/torch.linalg.svd.html#torch.linalg.svd "torch.linalg.svd")

+   [`torch.linalg.svdvals()`](generated/torch.linalg.svdvals.html#torch.linalg.svdvals "torch.linalg.svdvals")

返回类型

*_LinalgBackend*

```py
torch.backends.cuda.SDPBackend¶
```

别名为 `_SDPBackend`

```py
torch.backends.cuda.SDPAParams¶
```

别名为 `_SDPAParams`

```py
torch.backends.cuda.flash_sdp_enabled()¶
```

警告

此标志为测试版，可能会更改。

返回 flash 缩放点积注意力是否已启用。

```py
torch.backends.cuda.enable_mem_efficient_sdp(enabled)¶
```

警告

此标志为测试版，可能会更改。

启用或禁用内存高效的缩放点积注意力。

```py
torch.backends.cuda.mem_efficient_sdp_enabled()¶
```

警告

此标志为测试版，可能会更改。

返回内存高效的缩放点积注意力是否已启用。

```py
torch.backends.cuda.enable_flash_sdp(enabled)¶
```

警告

此标志为测试版，可能会更改。

启用或禁用 flash 缩放点积注意力。

```py
torch.backends.cuda.math_sdp_enabled()¶
```

警告

此标志为测试版，可能会更改。

返回 math 缩放点积注意力是否已启用。

```py
torch.backends.cuda.enable_math_sdp(enabled)¶
```

警告

此标志为测试版，可能会更改。

启用或禁用 math 缩放点积注意力。

```py
torch.backends.cuda.can_use_flash_attention(params, debug=False)¶
```

检查是否可以在 scaled_dot_product_attention 中使用 FlashAttention。

参数

+   **params** (*_SDPAParams*) – 包含查询、键、值张量、可选注意力掩码、丢弃率以及指示注意力是否因果的标志的 SDPAParams 实例。

+   **debug** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(在 Python v3.12 中)")) – 是否记录警告调试信息，说明为什么无法运行 FlashAttention。默认为False。

返回

如果可以使用给定参数，则为True；否则为False。

返回类型

[bool](https://docs.python.org/3/library/functions.html#bool "(在 Python v3.12 中)")

注意

此函数依赖于 PyTorch 的 CUDA 版本。在非CUDA环境中将返回False。

```py
torch.backends.cuda.can_use_efficient_attention(params, debug=False)¶
```

检查是否可以在 scaled_dot_product_attention 中使用 efficient_attention。

参数

+   **params** (*_SDPAParams*) – 包含查询、键、值张量、可选注意力掩码、丢弃率以及指示注意力是否因果的标志的 SDPAParams 实例。

+   **debug** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(在 Python v3.12 中)")) – 是否记录警告信息，说明为什么无法运行 efficient_attention。默认为False。

返回

如果可以使用给定参数，则为True；否则为False。

返回类型

[bool](https://docs.python.org/3/library/functions.html#bool "(在 Python v3.12 中)")

注意

此函数依赖于 PyTorch 的 CUDA 版本。在非CUDA环境中将返回False。

```py
torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=True)¶
```

警告

此标志为测试版，可能会更改。

此上下文管理器可用于临时启用或禁用缩放点积注意力的三个后端之一。退出上下文管理器时，将恢复标志的先前状态。## torch.backends.cudnn[](#module-torch.backends.cudnn "Permalink to this heading")

```py
torch.backends.cudnn.version()¶
```

返回 cuDNN 的版本。

```py
torch.backends.cudnn.is_available()¶
```

返回一个布尔值，指示当前是否可用 CUDNN。

```py
torch.backends.cudnn.enabled¶
```

一个控制 cuDNN 是否启用的 [`bool`](https://docs.python.org/3/library/functions.html#bool "(在 Python v3.12 中)")。

```py
torch.backends.cudnn.allow_tf32¶
```

一个[`bool`](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")，控制在Ampere或更新的GPU上cuDNN卷积中是否可以使用TensorFloat-32张量核心。请参阅[Ampere（以及更高版本）设备上的TensorFloat-32（TF32）](notes/cuda.html#tf32-on-ampere)。

```py
torch.backends.cudnn.deterministic¶
```

一个[`bool`](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")，如果为True，则导致cuDNN仅使用确定性卷积算法。另请参阅[`torch.are_deterministic_algorithms_enabled()`](generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled "torch.are_deterministic_algorithms_enabled")和[`torch.use_deterministic_algorithms()`](generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms "torch.use_deterministic_algorithms")。

```py
torch.backends.cudnn.benchmark¶
```

一个[`bool`](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")，如果为True，则导致cuDNN对多个卷积算法进行基准测试并选择最快的。

```py
torch.backends.cudnn.benchmark_limit¶
```

一个[`int`](https://docs.python.org/3/library/functions.html#int "(在Python v3.12中)")，指定torch.backends.cudnn.benchmark为True时尝试的cuDNN卷积算法的最大数量。将benchmark_limit设置为零以尝试每个可用算法。请注意，此设置仅影响通过cuDNN v8 API分派的卷积。  ## torch.backends.mps[](#module-torch.backends.mps "跳转到此标题的永久链接")

```py
torch.backends.mps.is_available()¶
```

返回一个指示当前是否可用MPS的布尔值。

返回类型

[bool](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")

```py
torch.backends.mps.is_built()¶
```

返回PyTorch是否构建有MPS支持。

请注意，这并不一定意味着MPS可用；只是如果在具有工作MPS驱动程序和设备的机器上运行此PyTorch二进制文件，我们将能够使用它。

返回类型

[bool](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")  ## torch.backends.mkl

```py
torch.backends.mkl.is_available()¶
```

返回PyTorch是否构建有MKL支持。

```py
class torch.backends.mkl.verbose(enable)¶
```

按需oneMKL详细功能。

为了更容易调试性能问题，oneMKL可以转储包含执行信息（如持续时间）的详细消息，同时执行内核。可以通过名为MKL_VERBOSE的环境变量调用详细功能。但是，这种方法在所有步骤中转储消息。这些是大量详细消息。此外，通常仅对单个迭代获取详细消息就足够用于调查性能问题。这种按需详细功能使得可以控制详细消息转储的范围。在以下示例中，仅为第二个推理转储详细消息。

```py
import torch
model(data)
with torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):
    model(data) 
```

参数

**level** – 详细级别 - `VERBOSE_OFF`：禁用详细 - `VERBOSE_ON`：启用详细  ## torch.backends.mkldnn[](#module-torch.backends.mkldnn "跳转到此标题的永久链接")

```py
torch.backends.mkldnn.is_available()¶
```

返回PyTorch是否构建有MKL-DNN支持。

```py
class torch.backends.mkldnn.verbose(level)¶
```

按需oneDNN（前MKL-DNN）详细功能。

为了更容易调试性能问题，oneDNN可以转储包含内核大小、输入数据大小和执行持续时间等信息的详细消息，同时执行内核。可以通过名为DNNL_VERBOSE的环境变量调用详细功能。但是，这种方法在所有步骤中转储消息。这些是大量详细消息。此外，通常仅对单个迭代获取详细消息就足够用于调查性能问题。这种按需详细功能使得可以控制详细消息转储的范围。在以下示例中，仅为第二个推理转储详细消息。

```py
import torch
model(data)
with torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):
    model(data) 
```

参数

**level** – 详细级别 - `VERBOSE_OFF`：禁用详细 - `VERBOSE_ON`：启用详细 - `VERBOSE_ON_CREATION`：启用详细，包括oneDNN内核创建  ## torch.backends.openmp[](#module-torch.backends.openmp "跳转到此标题的永久链接")

```py
torch.backends.openmp.is_available()¶
```

返回PyTorch是否构建有OpenMP支持。  ## torch.backends.opt_einsum[](#module-torch.backends.opt_einsum "跳转到此标题")

```py
torch.backends.opt_einsum.is_available()¶
```

返回一个指示opt_einsum当前是否可用的bool值。

返回类型

[bool](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")

```py
torch.backends.opt_einsum.get_opt_einsum()¶
```

如果当前可用，则返回opt_einsum包，否则返回None。

返回类型

[*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(在Python v3.12中)")

```py
torch.backends.opt_einsum.enabled¶
```

一个控制是否启用opt_einsum的`bool`（默认为`True`）。如果启用，torch.einsum将使用opt_einsum（[https://optimized-einsum.readthedocs.io/en/stable/path_finding.html](https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)）来计算更快性能的最佳收缩路径。

如果opt_einsum不可用，torch.einsum将退回到默认的从左到右的收缩路径。

```py
torch.backends.opt_einsum.strategy¶
```

一个指定当`torch.backends.opt_einsum.enabled`为`True`时要尝试哪些策略的`str`。默认情况下，torch.einsum将尝试“auto”策略，但也支持“greedy”和“optimal”策略。请注意，“optimal”策略在尝试所有可能路径时与输入数量的阶乘成正比。在opt_einsum的文档中查看更多细节（[https://optimized-einsum.readthedocs.io/en/stable/path_finding.html](https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)）。  ## torch.backends.xeon
