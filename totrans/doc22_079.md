# torch.sparse

> 原文：[https://pytorch.org/docs/stable/sparse.html](https://pytorch.org/docs/stable/sparse.html)

警告

PyTorch稀疏张量的API处于测试阶段，可能会在不久的将来发生变化。我们非常欢迎功能请求、错误报告和一般建议作为GitHub问题。

## 何时以及为什么使用稀疏性[](#why-and-when-to-use-sparsity "跳转到此标题的永久链接")

默认情况下，PyTorch将[`torch.Tensor`](张量.html#torch.Tensor "torch.Tensor")元素存储在连续的物理内存中。这导致了对需要快速访问元素的各种数组处理算法的高效实现。

现在，一些用户可能决定通过*元素大部分为零值*的张量来表示数据，如图邻接矩阵、修剪权重或点云。我们认识到这些是重要的应用程序，并旨在通过稀疏存储格式为这些用例提供性能优化。

多种稀疏存储格式，如COO、CSR/CSC、半结构、LIL等，多年来已经被开发出来。虽然它们在确切的布局上有所不同，但它们都通过高效表示零值元素来压缩数据。我们称未压缩的值为*指定*，与*未指定*、压缩元素相对。

通过压缩重复的零值，稀疏存储格式旨在节省各种CPU和GPU上的内存和计算资源。特别是对于高度稀疏或高度结构化的稀疏，这可能会对性能产生重大影响。因此，稀疏存储格式可以被视为性能优化。

像许多其他性能优化稀疏存储格式并不总是有利的。当尝试为您的用例使用稀疏格式时，您可能会发现执行时间增加而不是减少。

如果您在分析中预期性能会显著提高，但实际上却出现了降级，请随时打开一个GitHub问题。这有助于我们优先实现高效的内核和更广泛的性能优化。

我们使尝试不同的稀疏布局并在它们之间转换变得容易，而不对您特定应用程序的最佳选择发表意见。

## 功能概述

我们希望通过为每种布局提供转换例程，从给定的稠密张量构造稀疏张量变得简单。

在下一个示例中，我们将一个具有默认稠密（分布式）布局的2D张量转换为由COO内存布局支持的2D张量。在这种情况下，仅存储非零元素的值和索引。

```py
>>> a = torch.tensor([[0, 2.], [3, 0]])
>>> a.to_sparse()
tensor(indices=tensor([[0, 1],
 [1, 0]]),
 values=tensor([2., 3.]),
 size=(2, 2), nnz=2, layout=torch.sparse_coo) 
```

PyTorch目前支持[COO](#sparse-coo-docs)、[CSR](#sparse-csr-docs)、[CSC](#sparse-csc-docs)、[BSR](#sparse-bsr-docs)和[BSC](#sparse-bsc-docs)。

我们还有一个原型实现来支持：半结构稀疏<稀疏半结构文档>。更多细节请参考参考资料。

请注意，我们提供了这些格式的轻微概括。

批处理：诸如GPU之类的设备需要批处理以获得最佳性能，因此我们支持批处理维度。

我们目前提供了一个非常简单的批处理版本，其中稀疏格式的每个组件本身都被批处理。这也需要每个批次条目相同数量的指定元素。在这个示例中，我们从一个3D稠密张量构造一个3D（批处理）CSR张量。

```py
>>> t = torch.tensor([[[1., 0], [2., 3.]], [[4., 0], [5., 6.]]])
>>> t.dim()
3
>>> t.to_sparse_csr()
tensor(crow_indices=tensor([[0, 1, 3],
 [0, 1, 3]]),
 col_indices=tensor([[0, 0, 1],
 [0, 0, 1]]),
 values=tensor([[1., 2., 3.],
 [4., 5., 6.]]), size=(2, 2, 2), nnz=3,
 layout=torch.sparse_csr) 
```

稠密维度：另一方面，一些数据，如图嵌入，可能更适合被视为稀疏的向量集合，而不是标量。

在这个示例中，我们从一个3D分布式张量创建一个具有2个稀疏维度和1个稠密维度的3D混合COO张量。如果3D分布式张量中的整行都是零，则不会存储。但是，如果行中的任何值都是非零的，则整行都会被存储。这减少了索引的数量，因为我们只需要每行一个索引而不是每个元素一个索引。但它也增加了值的存储量。因为只有*完全*为零的行才能被发出，任何非零值元素的存在都会导致整行被存储。

```py
>>> t = torch.tensor([[[0., 0], [1., 2.]], [[0., 0], [3., 4.]]])
>>> t.to_sparse(sparse_dim=2)
tensor(indices=tensor([[0, 1],
 [1, 1]]),
 values=tensor([[1., 2.],
 [3., 4.]]),
 size=(2, 2, 2), nnz=2, layout=torch.sparse_coo) 
```

## 操作符概述

基本上，对具有稀疏存储格式的张量的操作与对具有步进（或其他）存储格式的张量的操作行为相同。存储的特殊性，即数据的物理布局，影响操作的性能，但不应影响语义。

我们正在积极增加稀疏张量的操作符覆盖范围。用户不应期望与密集张量相同级别的支持。请查看我们的[操作符](#sparse-ops-docs)文档以获取列表。

```py
>>> b = torch.tensor([[0, 0, 1, 2, 3, 0], [4, 5, 0, 6, 0, 0]])
>>> b_s = b.to_sparse_csr()
>>> b_s.cos()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: unsupported tensor layout: SparseCsr
>>> b_s.sin()
tensor(crow_indices=tensor([0, 3, 6]),
 col_indices=tensor([2, 3, 4, 0, 1, 3]),
 values=tensor([ 0.8415,  0.9093,  0.1411, -0.7568, -0.9589, -0.2794]),
 size=(2, 6), nnz=6, layout=torch.sparse_csr) 
```

如上例所示，我们不支持诸如cos之类的非零保留一元运算符。非零保留一元操作的输出将无法像输入那样充分利用稀疏存储格式，并可能导致内存的灾难性增加。我们依赖用户首先显式转换为密集张量，然后运行操作。

```py
>>> b_s.to_dense().cos()
tensor([[ 1.0000, -0.4161],
 [-0.9900,  1.0000]]) 
```

我们知道一些用户希望忽略压缩的零值，而不是保留操作的确切语义，例如cos。对于这一点，我们可以指向torch.masked及其MaskedTensor，后者也由稀疏存储格式和核支持。

还要注意，目前用户无法选择输出布局。例如，将稀疏张量添加到常规步进张量会导致步进张量。一些用户可能希望保持稀疏布局，因为他们知道结果仍然足够稀疏。

```py
>>> a + b.to_sparse()
tensor([[0., 3.],
 [3., 0.]]) 
```

我们承认，能够高效产生不同输出布局的核对于后续操作可能非常有用。后续操作可能会极大地受益于接收特定布局。我们正在开发一个API来控制结果布局，并认识到这是一个重要功能，可以为任何给定模型规划更优化的执行路径。

稀疏半结构张量

警告

稀疏半结构张量目前是一个原型功能，可能会发生变化。请随时提出问题以报告错误或分享反馈。

半结构稀疏性是首次在NVIDIA的Ampere架构中引入的稀疏数据布局。它也被称为**细粒度结构稀疏性**或**2:4结构稀疏性**。

这种稀疏布局存储每2n个元素中的n个元素，其中n由张量的数据类型（dtype）的宽度确定。最常用的dtype是float16，其中n=2，因此术语“2:4结构稀疏性”。

半结构稀疏性在[NVIDIA博客文章](https://developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt)中有更详细的解释。

在PyTorch中，半结构稀疏性是通过张量子类实现的。通过子类化，我们可以重写`__torch_dispatch__`，从而在执行矩阵乘法时使用更快的稀疏核。我们还可以将张量存储在子类中的压缩形式中，以减少内存开销。

在这种压缩形式中，稀疏张量仅保留*指定*元素和一些元数据，用于编码掩码。

注意

半结构稀疏张量的指定元素和元数据掩码一起存储在一个单一的扁平压缩张量中。它们被附加在一起形成一个连续的内存块。

压缩张量 = [原始张量的指定元素 | 元数据掩码]

对于大小为(r, c)的原始张量，我们期望前m * k // 2个元素是保留的元素，剩下的张量是元数据。

为了让用户更容易查看指定元素和掩码，可以使用`.indices()`和`.values()`分别访问掩码和指定元素。

+   `.values()`返回大小为(r, c//2)的张量中的指定元素，并具有与密集矩阵相同的数据类型。

+   `.indices()`返回一个大小为(r, c//2)的张量，元素类型为`torch.int16`（如果dtype为torch.float16或torch.bfloat16），元素类型为`torch.int32`（如果dtype为torch.int8）。

对于2:4稀疏张量，元数据开销很小 - 每个指定元素只有2位。

注意

重要的是要注意，`torch.float32`仅支持1:2稀疏性。因此，它不遵循上述相同的公式。

在这里，我们分解如何计算2:4稀疏张量的压缩比（稠密大小/稀疏大小）。

设(r, c) = 张量形状，e = 位宽(张量数据类型)，因此对于`torch.float16`和`torch.bfloat16`，e = 16，对于`torch.int8`，e = 8。

$M_{dense} = r \times c \times e \\ M_{sparse} = M_{specified} + M_{metadata} = r \times \frac{c}{2} \times e + r \times \frac{c}{2} \times 2 = \frac{rce}{2} + rc =rce(\frac{1}{2} +\frac{1}{e})$ Mdense​=r×c×eMsparse​=Mspecified​+Mmetadata​=r×2c​×e+r×2c​×2=2rce​+rc=rce(21​+e1​)

使用这些计算，我们可以确定原始稠密表示和新稀疏表示的总内存占用量。

这为我们提供了一个简单的压缩比公式，仅取决于张量数据类型的位宽。

$C = \frac{M_{sparse}}{M_{dense}} = \frac{1}{2} + \frac{1}{e}$ C=Mdense​Msparse​​=21​+e1​

通过使用这个公式，我们发现对于`torch.float16`或`torch.bfloat16`，压缩比为56.25%，对于`torch.int8`，压缩比为62.5%。

### 构建稀疏半结构张量

您可以通过简单使用`torch.to_sparse_semi_structured`函数将稠密张量转换为稀疏半结构张量。

请注意，由于硬件兼容性有限，我们仅支持CUDA张量用于半结构稀疏性的NVIDIA GPU。

以下数据类型支持半结构稀疏性。请注意，每种数据类型都有自己的形状约束和压缩因子。

| PyTorch数据类型 | 形状约束 | 压缩因子 | 稀疏模式 |
| --- | --- | --- | --- |
| `torch.float16` | 张量必须是2D且(r, c)都必须是64的正倍数 | 9/16 | 2:4 |
| `torch.bfloat16` | 张量必须是2D且(r, c)都必须是64的正倍数 | 9/16 | 2:4 |
| `torch.int8` | 张量必须是2D且(r, c)都必须是128的正倍数 | 10/16 | 2:4 |

要构建一个半结构稀疏张量，首先创建一个符合2:4（或半结构）稀疏格式的常规稠密张量。为此，我们将一个小的1x4条带平铺，创建一个16x16的稠密float16张量。之后，我们可以调用`to_sparse_semi_structured`函数对其进行压缩以加速推断。

```py
>>> from torch.sparse import to_sparse_semi_structured
>>> A = torch.Tensor([0, 0, 1, 1]).tile((128, 32)).half().cuda()
tensor([[0., 0., 1.,  ..., 0., 1., 1.],
 [0., 0., 1.,  ..., 0., 1., 1.],
 [0., 0., 1.,  ..., 0., 1., 1.],
 ...,
 [0., 0., 1.,  ..., 0., 1., 1.],
 [0., 0., 1.,  ..., 0., 1., 1.],
 [0., 0., 1.,  ..., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)
>>> A_sparse = to_sparse_semi_structured(A)
SparseSemiStructuredTensor(shape=torch.Size([128, 128]), transposed=False, values=tensor([[1., 1., 1.,  ..., 1., 1., 1.],
 [1., 1., 1.,  ..., 1., 1., 1.],
 [1., 1., 1.,  ..., 1., 1., 1.],
 ...,
 [1., 1., 1.,  ..., 1., 1., 1.],
 [1., 1., 1.,  ..., 1., 1., 1.],
 [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16), metadata=tensor([[-4370, -4370, -4370,  ..., -4370, -4370, -4370],
 [-4370, -4370, -4370,  ..., -4370, -4370, -4370],
 [-4370, -4370, -4370,  ..., -4370, -4370, -4370],
 ...,
 [-4370, -4370, -4370,  ..., -4370, -4370, -4370],
 [-4370, -4370, -4370,  ..., -4370, -4370, -4370],
 [-4370, -4370, -4370,  ..., -4370, -4370, -4370]], device='cuda:0',
dtype=torch.int16)) 
```

### 稀疏半结构张量操作

目前，以下操作支持半结构稀疏张量：

+   torch.addmm(偏置, 稠密, 稀疏.t())

+   torch.mm(稠密, 稀疏)

+   torch.mm(稀疏, 稠密)

+   aten.linear.default(稠密, 稀疏, 偏置)

+   aten.t.default(稀疏)

+   aten.t.detach(稀疏)

要使用这些操作，只需在您的张量以半结构稀疏格式具有0时，将`to_sparse_semi_structured(tensor)`的输出传递给`tensor`，就像这样：

```py
>>> a = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).half().cuda()
>>> b = torch.rand(64, 64).half().cuda()
>>> c = torch.mm(a, b)
>>> a_sparse = to_sparse_semi_structured(a)
>>> torch.allclose(c, torch.mm(a_sparse, b))
True 
```

### 使用半结构稀疏性加速nn.Linear

如果权重已经是半结构稀疏的，您可以通过几行代码加速模型中的线性层：

```py
>>> input = torch.rand(64, 64).half().cuda()
>>> mask = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).cuda().bool()
>>> linear = nn.Linear(64, 64).half().cuda()
>>> linear.weight = nn.Parameter(to_sparse_semi_structured(linear.weight.masked_fill(~mask, 0))) 
```  ## 稀疏COO张量

PyTorch实现了所谓的坐标格式，或COO格式，作为实现稀疏张量的存储格式之一。在COO格式中，指定的元素存储为元素索引和相应值的元组。特别是，

> +   指定元素的索引被收集在大小为(ndim, nse)的`indices`张量中，元素类型为`torch.int64`，
> +   
> +   相应的值被收集在大小为`(nse,)`的`values`张量中，并且具有任意整数或浮点数元素类型，

其中`ndim`是张量的维度，`nse`是指定元素的数量。

注意

稀疏COO张量的内存消耗至少为`(ndim * 8 + <元素类型大小（字节）>) * nse`字节（加上存储其他张量数据的恒定开销）。

分块张量的内存消耗至少为`product(<张量形状>) * <元素类型大小（字节）>`。

例如，一个大小为10,000 x 10,000的张量，包含100,000个非零的32位浮点数时，至少消耗的内存为`(2 * 8 + 4) * 100,000 = 2,000,000`字节，使用COO张量布局时，而使用默认的分块张量布局时为`10,000 * 10,000 * 4 = 400,000,000`字节。注意使用COO存储格式可以节省200倍的内存。

### 构造

稀疏的COO张量可以通过提供索引和数值的两个张量，以及稀疏张量的大小（当无法从索引和数值张量中推断出时），传递给[`torch.sparse_coo_tensor()`](generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor "torch.sparse_coo_tensor")函数来构建。

假设我们想要定义一个稀疏张量，位置(0, 2)处的条目为3，位置(1, 0)处的条目为4，位置(1, 2)处的条目为5。未指定的元素假定具有相同的值，填充值，默认为零。我们会这样写：

```py
>>> i = [[0, 1, 1],
 [2, 0, 2]]
>>> v =  [3, 4, 5]
>>> s = torch.sparse_coo_tensor(i, v, (2, 3))
>>> s
tensor(indices=tensor([[0, 1, 1],
 [2, 0, 2]]),
 values=tensor([3, 4, 5]),
 size=(2, 3), nnz=3, layout=torch.sparse_coo)
>>> s.to_dense()
tensor([[0, 0, 3],
 [4, 0, 5]]) 
```

请注意，输入的`i`不是索引元组的列表。如果想以这种方式编写索引，应在传递给稀疏构造函数之前进行转置：

```py
>>> i = [[0, 2], [1, 0], [1, 2]]
>>> v =  [3,      4,      5    ]
>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))
>>> # Or another equivalent formulation to get s
>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))
>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()
tensor([[0, 0, 3],
 [4, 0, 5]]) 
```

可以通过仅指定其大小来构建一个空的稀疏COO张量：

```py
>>> torch.sparse_coo_tensor(size=(2, 3))
tensor(indices=tensor([], size=(2, 0)),
 values=tensor([], size=(0,)),
 size=(2, 3), nnz=0, layout=torch.sparse_coo) 
```

### 稀疏混合COO张量[](#sparse-hybrid-coo-tensors "跳转到此标题的永久链接")

PyTorch实现了将标量值的稀疏张量扩展为具有（连续）张量值的稀疏张量。这样的张量被称为混合张量。

PyTorch混合COO张量通过允许`values`张量为多维张量来扩展稀疏COO张量，因此我们有：

> +   指定元素的索引被收集在大小为`(sparse_dims, nse)`且元素类型为`torch.int64`的`indices`张量中，
> +   
> +   相应的（张量）值被收集在大小为`(nse, dense_dims)`的`values`张量中，并且具有任意整数或浮点数元素类型。

注意

我们使用(M + K)维张量来表示N维稀疏混合张量，其中M和K分别是稀疏和密集维度的数量，使得M + K == N成立。

假设我们想要创建一个(2 + 1)维张量，位置(0, 2)处的条目为[3, 4]，位置(1, 0)处的条目为[5, 6]，位置(1, 2)处的条目为[7, 8]。我们会这样写

```py
>>> i = [[0, 1, 1],
 [2, 0, 2]]
>>> v =  [[3, 4], [5, 6], [7, 8]]
>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))
>>> s
tensor(indices=tensor([[0, 1, 1],
 [2, 0, 2]]),
 values=tensor([[3, 4],
 [5, 6],
 [7, 8]]),
 size=(2, 3, 2), nnz=3, layout=torch.sparse_coo) 
```

```py
>>> s.to_dense()
tensor([[[0, 0],
 [0, 0],
 [3, 4]],
 [[5, 6],
 [0, 0],
 [7, 8]]]) 
```

一般来说，如果`s`是一个稀疏COO张量，`M = s.sparse_dim()`，`K = s.dense_dim()`，那么我们有以下不变性：

> +   `M + K == len(s.shape) == s.ndim` - 张量的维度是稀疏和密集维度数量的总和，
> +   
> +   `s.indices().shape == (M, nse)` - 稀疏索引被显式存储，
> +   
> +   `s.values().shape == (nse,) + s.shape[M : M + K]` - 混合张量的值是K维张量，
> +   
> +   `s.values().layout == torch.strided` - 值以分块张量的形式存储。

注意

密集维度总是跟在稀疏维度后面，也就是不支持混合密集和稀疏维度。

注意

为了确保构建的稀疏张量具有一致的索引、数值和大小，可以通过`check_invariants=True`关键字参数在每个张量创建时启用不变性检查，或者全局使用[`torch.sparse.check_sparse_tensor_invariants`](generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants "torch.sparse.check_sparse_tensor_invariants")上下文管理器实例。默认情况下，稀疏张量的不变性检查是禁用的。### 未合并的稀疏COO张量

PyTorch稀疏COO张量格式允许稀疏*未合并*张量，在索引中可能存在重复坐标；在这种情况下，该索引处的值被解释为所有重复值条目的总和。例如，可以为相同索引`1`指定多个值`3`和`4`，这导致一个1-D未合并张量：

```py
>>> i = [[1, 1]]
>>> v =  [3, 4]
>>> s=torch.sparse_coo_tensor(i, v, (3,))
>>> s
tensor(indices=tensor([[1, 1]]),
 values=tensor(  [3, 4]),
 size=(3,), nnz=2, layout=torch.sparse_coo) 
```

而合并过程将使用求和将多值元素累积为单个值：

```py
>>> s.coalesce()
tensor(indices=tensor([[1]]),
 values=tensor([7]),
 size=(3,), nnz=1, layout=torch.sparse_coo) 
```

一般来说，[`torch.Tensor.coalesce()`](generated/torch.Tensor.coalesce.html#torch.Tensor.coalesce "torch.Tensor.coalesce")方法的输出是具有以下属性的稀疏张量：

+   指定张量元素的索引是唯一的，

+   索引按字典顺序排序，

+   [`torch.Tensor.is_coalesced()`](generated/torch.Tensor.is_coalesced.html#torch.Tensor.is_coalesced "torch.Tensor.is_coalesced") 返回 `True`。

注意

在大多数情况下，您不必关心稀疏张量是否已合并，因为大多数操作将在给定稀疏合并或未合并张量时完全相同。

然而，一些操作在未合并张量上实现效率更高，一些操作在合并张量上实现效率更高。

例如，稀疏COO张量的加法是通过简单地连接索引和值张量来实现的：

```py
>>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))
>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))
>>> a + b
tensor(indices=tensor([[0, 0, 1, 1]]),
 values=tensor([7, 8, 5, 6]),
 size=(2,), nnz=4, layout=torch.sparse_coo) 
```

如果反复执行可能产生重复条目的操作（例如，[`torch.Tensor.add()`](generated/torch.Tensor.add.html#torch.Tensor.add "torch.Tensor.add"))，应偶尔合并稀疏张量以防止它们变得过大。

另一方面，索引的字典顺序对于实现涉及许多元素选择操作的算法（例如切片或矩阵乘积）可能是有利的。

### 使用稀疏COO张量进行工作

让我们考虑以下示例：

```py
>>> i = [[0, 1, 1],
 [2, 0, 2]]
>>> v =  [[3, 4], [5, 6], [7, 8]]
>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2)) 
```

如上所述，稀疏COO张量是一个[`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor")实例，为了区分它与使用其他布局的张量实例，可以使用[`torch.Tensor.is_sparse`](generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse "torch.Tensor.is_sparse")或`torch.Tensor.layout`属性：

```py
>>> isinstance(s, torch.Tensor)
True
>>> s.is_sparse
True
>>> s.layout == torch.sparse_coo
True 
```

可以使用方法[`torch.Tensor.sparse_dim()`](generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim "torch.Tensor.sparse_dim")和[`torch.Tensor.dense_dim()`](generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim "torch.Tensor.dense_dim")分别获取稀疏和密集维度的数量。例如：

```py
>>> s.sparse_dim(), s.dense_dim()
(2, 1) 
```

如果`s`是一个稀疏COO张量，则可以使用方法[`torch.Tensor.indices()`](generated/torch.Tensor.indices.html#torch.Tensor.indices "torch.Tensor.indices")和[`torch.Tensor.values()`](generated/torch.Tensor.values.html#torch.Tensor.values "torch.Tensor.values")获取其COO格式数据。

注意

目前，只有当张量实例已合并时才能获取COO格式数据：

```py
>>> s.indices()
RuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first 
```

要获取未合并张量的COO格式数据，请使用`torch.Tensor._values()`和`torch.Tensor._indices()`：

```py
>>> s._indices()
tensor([[0, 1, 1],
 [2, 0, 2]]) 
```

警告

调用`torch.Tensor._values()`将返回一个*分离*张量。要跟踪梯度，必须使用`torch.Tensor.coalesce().values()`。

构造一个新的稀疏COO张量会导致一个未合并的张量：

```py
>>> s.is_coalesced()
False 
```

但可以使用[`torch.Tensor.coalesce()`](generated/torch.Tensor.coalesce.html#torch.Tensor.coalesce "torch.Tensor.coalesce")方法构造稀疏COO张量的合并副本：

```py
>>> s2 = s.coalesce()
>>> s2.indices()
tensor([[0, 1, 1],
 [2, 0, 2]]) 
```

在处理未压缩稀疏COO张量时，必须考虑未压缩数据的可加性：相同索引的值是一个求和的项，其求值给出对应张量元素的值。例如，对稀疏未压缩张量进行标量乘法可以通过将所有未压缩值与标量相乘来实现，因为 `c * (a + b) == c * a + c * b` 成立。然而，任何非线性操作，比如平方根，不能通过将操作应用于未压缩数据来实现，因为一般情况下 `sqrt(a + b) == sqrt(a) + sqrt(b)` 不成立。

对稀疏COO张量进行切片（带有正步长）仅支持密集维度。索引支持稀疏和密集维度：

```py
>>> s[1]
tensor(indices=tensor([[0, 2]]),
 values=tensor([[5, 6],
 [7, 8]]),
 size=(3, 2), nnz=2, layout=torch.sparse_coo)
>>> s[1, 0, 1]
tensor(6)
>>> s[1, 0, 1:]
tensor([6]) 
```

在PyTorch中，稀疏张量的填充值不能被明确指定，一般假定为零。然而，存在一些操作可能会以不同方式解释填充值。例如，[`torch.sparse.softmax()`](generated/torch.sparse.softmax.html#torch.sparse.softmax "torch.sparse.softmax") 计算softmax时假定填充值为负无穷。## 稀疏压缩张量[](#sparse-compressed-tensors "跳转到此标题的永久链接")

稀疏压缩张量代表一类稀疏张量，其共同特征是使用编码压缩某个维度的索引，从而在稀疏压缩张量的线性代数核上实现某些优化。这种编码基于[压缩稀疏行（CSR）](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format))格式，PyTorch稀疏压缩张量扩展了对稀疏张量批次的支持，允许多维张量值，并将稀疏张量值存储在密集块中。

注意

我们使用（B + M + K）维张量来表示一个N维稀疏压缩混合张量，其中B、M和K分别是批次、稀疏和密集维度的数量，满足 `B + M + K == N`。稀疏压缩张量的稀疏维度总是两个，`M == 2`。

注意

如果满足以下不变性，我们说一个索引张量 `compressed_indices` 使用CSR编码：

+   `compressed_indices` 是一个连续的步进为32位或64位整数张量

+   `compressed_indices` 的形状是 `(*batchsize, compressed_dim_size + 1)`，其中 `compressed_dim_size` 是压缩维度的数量（例如行或列）

+   `compressed_indices[..., 0] == 0` 其中 `...` 表示批次索引

+   `compressed_indices[..., compressed_dim_size] == nse` 其中 `nse` 是指定元素的数量

+   对于 `i=1, ..., compressed_dim_size`，`0 <= compressed_indices[..., i] - compressed_indices[..., i - 1] <= plain_dim_size`，其中 `plain_dim_size` 是平面维度的数量（与压缩维度正交，例如列或行）。

为了确保构建的稀疏张量具有一致的索引、值和大小，可以通过 `check_invariants=True` 关键字参数在每个张量创建时启用不变性检查，或者使用 [`torch.sparse.check_sparse_tensor_invariants`](generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants "torch.sparse.check_sparse_tensor_invariants") 上下文管理器实例进行全局设置。默认情况下，稀疏张量的不变性检查是禁用的。

注意

将稀疏压缩布局推广到N维张量可能会导致对指定元素数量的混淆。当稀疏压缩张量包含批量维度时，指定元素的数量将对应于每个批量的这些元素的数量。当稀疏压缩张量具有密集维度时，考虑的元素现在是K维数组。对于块稀疏压缩布局，2-D块被视为被指定的元素。以一个具有长度为`b`的批量维度和块形状为`p, q`的3维块稀疏张量为例。如果这个张量有`n`个指定元素，那么实际上我们有每批`n`个块被指定。这个张量将具有形状为`(b, n, p, q)`的`values`。指定元素数量的这种解释来自于所有稀疏压缩布局都源自于2维矩阵的压缩。批量维度被视为稀疏矩阵的堆叠，密集维度改变了元素的含义，从简单的标量值变为具有自己维度的数组。

### 稀疏CSR张量

CSR格式相对于COO格式的主要优势是更好地利用存储和更快的计算操作，例如使用MKL和MAGMA后端的稀疏矩阵-向量乘法。

在最简单的情况下，一个(0 + 2 + 0)维稀疏CSR张量由三个1-D张量组成：`crow_indices`、`col_indices`和`values`：

> +   `crow_indices`张量包含压缩的行索引。这是一个大小为`nrows + 1`（行数加1）的1-D张量。`crow_indices`的最后一个元素是指定元素的数量`nse`。该张量根据给定行的起始位置在`values`和`col_indices`中编码索引。张量中的每个连续数字减去前一个数字表示给定行中元素的数量。
> +   
> +   `col_indices`张量包含每个元素的列索引。这是一个大小为`nse`的1-D张量。
> +   
> +   `values`张量包含CSR张量元素的值。这是一个大小为`nse`的1-D张量。

注意

索引张量`crow_indices`和`col_indices`的元素类型应为`torch.int64`（默认）或`torch.int32`。如果要使用MKL启用的矩阵操作，请使用`torch.int32`。这是由于pytorch的默认链接是与使用32位整数索引的MKL LP64链接。

在一般情况下，(B + 2 + K)维稀疏CSR张量由两个(B + 1)维索引张量`crow_indices`和`col_indices`以及(1 + K)维`values`张量组成，使得

> +   `crow_indices.shape == (*batchsize, nrows + 1)`
> +   
> +   `col_indices.shape == (*batchsize, nse)`
> +   
> +   `values.shape == (nse, *densesize)`

稀疏CSR张量的形状为`(*batchsize, nrows, ncols, *densesize)`，其中`len(batchsize) == B`且`len(densesize) == K`。

注意

稀疏CSR张量的批次是相关的：所有批次中指定元素的数量必须相同。这种有点人为的约束允许有效地存储不同CSR批次的索引。

注意

稀疏和密集维度的数量可以使用[`torch.Tensor.sparse_dim()`](generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim "torch.Tensor.sparse_dim")和[`torch.Tensor.dense_dim()`](generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim "torch.Tensor.dense_dim")方法获得。批量维度可以从张量形状中计算得到：`batchsize = tensor.shape[:-tensor.sparse_dim() - tensor.dense_dim()]`。

注意

稀疏CSR张量的内存消耗至少为`(nrows * 8 + (8 + <元素类型大小字节> * prod(densesize)) * nse) * prod(batchsize)`字节（加上存储其他张量数据的恒定开销）。

使用[稀疏COO格式介绍中的示例数据](#sparse-coo-docs)相同，一个包含 100 000 个非零32位浮点数的 10 000 x 10 000 张量的内存消耗至少为`(10000 * 8 + (8 + 4 * 1) * 100 000) * 1 = 1 280 000`字节，使用CSR张量布局。请注意，与使用COO和分步格式相比，使用CSR存储格式可以节省1.6倍和310倍的存储空间。

#### 构造CSR张量[](#construction-of-csr-tensors "跳转到此标题")

可以直接使用[`torch.sparse_csr_tensor()`](generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor "torch.sparse_csr_tensor")函数构造稀疏CSR张量。用户必须分别提供行索引和列索引以及值张量，其中行索引必须使用CSR压缩编码指定。如果没有提供`size`参数，则将从`crow_indices`和`col_indices`中推断出`size`。

```py
>>> crow_indices = torch.tensor([0, 2, 4])
>>> col_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([1, 2, 3, 4])
>>> csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.float64)
>>> csr
tensor(crow_indices=tensor([0, 2, 4]),
 col_indices=tensor([0, 1, 0, 1]),
 values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
 dtype=torch.float64)
>>> csr.to_dense()
tensor([[1., 2.],
 [3., 4.]], dtype=torch.float64) 
```

注意

推断的`size`中稀疏维度的值是从`crow_indices`的大小和`col_indices`中的最大索引值计算而来的。如果列数需要大于推断的`size`中的列数，则必须明确指定`size`参数。

从分步或稀疏COO张量构造2-D稀疏CSR张量的最简单方法是使用[`torch.Tensor.to_sparse_csr()`](generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr "torch.Tensor.to_sparse_csr")方法。在（分步）张量中的任何零将被解释为稀疏张量中的缺失值：

```py
>>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)
>>> sp = a.to_sparse_csr()
>>> sp
tensor(crow_indices=tensor([0, 1, 3, 3]),
 col_indices=tensor([2, 0, 1]),
 values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64) 
```

#### CSR张量操作

稀疏矩阵-向量乘法可以使用`tensor.matmul()`方法执行。这是目前CSR张量上支持的唯一数学运算。

```py
>>> vec = torch.randn(4, 1, dtype=torch.float64)
>>> sp.matmul(vec)
tensor([[0.9078],
 [1.3180],
 [0.0000]], dtype=torch.float64) 
```  ### 稀疏CSC张量

稀疏CSC（压缩稀疏列）张量格式实现了CSC格式，用于存储具有扩展支持批量稀疏CSC张量和值为多维张量的二维张量。

注意

稀疏CSC张量在转置时本质上是稀疏CSR张量的转置，转置是关于交换稀疏维度的。

与[稀疏CSR张量](#sparse-csr-docs)类似，稀疏CSC张量由三个张量组成：`ccol_indices`、`row_indices`和`values`。

> +   `ccol_indices`张量包含压缩的列索引。这是一个形状为`(*batchsize, ncols + 1)`的(B + 1)-D张量。最后一个元素是指定元素的数量`nse`。该张量根据给定列开始的位置编码`values`和`row_indices`的索引。张量中的每个连续数字减去前一个数字表示给定列中元素的数量。
> +   
> +   `row_indices`张量包含每个元素的行索引。这是一个形状为`(*batchsize, nse)`的(B + 1)-D张量。
> +   
> +   `values`张量包含CSC张量元素的值。这是一个形状为`(nse, *densesize)`的(1 + K)-D张量。

#### CSC张量的构造[](#construction-of-csc-tensors "跳转到此标题")

可以直接使用[`torch.sparse_csc_tensor()`](generated/torch.sparse_csc_tensor.html#torch.sparse_csc_tensor "torch.sparse_csc_tensor")函数构造稀疏CSC张量。用户必须分别提供行索引和列索引以及值张量，其中列索引必须使用CSR压缩编码指定。如果没有提供`size`参数，则将从`row_indices`和`ccol_indices`张量中推断出`size`。

```py
>>> ccol_indices = torch.tensor([0, 2, 4])
>>> row_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([1, 2, 3, 4])
>>> csc = torch.sparse_csc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)
>>> csc
tensor(ccol_indices=tensor([0, 2, 4]),
 row_indices=tensor([0, 1, 0, 1]),
 values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
 dtype=torch.float64, layout=torch.sparse_csc)
>>> csc.to_dense()
tensor([[1., 3.],
 [2., 4.]], dtype=torch.float64) 
```

注意

稀疏CSC张量的构造函数在行索引参数之前有压缩的列索引参数。

(0 + 2 + 0)-维稀疏CSC张量可以使用[`torch.Tensor.to_sparse_csc()`](generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc "torch.Tensor.to_sparse_csc")方法从任何二维张量构造。在（分步）张量中的任何零将被解释为稀疏张量中的缺失值：

```py
>>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)
>>> sp = a.to_sparse_csc()
>>> sp
tensor(ccol_indices=tensor([0, 1, 2, 3, 3]),
 row_indices=tensor([1, 1, 0]),
 values=tensor([1., 2., 1.]), size=(3, 4), nnz=3, dtype=torch.float64,
 layout=torch.sparse_csc) 
```  ### 稀疏的BSR张量

稀疏的BSR（块压缩稀疏行）张量格式实现了BSR格式，用于存储二维张量，并扩展支持稀疏BSR张量的批处理和值为多维张量块的情况。

稀疏的BSR张量由三个张量组成：`crow_indices`、`col_indices`和`values`：

> +   `crow_indices`张量包含压缩的行索引。这是一个形状为`(*batchsize, nrowblocks + 1)`的(B + 1)-D张量。最后一个元素是指定块的数量`nse`。该张量根据给定列块的起始位置编码`values`和`col_indices`中的索引。张量中的每个连续数字减去前一个数字表示给定行中块的数量。
> +   
> +   `col_indices`张量包含每个元素的列块索引。这是一个形状为`(*batchsize, nse)`的(B + 1)-D张量。
> +   
> +   `values`张量包含稀疏的BSR张量元素的值，收集到二维块中。这是一个形状为`(nse, nrowblocks, ncolblocks, *densesize)`的(1 + 2 + K)-D张量。

#### 构建BSR张量[](#construction-of-bsr-tensors "跳转到此标题的永久链接")

稀疏的BSR张量可以直接通过使用[`torch.sparse_bsr_tensor()`](generated/torch.sparse_bsr_tensor.html#torch.sparse_bsr_tensor "torch.sparse_bsr_tensor")函数来构建。用户必须分别提供行和列块索引以及数值张量，其中行块索引必须使用CSR压缩编码指定。如果没有提供`size`参数，它将从`crow_indices`和`col_indices`张量中推断出来。

```py
>>> crow_indices = torch.tensor([0, 2, 4])
>>> col_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]],
...                        [[3, 4, 5], [9, 10, 11]],
...                        [[12, 13, 14], [18, 19, 20]],
...                        [[15, 16, 17], [21, 22, 23]]])
>>> bsr = torch.sparse_bsr_tensor(crow_indices, col_indices, values, dtype=torch.float64)
>>> bsr
tensor(crow_indices=tensor([0, 2, 4]),
 col_indices=tensor([0, 1, 0, 1]),
 values=tensor([[[ 0.,  1.,  2.],
 [ 6.,  7.,  8.]],
 [[ 3.,  4.,  5.],
 [ 9., 10., 11.]],
 [[12., 13., 14.],
 [18., 19., 20.]],
 [[15., 16., 17.],
 [21., 22., 23.]]]),
 size=(4, 6), nnz=4, dtype=torch.float64, layout=torch.sparse_bsr)
>>> bsr.to_dense()
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],
 [ 6.,  7.,  8.,  9., 10., 11.],
 [12., 13., 14., 15., 16., 17.],
 [18., 19., 20., 21., 22., 23.]], dtype=torch.float64) 
```

可以使用[`torch.Tensor.to_sparse_bsr()`](generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr "torch.Tensor.to_sparse_bsr")方法从任何二维张量构建(0 + 2 + 0)-维稀疏的BSR张量，该方法还需要指定值块大小：

```py
>>> dense = torch.tensor([[0, 1, 2, 3, 4, 5],
...                       [6, 7, 8, 9, 10, 11],
...                       [12, 13, 14, 15, 16, 17],
...                       [18, 19, 20, 21, 22, 23]])
>>> bsr = dense.to_sparse_bsr(blocksize=(2, 3))
>>> bsr
tensor(crow_indices=tensor([0, 2, 4]),
 col_indices=tensor([0, 1, 0, 1]),
 values=tensor([[[ 0,  1,  2],
 [ 6,  7,  8]],
 [[ 3,  4,  5],
 [ 9, 10, 11]],
 [[12, 13, 14],
 [18, 19, 20]],
 [[15, 16, 17],
 [21, 22, 23]]]), size=(4, 6), nnz=4,
 layout=torch.sparse_bsr) 
```  ### 稀疏的BSC张量

稀疏的BSC（块压缩稀疏列）张量格式实现了BSC格式，用于存储二维张量，并扩展支持稀疏BSC张量的批处理和值为多维张量块的情况。

稀疏的BSC张量由三个张量组成：`ccol_indices`、`row_indices`和`values`：

> +   `ccol_indices`张量包含压缩的列索引。这是一个形状为`(*batchsize, ncolblocks + 1)`的(B + 1)-D张量。最后一个元素是指定块的数量`nse`。该张量根据给定行块的起始位置编码`values`和`row_indices`中的索引。张量中的每个连续数字减去前一个数字表示给定列中块的数量。
> +   
> +   `row_indices`张量包含每个元素的行块索引。这是一个形状为`(*batchsize, nse)`的(B + 1)-D张量。
> +   
> +   `values`张量包含稀疏的BSC张量元素的值，收集到二维块中。这是一个形状为`(nse, nrowblocks, ncolblocks, *densesize)`的(1 + 2 + K)-D张量。

#### 构建BSC张量[](#construction-of-bsc-tensors "跳转到此标题的永久链接")

稀疏的BSC张量可以直接通过使用[`torch.sparse_bsc_tensor()`](generated/torch.sparse_bsc_tensor.html#torch.sparse_bsc_tensor "torch.sparse_bsc_tensor")函数来构建。用户必须分别提供行和列块索引以及数值张量，其中列块索引必须使用CSR压缩编码指定。如果没有提供`size`参数，它将从`ccol_indices`和`row_indices`张量中推断出来。

```py
>>> ccol_indices = torch.tensor([0, 2, 4])
>>> row_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]],
...                        [[3, 4, 5], [9, 10, 11]],
...                        [[12, 13, 14], [18, 19, 20]],
...                        [[15, 16, 17], [21, 22, 23]]])
>>> bsc = torch.sparse_bsc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)
>>> bsc
tensor(ccol_indices=tensor([0, 2, 4]),
 row_indices=tensor([0, 1, 0, 1]),
 values=tensor([[[ 0.,  1.,  2.],
 [ 6.,  7.,  8.]],
 [[ 3.,  4.,  5.],
 [ 9., 10., 11.]],
 [[12., 13., 14.],
 [18., 19., 20.]],
 [[15., 16., 17.],
 [21., 22., 23.]]]), size=(4, 6), nnz=4,
 dtype=torch.float64, layout=torch.sparse_bsc) 
```

### 用于处理稀疏压缩张量的工具[](#tools-for-working-with-sparse-compressed-tensors "跳转到此标题的永久链接")

所有稀疏压缩张量 — CSR、CSC、BSR 和 BSC 张量 — 在概念上非常相似，它们的索引数据分为两部分：所谓的压缩索引使用 CSR 编码，而所谓的普通索引与压缩索引正交。这使得这些张量上的各种工具可以共享相同的实现，这些实现由张量布局参数化。

#### 稀疏压缩张量的构造[](#construction-of-sparse-compressed-tensors "Permalink to this heading")

CSR、CSC、BSR 和 CSC 张量可以通过使用 [`torch.sparse_compressed_tensor()`](generated/torch.sparse_compressed_tensor.html#torch.sparse_compressed_tensor "torch.sparse_compressed_tensor") 函数构建，该函数具有与上述讨论的构造函数 [`torch.sparse_csr_tensor()`](generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor "torch.sparse_csr_tensor")、[`torch.sparse_csc_tensor()`](generated/torch.sparse_csc_tensor.html#torch.sparse_csc_tensor "torch.sparse_csc_tensor")、[`torch.sparse_bsr_tensor()`](generated/torch.sparse_bsr_tensor.html#torch.sparse_bsr_tensor "torch.sparse_bsr_tensor") 和 [`torch.sparse_bsc_tensor()`](generated/torch.sparse_bsc_tensor.html#torch.sparse_bsc_tensor "torch.sparse_bsc_tensor") 相同的接口，但需要额外的 `layout` 参数。以下示例说明了使用相同输入数据通过指定相应的布局参数来构建 CSR 和 CSC 张量的方法，该参数传递给 [`torch.sparse_compressed_tensor()`](generated/torch.sparse_compressed_tensor.html#torch.sparse_compressed_tensor "torch.sparse_compressed_tensor") 函数：

```py
>>> compressed_indices = torch.tensor([0, 2, 4])
>>> plain_indices = torch.tensor([0, 1, 0, 1])
>>> values = torch.tensor([1, 2, 3, 4])
>>> csr = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csr)
>>> csr
tensor(crow_indices=tensor([0, 2, 4]),
 col_indices=tensor([0, 1, 0, 1]),
 values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,
 layout=torch.sparse_csr)
>>> csc = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csc)
>>> csc
tensor(ccol_indices=tensor([0, 2, 4]),
 row_indices=tensor([0, 1, 0, 1]),
 values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,
 layout=torch.sparse_csc)
>>> (csr.transpose(0, 1).to_dense() == csc.to_dense()).all()
tensor(True) 
```  ## 支持的操作[](#supported-operations "Permalink to this heading")

### 线性代数操作

以下表格总结了在稀疏矩阵上支持的线性代数操作，其中操作数的布局可能会有所不同。这里 `T[layout]` 表示具有给定布局的张量。类似地，`M[layout]` 表示矩阵（2-D PyTorch 张量），`V[layout]` 表示向量（1-D PyTorch 张量）。此外，`f` 表示标量（浮点数或 0-D PyTorch 张量），`*` 表示逐元素乘法，`@` 表示矩阵乘法。

| PyTorch 操作 | 稀疏梯度？ | 布局签名 |
| --- | --- | --- |
| [`torch.mv()`](generated/torch.mv.html#torch.mv "torch.mv") | 否 | `M[sparse_coo] @ V[strided] -> V[strided]` |
| [`torch.mv()`](generated/torch.mv.html#torch.mv "torch.mv") | 否 | `M[sparse_csr] @ V[strided] -> V[strided]` |
| [`torch.matmul()`](generated/torch.matmul.html#torch.matmul "torch.matmul") | 否 | `M[sparse_coo] @ M[strided] -> M[strided]` |
| [`torch.matmul()`](generated/torch.matmul.html#torch.matmul "torch.matmul") | 否 | `M[sparse_csr] @ M[strided] -> M[strided]` |
| [`torch.matmul()`](generated/torch.matmul.html#torch.matmul "torch.matmul") | 否 | `M[SparseSemiStructured] @ M[strided] -> M[strided]` |
| [`torch.matmul()`](generated/torch.matmul.html#torch.matmul "torch.matmul") | 否 | `M[strided] @ M[SparseSemiStructured] -> M[strided]` |
| [`torch.mm()`](generated/torch.mm.html#torch.mm "torch.mm") | 否 | `M[sparse_coo] @ M[strided] -> M[strided]` |
| [`torch.mm()`](generated/torch.mm.html#torch.mm "torch.mm") | 否 | `M[SparseSemiStructured] @ M[strided] -> M[strided]` |
| [`torch.mm()`](generated/torch.mm.html#torch.mm "torch.mm") | 否 | `M[strided] @ M[SparseSemiStructured] -> M[strided]` |
| [`torch.sparse.mm()`](generated/torch.sparse.mm.html#torch.sparse.mm "torch.sparse.mm") | 是 | `M[sparse_coo] @ M[strided] -> M[strided]` |
| [`torch.smm()`](generated/torch.smm.html#torch.smm "torch.smm") | 否 | `M[sparse_coo] @ M[strided] -> M[sparse_coo]` |
| [`torch.hspmm()`](generated/torch.hspmm.html#torch.hspmm "torch.hspmm") | 否 | `M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo]` |
| [`torch.bmm()`](generated/torch.bmm.html#torch.bmm "torch.bmm") | 否 | `T[sparse_coo] @ T[strided] -> T[strided]` |
| [`torch.addmm()`](generated/torch.addmm.html#torch.addmm "torch.addmm") | 否 | `f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]` |
| [`torch.addmm()`](generated/torch.addmm.html#torch.addmm "torch.addmm") | 否 | `f * M[strided] + f * (M[SparseSemiStructured] @ M[strided]) -> M[strided]` |
| [`torch.addmm()`](generated/torch.addmm.html#torch.addmm "torch.addmm") | 否 | `f * M[strided] + f * (M[strided] @ M[SparseSemiStructured]) -> M[strided]` |
| [`torch.sparse.addmm()`](generated/torch.sparse.addmm.html#torch.sparse.addmm "torch.sparse.addmm") | 是 | `f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]` |
| [`torch.sspaddmm()`](generated/torch.sspaddmm.html#torch.sspaddmm "torch.sspaddmm") | 否 | `f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo]` |
| [`torch.lobpcg()`](generated/torch.lobpcg.html#torch.lobpcg "torch.lobpcg") | 否 | `GENEIG(M[sparse_coo]) -> M[strided], M[strided]` |
| [`torch.pca_lowrank()`](generated/torch.pca_lowrank.html#torch.pca_lowrank "torch.pca_lowrank") | 是 | `PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided]` |
| [`torch.svd_lowrank()`](generated/torch.svd_lowrank.html#torch.svd_lowrank "torch.svd_lowrank") | 是 | `SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]` |

“Sparse grad?”列指示PyTorch操作是否支持对稀疏矩阵参数进行反向传播。除了[`torch.smm()`](generated/torch.smm.html#torch.smm "torch.smm")之外，所有PyTorch操作都支持对分步矩阵参数进行反向传播。

注意

目前，PyTorch不支持使用布局签名`M[strided] @ M[sparse_coo]`进行矩阵乘法。然而，应用程序仍然可以使用矩阵关系`D @ S == (S.t() @ D.t()).t()`来计算这个。

### 张量方法和稀疏

以下张量方法与稀疏张量相关：

| [`Tensor.is_sparse`](generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse "torch.Tensor.is_sparse") | 如果张量使用稀疏COO存储布局，则返回`True`，否则返回`False`。 |
| --- | --- |
| [`Tensor.is_sparse_csr`](generated/torch.Tensor.is_sparse_csr.html#torch.Tensor.is_sparse_csr "torch.Tensor.is_sparse_csr") | 如果张量使用稀疏CSR存储布局，则返回`True`，否则返回`False`。 |
| [`Tensor.dense_dim`](generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim "torch.Tensor.dense_dim") | 返回[sparse tensor](#sparse-docs) `self`中的稠密维度数量。 |
| [`Tensor.sparse_dim`](generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim "torch.Tensor.sparse_dim") | 返回[sparse tensor](#sparse-docs) `self`中的稀疏维度数量。 |
| [`Tensor.sparse_mask`](generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask "torch.Tensor.sparse_mask") | 返回一个由稀疏张量`mask`的索引过滤的分步张量`self`的新[稀疏张量](#sparse-docs)。 |
| [`Tensor.to_sparse`](generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse "torch.Tensor.to_sparse") | 返回张量的稀疏副本。 |
| [`Tensor.to_sparse_coo`](generated/torch.Tensor.to_sparse_coo.html#torch.Tensor.to_sparse_coo "torch.Tensor.to_sparse_coo") | 将张量转换为[坐标格式](#sparse-coo-docs)。 |
| [`Tensor.to_sparse_csr`](generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr "torch.Tensor.to_sparse_csr") | 将张量转换为压缩行存储格式（CSR）。 |
| [`Tensor.to_sparse_csc`](generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc "torch.Tensor.to_sparse_csc") | 将张量转换为压缩列存储（CSC）格式。 |
| [`Tensor.to_sparse_bsr`](generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr "torch.Tensor.to_sparse_bsr") | 将张量转换为给定块大小的块稀疏行（BSR）存储格式。 |
| [`Tensor.to_sparse_bsc`](generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc "torch.Tensor.to_sparse_bsc") | 将张量转换为给定块大小的块稀疏列（BSC）存储格式。 |
| [`Tensor.to_dense`](generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense "torch.Tensor.to_dense") | 如果`self`不是分步张量，则创建`self`的分步副本，否则返回`self`。 |
| [`Tensor.values`](generated/torch.Tensor.values.html#torch.Tensor.values "torch.Tensor.values") | 返回[稀疏COO张量](#sparse-coo-docs)的值张量。 |

以下张量方法特定于稀疏COO张量：

| [`Tensor.coalesce`](generated/torch.Tensor.coalesce.html#torch.Tensor.coalesce "torch.Tensor.coalesce") | 如果`self`是[未合并的张量](#sparse-uncoalesced-coo-docs)，则返回`self`的合并副本。 |
| --- | --- |
| [`Tensor.sparse_resize_`](generated/torch.Tensor.sparse_resize_.html#torch.Tensor.sparse_resize_ "torch.Tensor.sparse_resize_") | 调整`self`[稀疏张量](#sparse-docs)到所需大小和稀疏和密集维度的数量。 |
| [`Tensor.sparse_resize_and_clear_`](generated/torch.Tensor.sparse_resize_and_clear_.html#torch.Tensor.sparse_resize_and_clear_ "torch.Tensor.sparse_resize_and_clear_") | 从[稀疏张量](#sparse-docs)`self`中删除所有指定元素，并将`self`调整为所需大小和稀疏和密集维度的数量。 |
| [`Tensor.is_coalesced`](generated/torch.Tensor.is_coalesced.html#torch.Tensor.is_coalesced "torch.Tensor.is_coalesced") | 如果`self`是已合并的[稀疏COO张量](#sparse-coo-docs)，则返回`True`，否则返回`False`。 |
| [`Tensor.indices`](generated/torch.Tensor.indices.html#torch.Tensor.indices "torch.Tensor.indices") | 返回[稀疏COO张量](#sparse-coo-docs)的索引张量。 |

以下方法特定于[稀疏CSR张量](#sparse-csr-docs)和[稀疏BSR张量](#sparse-bsr-docs)：

| [`Tensor.crow_indices`](generated/torch.Tensor.crow_indices.html#torch.Tensor.crow_indices "torch.Tensor.crow_indices") | 返回包含`self`张量的压缩行索引的张量，当`self`是布局为`sparse_csr`的稀疏CSR张量时。 |
| --- | --- |
| [`Tensor.col_indices`](generated/torch.Tensor.col_indices.html#torch.Tensor.col_indices "torch.Tensor.col_indices") | 返回包含`self`张量的列索引的张量，当`self`是布局为`sparse_csr`的稀疏CSR张量时。 |

以下方法特定于[稀疏CSC张量](#sparse-csc-docs)和[稀疏BSC张量](#sparse-bsc-docs)：

| [`Tensor.row_indices`](generated/torch.Tensor.row_indices.html#torch.Tensor.row_indices "torch.Tensor.row_indices") |  |
| --- | --- |
| [`Tensor.ccol_indices`](generated/torch.Tensor.ccol_indices.html#torch.Tensor.ccol_indices "torch.Tensor.ccol_indices") |  |

以下张量方法支持稀疏COO张量：

[`add()`](generated/torch.Tensor.add.html#torch.Tensor.add "torch.Tensor.add") [`add_()`](generated/torch.Tensor.add_.html#torch.Tensor.add_ "torch.Tensor.add_") [`addmm()`](generated/torch.Tensor.addmm.html#torch.Tensor.addmm "torch.Tensor.addmm") [`addmm_()`](generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_ "torch.Tensor.addmm_") [`any()`](generated/torch.Tensor.any.html#torch.Tensor.any "torch.Tensor.any") [`asin()`](generated/torch.Tensor.asin.html#torch.Tensor.asin "torch.Tensor.asin") [`asin_()`](generated/torch.Tensor.asin_.html#torch.Tensor.asin_ "torch.Tensor.asin_") [`arcsin()`](generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin "torch.Tensor.arcsin") [`arcsin_()`](generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_ "torch.Tensor.arcsin_") [`bmm()`](generated/torch.Tensor.bmm.html#torch.Tensor.bmm "torch.Tensor.bmm") [`clone()`](generated/torch.Tensor.clone.html#torch.Tensor.clone "torch.Tensor.clone") [`deg2rad()`](generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad "torch.Tensor.deg2rad") `deg2rad_()` [`detach()`](generated/torch.Tensor.detach.html#torch.Tensor.detach "torch.Tensor.detach") [`detach_()`](generated/torch.Tensor.detach_.html#torch.Tensor.detach_ "torch.Tensor.detach_") [`dim()`](generated/torch.Tensor.dim.html#torch.Tensor.dim "torch.Tensor.dim") [`div()`](generated/torch.Tensor.div.html#torch.Tensor.div "torch.Tensor.div") [`div_()`](generated/torch.Tensor.div_.html#torch.Tensor.div_ "torch.Tensor.div_") [`floor_divide()`](generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide "torch.Tensor.floor_divide") [`floor_divide_()`](generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_ "torch.Tensor.floor_divide_") [`get_device()`](generated/torch.Tensor.get_device.html#torch.Tensor.get_device "torch.Tensor.get_device") [`index_select()`](generated/torch.Tensor.index_select.html#torch.Tensor.index_select "torch.Tensor.index_select") [`isnan()`](generated/torch.Tensor.isnan.html#torch.Tensor.isnan "torch.Tensor.isnan") [`log1p()`](generated/torch.Tensor.log1p.html#torch.Tensor.log1p "torch.Tensor.log1p") [`log1p_()`](generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_ "torch.Tensor.log1p_") [`mm()`](generated/torch.Tensor.mm.html#torch.Tensor.mm "torch.Tensor.mm") [`mul()`](generated/torch.Tensor.mul.html#torch.Tensor.mul "torch.Tensor.mul") [`mul_()`](generated/torch.Tensor.mul_.html#torch.Tensor.mul_ "torch.Tensor.mul_") [`mv()`](generated/torch.Tensor.mv.html#torch.Tensor.mv "torch.Tensor.mv") [`narrow_copy()`](generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy "torch.Tensor.narrow_copy") [`neg()`](generated/torch.Tensor.neg.html#torch.Tensor.neg "torch.Tensor.neg") [`neg_()`](generated/torch.Tensor.neg_.html#torch.Tensor.neg_ "torch.Tensor.neg_") [`negative()`](generated/torch.Tensor.negative.html#torch.Tensor.negative "torch.Tensor.negative") [`negative_()`](generated/torch.Tensor.negative_.html#torch.Tensor.negative_ "torch.Tensor.negative_") [`numel()`](generated/torch.Tensor.numel.html#torch.Tensor.numel "torch.Tensor.numel") [`rad2deg()`](generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg "torch.Tensor.rad2deg") `rad2deg_()` [`resize_as_()`](generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_ "torch.Tensor.resize_as_") [`size()`](generated/torch.Tensor.size.html#torch.Tensor.size "torch.Tensor.size") [`pow()`](generated/torch.Tensor.pow.html#torch.Tensor.pow "torch.Tensor.pow") [`sqrt()`](generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt "torch.Tensor.sqrt") [`square()`](generated/torch.Tensor.square.html#torch.Tensor.square "torch.Tensor.square") [`smm()`](generated/torch.Tensor.smm.html#torch.Tensor.smm "torch.Tensor.smm") [`sspaddmm()`](generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm "torch.Tensor.sspaddmm") [`sub()`](generated/torch.Tensor.sub.html#torch.Tensor.sub "torch.Tensor.sub") [`sub_()`](generated/torch.Tensor.sub_.html#torch.Tensor.sub_ "torch.Tensor.sub_") [`t()`](generated/torch.Tensor.t.html#torch.Tensor.t "torch.Tensor.t") [`t_()`](generated/torch.Tensor.t_.html#torch.Tensor.t_ "torch.Tensor.t_") [`transpose()`](generated/torch.Tensor.transpose.html#torch.Tensor.transpose "torch.Tensor.transpose") [`transpose_()`](generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_ "torch.Tensor.transpose_") [`zero_()`](generated/torch.Tensor.zero_.html#torch.Tensor.zero_ "torch.Tensor.zero_")

### 针对稀疏张量的Torch函数[](#torch-functions-specific-to-sparse-tensors "跳转到此标题")

| [`sparse_coo_tensor`](generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor "torch.sparse_coo_tensor") | 使用给定的`indices`构造一个[COO（坐标）格式的稀疏张量](#sparse-coo-docs)，其中包含指定的值。 |
| --- | --- |
| [`sparse_csr_tensor`](generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor "torch.sparse_csr_tensor") | 使用给定的`crow_indices`和`col_indices`构造一个[CSR（压缩稀疏行）格式的稀疏张量](#sparse-csr-docs)，其中包含指定的值。 |
| [`sparse_csc_tensor`](generated/torch.sparse_csc_tensor.html#torch.sparse_csc_tensor "torch.sparse_csc_tensor") | 使用给定的`ccol_indices`和`row_indices`构造一个[CSC（压缩稀疏列）格式的稀疏张量](#sparse-csc-docs)，其中包含指定的值。 |
| [`sparse_bsr_tensor`](generated/torch.sparse_bsr_tensor.html#torch.sparse_bsr_tensor "torch.sparse_bsr_tensor") | 使用给定的`crow_indices`和`col_indices`构造一个[BSR（块压缩稀疏行）格式的稀疏张量](#sparse-bsr-docs)，其中包含指定的二维块。 |
| [`sparse_bsc_tensor`](generated/torch.sparse_bsc_tensor.html#torch.sparse_bsc_tensor "torch.sparse_bsc_tensor") | 使用给定的`ccol_indices`和`row_indices`构造一个[BSC（块压缩稀疏列）格式的稀疏张量](#sparse-bsc-docs)，其中包含指定的二维块。 |
| [`sparse_compressed_tensor`](generated/torch.sparse_compressed_tensor.html#torch.sparse_compressed_tensor "torch.sparse_compressed_tensor") | 使用给定的`compressed_indices`和`plain_indices`构造一个[压缩稀疏格式的张量 - CSR、CSC、BSR或BSC -](#sparse-compressed-docs)，其中包含指定的值。 |
| [`sparse.sum`](generated/torch.sparse.sum.html#torch.sparse.sum "torch.sparse.sum") | 返回给定稀疏张量每行的和。 |
| [`sparse.addmm`](generated/torch.sparse.addmm.html#torch.sparse.addmm "torch.sparse.addmm") | 此函数在前向传播中与[`torch.addmm()`](generated/torch.addmm.html#torch.addmm "torch.addmm")完全相同，只是它支持稀疏COO矩阵 `mat1` 的反向传播。 |
| [`sparse.sampled_addmm`](generated/torch.sparse.sampled_addmm.html#torch.sparse.sampled_addmm "torch.sparse.sampled_addmm") | 在`input`的稀疏模式指定的位置上，对密集矩阵`mat1`和`mat2`执行矩阵乘法。 |
| [`sparse.mm`](generated/torch.sparse.mm.html#torch.sparse.mm "torch.sparse.mm") | 对稀疏矩阵 `mat1` 执行矩阵乘法。 |
| [`sspaddmm`](generated/torch.sspaddmm.html#torch.sspaddmm "torch.sspaddmm") | 将稀疏张量 `mat1` 与密集张量 `mat2` 相乘，然后将稀疏张量 `input` 加到结果中。 |
| [`hspmm`](generated/torch.hspmm.html#torch.hspmm "torch.hspmm") | 对一个[稀疏COO矩阵](#sparse-coo-docs) `mat1` 和一个分块矩阵 `mat2` 执行矩阵乘法。 |
| [`smm`](generated/torch.smm.html#torch.smm "torch.smm") | 对稀疏矩阵 `input` 和密集矩阵 `mat` 执行矩阵乘法。 |
| [`sparse.softmax`](generated/torch.sparse.softmax.html#torch.sparse.softmax "torch.sparse.softmax") | 应用softmax函数。 |
| [`sparse.log_softmax`](generated/torch.sparse.log_softmax.html#torch.sparse.log_softmax "torch.sparse.log_softmax") | 应用softmax函数后跟对数函数。 |
| [`sparse.spdiags`](generated/torch.sparse.spdiags.html#torch.sparse.spdiags "torch.sparse.spdiags") | 通过将`diagonals`的行值沿输出的指定对角线放置，创建一个稀疏的二维张量。 |

### 其他函数

以下[`torch`](torch.html#module-torch "torch")函数支持稀疏张量：

[`cat()`](generated/torch.cat.html#torch.cat "torch.cat") [`dstack()`](generated/torch.dstack.html#torch.dstack "torch.dstack") [`empty()`](generated/torch.empty.html#torch.empty "torch.empty") [`empty_like()`](generated/torch.empty_like.html#torch.empty_like "torch.empty_like") [`hstack()`](generated/torch.hstack.html#torch.hstack "torch.hstack") [`index_select()`](generated/torch.index_select.html#torch.index_select "torch.index_select") [`is_complex()`](generated/torch.is_complex.html#torch.is_complex "torch.is_complex") [`is_floating_point()`](generated/torch.is_floating_point.html#torch.is_floating_point "torch.is_floating_point") [`is_nonzero()`](generated/torch.is_nonzero.html#torch.is_nonzero "torch.is_nonzero") `is_same_size()` `is_signed()` [`is_tensor()`](generated/torch.is_tensor.html#torch.is_tensor "torch.is_tensor") [`lobpcg()`](generated/torch.lobpcg.html#torch.lobpcg "torch.lobpcg") [`mm()`](generated/torch.mm.html#torch.mm "torch.mm") `native_norm()` [`pca_lowrank()`](generated/torch.pca_lowrank.html#torch.pca_lowrank "torch.pca_lowrank") [`select()`](generated/torch.select.html#torch.select "torch.select") [`stack()`](generated/torch.stack.html#torch.stack "torch.stack") [`svd_lowrank()`](generated/torch.svd_lowrank.html#torch.svd_lowrank "torch.svd_lowrank") [`unsqueeze()`](generated/torch.unsqueeze.html#torch.unsqueeze "torch.unsqueeze") [`vstack()`](generated/torch.vstack.html#torch.vstack "torch.vstack") [`zeros()`](generated/torch.zeros.html#torch.zeros "torch.zeros") [`zeros_like()`](generated/torch.zeros_like.html#torch.zeros_like "torch.zeros_like")

要管理检查稀疏张量不变性，请参见：

| [`sparse.check_sparse_tensor_invariants`](generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants "torch.sparse.check_sparse_tensor_invariants") | 用于控制检查稀疏张量不变性的工具。 |
| --- | --- |

要在[`gradcheck()`](autograd.html#module-torch.autograd.gradcheck "torch.autograd.gradcheck")函数中使用稀疏张量，请参见：

| [`sparse.as_sparse_gradcheck`](generated/torch.sparse.as_sparse_gradcheck.html#torch.sparse.as_sparse_gradcheck "torch.sparse.as_sparse_gradcheck") | 装饰函数，用于扩展稀疏张量的 gradcheck。 |
| --- | --- |

### 一元函数

我们的目标是支持所有保持零值的一元函数。

如果您发现我们缺少您需要的保持零值的一元函数，请随时鼓励您为功能请求打开问题。在打开问题之前，请务必先尝试搜索功能。

以下运算符目前支持稀疏 COO/CSR/CSC/BSR/CSR 张量输入。

[`abs()`](generated/torch.abs.html#torch.abs "torch.abs") [`asin()`](generated/torch.asin.html#torch.asin "torch.asin") [`asinh()`](generated/torch.asinh.html#torch.asinh "torch.asinh") [`atan()`](generated/torch.atan.html#torch.atan "torch.atan") [`atanh()`](generated/torch.atanh.html#torch.atanh "torch.atanh") [`ceil()`](generated/torch.ceil.html#torch.ceil "torch.ceil") [`conj_physical()`](generated/torch.conj_physical.html#torch.conj_physical "torch.conj_physical") [`floor()`](generated/torch.floor.html#torch.floor "torch.floor") [`log1p()`](generated/torch.log1p.html#torch.log1p "torch.log1p") [`neg()`](generated/torch.neg.html#torch.neg "torch.neg") [`round()`](generated/torch.round.html#torch.round "torch.round") [`sin()`](generated/torch.sin.html#torch.sin "torch.sin") [`sinh()`](generated/torch.sinh.html#torch.sinh "torch.sinh") [`sign()`](generated/torch.sign.html#torch.sign "torch.sign") [`sgn()`](generated/torch.sgn.html#torch.sgn "torch.sgn") [`signbit()`](generated/torch.signbit.html#torch.signbit "torch.signbit") [`tan()`](generated/torch.tan.html#torch.tan "torch.tan") [`tanh()`](generated/torch.tanh.html#torch.tanh "torch.tanh") [`trunc()`](generated/torch.trunc.html#torch.trunc "torch.trunc") [`expm1()`](generated/torch.expm1.html#torch.expm1 "torch.expm1") [`sqrt()`](generated/torch.sqrt.html#torch.sqrt "torch.sqrt") [`angle()`](generated/torch.angle.html#torch.angle "torch.angle") [`isinf()`](generated/torch.isinf.html#torch.isinf "torch.isinf") [`isposinf()`](generated/torch.isposinf.html#torch.isposinf "torch.isposinf") [`isneginf()`](generated/torch.isneginf.html#torch.isneginf "torch.isneginf") [`isnan()`](generated/torch.isnan.html#torch.isnan "torch.isnan") [`erf()`](generated/torch.erf.html#torch.erf "torch.erf") [`erfinv()`](generated/torch.erfinv.html#torch.erfinv "torch.erfinv")
