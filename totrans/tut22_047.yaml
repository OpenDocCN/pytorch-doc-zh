- en: Language Modeling with nn.Transformer and torchtext
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用nn.Transformer和torchtext进行语言建模
- en: 原文：[https://pytorch.org/tutorials/beginner/transformer_tutorial.html](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/beginner/transformer_tutorial.html](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-beginner-transformer-tutorial-py) to download
    the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-beginner-transformer-tutorial-py)下载完整示例代码
- en: This is a tutorial on training a model to predict the next word in a sequence
    using the [nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)
    module.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于使用[nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)模块训练模型以预测序列中下一个单词的教程。
- en: The PyTorch 1.2 release includes a standard transformer module based on the
    paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf). Compared
    to Recurrent Neural Networks (RNNs), the transformer model has proven to be superior
    in quality for many sequence-to-sequence tasks while being more parallelizable.
    The `nn.Transformer` module relies entirely on an attention mechanism (implemented
    as [nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html))
    to draw global dependencies between input and output. The `nn.Transformer` module
    is highly modularized such that a single component (e.g., [nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html))
    can be easily adapted/composed.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 1.2版本包含了一个基于论文[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)的标准transformer模块。与循环神经网络（RNNs）相比，transformer模型在许多序列到序列任务中已被证明质量更优，同时更易并行化。`nn.Transformer`模块完全依赖于注意力机制（实现为[nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)）来绘制输入和输出之间的全局依赖关系。`nn.Transformer`模块高度模块化，使得单个组件（例如[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)）可以轻松适应/组合。
- en: '![../_images/transformer_architecture.jpg](../Images/4b79dddf1ff54b9384754144d8246d9b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/transformer_architecture.jpg](../Images/4b79dddf1ff54b9384754144d8246d9b.png)'
- en: Define the model
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义模型
- en: In this tutorial, we train a `nn.TransformerEncoder` model on a causal language
    modeling task. Please note that this tutorial does not cover the training of [nn.TransformerDecoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder),
    as depicted in the right half of the diagram above. The language modeling task
    is to assign a probability for the likelihood of a given word (or a sequence of
    words) to follow a sequence of words. A sequence of tokens are passed to the embedding
    layer first, followed by a positional encoding layer to account for the order
    of the word (see the next paragraph for more details). The `nn.TransformerEncoder`
    consists of multiple layers of [nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html).
    Along with the input sequence, a square attention mask is required because the
    self-attention layers in `nn.TransformerDecoder` are only allowed to attend the
    earlier positions in the sequence. For the language modeling task, any tokens
    on the future positions should be masked. This masking, combined with fact that
    the output embeddings are offset with later positions ensures that the predictions
    for position i can depend only on the known outputs at positions less than i.
    To produce a probability distribution over output words, the output of the `nn.TransformerEncoder`
    model is passed through a linear layer to output unnormalized logits. The log-softmax
    function isn’t applied here due to the later use of [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html),
    which requires the inputs to be unnormalized logits.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们在因果语言建模任务上训练一个`nn.TransformerEncoder`模型。请注意，本教程不涵盖[nn.TransformerDecoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder)的训练，如上图右半部所示。语言建模任务是为给定单词（或一系列单词）在一系列单词后出现的概率分配一个概率。一系列标记首先传递到嵌入层，然后是一个位置编码层，以考虑单词的顺序（有关更多细节，请参见下一段）。`nn.TransformerEncoder`由多个[nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)层组成。除了输入序列外，还需要一个方形的注意力掩码，因为`nn.TransformerDecoder`中的自注意力层只能关注序列中较早的位置。对于语言建模任务，未来位置上的任何标记都应该被屏蔽。这种屏蔽，加上输出嵌入与后续位置的偏移，确保位置i的预测仅依赖于位置小于i的已知输出。为了生成输出单词的概率分布，`nn.TransformerEncoder`模型的输出通过一个线性层传递以输出未归一化的logits。这里不应用log-softmax函数，因为后续使用[CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)需要输入为未归一化的logits。
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`PositionalEncoding` module injects some information about the relative or
    absolute position of the tokens in the sequence. The positional encodings have
    the same dimension as the embeddings so that the two can be summed. Here, we use
    `sine` and `cosine` functions of different frequencies.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`PositionalEncoding`模块向序列中的标记注入了一些关于相对或绝对位置的信息。位置编码与嵌入的维度相同，因此两者可以相加。在这里，我们使用不同频率的`sine`和`cosine`函数。'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load and batch data
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和批处理数据
- en: This tutorial uses `torchtext` to generate Wikitext-2 dataset. To access torchtext
    datasets, please install torchdata following instructions at [https://github.com/pytorch/data](https://github.com/pytorch/data).
    %%
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程使用`torchtext`生成Wikitext-2数据集。要访问torchtext数据集，请按照[https://github.com/pytorch/data](https://github.com/pytorch/data)上的说明安装torchdata。%%
- en: '[PRE2]'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The vocab object is built based on the train dataset and is used to numericalize
    tokens into tensors. Wikitext-2 represents rare tokens as <unk>.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: vocab对象是基于训练数据集构建的，并用于将标记数值化为张量。Wikitext-2将稀有标记表示为<unk>。
- en: Given a 1-D vector of sequential data, `batchify()` arranges the data into `batch_size`
    columns. If the data does not divide evenly into `batch_size` columns, then the
    data is trimmed to fit. For instance, with the alphabet as the data (total length
    of 26) and `batch_size=4`, we would divide the alphabet into sequences of length
    6, resulting in 4 of such sequences.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个一维顺序数据向量，`batchify()`将数据排列成`batch_size`列。如果数据不能完全分成`batch_size`列，则将数据修剪以适应。例如，对于字母表作为数据（总长度为26）和`batch_size=4`，我们将字母表分成长度为6的序列，结果为4个这样的序列。
- en: \[\begin{bmatrix} \text{A} & \text{B} & \text{C} & \ldots & \text{X} & \text{Y}
    & \text{Z} \end{bmatrix} \Rightarrow \begin{bmatrix} \begin{bmatrix}\text{A} \\
    \text{B} \\ \text{C} \\ \text{D} \\ \text{E} \\ \text{F}\end{bmatrix} & \begin{bmatrix}\text{G}
    \\ \text{H} \\ \text{I} \\ \text{J} \\ \text{K} \\ \text{L}\end{bmatrix} & \begin{bmatrix}\text{M}
    \\ \text{N} \\ \text{O} \\ \text{P} \\ \text{Q} \\ \text{R}\end{bmatrix} & \begin{bmatrix}\text{S}
    \\ \text{T} \\ \text{U} \\ \text{V} \\ \text{W} \\ \text{X}\end{bmatrix} \end{bmatrix}
    \]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{bmatrix} \text{A} & \text{B} & \text{C} & \ldots & \text{X} & \text{Y}
    & \text{Z} \end{bmatrix} \Rightarrow \begin{bmatrix} \begin{bmatrix}\text{A} \\
    \text{B} \\ \text{C} \\ \text{D} \\ \text{E} \\ \text{F}\end{bmatrix} & \begin{bmatrix}\text{G}
    \\ \text{H} \\ \text{I} \\ \text{J} \\ \text{K} \\ \text{L}\end{bmatrix} & \begin{bmatrix}\text{M}
    \\ \text{N} \\ \text{O} \\ \text{P} \\ \text{Q} \\ \text{R}\end{bmatrix} & \begin{bmatrix}\text{S}
    \\ \text{T} \\ \text{U} \\ \text{V} \\ \text{W} \\ \text{X}\end{bmatrix} \end{bmatrix}
    \]
- en: Batching enables more parallelizable processing. However, batching means that
    the model treats each column independently; for example, the dependence of `G`
    and `F` can not be learned in the example above.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理使处理更具并行性。但是，批处理意味着模型独立处理每一列；例如，上面示例中`G`和`F`的依赖关系无法学习。
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Functions to generate input and target sequence
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成输入和目标序列的函数
- en: '`get_batch()` generates a pair of input-target sequences for the transformer
    model. It subdivides the source data into chunks of length `bptt`. For the language
    modeling task, the model needs the following words as `Target`. For example, with
    a `bptt` value of 2, we’d get the following two Variables for `i` = 0:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_batch()`为transformer模型生成一对输入-目标序列。它将源数据细分为长度为`bptt`的块。对于语言建模任务，模型需要以下单词作为`Target`。例如，对于`bptt`值为2，我们会得到`i`=0时的以下两个变量：'
- en: '![../_images/transformer_input_target.png](../Images/20ef8681366b44461cf49d1ab98ab8f2.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/transformer_input_target.png](../Images/20ef8681366b44461cf49d1ab98ab8f2.png)'
- en: It should be noted that the chunks are along dimension 0, consistent with the
    `S` dimension in the Transformer model. The batch dimension `N` is along dimension
    1.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，分块沿着维度0，与Transformer模型中的`S`维度一致。批处理维度`N`沿着维度1。
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Initiate an instance
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化一个实例
- en: The model hyperparameters are defined below. The `vocab` size is equal to the
    length of the vocab object.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 模型超参数如下所定义。`vocab`大小等于词汇对象的长度。
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Run the model
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行模型
- en: We use [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
    with the [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)
    (stochastic gradient descent) optimizer. The learning rate is initially set to
    5.0 and follows a [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)
    schedule. During training, we use [nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)
    to prevent gradients from exploding.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用[CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)和[SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)（随机梯度下降）优化器。学习率最初设置为5.0，并遵循[StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)调度。在训练过程中，我们使用[nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)来防止梯度爆炸。
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Loop over epochs. Save the model if the validation loss is the best we’ve seen
    so far. Adjust the learning rate after each epoch.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 循环遍历每个epoch。如果验证损失是迄今为止最佳的，则保存模型。每个epoch后调整学习率。
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Evaluate the best model on the test dataset
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试数据集上评估最佳模型
- en: '[PRE10]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Total running time of the script:** ( 4 minutes 31.006 seconds)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（4分钟31.006秒）'
- en: '[`Download Python source code: transformer_tutorial.py`](../_downloads/aa3898eb04d468790e00cb42405b1c23/transformer_tutorial.py)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：transformer_tutorial.py`](../_downloads/aa3898eb04d468790e00cb42405b1c23/transformer_tutorial.py)'
- en: '[`Download Jupyter notebook: transformer_tutorial.ipynb`](../_downloads/9cf2d4ead514e661e20d2070c9bf7324/transformer_tutorial.ipynb)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：transformer_tutorial.ipynb`](../_downloads/9cf2d4ead514e661e20d2070c9bf7324/transformer_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)'
