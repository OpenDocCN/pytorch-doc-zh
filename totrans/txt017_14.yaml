- en: torchtext.models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torchtext.models
- en: 原文：[https://pytorch.org/text/stable/models.html](https://pytorch.org/text/stable/models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/text/stable/models.html](https://pytorch.org/text/stable/models.html)'
- en: '## RobertaBundle[](#robertabundle "Permalink to this heading")'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '## RobertaBundle[](#robertabundle "跳转到此标题")'
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Example - Pretrained base xlmr encoder
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 - 预训练基本xlmr编码器
- en: '[PRE1]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Example - Pretrained large xlmr encoder attached to un-initialized classification
    head
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 - 预训练的大型xlmr编码器附加到未初始化的分类头部
- en: '[PRE2]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Example - User-specified configuration and checkpoint
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 - 用户指定的配置和检查点
- en: '[PRE3]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Parameters:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**head** (*nn.Module*) – A module to be attached to the encoder to perform
    specific task. If provided, it will replace the default member head (Default:
    `None`)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**head**（*nn.Module*）- 一个要附加到编码器上以执行特定任务的模块。如果提供，它将替换默认成员头（默认值：`None`）'
- en: '**load_weights** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Indicates whether or not to load weights if available.
    (Default: `True`)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**load_weights**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在Python v3.12中)")）- 指示是否加载可用的权重。（默认值：`True`）'
- en: '**freeze_encoder** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Indicates whether or not to freeze the encoder weights.
    (Default: `False`)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**freeze_encoder**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在Python v3.12中)")）- 指示是否冻结编码器权重。（默认值：`False`）'
- en: '**dl_kwargs** (*dictionary of keyword arguments*) – Passed to [`torch.hub.load_state_dict_from_url()`](https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url
    "(in PyTorch v2.1)"). (Default: `None`)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dl_kwargs**（*关键字参数的字典*）- 传递给[`torch.hub.load_state_dict_from_url()`](https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url
    "(在PyTorch v2.1中)")。（默认值：`None`）'
- en: XLMR_BASE_ENCODER[](#xlmr-base-encoder "Permalink to this heading")
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLMR_BASE_ENCODER[](#xlmr-base-encoder "跳转到此标题")
- en: '[PRE5]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: XLM-R Encoder with Base configuration
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 带有基本配置的XLM-R编码器
- en: The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation
    Learning at Scale <https://arxiv.org/abs/1911.02116>. It is a large multi-lingual
    language model, trained on 2.5TB of filtered CommonCrawl data and based on the
    RoBERTa model architecture.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: XLM-RoBERTa模型是在规模上进行无监督跨语言表示学习提出的<https://arxiv.org/abs/1911.02116>。它是一个大型多语言语言模型，训练于2.5TB的经过筛选的CommonCrawl数据，并基于RoBERTa模型架构。
- en: Originally published by the authors of XLM-RoBERTa under MIT License and redistributed
    with the same license. [[License](https://github.com/pytorch/fairseq/blob/main/LICENSE),
    [Source](https://github.com/pytorch/fairseq/tree/main/examples/xlmr#pre-trained-models)]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最初由XLM-RoBERTa的作者根据MIT许可证发布，并以相同的许可证重新分发。[[许可证](https://github.com/pytorch/fairseq/blob/main/LICENSE)，[来源](https://github.com/pytorch/fairseq/tree/main/examples/xlmr#pre-trained-models)]
- en: Please refer to [`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle
    "torchtext.models.RobertaBundle") for the usage.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle "torchtext.models.RobertaBundle")以获取用法。
- en: XLMR_LARGE_ENCODER[](#xlmr-large-encoder "Permalink to this heading")
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLMR_LARGE_ENCODER[](#xlmr-large-encoder "跳转到此标题")
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: XLM-R Encoder with Large configuration
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 带有大型配置的XLM-R编码器
- en: The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation
    Learning at Scale <https://arxiv.org/abs/1911.02116>. It is a large multi-lingual
    language model, trained on 2.5TB of filtered CommonCrawl data and based on the
    RoBERTa model architecture.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: XLM-RoBERTa模型是在规模上进行无监督跨语言表示学习提出的<https://arxiv.org/abs/1911.02116>。它是一个大型多语言语言模型，训练于2.5TB的经过筛选的CommonCrawl数据，并基于RoBERTa模型架构。
- en: Originally published by the authors of XLM-RoBERTa under MIT License and redistributed
    with the same license. [[License](https://github.com/pytorch/fairseq/blob/main/LICENSE),
    [Source](https://github.com/pytorch/fairseq/tree/main/examples/xlmr#pre-trained-models)]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最初由XLM-RoBERTa的作者根据MIT许可证发布，并以相同的许可证重新分发。[[许可证](https://github.com/pytorch/fairseq/blob/main/LICENSE)，[来源](https://github.com/pytorch/fairseq/tree/main/examples/xlmr#pre-trained-models)]
- en: Please refer to [`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle
    "torchtext.models.RobertaBundle") for the usage.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle "torchtext.models.RobertaBundle")以获取用法。
- en: ROBERTA_BASE_ENCODER[](#roberta-base-encoder "Permalink to this heading")
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROBERTA_BASE_ENCODER[](#roberta-base-encoder "跳转到此标题")
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Roberta Encoder with Base configuration
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 带有基本配置的Roberta编码器
- en: RoBERTa iterates on BERT’s pretraining procedure, including training the model
    longer, with bigger batches over more data; removing the next sentence prediction
    objective; training on longer sequences; and dynamically changing the masking
    pattern applied to the training data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa在BERT的预训练过程中进行迭代，包括更长时间地训练模型，使用更大的批次处理更多数据；移除下一个句子预测目标；在更长的序列上进行训练；并动态地改变应用于训练数据的掩码模式。
- en: 'The RoBERTa model was pretrained on the reunion of five datasets: BookCorpus,
    English Wikipedia, CC-News, OpenWebText, and STORIES. Together theses datasets
    contain over a 160GB of text.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa模型是在五个数据集的汇总上进行预训练的：BookCorpus，英文维基百科，CC-News，OpenWebText和STORIES。这些数据集一起包含超过160GB的文本。
- en: Originally published by the authors of RoBERTa under MIT License and redistributed
    with the same license. [[License](https://github.com/pytorch/fairseq/blob/main/LICENSE),
    [Source](https://github.com/pytorch/fairseq/tree/main/examples/roberta#pre-trained-models)]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最初由RoBERTa的作者根据MIT许可证发布，并以相同的许可证重新分发。[[许可证](https://github.com/pytorch/fairseq/blob/main/LICENSE)，[来源](https://github.com/pytorch/fairseq/tree/main/examples/roberta#pre-trained-models)]
- en: Please refer to [`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle
    "torchtext.models.RobertaBundle") for the usage.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle "torchtext.models.RobertaBundle")以获取用法。
- en: ROBERTA_LARGE_ENCODER[](#roberta-large-encoder "Permalink to this heading")
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROBERTA_LARGE_ENCODER[](#roberta-large-encoder "跳转到此标题")
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Roberta Encoder with Large configuration
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 带有大型配置的Roberta编码器
- en: RoBERTa iterates on BERT’s pretraining procedure, including training the model
    longer, with bigger batches over more data; removing the next sentence prediction
    objective; training on longer sequences; and dynamically changing the masking
    pattern applied to the training data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa在BERT的预训练过程上进行了迭代，包括更长时间地训练模型，使用更大的批次处理更多数据；移除下一个句子预测目标；在更长的序列上进行训练；以及动态地改变应用于训练数据的掩码模式。
- en: 'The RoBERTa model was pretrained on the reunion of five datasets: BookCorpus,
    English Wikipedia, CC-News, OpenWebText, and STORIES. Together theses datasets
    contain over a 160GB of text.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa模型是在五个数据集的基础上进行预训练的：BookCorpus、英文维基百科、CC-News、OpenWebText和STORIES。这些数据集总共包含超过160GB的文本。
- en: Originally published by the authors of RoBERTa under MIT License and redistributed
    with the same license. [[License](https://github.com/pytorch/fairseq/blob/main/LICENSE),
    [Source](https://github.com/pytorch/fairseq/tree/main/examples/roberta#pre-trained-models)]
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最初由RoBERTa的作者在MIT许可下发布，并以相同许可重新分发。[[许可证](https://github.com/pytorch/fairseq/blob/main/LICENSE),
    [来源](https://github.com/pytorch/fairseq/tree/main/examples/roberta#pre-trained-models)]
- en: Please refer to [`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle
    "torchtext.models.RobertaBundle") for the usage.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle "torchtext.models.RobertaBundle")进行使用。
