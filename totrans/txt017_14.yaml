- en: torchtext.models¶
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/text/stable/models.html](https://pytorch.org/text/stable/models.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '## RobertaBundle[¶](#robertabundle "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Example - Pretrained base xlmr encoder
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Example - Pretrained large xlmr encoder attached to un-initialized classification
    head
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Example - User-specified configuration and checkpoint
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**head** (*nn.Module*) – A module to be attached to the encoder to perform
    specific task. If provided, it will replace the default member head (Default:
    `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**load_weights** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Indicates whether or not to load weights if available.
    (Default: `True`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**freeze_encoder** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Indicates whether or not to freeze the encoder weights.
    (Default: `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dl_kwargs** (*dictionary of keyword arguments*) – Passed to [`torch.hub.load_state_dict_from_url()`](https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url
    "(in PyTorch v2.1)"). (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLMR_BASE_ENCODER[¶](#xlmr-base-encoder "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: XLM-R Encoder with Base configuration
  prefs: []
  type: TYPE_NORMAL
- en: The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation
    Learning at Scale <https://arxiv.org/abs/1911.02116>. It is a large multi-lingual
    language model, trained on 2.5TB of filtered CommonCrawl data and based on the
    RoBERTa model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Originally published by the authors of XLM-RoBERTa under MIT License and redistributed
    with the same license. [[License](https://github.com/pytorch/fairseq/blob/main/LICENSE),
    [Source](https://github.com/pytorch/fairseq/tree/main/examples/xlmr#pre-trained-models)]
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle
    "torchtext.models.RobertaBundle") for the usage.
  prefs: []
  type: TYPE_NORMAL
- en: XLMR_LARGE_ENCODER[¶](#xlmr-large-encoder "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: XLM-R Encoder with Large configuration
  prefs: []
  type: TYPE_NORMAL
- en: The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation
    Learning at Scale <https://arxiv.org/abs/1911.02116>. It is a large multi-lingual
    language model, trained on 2.5TB of filtered CommonCrawl data and based on the
    RoBERTa model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Originally published by the authors of XLM-RoBERTa under MIT License and redistributed
    with the same license. [[License](https://github.com/pytorch/fairseq/blob/main/LICENSE),
    [Source](https://github.com/pytorch/fairseq/tree/main/examples/xlmr#pre-trained-models)]
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle
    "torchtext.models.RobertaBundle") for the usage.
  prefs: []
  type: TYPE_NORMAL
- en: ROBERTA_BASE_ENCODER[¶](#roberta-base-encoder "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Roberta Encoder with Base configuration
  prefs: []
  type: TYPE_NORMAL
- en: RoBERTa iterates on BERT’s pretraining procedure, including training the model
    longer, with bigger batches over more data; removing the next sentence prediction
    objective; training on longer sequences; and dynamically changing the masking
    pattern applied to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RoBERTa model was pretrained on the reunion of five datasets: BookCorpus,
    English Wikipedia, CC-News, OpenWebText, and STORIES. Together theses datasets
    contain over a 160GB of text.'
  prefs: []
  type: TYPE_NORMAL
- en: Originally published by the authors of RoBERTa under MIT License and redistributed
    with the same license. [[License](https://github.com/pytorch/fairseq/blob/main/LICENSE),
    [Source](https://github.com/pytorch/fairseq/tree/main/examples/roberta#pre-trained-models)]
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle
    "torchtext.models.RobertaBundle") for the usage.
  prefs: []
  type: TYPE_NORMAL
- en: ROBERTA_LARGE_ENCODER[¶](#roberta-large-encoder "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Roberta Encoder with Large configuration
  prefs: []
  type: TYPE_NORMAL
- en: RoBERTa iterates on BERT’s pretraining procedure, including training the model
    longer, with bigger batches over more data; removing the next sentence prediction
    objective; training on longer sequences; and dynamically changing the masking
    pattern applied to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RoBERTa model was pretrained on the reunion of five datasets: BookCorpus,
    English Wikipedia, CC-News, OpenWebText, and STORIES. Together theses datasets
    contain over a 160GB of text.'
  prefs: []
  type: TYPE_NORMAL
- en: Originally published by the authors of RoBERTa under MIT License and redistributed
    with the same license. [[License](https://github.com/pytorch/fairseq/blob/main/LICENSE),
    [Source](https://github.com/pytorch/fairseq/tree/main/examples/roberta#pre-trained-models)]
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [`torchtext.models.RobertaBundle()`](#torchtext.models.RobertaBundle
    "torchtext.models.RobertaBundle") for the usage.
  prefs: []
  type: TYPE_NORMAL
