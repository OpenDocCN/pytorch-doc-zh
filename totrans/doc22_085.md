# torch.utils.checkpoint

> 原文：[https://pytorch.org/docs/stable/checkpoint.html](https://pytorch.org/docs/stable/checkpoint.html)

注意

检查点是通过在反向传播期间为每个检查点段重新运行一个前向传播段来实现的。这可能会导致持久状态（如RNG状态）比没有检查点更快地前进。默认情况下，检查点包含逻辑来调整RNG状态，使得使用RNG的检查点通过（例如通过dropout）与未检查点的通过相比具有确定性输出。存储和恢复RNG状态的逻辑可能会导致适度的性能损失，具体取决于检查点操作的运行时。如果不需要与未检查点的通过相比的确定性输出，请在`checkpoint`或`checkpoint_sequential`中提供`preserve_rng_state=False`以省略在每个检查点期间存储和恢复RNG状态。

存储逻辑会保存并恢复CPU和另一设备类型（通过`_infer_device_type`从张量参数中排除CPU张量推断设备类型）的RNG状态到`run_fn`。如果有多个设备，设备状态只会为单个设备类型的设备保存，其余设备将被忽略。因此，如果任何检查点函数涉及随机性，可能会导致梯度不正确。（请注意，如果检测到CUDA设备，则会优先考虑；否则，将选择遇到的第一个设备。）如果没有CPU张量，则将保存和恢复默认设备类型状态（默认值为cuda，可以通过`DefaultDeviceType`设置为其他设备）。然而，该逻辑无法预测用户是否会在`run_fn`内将张量移动到新设备。因此，如果您在`run_fn`内将张量移动到新设备（“新”意味着不属于[当前设备+张量参数的设备]集合），则与未检查点的通过相比，确定性输出永远无法保证。

```py
torch.utils.checkpoint.checkpoint(function, *args, use_reentrant=None, context_fn=<function noop_context_fn>, determinism_check='default', debug=False, **kwargs)¶
```

检查点一个模型或模型的一部分。

激活检查点是一种以计算换内存的技术。在检查点区域内，前向计算省略了为反向保留的张量，而是在反向传播期间重新计算它们。激活检查点可以应用于模型的任何部分。

目前有两种可用的检查点实现，由`use_reentrant`参数确定。建议您使用`use_reentrant=False`。请参考下面的注意事项以讨论它们之间的区别。

警告

如果在反向传播期间的`function`调用与正向传播不同，例如由于全局变量，那么被检查点化的版本可能不等价，可能会导致错误被引发或导致梯度错误地计算。

警告

如果您使用的是`use_reentrant=True`变体（这是当前的默认值），请参考下面的注意事项以获取重要考虑因素和潜在限制。

注意

检查点的可重入变体（`use_reentrant=True`）和非可重入变体（`use_reentrant=False`）在以下方面有所不同：

+   非可重入检查点在所有需要的中间激活重新计算后立即停止重新计算。此功能默认启用，但可以使用`set_checkpoint_early_stop()`禁用。可重入检查点在反向传播期间始终重新计算`function`的全部内容。

+   可重入变体在正向传播期间不记录自动求导图，因为它在[`torch.no_grad()`](generated/torch.no_grad.html#torch.no_grad "torch.no_grad")下运行正向传播。非可重入版本记录自动求导图，允许在检查点区域内对图进行反向传播。

+   可重入检查点仅支持后向传递的[`torch.autograd.backward()`](generated/torch.autograd.backward.html#torch.autograd.backward "torch.autograd.backward") API，不带其输入参数，而非可重入版本支持执行后向传递的所有方式。

+   至少一个输入和输出必须具有`requires_grad=True`，以便支持可重入变体。如果不满足此条件，模型的被检查部分将没有梯度。非可重入版本不具有此要求。

+   可重入版本不考虑嵌套结构中的张量（例如自定义对象、列表、字典等）是否参与自动梯度，而非可重入版本则考虑。

+   可重入检查点不支持从计算图中分离的张量的检查点区域，而非可重入版本支持。对于可重入变体，如果检查点段包含使用`detach()`分离的张量或使用[`torch.no_grad()`](generated/torch.no_grad.html#torch.no_grad "torch.no_grad")的张量，后向传递将引发错误。这是因为`checkpoint`使所有输出都需要梯度，当在模型中定义张量为无梯度时会出现问题。为了避免这种情况，在`checkpoint`函数之外分离张量。

参数

+   **function** - 描述模型或模型部分前向传递中要运行的内容。它还应该知道如何处理作为元组传递的输入。例如，在LSTM中，如果用户传递了`(activation, hidden)`，`function`应该正确地将第一个输入用作`activation`，第二个输入用作`hidden`

+   **preserve_rng_state** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")*,* *optional*) - 在每个检查点期间省略保存和恢复RNG状态。默认值：`True`

+   **use_reentrant** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")*,* *optional*) - 使用需要可重入自动梯度的检查点实现。如果指定了`use_reentrant=False`，`checkpoint`将使用不需要可重入自动梯度的实现。这允许`checkpoint`支持额外功能，例如与`torch.autograd.grad`正常工作以及支持输入到被检查函数的关键字参数。请注意，未来版本的PyTorch将默认为`use_reentrant=False`。默认值：`True`

+   **context_fn** (*Callable**,* *optional*) - 返回两个上下文管理器元组的可调用函数。函数及其重新计算将分别在第一个和第二个上下文管理器下运行。只有在`use_reentrant=False`时才支持此参数。

+   **determinism_check** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)")*,* *optional*) - 指定要执行的确定性检查的字符串。默认情况下设置为`"default"`，它会将重新计算的张量的形状、数据类型和设备与保存的张量进行比较。要关闭此检查，请指定`"none"`。目前这是唯一支持的两个值。如果您希望看到更多确定性检查，请提出问题。只有在`use_reentrant=False`时支持此参数，如果`use_reentrant=True`，则确定性检查始终被禁用。

+   **debug** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")*,* *optional*) - 如果为`True`，错误消息还将包括原始前向计算期间运行的操作的跟踪，以及重新计算。只有在`use_reentrant=False`时支持此参数。

+   **args** - 包含传递给`function`的输入的元组

返回

在`*args`上运行`function`的输出

```py
torch.utils.checkpoint.checkpoint_sequential(functions, segments, input, use_reentrant=None, **kwargs)¶
```

检查点一个顺序模型以节省内存。

顺序模型按顺序执行一系列模块/函数。因此，我们可以将这样的模型分成各个段，并对每个段进行检查点。除最后一个外，所有段都不会存储中间激活。每个检查点段的输入将被保存以便在反向传播中重新运行该段。

警告

如果您使用`use_reentrant=True`变体（这是默认值），请参阅：func:`~torch.utils.checkpoint.checkpoint`以获取此变体的重要注意事项和限制。建议您使用`use_reentrant=False`。

参数

+   **functions** - 一个[`torch.nn.Sequential`](generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential")或者要依次运行的模块或函数列表（组成模型）。

+   **segments** - 要在模型中创建的块数

+   **input** - 作为`functions`输入的张量

+   **preserve_rng_state**（[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")，可选） - 在每个检查点期间省略保存和恢复RNG状态。默认值：`True`

+   **use_reentrant**（[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")，可选） - 使用需要可重入自动求导的检查点实现。如果指定了`use_reentrant=False`，`checkpoint`将使用不需要可重入自动求导的实现。这允许`checkpoint`支持额外的功能，例如与`torch.autograd.grad`正常工作以及支持传递给检查点函数的关键字参数。默认值：`True`

返回值

在`*inputs`上依次运行`functions`的输出

示例

```py
>>> model = nn.Sequential(...)
>>> input_var = checkpoint_sequential(model, chunks, input_var) 
```

```py
torch.utils.checkpoint.set_checkpoint_debug_enabled(enabled)¶
```

上下文管理器，设置检查点在运行时是否应该打印额外的调试信息。有关更多信息，请参阅[`checkpoint()`](#torch.utils.checkpoint.checkpoint "torch.utils.checkpoint.checkpoint")中的`debug`标志。请注意，当设置时，此上下文管理器会覆盖传递给检查点的`debug`值。要推迟到本地设置，请将`None`传递给此上下文。

参数

**enabled**（[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")） - 检查点是否应该打印调试信息。默认值为'None'。
