- en: torch.optim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[`torch.optim`](#module-torch.optim "torch.optim") is a package implementing
    various optimization algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Most commonly used methods are already supported, and the interface is general
    enough, so that more sophisticated ones can also be easily integrated in the future.
  prefs: []
  type: TYPE_NORMAL
- en: How to use an optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use [`torch.optim`](#module-torch.optim "torch.optim") you have to construct
    an optimizer object that will hold the current state and will update the parameters
    based on the computed gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing it
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To construct an [`Optimizer`](#torch.optim.Optimizer "torch.optim.Optimizer")
    you have to give it an iterable containing the parameters (all should be `Variable`
    s) to optimize. Then, you can specify optimizer-specific options such as the learning
    rate, weight decay, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Per-parameter options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[`Optimizer`](#torch.optim.Optimizer "torch.optim.Optimizer") s also support
    specifying per-parameter options. To do this, instead of passing an iterable of
    `Variable` s, pass in an iterable of [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)") s. Each of them will define a separate parameter group, and
    should contain a `params` key, containing a list of parameters belonging to it.
    Other keys should match the keyword arguments accepted by the optimizers, and
    will be used as optimization options for this group.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can still pass options as keyword arguments. They will be used as defaults,
    in the groups that didn’t override them. This is useful when you only want to
    vary a single option, while keeping all others consistent between parameter groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, this is very useful when one wants to specify per-layer learning
    rates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This means that `model.base`’s parameters will use the default learning rate
    of `1e-2`, `model.classifier`’s parameters will use a learning rate of `1e-3`,
    and a momentum of `0.9` will be used for all parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Taking an optimization step[](#taking-an-optimization-step "Permalink to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All optimizers implement a [`step()`](generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step
    "torch.optim.Optimizer.step") method, that updates the parameters. It can be used
    in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '`optimizer.step()`'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is a simplified version supported by most optimizers. The function can
    be called once the gradients are computed using e.g. `backward()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`optimizer.step(closure)`'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate
    the function multiple times, so you have to pass in a closure that allows them
    to recompute your model. The closure should clear the gradients, compute the loss,
    and return it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '## Base class'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Base class for all optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Parameters need to be specified as collections that have a deterministic ordering
    that is consistent between runs. Examples of objects that don’t satisfy those
    properties are sets and iterators over values of dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**params** (*iterable*) – an iterable of [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor") s or [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)") s. Specifies what Tensors should be optimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**defaults** ([*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict
    "(in Python v3.12)")*[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* [*Any*](https://docs.python.org/3/library/typing.html#typing.Any
    "(in Python v3.12)")*]*) – (dict): a dict containing default values of optimization
    options (used when a parameter group doesn’t specify them).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [`Optimizer.add_param_group`](generated/torch.optim.Optimizer.add_param_group.html#torch.optim.Optimizer.add_param_group
    "torch.optim.Optimizer.add_param_group") | Add a param group to the [`Optimizer`](#torch.optim.Optimizer
    "torch.optim.Optimizer") s param_groups. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Optimizer.load_state_dict`](generated/torch.optim.Optimizer.load_state_dict.html#torch.optim.Optimizer.load_state_dict
    "torch.optim.Optimizer.load_state_dict") | Loads the optimizer state. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Optimizer.state_dict`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict") | Returns the state of the optimizer as a
    [`dict`](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)").
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Optimizer.step`](generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step
    "torch.optim.Optimizer.step") | Performs a single optimization step (parameter
    update). |'
  prefs: []
  type: TYPE_TB
- en: '| [`Optimizer.zero_grad`](generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad
    "torch.optim.Optimizer.zero_grad") | Resets the gradients of all optimized [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor") s. |'
  prefs: []
  type: TYPE_TB
- en: Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`Adadelta`](generated/torch.optim.Adadelta.html#torch.optim.Adadelta "torch.optim.Adadelta")
    | Implements Adadelta algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Adagrad`](generated/torch.optim.Adagrad.html#torch.optim.Adagrad "torch.optim.Adagrad")
    | Implements Adagrad algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Adam`](generated/torch.optim.Adam.html#torch.optim.Adam "torch.optim.Adam")
    | Implements Adam algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`AdamW`](generated/torch.optim.AdamW.html#torch.optim.AdamW "torch.optim.AdamW")
    | Implements AdamW algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`SparseAdam`](generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam
    "torch.optim.SparseAdam") | SparseAdam implements a masked version of the Adam
    algorithm suitable for sparse gradients. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Adamax`](generated/torch.optim.Adamax.html#torch.optim.Adamax "torch.optim.Adamax")
    | Implements Adamax algorithm (a variant of Adam based on infinity norm). |'
  prefs: []
  type: TYPE_TB
- en: '| [`ASGD`](generated/torch.optim.ASGD.html#torch.optim.ASGD "torch.optim.ASGD")
    | Implements Averaged Stochastic Gradient Descent. |'
  prefs: []
  type: TYPE_TB
- en: '| [`LBFGS`](generated/torch.optim.LBFGS.html#torch.optim.LBFGS "torch.optim.LBFGS")
    | Implements L-BFGS algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`NAdam`](generated/torch.optim.NAdam.html#torch.optim.NAdam "torch.optim.NAdam")
    | Implements NAdam algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`RAdam`](generated/torch.optim.RAdam.html#torch.optim.RAdam "torch.optim.RAdam")
    | Implements RAdam algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`RMSprop`](generated/torch.optim.RMSprop.html#torch.optim.RMSprop "torch.optim.RMSprop")
    | Implements RMSprop algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Rprop`](generated/torch.optim.Rprop.html#torch.optim.Rprop "torch.optim.Rprop")
    | Implements the resilient backpropagation algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`SGD`](generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD")
    | Implements stochastic gradient descent (optionally with momentum). |'
  prefs: []
  type: TYPE_TB
- en: Many of our algorithms have various implementations optimized for performance,
    readability and/or generality, so we attempt to default to the generally fastest
    implementation for the current device if no particular implementation has been
    specified by the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have 3 major categories of implementations: for-loop, foreach (multi-tensor),
    and fused. The most straightforward implementations are for-loops over the parameters
    with big chunks of computation. For-looping is usually slower than our foreach
    implementations, which combine parameters into a multi-tensor and run the big
    chunks of computation all at once, thereby saving many sequential kernel calls.
    A few of our optimizers have even faster fused implementations, which fuse the
    big chunks of computation into one kernel. We can think of foreach implementations
    as fusing horizontally and fused implementations as fusing vertically on top of
    that.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the performance ordering of the 3 implementations is fused > foreach
    > for-loop. So when applicable, we default to foreach over for-loop. Applicable
    means the foreach implementation is available, the user has not specified any
    implementation-specific kwargs (e.g., fused, foreach, differentiable), and all
    tensors are native and on CUDA. Note that while fused should be even faster than
    foreach, the implementations are newer and we would like to give them more bake-in
    time before flipping the switch everywhere. You are welcome to try them out though!
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is a table showing the available and default implementations of each
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Default | Has foreach? | Has fused? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`Adadelta`](generated/torch.optim.Adadelta.html#torch.optim.Adadelta "torch.optim.Adadelta")
    | foreach | yes | no |'
  prefs: []
  type: TYPE_TB
- en: '| [`Adagrad`](generated/torch.optim.Adagrad.html#torch.optim.Adagrad "torch.optim.Adagrad")
    | foreach | yes | no |'
  prefs: []
  type: TYPE_TB
- en: '| [`Adam`](generated/torch.optim.Adam.html#torch.optim.Adam "torch.optim.Adam")
    | foreach | yes | yes |'
  prefs: []
  type: TYPE_TB
- en: '| [`AdamW`](generated/torch.optim.AdamW.html#torch.optim.AdamW "torch.optim.AdamW")
    | foreach | yes | yes |'
  prefs: []
  type: TYPE_TB
- en: '| [`SparseAdam`](generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam
    "torch.optim.SparseAdam") | for-loop | no | no |'
  prefs: []
  type: TYPE_TB
- en: '| [`Adamax`](generated/torch.optim.Adamax.html#torch.optim.Adamax "torch.optim.Adamax")
    | foreach | yes | no |'
  prefs: []
  type: TYPE_TB
- en: '| [`ASGD`](generated/torch.optim.ASGD.html#torch.optim.ASGD "torch.optim.ASGD")
    | foreach | yes | no |'
  prefs: []
  type: TYPE_TB
- en: '| [`LBFGS`](generated/torch.optim.LBFGS.html#torch.optim.LBFGS "torch.optim.LBFGS")
    | for-loop | no | no |'
  prefs: []
  type: TYPE_TB
- en: '| [`NAdam`](generated/torch.optim.NAdam.html#torch.optim.NAdam "torch.optim.NAdam")
    | foreach | yes | no |'
  prefs: []
  type: TYPE_TB
- en: '| [`RAdam`](generated/torch.optim.RAdam.html#torch.optim.RAdam "torch.optim.RAdam")
    | foreach | yes | no |'
  prefs: []
  type: TYPE_TB
- en: '| [`RMSprop`](generated/torch.optim.RMSprop.html#torch.optim.RMSprop "torch.optim.RMSprop")
    | foreach | yes | no |'
  prefs: []
  type: TYPE_TB
- en: '| [`Rprop`](generated/torch.optim.Rprop.html#torch.optim.Rprop "torch.optim.Rprop")
    | foreach | yes | no |'
  prefs: []
  type: TYPE_TB
- en: '| [`SGD`](generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD")
    | foreach | yes | no |'
  prefs: []
  type: TYPE_TB
- en: How to adjust learning rate[](#how-to-adjust-learning-rate "Permalink to this
    heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`torch.optim.lr_scheduler`](#module-torch.optim.lr_scheduler "torch.optim.lr_scheduler")
    provides several methods to adjust the learning rate based on the number of epochs.
    [`torch.optim.lr_scheduler.ReduceLROnPlateau`](generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau
    "torch.optim.lr_scheduler.ReduceLROnPlateau") allows dynamic learning rate reducing
    based on some validation measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning rate scheduling should be applied after optimizer’s update; e.g.,
    you should write your code this way:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Most learning rate schedulers can be called back-to-back (also referred to as
    chaining schedulers). The result is that each scheduler is applied one after the
    other on the learning rate obtained by the one preceding it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In many places in the documentation, we will use the following template to refer
    to schedulers algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called
    before the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way.
    If you use the learning rate scheduler (calling `scheduler.step()`) before the
    optimizer’s update (calling `optimizer.step()`), this will skip the first value
    of the learning rate schedule. If you are unable to reproduce results after upgrading
    to PyTorch 1.1.0, please check if you are calling `scheduler.step()` at the wrong
    time.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`lr_scheduler.LambdaLR`](generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR
    "torch.optim.lr_scheduler.LambdaLR") | Sets the learning rate of each parameter
    group to the initial lr times a given function. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.MultiplicativeLR`](generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR
    "torch.optim.lr_scheduler.MultiplicativeLR") | Multiply the learning rate of each
    parameter group by the factor given in the specified function. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.StepLR`](generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR
    "torch.optim.lr_scheduler.StepLR") | Decays the learning rate of each parameter
    group by gamma every step_size epochs. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.MultiStepLR`](generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR
    "torch.optim.lr_scheduler.MultiStepLR") | Decays the learning rate of each parameter
    group by gamma once the number of epoch reaches one of the milestones. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.ConstantLR`](generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR
    "torch.optim.lr_scheduler.ConstantLR") | Decays the learning rate of each parameter
    group by a small constant factor until the number of epoch reaches a pre-defined
    milestone: total_iters. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.LinearLR`](generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR
    "torch.optim.lr_scheduler.LinearLR") | Decays the learning rate of each parameter
    group by linearly changing small multiplicative factor until the number of epoch
    reaches a pre-defined milestone: total_iters. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.ExponentialLR`](generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR
    "torch.optim.lr_scheduler.ExponentialLR") | Decays the learning rate of each parameter
    group by gamma every epoch. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.PolynomialLR`](generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR
    "torch.optim.lr_scheduler.PolynomialLR") | Decays the learning rate of each parameter
    group using a polynomial function in the given total_iters. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.CosineAnnealingLR`](generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR
    "torch.optim.lr_scheduler.CosineAnnealingLR") | Set the learning rate of each
    parameter group using a cosine annealing schedule, where <math><semantics><mrow><msub><mi>η</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">\eta_{max}</annotation></semantics></math>ηmax​ is
    set to the initial lr and <math><semantics><mrow><msub><mi>T</mi><mrow><mi>c</mi><mi>u</mi><mi>r</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">T_{cur}</annotation></semantics></math>Tcur​ is the
    number of epochs since the last restart in SGDR: |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.ChainedScheduler`](generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler
    "torch.optim.lr_scheduler.ChainedScheduler") | Chains list of learning rate schedulers.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.SequentialLR`](generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR
    "torch.optim.lr_scheduler.SequentialLR") | Receives the list of schedulers that
    is expected to be called sequentially during optimization process and milestone
    points that provides exact intervals to reflect which scheduler is supposed to
    be called at a given epoch. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.ReduceLROnPlateau`](generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau
    "torch.optim.lr_scheduler.ReduceLROnPlateau") | Reduce learning rate when a metric
    has stopped improving. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.CyclicLR`](generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR
    "torch.optim.lr_scheduler.CyclicLR") | Sets the learning rate of each parameter
    group according to cyclical learning rate policy (CLR). |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.OneCycleLR`](generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR
    "torch.optim.lr_scheduler.OneCycleLR") | Sets the learning rate of each parameter
    group according to the 1cycle learning rate policy. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lr_scheduler.CosineAnnealingWarmRestarts`](generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts") | Set the learning rate
    of each parameter group using a cosine annealing schedule, where <math><semantics><mrow><msub><mi>η</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">\eta_{max}</annotation></semantics></math>ηmax​ is
    set to the initial lr, <math><semantics><mrow><msub><mi>T</mi><mrow><mi>c</mi><mi>u</mi><mi>r</mi></mrow></msub></mrow><annotation
    encoding="application/x-tex">T_{cur}</annotation></semantics></math>Tcur​ is the
    number of epochs since the last restart and <math><semantics><mrow><msub><mi>T</mi><mi>i</mi></msub></mrow><annotation
    encoding="application/x-tex">T_{i}</annotation></semantics></math>Ti​ is the number
    of epochs between two warm restarts in SGDR: |'
  prefs: []
  type: TYPE_TB
- en: Weight Averaging (SWA and EMA)[](#weight-averaging-swa-and-ema "Permalink to
    this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`torch.optim.swa_utils`](#module-torch.optim.swa_utils "torch.optim.swa_utils")
    implements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA).
    In particular, the `torch.optim.swa_utils.AveragedModel` class implements SWA
    and EMA models, `torch.optim.swa_utils.SWALR` implements the SWA learning rate
    scheduler and `torch.optim.swa_utils.update_bn()` is a utility function used to
    update SWA/EMA batch normalization statistics at the end of training.'
  prefs: []
  type: TYPE_NORMAL
- en: SWA has been proposed in [Averaging Weights Leads to Wider Optima and Better
    Generalization](https://arxiv.org/abs/1803.05407).
  prefs: []
  type: TYPE_NORMAL
- en: EMA is a widely known technique to reduce the training time by reducing the
    number of weight updates needed. It is a variation of [Polyak averaging](https://paperswithcode.com/method/polyak-averaging),
    but using exponential weights instead of equal weights across iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing averaged models[](#constructing-averaged-models "Permalink to
    this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The AveragedModel class serves to compute the weights of the SWA or EMA model.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create an SWA averaged model by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'EMA models are constructed by specifying the `multi_avg_fn` argument as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Decay is a parameter between 0 and 1 that controls how fast the averaged parameters
    are decayed. If not provided to `get_ema_multi_avg_fn`, the default is 0.999.
  prefs: []
  type: TYPE_NORMAL
- en: '`get_ema_multi_avg_fn` returns a function that applies the following EMA equation
    to the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><semantics><mrow><msubsup><mi>W</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mtext>EMA</mtext></msubsup><mo>=</mo><mi>α</mi><msubsup><mi>W</mi><mi>t</mi><mtext>EMA</mtext></msubsup><mo>+</mo><mo
    stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="false">)</mo><msubsup><mi>W</mi><mi>t</mi><mtext>model</mtext></msubsup></mrow><annotation
    encoding="application/x-tex">W^\textrm{EMA}_{t+1} = \alpha W^\textrm{EMA}_{t}
    + (1 - \alpha) W^\textrm{model}_t</annotation></semantics></math> Wt+1EMA​=αWtEMA​+(1−α)Wtmodel​
  prefs: []
  type: TYPE_NORMAL
- en: where alpha is the EMA decay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here the model `model` can be an arbitrary [`torch.nn.Module`](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module") object. `averaged_model` will keep track of the running averages
    of the parameters of the `model`. To update these averages, you should use the
    `update_parameters()` function after the optimizer.step():'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For SWA and EMA, this call is usually done right after the optimizer `step()`.
    In the case of SWA, this is usually skipped for some numbers of steps at the beginning
    of the training.
  prefs: []
  type: TYPE_NORMAL
- en: Custom averaging strategies[](#custom-averaging-strategies "Permalink to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By default, `torch.optim.swa_utils.AveragedModel` computes a running equal
    average of the parameters that you provide, but you can also use custom averaging
    functions with the `avg_fn` or `multi_avg_fn` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`avg_fn` allows defining a function operating on each parameter tuple (averaged
    parameter, model parameter) and should return the new averaged parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multi_avg_fn` allows defining more efficient operations acting on a tuple
    of parameter lists, (averaged parameter list, model parameter list), at the same
    time, for example using the `torch._foreach*` functions. This function must update
    the averaged parameters in-place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example `ema_model` computes an exponential moving average
    using the `avg_fn` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following example `ema_model` computes an exponential moving average
    using the more efficient `multi_avg_fn` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: SWA learning rate schedules[](#swa-learning-rate-schedules "Permalink to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Typically, in SWA the learning rate is set to a high constant value. `SWALR`
    is a learning rate scheduler that anneals the learning rate to a fixed value,
    and then keeps it constant. For example, the following code creates a scheduler
    that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs
    within each parameter group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You can also use cosine annealing to a fixed value instead of linear annealing
    by setting `anneal_strategy="cos"`.
  prefs: []
  type: TYPE_NORMAL
- en: Taking care of batch normalization[](#taking-care-of-batch-normalization "Permalink
    to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`update_bn()` is a utility function that allows to compute the batchnorm statistics
    for the SWA model on a given dataloader `loader` at the end of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`update_bn()` applies the `swa_model` to every element in the dataloader and
    computes the activation statistics for each batch normalization layer in the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`update_bn()` assumes that each batch in the dataloader `loader` is either
    a tensors or a list of tensors where the first element is the tensor that the
    network `swa_model` should be applied to. If your dataloader has a different structure,
    you can update the batch normalization statistics of the `swa_model` by doing
    a forward pass with the `swa_model` on each element of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting it all together: SWA[](#putting-it-all-together-swa "Permalink to
    this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the example below, `swa_model` is the SWA model that accumulates the averages
    of the weights. We train the model for a total of 300 epochs and we switch to
    the SWA learning rate schedule and start to collect SWA averages of the parameters
    at epoch 160:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting it all together: EMA[](#putting-it-all-together-ema "Permalink to
    this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the example below, `ema_model` is the EMA model that accumulates the exponentially-decayed
    averages of the weights with a decay rate of 0.999. We train the model for a total
    of 300 epochs and start to collect EMA averages immediately.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
