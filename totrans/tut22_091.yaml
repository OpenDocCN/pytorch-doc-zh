- en: Extending dispatcher for a new backend in C++
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在C++中为新后端扩展调度程序
- en: 原文：[https://pytorch.org/tutorials/advanced/extend_dispatcher.html](https://pytorch.org/tutorials/advanced/extend_dispatcher.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/advanced/extend_dispatcher.html](https://pytorch.org/tutorials/advanced/extend_dispatcher.html)
- en: In this tutorial we will walk through all necessary steps to extend the dispatcher
    to add a new device living outside `pytorch/pytorch` repo and maintain it to keep
    in sync with native PyTorch devices. Here we’ll assume that you’re familiar with
    how to [register a dispatched operator in C++](dispatcher) and how to write a
    [custom autograd function](cpp_autograd).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将逐步介绍扩展调度程序的所有必要步骤，以添加一个位于`pytorch/pytorch`存储库之外的新设备，并保持与原生PyTorch设备同步。在这里，我们假设您熟悉如何[在C++中注册调度运算符](dispatcher)以及如何编写[自定义自动微分函数](cpp_autograd)。
- en: Note
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This tutorial touches a lot of internal components inside PyTorch which are
    being actively improved, please expect changes to APIs if you decide to follow
    this tutorial. We’ll keep this tutorial up to date with the latest APIs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程涉及PyTorch内部许多正在积极改进的组件，请在决定跟随本教程时预期API的更改。我们将保持本教程与最新的API保持同步。
- en: What’s a new backend?
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是新后端？
- en: 'Adding a new backend to PyTorch requires a lot of developement and maintainence
    from backend extenders. Before adding a new backend, let’s first consider a few
    common use cases and recommended solutions for them:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 向PyTorch添加一个新后端需要来自后端扩展者的大量开发和维护。在添加新后端之前，让我们首先考虑一些常见用例和推荐的解决方案：
- en: If you have new algorithms for an existing PyTorch operator, send a PR to PyTorch.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您有现有PyTorch运算符的新算法，请向PyTorch发送一个PR。
- en: If you want to propose a new operator, send a feature request/PR to PyTorch.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想提出一个新的运算符，请向PyTorch发送一个功能请求/PR。
- en: If you want to add support for a new device/hardware like Google TPU and customized
    chips, which often requires using hardware-specific API to write kernels, follow
    this tutorial and add a out-of-tree backend to PyTorch.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想为新设备/硬件（如Google TPU和定制芯片）添加支持，通常需要使用特定于硬件的API来编写内核，请按照本教程并向PyTorch添加一个树外后端。
- en: If you want to add support for existing operators but with a different Tensor
    layout/representation like sparse and quantized, which enforces your kernels to
    be written in a way that’s more efficient given the layout/representation limitation,
    follow this tutorial and add a out-of-tree backend to PyTorch.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想为现有运算符添加支持，但使用不同的张量布局/表示，如稀疏和量化，这将强制您的内核以更有效的方式编写，考虑到布局/表示限制，请按照本教程并向PyTorch添加一个树外后端。
- en: In this tutorial we’ll mainly focus on adding a new out-of-tree device below.
    Adding out-of-tree support for a different tensor layout might share many common
    steps with devices, but we haven’t seen an example of such integrations yet so
    it might require addtional work from PyTorch to support it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将主要关注添加一个新的树外设备。为不同张量布局添加树外支持可能与设备共享许多常见步骤，但我们尚未看到这种集成的示例，因此可能需要PyTorch进行额外的工作来支持它。
- en: Get a dispatch key for your backend
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为您的后端获取一个调度键
- en: PyTorch operators are implemented in C++ and made available in Python frontend
    through Python bindings. The PyTorch dispatcher divides the implementation of
    an operator into multiple kernels, each of which is associated with a specific
    dispatch key. Supporting a new backend in PyTorch essentially means writing a
    kernel for each PyTorch operator in C++ and then registering them to a dispatch
    key representing your customized backend in the dispatcher.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch运算符是用C++实现的，并通过Python绑定在Python前端中提供。PyTorch调度程序将运算符的实现分为多个内核，每个内核与特定的调度键相关联。在PyTorch中支持一个新后端基本上意味着为C++中的每个PyTorch运算符编写一个内核，然后将它们注册到调度程序中代表您定制后端的调度键。
- en: 'Dispatch key is your identifier in the dispatcher system. The dispatcher looks
    at the dispatch keys carried on input tensors and calls the right kernel accordingly.
    PyTorch provides three reserved dispatch keys (and their corresponding Autograd
    keys) for prototyping out-of-tree backend extensions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 调度键是调度系统中的标识符。调度程序查看输入张量上携带的调度键，并相应地调用正确的内核。PyTorch为原型化树外后端扩展提供了三个预留的调度键（以及它们对应的Autograd键）：
- en: PrivateUse1/AutogradPrivateUse1
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PrivateUse1/AutogradPrivateUse1
- en: PrivateUse2/AutogradPrivateUse2
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PrivateUse2/AutogradPrivateUse2
- en: PrivateUse3/AutogradPrivateUse3
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PrivateUse3/AutogradPrivateUse3
- en: You can choose any of keys above to prototype your customized backend. To create
    a Tensor on `PrivateUse1` backend, you need to set dispatch key in `TensorImpl`
    constructor.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择上述任何键来原型化您的定制后端。要在`PrivateUse1`后端上创建一个张量，您需要在`TensorImpl`构造函数中设置调度键。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that `TensorImpl` class above assumes your Tensor is backed by a storage
    like CPU/CUDA. We also provide `OpaqueTensorImpl` for backends without a storage.
    And you might need to tweak/override certain methods to fit your customized hardware.
    One example in pytorch repo is [Vulkan TensorImpl](https://github.com/pytorch/pytorch/blob/1.7/aten/src/ATen/native/vulkan/VulkanOpaqueTensorImpl.h).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上面的`TensorImpl`类假定您的张量由类似CPU/CUDA的存储支持。我们还提供了`OpaqueTensorImpl`，用于没有存储的后端。您可能需要调整/覆盖某些方法以适应您的定制硬件。PyTorch存储库中的一个示例是[Vulkan
    TensorImpl](https://github.com/pytorch/pytorch/blob/1.7/aten/src/ATen/native/vulkan/VulkanOpaqueTensorImpl.h)。
- en: Note
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Once the prototype is done and you plan to do regular releases for your backend
    extension, please feel free to submit a PR to `pytorch/pytorch` to reserve a dedicated
    dispath key for your backend.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦原型完成，并且您计划为您的后端扩展进行定期发布，请随时向`pytorch/pytorch`提交一个PR，以保留一个专用的调度键给您的后端。
- en: Get the full list of PyTorch operators
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取PyTorch运算符的完整列表
- en: 'PyTorch provides a full list of extensible C++ operators in generated file
    `build/aten/src/ATen/RegistrationDeclarations.h`. This file is only available
    after building PyTorch from source. Here’s a snippet of the file:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了一个生成文件`build/aten/src/ATen/RegistrationDeclarations.h`中的可扩展C++运算符的完整列表。此文件仅在从源代码构建PyTorch后才可用。以下是文件的一部分：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There’re multiple fields associated with a single operator. Let’s break it
    down using `abs_out` as an example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与单个运算符相关联的多个字段。让我们以 `abs_out` 为例进行详细说明：
- en: '`Tensor & abs_out(Tensor & out, const Tensor & self);` is the C++ signature
    of the operator, your C++ kernel should match this signature exactly.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tensor & abs_out(Tensor & out, const Tensor & self);` 是运算符的 C++ 签名，您的 C++
    内核应该与此签名完全匹配。'
- en: '`aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)` is the unique
    schema representing the operator, which also contains aliasing and mutation annotations
    compared to the C++ signature. This is the unique identifier the dispatcher uses
    to find an operator.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)` 是表示运算符的唯一模式，与
    C++ 签名相比，还包含别名和突变注释。这是调度器用来查找运算符的唯一标识符。'
- en: '`dispatch` and `default` are boolean fields that provide information about
    what native PyTorch kernels can do, thus implies whether it’s required for backend
    extenders to implement the kernel. More details can be found in [register kernels
    for the new backend](#register-kernel).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dispatch` 和 `default` 是布尔字段，提供了关于原生 PyTorch 内核能够做什么的信息，因此暗示了是否需要后端扩展者实现该内核。更多细节可以在
    [为新后端注册内核](#register-kernel) 中找到。'
- en: '## Register kernels for the new backend'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '## 为新后端注册内核'
- en: 'To register your kernels to PyTorch dispatcher, you can use the `TORCH_LIBRARY_IMPL`
    API described in [Registering a Dispatched Operator in C++](dispatcher):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要将您的内核注册到 PyTorch 调度器中，您可以使用 [在 C++ 中注册分发运算符](dispatcher) 中描述的 `TORCH_LIBRARY_IMPL`
    API：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now let’s zoom in and what operator requires a kernel from a customized backend
    and what’s inside the kernels exactly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解哪些运算符需要来自定制后端的内核以及这些内核的具体内容。
- en: PyTorch currently has more than 1600 operators and it’s still growing. It’s
    unrealistic for backend extensions to keep up with this speed. Even for native
    backends like CPU or CUDA, it often requires a lot of work to write dedicated
    kernels for every new op.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 目前有超过 1600 个运算符，而且仍在增长。对于后端扩展来说，跟上这种速度是不现实的。即使对于像 CPU 或 CUDA 这样的原生后端，通常也需要大量工作为每个新运算符编写专用内核。
- en: Fortunately, some native PyTorch kernels are written in a way that they decompose
    to combination of several known operators. In other words, you only need to implement
    a set of known operators (ops that require registration below) instead of all
    PyTorch operators.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，一些原生 PyTorch 内核是以一种方式编写的，它们分解为几个已知运算符的组合。换句话说，您只需要实现一组已知运算符（下面需要注册的运算符）而不是所有
    PyTorch 运算符。
- en: 'PyTorch operators can be classified into two categories:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 运算符可以分为两类：
- en: 'Ops that require registration: PyTorch native implementation for these ops
    is backend specific and thus it’s required to provide a kernel for customized
    backend. Otherwise calling such op on the customized backend will error out.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要注册的运算符：这些运算符的 PyTorch 原生实现是特定于后端的，因此需要为定制后端提供内核。否则，在定制后端上调用此类运算符将导致错误。
- en: In `RegistrationDeclarations.h` these operators have `dispatch` set to True
    *and* `default` set to False in the metadata found in their accompanying comments.
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `RegistrationDeclarations.h` 中，这些运算符在其附带的注释中的元数据中，`dispatch` 设置为 True *并且*
    `default` 设置为 False。
- en: 'Registration is optional: backend extenders can skip registering to these ops
    without sacrificing any support. However, if a backend extender wants to override
    the default kernel provided by PyTorch, they can still register their customized
    kernel to their backend and the dispatcher will use it for your backend only.
    For example, current implementation of PyTorch’s `max_pool2d` returns `indices`
    as part of forward outputs which creates overhead in torch_xla, so torch_xla registers
    its own kernel for `max_pool2d` instead.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册是可选的：后端扩展者可以跳过为这些操作注册而不会牺牲任何支持。然而，如果后端扩展者想要覆盖 PyTorch 提供的默认内核，他们仍然可以将他们定制的内核注册到他们的后端，调度器将仅在您的后端中使用它。例如，PyTorch
    的 `max_pool2d` 的当前实现返回 `indices` 作为前向输出的一部分，这在 torch_xla 中创建了开销，因此 torch_xla 为
    `max_pool2d` 注册了自己的内核。
- en: In `RegistrationDeclarations.h` these operators have `dispatch` set to False
    *or* `default` set to True in the metadata found in their accompanying comments.
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `RegistrationDeclarations.h` 中，这些运算符在其附带的注释中的元数据中，`dispatch` 设置为 False *或*
    `default` 设置为 True。
- en: Autograd support for the new backend
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新后端的自动求导支持
- en: Gradient formulas are mostly purely mathematical and thus are general for all
    backends. PyTorch often registers a kernel to alias dispatch key Autograd, which
    means it can be used by all backends.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度公式大多是纯数学的，因此对于所有后端都是通用的。PyTorch 经常注册一个用于别名调度键 Autograd 的内核，这意味着它可以被所有后端使用。
- en: For these operators you don’t have to worry about their derivative formulas,
    you can just write forward definitions for operators in `RegistrationDeclarations.h`
    and PyTorch handles backward for you automatically.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些运算符，您不必担心它们的导数公式，您只需在 `RegistrationDeclarations.h` 中为运算符编写前向定义，PyTorch 将自动为您处理后向。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In some cases, PyTorch backward kernel implementations are also device specific
    so that they can squeeze out max performance out of each backend. For those operators
    you’ll see op_backward showing up in `RegistrationDeclarations.h` as *required
    registration* as well.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，PyTorch 的反向内核实现也是特定于设备的，以便从每个后端中挤出最大性能。对于这些运算符，您将在 `RegistrationDeclarations.h`
    中看到 op_backward 出现为 *必需注册*。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In a few *rare* cases, PyTorch’s gradient formula for certain operators may
    have assumptions that don’t generalize for all backends. In those cases backend
    extenders can optionally override PyTorch Autograd layer by registering a kernel
    from torch::autograd::Function to the corresponding dispatch key (for example,
    AutogradPrivateUse1 if you’re using PrivateUse1 for your backend):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些 *罕见* 情况下，PyTorch 对于某些运算符的梯度公式可能有不适用于所有后端的假设。在这些情况下，后端扩展者可以选择通过将来自 torch::autograd::Function
    的内核注册到相应的调度键（例如，如果您的后端使用 PrivateUse1，则为 AutogradPrivateUse1）来覆盖 PyTorch 的 Autograd
    层：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With this trick you have full control over both training and inference behavior
    for `my_add` operator in your backend. Here’s [an example](https://github.com/pytorch/xla/blob/r1.7/torch_xla/csrc/aten_autograd_ops.h)
    in the `pytorch/xla` repository.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种技巧，您可以完全控制后端中`my_add`运算符的训练和推理行为。这里是`pytorch/xla`存储库中的[一个示例](https://github.com/pytorch/xla/blob/r1.7/torch_xla/csrc/aten_autograd_ops.h)。
- en: Build an extension
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建扩展
- en: 'Out-of-tree backend is supported by adding a C++ extension to PyTorch. Once
    you have kernels and registrations ready, you can build a C++ extension by writing
    a `setup.py` script that uses `setuptools` to compile C++ code. Here’s a simplified
    example from [pytorch/xla repo](https://github.com/pytorch/xla/blob/master/setup.py):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向PyTorch添加C++扩展来支持外部后端。一旦您准备好内核和注册，您可以通过编写一个使用`setuptools`编译C++代码的`setup.py`脚本来构建C++扩展。以下是来自[pytorch/xla存储库](https://github.com/pytorch/xla/blob/master/setup.py)的一个简化示例：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: See [our C++ extension tutorial](https://pytorch.org/tutorials/advanced/cpp_extension.html#building-with-setuptools)
    for more details.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参阅[我们的C++扩展教程](https://pytorch.org/tutorials/advanced/cpp_extension.html#building-with-setuptools)。
- en: Custom operator support
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义运算符支持
- en: Your new backend should work seamlessly with [customized operators extended
    in python](https://pytorch.org/docs/stable/notes/extending.html) without writing
    any new kernels as long as the customized operator is composed of existing PyTorch
    operators (which are already supported by your backend).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您的新后端应该与[在Python中扩展的自定义运算符](https://pytorch.org/docs/stable/notes/extending.html)无缝配合，而无需编写任何新的内核，只要自定义运算符由现有PyTorch运算符组成（这些运算符已受到您的后端支持）。
- en: For [custom operators extended in C++](cpp_autograd) they often come with a
    [backend specific C++ kernel implementation e.g. nms kernel in torchvsion](https://github.com/pytorch/vision/blob/master/torchvision/csrc/ops/cuda/nms_kernel.cu)
    as well as [a customized Python API e.g. torch.ops.torchvision.nms](https://github.com/pytorch/vision/blob/master/torchvision/csrc/ops/nms.cpp#L18).
    To support these operators, backend extenders will need to write a C++ kernel
    for your backend and properly register it to the corresponding namespace in the
    dispatcher similar to supporting PyTorch native operators. Alternatively you could
    also add a customized API in your extension e.g `torch_xla.core.functions.nms`
    for these adhoc requests.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[在C++中扩展的自定义运算符](cpp_autograd)，它们通常带有[后端特定的C++内核实现，例如torchvsion中的nms内核](https://github.com/pytorch/vision/blob/master/torchvision/csrc/ops/cuda/nms_kernel.cu)，以及[自定义的Python
    API，例如torch.ops.torchvision.nms](https://github.com/pytorch/vision/blob/master/torchvision/csrc/ops/nms.cpp#L18)。为了支持这些运算符，后端扩展者需要为您的后端编写一个C++内核，并将其正确注册到分发器中的相应命名空间，类似于支持PyTorch原生运算符。或者，您还可以在您的扩展中添加一个自定义API，例如`torch_xla.core.functions.nms`，以满足这些临时请求。
- en: JIT support
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JIT支持
- en: As we mentioned in [Registering a Dispatched Operator in C++](dispatcher), kernels
    registered through m.impl() API support being called in both unboxed and boxed
    ways. In other words your customized backend can also work with our JIT tracing/scripting
    frontend just like the in-tree backends like CPU or CUDA do. You could potentially
    also write specialized optimization passes for your backend on a JIT graph. But
    we will not discuss it here since we haven’t finalized the integration point in
    JIT, so the current backend support will focus on the eager frontend for now.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[在C++中注册分发运算符](dispatcher)中提到的，通过m.impl() API注册的内核支持以未装箱和装箱方式调用。换句话说，您的定制后端也可以与我们的JIT跟踪/脚本前端一起工作，就像树内后端（如CPU或CUDA）一样。您还可以为JIT图编写专门的优化传递，但我们不会在这里讨论，因为我们尚未确定JIT中的集成点，因此当前后端支持将重点放在急切的前端上。
- en: Testing your backend against native PyTorch backends
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对原生PyTorch后端进行测试
- en: PyTorch lets tests run on multiple device types using its [generic device type
    testing framework](https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_device_type.py).
    You can find details about [how tests use it](https://github.com/pytorch/pytorch/blob/5a8198eb3c594aa18352930fd21f3c25bd7b7100/torch/testing/_internal/common_device_type.py#L23)
    and information about [how to add a new device type](https://github.com/pytorch/pytorch/blob/5a8198eb3c594aa18352930fd21f3c25bd7b7100/torch/testing/_internal/common_device_type.py#L369).
    Once added, PyTorch tests using the generic device type testing framework will
    be run using your device type, too. See [this Wiki page](https://github.com/pytorch/pytorch/wiki/Writing-tests-that-run-on-all-available-device-types)
    for an example of how tests are instantiated.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch允许使用其[通用设备类型测试框架](https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_device_type.py)在多种设备类型上运行测试。您可以在[测试如何使用它](https://github.com/pytorch/pytorch/blob/5a8198eb3c594aa18352930fd21f3c25bd7b7100/torch/testing/_internal/common_device_type.py#L23)以及[如何添加新设备类型](https://github.com/pytorch/pytorch/blob/5a8198eb3c594aa18352930fd21f3c25bd7b7100/torch/testing/_internal/common_device_type.py#L369)方面找到详细信息。一旦添加，使用通用设备类型测试框架的PyTorch测试也将使用您的设备类型运行。查看[此Wiki页面](https://github.com/pytorch/pytorch/wiki/Writing-tests-that-run-on-all-available-device-types)以了解测试如何实例化的示例。
- en: Running PyTorch’s existing test suites with your device type is important to
    ensure correctness, but not all PyTorch features are supported by every device
    type. The generic device type testing framework allows for considerable customization
    so that device types can select which tests to run, which dtypes they support,
    and even which precisions to use when comparing tensors for equality.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您的设备类型运行PyTorch现有的测试套件非常重要，以确保正确性，但并非所有PyTorch功能都受到每种设备类型的支持。通用设备类型测试框架允许进行相当大的定制，以便设备类型可以选择运行哪些测试，支持哪些数据类型，甚至在比较张量相等性时使用哪些精度。
- en: An example device type that uses the generic device type testing framework and
    doesn’t ship with PyTorch is XLA. See [its extension of the generic device type
    testing framework](https://github.com/pytorch/xla/blob/master/test/pytorch_test_base.py),
    which contains examples of block listing tests, block listing dtypes, and overriding
    test precision.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用通用设备类型测试框架并且不随PyTorch一起提供的示例设备类型是XLA。请参阅[其对通用设备类型测试框架的扩展](https://github.com/pytorch/xla/blob/master/test/pytorch_test_base.py)，其中包含了测试块列表、数据类型块列表和覆盖测试精度的示例。
- en: The generic device type testing framework is actively developed. To request
    a feature please file an issue on PyTorch’s Github.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通用设备类型测试框架正在积极开发中。要请求功能，请在 PyTorch 的 Github 上提交问题。
- en: Backward Compatibility
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向后兼容性
- en: Currently PyTorch can’t guarantee backward compatibility for registered operators.
    Operators, as well as their schemas, might be added/modified/deleted as needed.
    Registered kernels must be *exactly* the same as PyTorch version. If PyTorch adds
    more parameters ( even with defaults) for an operator, your old registration won’t
    work until it’s updated to match PyTorch’s new signature.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，PyTorch 无法保证已注册运算符的向后兼容性。运算符及其模式可能会根据需要进行添加/修改/删除。注册的内核必须与 PyTorch 版本完全相同。如果
    PyTorch 为运算符添加更多参数（即使有默认值），您的旧注册将无法工作，直到更新以匹配 PyTorch 的新签名为止。
- en: As a result, we *highly recommend* out-of-tree backend extenders only sync with
    major PyTorch releases to minimize interruptions in development. PyTorch is on
    a quarterly release cadence. Backend extenders should join the *#announcement*
    channel at [pytorch.slack.com](http://pytorch.slack.com/) to get latest updates
    on releases.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们*强烈建议*独立存储后端扩展器仅与主要的 PyTorch 发布同步，以最大程度地减少开发中的中断。PyTorch 按季度发布。后端扩展器应该加入
    *#announcement* 频道，以获取有关发布的最新更新。
- en: Known issues & additional notes
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 已知问题和其他说明
- en: Not all test suites are device generic yet. Extensible test classes can be found
    by searching `instantiate_device_type_tests` in PyTorch codebase, e.g `TestTorchDeviceType,
    TestViewOps, TestTensorDeviceOps, TestTypePromotion` etc.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有测试套件都是设备通用的。可以通过在 PyTorch 代码库中搜索 `instantiate_device_type_tests` 来找到可扩展的测试类，例如
    `TestTorchDeviceType, TestViewOps, TestTensorDeviceOps, TestTypePromotion` 等。
- en: There’s no extension point in C++ for serializing a python Tensor object on
    customized backend. Currently you can only extend it by modifying [PyTorch Tensor
    __reduce_ex__ method](https://github.com/pytorch/pytorch/blob/5640b79bf8a5412a0209a919c05c811d5427cc12/torch/tensor.py#L83-L150)
    or monkey patching in out-of-tree repository.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 C++ 中没有扩展点用于在自定义后端上序列化 Python 张量对象。目前，您只能通过修改 [PyTorch 张量 __reduce_ex__ 方法](https://github.com/pytorch/pytorch/blob/5640b79bf8a5412a0209a919c05c811d5427cc12/torch/tensor.py#L83-L150)
    或在独立存储库中进行 monkey patching 来扩展它。
- en: If your backend doesn’t allow direct memory access, you should pay additional
    attention to supporting view ops since they’re supposed to share storage. Changes
    to view tensor need to propagated to its base tensor and vice versa.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的后端不允许直接访问内存，则应特别注意支持视图操作，因为它们应该共享存储。对视图张量的更改需要传播到其基张量，反之亦然。
- en: There’s no extension point in C++ for Optimizer if your backend doesn’t work
    with the native PyTorch Optimizers, e.g. need to carry the states to be updated
    in backward like torch-xla. Such use cases currently can only be done through
    adding customized API or monkey patching in out-of-tree repository.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的后端无法与原生 PyTorch 优化器一起使用，则在 C++ 中没有优化器的扩展点，例如需要在向后传递时携带状态以更新像 torch-xla 这样的优化器。目前，这种用例只能通过添加自定义
    API 或在独立存储库中进行 monkey patching 来实现。
- en: Future Work
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来工作
- en: 'Making every component in PyTorch extensible for an out-of-tree backend seamless
    requires a lot of changes to PyTorch internals. Here are a few items that we’re
    actively working on might improve the experience in the future:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使 PyTorch 中的每个组件都对于独立存储后端无缝扩展需要对 PyTorch 内部进行大量更改。以下是我们正在积极努力改进的一些项目，可能会在未来改善体验：
- en: Improve test coverage of generic testing framework.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进通用测试框架的测试覆盖率。
- en: Improve `Math` kernel coverage and more comprehensive tests to make sure `Math`
    kernel bahavior matches other backends like `CPU/CUDA`.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进 `Math` 内核覆盖率和更全面的测试，以确保 `Math` 内核行为与其他后端（如 `CPU/CUDA`）匹配。
- en: Refactor `RegistrationDeclarations.h` to carry the minimal information and reuse
    PyTorch’s codegen as much as possible.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重构 `RegistrationDeclarations.h`，尽可能携带最少的信息并重复使用 PyTorch 的代码生成。
- en: Support a backend fallback kernel to automatic convert inputs to CPU and convert
    the result back to the customized backend. This will allow “full” operator coverage
    even though you don’t have kernels written for every operator.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持后端回退内核，自动将输入转换为 CPU 并将结果转换回自定义后端。这将允许“完整”运算符覆盖，即使您没有为每个运算符编写内核。
- en: Stay in touch
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保持联系
- en: Please use [PyTorch dev discussions](https://dev-discuss.pytorch.org/) for questions
    and discussions. If you have any feature requests or bug reports, please [file
    an issue on github](https://github.com/pytorch/pytorch/issues).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请使用 [PyTorch 开发讨论](https://dev-discuss.pytorch.org/) 进行问题和讨论。如果您有任何功能请求或错误报告，请在
    github 上提交问题（https://github.com/pytorch/pytorch/issues）。
- en: If you’re interested in helping in any of the future work items above (e.g adding
    more `Math` kernels for PyTorch operators in C++), please reach out to us through
    Github or Slack!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣帮助上述任何未来工作项目（例如在 C++ 中为 PyTorch 运算符添加更多 `Math` 内核），请通过 Github 或 Slack
    与我们联系！
