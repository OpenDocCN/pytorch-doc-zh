- en: Distributed Data Parallel in PyTorch - Video Tutorials
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch中的分布式数据并行 - 视频教程
- en: 原文：[https://pytorch.org/tutorials/beginner/ddp_series_intro.html](https://pytorch.org/tutorials/beginner/ddp_series_intro.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/beginner/ddp_series_intro.html](https://pytorch.org/tutorials/beginner/ddp_series_intro.html)
- en: '**Introduction** || [What is DDP](ddp_series_theory.html) || [Single-Node Multi-GPU
    Training](ddp_series_multigpu.html) || [Fault Tolerance](ddp_series_fault_tolerance.html)
    || [Multi-Node training](../intermediate/ddp_series_multinode.html) || [minGPT
    Training](../intermediate/ddp_series_minGPT.html)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍** || [什么是DDP](ddp_series_theory.html) || [单节点多GPU训练](ddp_series_multigpu.html)
    || [容错性](ddp_series_fault_tolerance.html) || [多节点训练](../intermediate/ddp_series_multinode.html)
    || [minGPT训练](../intermediate/ddp_series_minGPT.html)'
- en: 'Authors: [Suraj Subramanian](https://github.com/suraj813)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：[Suraj Subramanian](https://github.com/suraj813)
- en: Follow along with the video below or on [youtube](https://www.youtube.com/watch/-K3bZYHYHEA).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随下面的视频或在[youtube](https://www.youtube.com/watch/-K3bZYHYHEA)上观看。
- en: '[https://www.youtube.com/embed/-K3bZYHYHEA](https://www.youtube.com/embed/-K3bZYHYHEA)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/-K3bZYHYHEA](https://www.youtube.com/embed/-K3bZYHYHEA)'
- en: This series of video tutorials walks you through distributed training in PyTorch
    via DDP.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列视频教程将带您了解通过DDP在PyTorch中进行分布式训练。
- en: The series starts with a simple non-distributed training job, and ends with
    deploying a training job across several machines in a cluster. Along the way,
    you will also learn about [torchrun](https://pytorch.org/docs/stable/elastic/run.html)
    for fault-tolerant distributed training.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该系列从简单的非分布式训练作业开始，最终部署到集群中的多台机器上进行训练。在此过程中，您还将了解到关于[torchrun](https://pytorch.org/docs/stable/elastic/run.html)用于容错分布式训练。
- en: The tutorial assumes a basic familiarity with model training in PyTorch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程假定您对PyTorch中的模型训练有基本的了解。
- en: Running the code
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行代码
- en: You will need multiple CUDA GPUs to run the tutorial code. Typically, this can
    be done on a cloud instance with multiple GPUs (the tutorials use an Amazon EC2
    P3 instance with 4 GPUs).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要多个CUDA GPU来运行教程代码。通常可以在具有多个GPU的云实例上完成此操作（教程使用具有4个GPU的Amazon EC2 P3实例）。
- en: The tutorial code is hosted in this [github repo](https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-series).
    Clone the repository and follow along!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 教程代码托管在这个[github仓库](https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-series)。克隆该仓库并跟随教程！
- en: Tutorial sections
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教程部分
- en: Introduction (this page)
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍（本页）
- en: '[What is DDP?](ddp_series_theory.html) Gently introduces what DDP is doing
    under the hood'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DDP是什么？](ddp_series_theory.html) 温和地介绍了DDP在幕后的工作'
- en: '[Single-Node Multi-GPU Training](ddp_series_multigpu.html) Training models
    using multiple GPUs on a single machine'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[单节点多GPU训练](ddp_series_multigpu.html) 在单台机器上使用多个GPU训练模型'
- en: '[Fault-tolerant distributed training](ddp_series_fault_tolerance.html) Making
    your distributed training job robust with torchrun'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[容错分布式训练](ddp_series_fault_tolerance.html) 使用torchrun使您的分布式训练工作更加稳健'
- en: '[Multi-Node training](../intermediate/ddp_series_multinode.html) Training models
    using multiple GPUs on multiple machines'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[多节点训练](../intermediate/ddp_series_multinode.html) 使用多台机器上的多个GPU进行模型训练'
- en: '[Training a GPT model with DDP](../intermediate/ddp_series_minGPT.html) “Real-world”
    example of training a [minGPT](https://github.com/karpathy/minGPT) model with
    DDP'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用DDP训练GPT模型](../intermediate/ddp_series_minGPT.html) 使用DDP训练[minGPT](https://github.com/karpathy/minGPT)模型的“真实世界”示例'
