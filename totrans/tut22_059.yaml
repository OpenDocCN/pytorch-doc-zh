- en: Reinforcement Learning (PPO) with TorchRL Tutorial
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TorchRL的强化学习（PPO）教程
- en: 原文：[https://pytorch.org/tutorials/intermediate/reinforcement_ppo.html](https://pytorch.org/tutorials/intermediate/reinforcement_ppo.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/intermediate/reinforcement_ppo.html](https://pytorch.org/tutorials/intermediate/reinforcement_ppo.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-intermediate-reinforcement-ppo-py) to download
    the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-intermediate-reinforcement-ppo-py)下载完整示例代码
- en: '**Author**: [Vincent Moens](https://github.com/vmoens)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：[Vincent Moens](https://github.com/vmoens)'
- en: This tutorial demonstrates how to use PyTorch and `torchrl` to train a parametric
    policy network to solve the Inverted Pendulum task from the [OpenAI-Gym/Farama-Gymnasium
    control library](https://github.com/Farama-Foundation/Gymnasium).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程演示了如何使用PyTorch和`torchrl`来训练一个参数化策略网络，以解决来自[OpenAI-Gym/Farama-Gymnasium控制库](https://github.com/Farama-Foundation/Gymnasium)的倒立摆任务。
- en: '![Inverted pendulum](../Images/68def57aa25e2691b9ecd28c9fb0687c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![倒立摆](../Images/68def57aa25e2691b9ecd28c9fb0687c.png)'
- en: Inverted pendulum
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 倒立摆
- en: 'Key learnings:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 关键学习：
- en: How to create an environment in TorchRL, transform its outputs, and collect
    data from this environment;
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在TorchRL中创建环境，转换其输出，并从该环境收集数据;
- en: How to make your classes talk to each other using [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))");
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用[`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))")使您的类彼此通信;
- en: 'The basics of building your training loop with TorchRL:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TorchRL构建训练循环的基础知识：
- en: How to compute the advantage signal for policy gradient methods;
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算策略梯度方法的优势信号;
- en: How to create a stochastic policy using a probabilistic neural network;
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用概率神经网络创建随机策略;
- en: How to create a dynamic replay buffer and sample from it without repetition.
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何创建一个动态回放缓冲区，并从中进行无重复采样。
- en: 'We will cover six crucial components of TorchRL:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍TorchRL的六个关键组件：
- en: '[environments](https://pytorch.org/rl/reference/envs.html)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[环境](https://pytorch.org/rl/reference/envs.html)'
- en: '[transforms](https://pytorch.org/rl/reference/envs.html#transforms)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[转换](https://pytorch.org/rl/reference/envs.html#transforms)'
- en: '[models (policy and value function)](https://pytorch.org/rl/reference/modules.html)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模型（策略和价值函数）](https://pytorch.org/rl/reference/modules.html)'
- en: '[loss modules](https://pytorch.org/rl/reference/objectives.html)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[损失模块](https://pytorch.org/rl/reference/objectives.html)'
- en: '[data collectors](https://pytorch.org/rl/reference/collectors.html)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据收集器](https://pytorch.org/rl/reference/collectors.html)'
- en: '[replay buffers](https://pytorch.org/rl/reference/data.html#replay-buffers)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[回放缓冲区](https://pytorch.org/rl/reference/data.html#replay-buffers)'
- en: 'If you are running this in Google Colab, make sure you install the following
    dependencies:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Google Colab中运行此代码，请确保安装以下依赖项：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Proximal Policy Optimization (PPO) is a policy-gradient algorithm where a batch
    of data is being collected and directly consumed to train the policy to maximise
    the expected return given some proximality constraints. You can think of it as
    a sophisticated version of [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf),
    the foundational policy-optimization algorithm. For more information, see the
    [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) paper.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Proximal Policy Optimization（PPO）是一种策略梯度算法，其中收集一批数据，并直接用于训练策略以最大化给定一些近似约束条件下的预期回报。您可以将其视为[REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)的复杂版本，这是基础策略优化算法。有关更多信息，请参阅[Proximal
    Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)论文。
- en: PPO is usually regarded as a fast and efficient method for online, on-policy
    reinforcement algorithm. TorchRL provides a loss-module that does all the work
    for you, so that you can rely on this implementation and focus on solving your
    problem rather than re-inventing the wheel every time you want to train a policy.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: PPO通常被认为是一种快速高效的在线、基于策略的强化学习算法。TorchRL提供了一个损失模块，可以为您完成所有工作，这样您就可以依赖这个实现，专注于解决问题，而不是每次想要训练策略时都要重新发明轮子。
- en: 'For completeness, here is a brief overview of what the loss computes, even
    though this is taken care of by our [`ClipPPOLoss`](https://pytorch.org/rl/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss
    "(in torchrl vmain (0.4.0 ))") module—the algorithm works as follows: 1\. we will
    sample a batch of data by playing the policy in the environment for a given number
    of steps. 2\. Then, we will perform a given number of optimization steps with
    random sub-samples of this batch using a clipped version of the REINFORCE loss.
    3\. The clipping will put a pessimistic bound on our loss: lower return estimates
    will be favored compared to higher ones. The precise formula of the loss is:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整起见，这里简要概述了损失的计算过程，尽管这已经由我们的[`ClipPPOLoss`](https://pytorch.org/rl/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss
    "(in torchrl vmain (0.4.0 ))")模块处理——算法的工作方式如下：1. 通过在环境中执行策略一定数量的步骤来采样一批数据。2. 然后，我们将使用该批次的随机子样本执行一定数量的优化步骤，使用REINFORCE损失的剪切版本。3.
    剪切将对我们的损失设置一个悲观的边界：较低的回报估计将优先于较高的回报估计。损失的精确公式如下：
- en: \[L(s,a,\theta_k,\theta) = \min\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}
    A^{\pi_{\theta_k}}(s,a), \;\; g(\epsilon, A^{\pi_{\theta_k}}(s,a)) \right),\]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: \[L(s,a,\theta_k,\theta) = \min\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}
    A^{\pi_{\theta_k}}(s,a), \;\; g(\epsilon, A^{\pi_{\theta_k}}(s,a)) \right),\]
- en: 'There are two components in that loss: in the first part of the minimum operator,
    we simply compute an importance-weighted version of the REINFORCE loss (for example,
    a REINFORCE loss that we have corrected for the fact that the current policy configuration
    lags the one that was used for the data collection). The second part of that minimum
    operator is a similar loss where we have clipped the ratios when they exceeded
    or were below a given pair of thresholds.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在该损失中有两个组件：在最小运算符的第一部分中，我们简单地计算REINFORCE损失的重要性加权版本（例如，我们已经校正了当前策略配置滞后于用于数据收集的策略配置的事实）。最小运算符的第二部分是一个类似的损失，当比率超过或低于给定的一对阈值时，我们对比率进行了剪切。
- en: This loss ensures that whether the advantage is positive or negative, policy
    updates that would produce significant shifts from the previous configuration
    are being discouraged.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种损失确保了无论优势是正面还是负面，都会阻止会导致与先前配置产生显著变化的策略更新。
- en: 'This tutorial is structured as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程结构如下：
- en: First, we will define a set of hyperparameters we will be using for training.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义一组用于训练的超参数。
- en: Next, we will focus on creating our environment, or simulator, using TorchRL’s
    wrappers and transforms.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将专注于使用TorchRL的包装器和转换器创建我们的环境或模拟器。
- en: Next, we will design the policy network and the value model, which is indispensable
    to the loss function. These modules will be used to configure our loss module.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设计策略网络和价值模型，这对于损失函数是必不可少的。这些模块将用于配置我们的损失模块。
- en: Next, we will create the replay buffer and data loader.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建重放缓冲区和数据加载器。
- en: Finally, we will run our training loop and analyze the results.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将运行训练循环并分析结果。
- en: 'Throughout this tutorial, we’ll be using the `tensordict` library. [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") is the lingua franca of TorchRL: it helps us
    abstract what a module reads and writes and care less about the specific data
    description and more about the algorithm itself.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用`tensordict`库。[`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))")是TorchRL的通用语言：它帮助我们抽象出模块读取和写入的内容，更少关心具体的数据描述，更多关注算法本身。
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Define Hyperparameters
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义超参数
- en: We set the hyperparameters for our algorithm. Depending on the resources available,
    one may choose to execute the policy on GPU or on another device. The `frame_skip`
    will control how for how many frames is a single action being executed. The rest
    of the arguments that count frames must be corrected for this value (since one
    environment step will actually return `frame_skip` frames).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置算法的超参数。根据可用资源，可以选择在GPU上或在另一设备上执行策略。`frame_skip`将控制执行单个动作需要多少帧。其余计算帧数的参数必须根据这个值进行校正（因为一个环境步骤实际上会返回`frame_skip`帧）。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Data collection parameters
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集参数
- en: 'When collecting data, we will be able to choose how big each batch will be
    by defining a `frames_per_batch` parameter. We will also define how many frames
    (such as the number of interactions with the simulator) we will allow ourselves
    to use. In general, the goal of an RL algorithm is to learn to solve the task
    as fast as it can in terms of environment interactions: the lower the `total_frames`
    the better. We also define a `frame_skip`: in some contexts, repeating the same
    action multiple times over the course of a trajectory may be beneficial as it
    makes the behavior more consistent and less erratic. However, “skipping” too many
    frames will hamper training by reducing the reactivity of the actor to observation
    changes.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集数据时，我们可以通过定义一个`frames_per_batch`参数来选择每个批次的大小。我们还将定义我们允许自己使用多少帧（例如与模拟器的交互次数）。一般来说，RL算法的目标是尽快学会解决任务，以尽可能少的环境交互次数为目标：`total_frames`越低越好。我们还定义了一个`frame_skip`：在某些情况下，重复在轨迹过程中多次执行相同动作可能是有益的，因为这会使行为更一致，更少出现异常。然而，“跳过”太多帧会通过降低演员对观察变化的反应性来阻碍训练。
- en: When using `frame_skip` it is good practice to correct the other frame counts
    by the number of frames we are grouping together. If we configure a total count
    of X frames for training but use a `frame_skip` of Y, we will be actually collecting
    `XY` frames in total which exceeds our predefined budget.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`frame_skip`时，最好根据我们正在组合在一起的帧数来校正其他帧数。如果我们为训练配置了X帧的总数，但使用了Y的`frame_skip`，那么我们实际上将总共收集`XY`帧，这超出了我们预先定义的预算。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PPO parameters
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PPO参数
- en: 'At each data collection (or batch collection) we will run the optimization
    over a certain number of *epochs*, each time consuming the entire data we just
    acquired in a nested training loop. Here, the `sub_batch_size` is different from
    the `frames_per_batch` here above: recall that we are working with a “batch of
    data” coming from our collector, which size is defined by `frames_per_batch`,
    and that we will further split in smaller sub-batches during the inner training
    loop. The size of these sub-batches is controlled by `sub_batch_size`.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次数据收集（或批量收集）中，我们将在一定数量的*epochs*上运行优化，每次都会在嵌套的训练循环中消耗我们刚刚获取的所有数据。在这里，`sub_batch_size`与上面的`frames_per_batch`不同：请记住，我们正在处理来自我们收集器的“数据批次”，其大小由`frames_per_batch`定义，并且我们将在内部训练循环中进一步分割为更小的子批次。这些子批次的大小由`sub_batch_size`控制。
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Define an environment
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个环境
- en: 'In RL, an *environment* is usually the way we refer to a simulator or a control
    system. Various libraries provide simulation environments for reinforcement learning,
    including Gymnasium (previously OpenAI Gym), DeepMind Control Suite, and many
    others. As a general library, TorchRL’s goal is to provide an interchangeable
    interface to a large panel of RL simulators, allowing you to easily swap one environment
    with another. For example, creating a wrapped gym environment can be achieved
    with few characters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，*环境*通常是我们指代模拟器或控制系统的方式。各种库提供了用于强化学习的模拟环境，包括Gymnasium（之前是OpenAI Gym）、DeepMind
    Control Suite等。作为一个通用库，TorchRL的目标是为大量RL模拟器提供可互换的接口，使您可以轻松地将一个环境与另一个环境进行交换。例如，可以用很少的字符创建一个包装的gym环境：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'There are a few things to notice in this code: first, we created the environment
    by calling the `GymEnv` wrapper. If extra keyword arguments are passed, they will
    be transmitted to the `gym.make` method, hence covering the most common environment
    construction commands. Alternatively, one could also directly create a gym environment
    using `gym.make(env_name, **kwargs)` and wrap it in a GymWrapper class.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中有几点需要注意：首先，我们通过调用 `GymEnv` 包装器创建了环境。如果传递了额外的关键字参数，它们将传递给 `gym.make` 方法，因此涵盖了最常见的环境构建命令。或者，也可以直接使用
    `gym.make(env_name, **kwargs)` 创建 gym 环境，并将其包装在 GymWrapper 类中。
- en: 'Also the `device` argument: for gym, this only controls the device where input
    action and observed states will be stored, but the execution will always be done
    on CPU. The reason for this is simply that gym does not support on-device execution,
    unless specified otherwise. For other libraries, we have control over the execution
    device and, as much as we can, we try to stay consistent in terms of storing and
    execution backends.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 还有 `device` 参数：对于 gym，这只控制输入动作和观察状态将存储在的设备，但执行始终在 CPU 上进行。原因很简单，即 gym 不支持在设备上执行，除非另有说明。对于其他库，我们可以控制执行设备，并且尽可能保持存储和执行后端的一致性。
- en: Transforms
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换
- en: We will append some transforms to our environments to prepare the data for the
    policy. In Gym, this is usually achieved via wrappers. TorchRL takes a different
    approach, more similar to other pytorch domain libraries, through the use of transforms.
    To add transforms to an environment, one should simply wrap it in a [`TransformedEnv`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv
    "(in torchrl vmain (0.4.0 ))") instance and append the sequence of transforms
    to it. The transformed environment will inherit the device and meta-data of the
    wrapped environment, and transform these depending on the sequence of transforms
    it contains.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向我们的环境附加一些转换，以准备数据供策略使用。在 Gym 中，通常通过包装器来实现这一点。TorchRL 采用了一种不同的方法，更类似于其他 pytorch
    领域库，通过使用转换。要向环境添加转换，只需将其包装在一个 [`TransformedEnv`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv
    "(在 torchrl vmain (0.4.0 ))") 实例中，并将转换序列附加到其中。转换后的环境将继承包装环境的设备和元数据，并根据包含的转换序列进行转换。
- en: Normalization
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 归一化
- en: 'The first to encode is a normalization transform. As a rule of thumbs, it is
    preferable to have data that loosely match a unit Gaussian distribution: to obtain
    this, we will run a certain number of random steps in the environment and compute
    the summary statistics of these observations.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要进行编码的是归一化转换。通常情况下，最好让数据大致符合单位高斯分布：为了实现这一点，我们将在环境中运行一定数量的随机步骤，并计算这些观察结果的摘要统计信息。
- en: 'We’ll append two other transforms: the [`DoubleToFloat`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.DoubleToFloat.html#torchrl.envs.transforms.DoubleToFloat
    "(in torchrl vmain (0.4.0 ))") transform will convert double entries to single-precision
    numbers, ready to be read by the policy. The [`StepCounter`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.StepCounter.html#torchrl.envs.transforms.StepCounter
    "(in torchrl vmain (0.4.0 ))") transform will be used to count the steps before
    the environment is terminated. We will use this measure as a supplementary measure
    of performance.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将附加另外两个转换：[`DoubleToFloat`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.DoubleToFloat.html#torchrl.envs.transforms.DoubleToFloat
    "(在 torchrl vmain (0.4.0 ))") 转换将双精度条目转换为单精度数字，以便策略读取。[`StepCounter`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.StepCounter.html#torchrl.envs.transforms.StepCounter
    "(在 torchrl vmain (0.4.0 ))") 转换将用于计算环境终止之前的步数。我们将使用这个度量作为性能的补充度量。
- en: 'As we will see later, many of the TorchRL’s classes rely on [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") to communicate. You could think of it as a python
    dictionary with some extra tensor features. In practice, this means that many
    modules we will be working with need to be told what key to read (`in_keys`) and
    what key to write (`out_keys`) in the `tensordict` they will receive. Usually,
    if `out_keys` is omitted, it is assumed that the `in_keys` entries will be updated
    in-place. For our transforms, the only entry we are interested in is referred
    to as `"observation"` and our transform layers will be told to modify this entry
    and this entry only:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在后面看到的，TorchRL 的许多类依赖于 [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(在 tensordict vmain (0.4.0 ))") 进行通信。您可以将其视为带有一些额外张量功能的 python 字典。实际上，这意味着我们将要使用的许多模块需要告诉它们在它们将接收的
    `tensordict` 中读取哪个键（`in_keys`）和写入哪个键（`out_keys`）。通常情况下，如果省略了 `out_keys`，则假定 `in_keys`
    条目将被原地更新。对于我们的转换，我们感兴趣的唯一条目被称为 `"observation"`，我们将告诉我们的转换层修改这个条目，仅限于这个条目：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you may have noticed, we have created a normalization layer but we did not
    set its normalization parameters. To do this, [`ObservationNorm`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm
    "(in torchrl vmain (0.4.0 ))") can automatically gather the summary statistics
    of our environment:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能已经注意到的，我们已经创建了一个归一化层，但我们没有设置其归一化参数。为了做到这一点，[`ObservationNorm`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm
    "(在 torchrl vmain (0.4.0 ))") 可以自动收集我们环境的摘要统计信息：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The [`ObservationNorm`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm
    "(in torchrl vmain (0.4.0 ))") transform has now been populated with a location
    and a scale that will be used to normalize the data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[`ObservationNorm`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm
    "(在 torchrl vmain (0.4.0 ))") 转换现在已经填充了一个位置和一个比例，将用于归一化数据。'
- en: 'Let us do a little sanity check for the shape of our summary stats:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对我们的摘要统计信息的形状进行一些简单的检查：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: An environment is not only defined by its simulator and transforms, but also
    by a series of metadata that describe what can be expected during its execution.
    For efficiency purposes, TorchRL is quite stringent when it comes to environment
    specs, but you can easily check that your environment specs are adequate. In our
    example, the `GymWrapper` and `GymEnv` that inherits from it already take care
    of setting the proper specs for your environment so you should not have to care
    about this.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个环境不仅由其模拟器和转换定义，还由一系列描述其执行期间可以预期到的元数据定义。出于效率目的，当涉及环境规范时，TorchRL是相当严格的，但您可以轻松检查您的环境规范是否合适。在我们的示例中，`GymWrapper`和从中继承的`GymEnv`已经负责为您的环境设置适当的规范，因此您不必担心这一点。
- en: 'Nevertheless, let’s see a concrete example using our transformed environment
    by looking at its specs. There are three specs to look at: `observation_spec`
    which defines what is to be expected when executing an action in the environment,
    `reward_spec` which indicates the reward domain and finally the `input_spec` (which
    contains the `action_spec`) and which represents everything an environment requires
    to execute a single step.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，让我们通过查看其规范来看一个使用我们转换后的环境的具体示例。有三个规范要查看：`observation_spec`定义了在环境中执行动作时可以预期的内容，`reward_spec`指示奖励域，最后是`input_spec`（其中包含`action_spec`），它代表环境执行单个步骤所需的一切。
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'the `check_env_specs()` function runs a small rollout and compares its output
    against the environment specs. If no error is raised, we can be confident that
    the specs are properly defined:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`check_env_specs()`函数运行一个小的执行，并将其输出与环境规范进行比较。如果没有引发错误，我们可以确信规范已经正确定义：'
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For fun, let’s see what a simple random rollout looks like. You can call env.rollout(n_steps)
    and get an overview of what the environment inputs and outputs look like. Actions
    will automatically be drawn from the action spec domain, so you don’t need to
    care about designing a random sampler.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，让我们看看简单的随机执行是什么样子的。您可以调用env.rollout(n_steps)并查看环境输入和输出的概况。动作将自动从动作规范域中绘制，因此您无需担心设计随机采样器。
- en: 'Typically, at each step, an RL environment receives an action as input, and
    outputs an observation, a reward and a done state. The observation may be composite,
    meaning that it could be composed of more than one tensor. This is not a problem
    for TorchRL, since the whole set of observations is automatically packed in the
    output [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))"). After executing a rollout (for example, a sequence
    of environment steps and random action generations) over a given number of steps,
    we will retrieve a [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") instance with a shape that matches this trajectory
    length:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在每一步中，RL环境接收一个动作作为输入，并输出一个观察、一个奖励和一个完成状态。观察可能是复合的，这意味着它可能由多个张量组成。这对于TorchRL来说不是问题，因为所有的观察集合都会自动打包在输出的[`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))")中。在执行一个执行（例如，一系列环境步骤和随机动作生成）一定数量的步骤后，我们将检索到一个形状与此轨迹长度匹配的[`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))")实例：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our rollout data has a shape of `torch.Size([3])`, which matches the number
    of steps we ran it for. The `"next"` entry points to the data coming after the
    current step. In most cases, the `"next"` data at time t matches the data at `t+1`,
    but this may not be the case if we are using some specific transformations (for
    example, multi-step).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的执行数据的形状是`torch.Size([3])`，与我们运行的步数相匹配。`"next"`条目指向当前步骤之后的数据。在大多数情况下，时间t的`"next"`数据与`t+1`时刻的数据匹配，但如果我们使用一些特定的转换（例如，多步），这可能不是情况。
- en: Policy
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略
- en: PPO utilizes a stochastic policy to handle exploration. This means that our
    neural network will have to output the parameters of a distribution, rather than
    a single value corresponding to the action taken.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: PPO利用随机策略来处理探索。这意味着我们的神经网络将不得不输出一个分布的参数，而不是与采取的动作对应的单个值。
- en: 'As the data is continuous, we use a Tanh-Normal distribution to respect the
    action space boundaries. TorchRL provides such distribution, and the only thing
    we need to care about is to build a neural network that outputs the right number
    of parameters for the policy to work with (a location, or mean, and a scale):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据是连续的，我们使用Tanh-Normal分布来尊重动作空间的边界。TorchRL提供了这样的分布，我们唯一需要关心的是构建一个神经网络，以输出策略所需的正确数量的参数（位置或均值，以及尺度）：
- en: \[f_{\theta}(\text{observation}) = \mu_{\theta}(\text{observation}), \sigma^{+}_{\theta}(\text{observation})\]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \[f_{\theta}(\text{observation}) = \mu_{\theta}(\text{observation}), \sigma^{+}_{\theta}(\text{observation})\]
- en: The only extra-difficulty that is brought up here is to split our output in
    two equal parts and map the second to a strictly positive space.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一增加的困难是将我们的输出分成两个相等的部分，并将第二部分映射到严格正空间。
- en: 'We design the policy in three steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分三步设计策略：
- en: Define a neural network `D_obs` -> `2 * D_action`. Indeed, our `loc` (mu) and
    `scale` (sigma) both have dimension `D_action`.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个神经网络`D_obs` -> `2 * D_action`。确实，我们的`loc`（mu）和`scale`（sigma）都具有维度`D_action`。
- en: Append a `NormalParamExtractor` to extract a location and a scale (for example,
    splits the input in two equal parts and applies a positive transformation to the
    scale parameter).
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 附加一个`NormalParamExtractor`来提取位置和尺度（例如，将输入分成两个相等的部分，并对尺度参数应用正变换）。
- en: Create a probabilistic [`TensorDictModule`](https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule
    "(in tensordict vmain (0.4.0 ))") that can generate this distribution and sample
    from it.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个可以生成此分布并从中采样的概率[`TensorDictModule`](https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule
    "(in tensordict vmain (0.4.0 ))")。
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To enable the policy to “talk” with the environment through the `tensordict`
    data carrier, we wrap the `nn.Module` in a [`TensorDictModule`](https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule
    "(in tensordict vmain (0.4.0 ))"). This class will simply ready the `in_keys`
    it is provided with and write the outputs in-place at the registered `out_keys`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使策略通过`tensordict`数据载体与环境“交流”，我们将`nn.Module`包装在[`TensorDictModule`](https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule
    "(在tensordict vmain (0.4.0))")中。这个类将简单地准备好它提供的`in_keys`，并将输出就地写入注册的`out_keys`。
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We now need to build a distribution out of the location and scale of our normal
    distribution. To do so, we instruct the [`ProbabilisticActor`](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor
    "(in torchrl vmain (0.4.0 ))") class to build a [`TanhNormal`](https://pytorch.org/rl/reference/generated/torchrl.modules.TanhNormal.html#torchrl.modules.TanhNormal
    "(in torchrl vmain (0.4.0 ))") out of the location and scale parameters. We also
    provide the minimum and maximum values of this distribution, which we gather from
    the environment specs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要根据正态分布的位置和尺度构建一个分布。为此，我们指示[`ProbabilisticActor`](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor
    "(在torchrl vmain (0.4.0))")类根据位置和尺度参数构建一个[`TanhNormal`](https://pytorch.org/rl/reference/generated/torchrl.modules.TanhNormal.html#torchrl.modules.TanhNormal
    "(在torchrl vmain (0.4.0)")。我们还提供这个分布的最小值和最大值，这些值是从环境规格中获取的。
- en: The name of the `in_keys` (and hence the name of the `out_keys` from the [`TensorDictModule`](https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule
    "(in tensordict vmain (0.4.0 ))") above) cannot be set to any value one may like,
    as the [`TanhNormal`](https://pytorch.org/rl/reference/generated/torchrl.modules.TanhNormal.html#torchrl.modules.TanhNormal
    "(in torchrl vmain (0.4.0 ))") distribution constructor will expect the `loc`
    and `scale` keyword arguments. That being said, [`ProbabilisticActor`](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor
    "(in torchrl vmain (0.4.0 ))") also accepts `Dict[str, str]` typed `in_keys` where
    the key-value pair indicates what `in_key` string should be used for every keyword
    argument that is to be used.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`in_keys`的名称（因此上面的[`TensorDictModule`](https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule
    "(在tensordict vmain (0.4.0))")的`out_keys`的名称）不能设置为任何一个可能喜欢的值，因为[`TanhNormal`](https://pytorch.org/rl/reference/generated/torchrl.modules.TanhNormal.html#torchrl.modules.TanhNormal
    "(在torchrl vmain (0.4.0))")分布构造函数将期望`loc`和`scale`关键字参数。也就是说，[`ProbabilisticActor`](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor
    "(在torchrl vmain (0.4.0))")还接受`Dict[str, str]`类型的`in_keys`，其中键值对指示每个要使用的关键字参数的`in_key`字符串应该用于。'
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Value network
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 价值网络
- en: The value network is a crucial component of the PPO algorithm, even though it
    won’t be used at inference time. This module will read the observations and return
    an estimation of the discounted return for the following trajectory. This allows
    us to amortize learning by relying on the some utility estimation that is learned
    on-the-fly during training. Our value network share the same structure as the
    policy, but for simplicity we assign it its own set of parameters.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 价值网络是PPO算法的关键组件，尽管在推断时不会使用。这个模块将读取观察结果，并返回对接下来轨迹的折扣回报的估计。这使我们能够通过依赖在训练过程中动态学习的一些效用估计来分期学习。我们的价值网络与策略具有相同的结构，但为简单起见，我们为其分配了自己的一组参数。
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'let’s try our policy and value modules. As we said earlier, the usage of [`TensorDictModule`](https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule
    "(in tensordict vmain (0.4.0 ))") makes it possible to directly read the output
    of the environment to run these modules, as they know what information to read
    and where to write it:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试我们的策略和价值模块。正如我们之前所说，使用[`TensorDictModule`](https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule
    "(在tensordict vmain (0.4.0))")使得直接读取环境的输出来运行这些模块成为可能，因为它们知道要读取什么信息以及在哪里写入它：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Data collector
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集器
- en: 'TorchRL provides a set of [DataCollector classes](https://pytorch.org/rl/reference/collectors.html).
    Briefly, these classes execute three operations: reset an environment, compute
    an action given the latest observation, execute a step in the environment, and
    repeat the last two steps until the environment signals a stop (or reaches a done
    state).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: TorchRL提供了一组[DataCollector类](https://pytorch.org/rl/reference/collectors.html)。简而言之，这些类执行三个操作：重置环境，根据最新观察计算动作，执行环境中的一步，并重复最后两个步骤，直到环境发出停止信号（或达到完成状态）。
- en: They allow you to control how many frames to collect at each iteration (through
    the `frames_per_batch` parameter), when to reset the environment (through the
    `max_frames_per_traj` argument), on which `device` the policy should be executed,
    etc. They are also designed to work efficiently with batched and multiprocessed
    environments.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它们允许您控制每次迭代收集多少帧（通过`frames_per_batch`参数），何时重置环境（通过`max_frames_per_traj`参数），策略应该在哪个`device`上执行等。它们还被设计为与批处理和多进程环境高效地配合工作。
- en: 'The simplest data collector is the [`SyncDataCollector`](https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.SyncDataCollector.html#torchrl.collectors.collectors.SyncDataCollector
    "(in torchrl vmain (0.4.0 ))"): it is an iterator that you can use to get batches
    of data of a given length, and that will stop once a total number of frames (`total_frames`)
    have been collected. Other data collectors ([`MultiSyncDataCollector`](https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiSyncDataCollector.html#torchrl.collectors.collectors.MultiSyncDataCollector
    "(in torchrl vmain (0.4.0 ))") and [`MultiaSyncDataCollector`](https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector
    "(in torchrl vmain (0.4.0 ))")) will execute the same operations in synchronous
    and asynchronous manner over a set of multiprocessed workers.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的数据收集器是[`SyncDataCollector`](https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.SyncDataCollector.html#torchrl.collectors.collectors.SyncDataCollector
    "(在torchrl vmain (0.4.0)中")：它是一个迭代器，您可以使用它来获取给定长度的数据批次，并且一旦收集到总帧数（`total_frames`），它将停止。其他数据收集器（[`MultiSyncDataCollector`](https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiSyncDataCollector.html#torchrl.collectors.collectors.MultiSyncDataCollector
    "(在torchrl vmain (0.4.0)中")和[`MultiaSyncDataCollector`](https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector
    "(在torchrl vmain (0.4.0)中")）将以同步和异步方式在一组多进程工作者上执行相同的操作。
- en: As for the policy and environment before, the data collector will return [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") instances with a total number of elements that
    will match `frames_per_batch`. Using [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") to pass data to the training loop allows you
    to write data loading pipelines that are 100% oblivious to the actual specificities
    of the rollout content.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的策略和环境一样，数据收集器将返回[`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(在tensordict vmain (0.4.0)中)")实例，其中元素的总数将与`frames_per_batch`匹配。使用[`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(在tensordict vmain (0.4.0)中)")将数据传递给训练循环，可以编写数据加载管道，完全不受回滚内容的实际特定性的影响。
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Replay buffer
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重放缓冲区
- en: Replay buffers are a common building piece of off-policy RL algorithms. In on-policy
    contexts, a replay buffer is refilled every time a batch of data is collected,
    and its data is repeatedly consumed for a certain number of epochs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 重放缓冲区是离策略RL算法的常见构建组件。在策略上下文中，每次收集一批数据时都会重新填充重放缓冲区，并且其数据将被重复消耗一定数量的时期。
- en: 'TorchRL’s replay buffers are built using a common container [`ReplayBuffer`](https://pytorch.org/rl/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer
    "(in torchrl vmain (0.4.0 ))") which takes as argument the components of the buffer:
    a storage, a writer, a sampler and possibly some transforms. Only the storage
    (which indicates the replay buffer capacity) is mandatory. We also specify a sampler
    without repetition to avoid sampling multiple times the same item in one epoch.
    Using a replay buffer for PPO is not mandatory and we could simply sample the
    sub-batches from the collected batch, but using these classes make it easy for
    us to build the inner training loop in a reproducible way.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: TorchRL的重放缓冲区使用一个通用容器[`ReplayBuffer`](https://pytorch.org/rl/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer
    "(在torchrl vmain (0.4.0)中")，它以缓冲区的组件作为参数：存储、写入器、采样器和可能的一些转换。只有存储（指示重放缓冲区容量）是强制性的。我们还指定了一个无重复的采样器，以避免在一个时期内多次采样相同的项目。对于PPO来说，使用重放缓冲区并不是强制性的，我们可以简单地从收集的批次中采样子批次，但使用这些类使我们能够以可重复的方式构建内部训练循环。
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Loss function
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'The PPO loss can be directly imported from TorchRL for convenience using the
    [`ClipPPOLoss`](https://pytorch.org/rl/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss
    "(in torchrl vmain (0.4.0 ))") class. This is the easiest way of utilizing PPO:
    it hides away the mathematical operations of PPO and the control flow that goes
    with it.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接从TorchRL中导入PPO损失以方便使用[`ClipPPOLoss`](https://pytorch.org/rl/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss
    "(在torchrl vmain (0.4.0)中")类。这是利用PPO的最简单方法：它隐藏了PPO的数学运算和相关控制流程。
- en: PPO requires some “advantage estimation” to be computed. In short, an advantage
    is a value that reflects an expectancy over the return value while dealing with
    the bias / variance tradeoff. To compute the advantage, one just needs to (1)
    build the advantage module, which utilizes our value operator, and (2) pass each
    batch of data through it before each epoch. The GAE module will update the input
    `tensordict` with new `"advantage"` and `"value_target"` entries. The `"value_target"`
    is a gradient-free tensor that represents the empirical value that the value network
    should represent with the input observation. Both of these will be used by [`ClipPPOLoss`](https://pytorch.org/rl/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss
    "(in torchrl vmain (0.4.0 ))") to return the policy and value losses.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: PPO需要计算一些“优势估计”。简而言之，优势是反映在处理偏差/方差折衷时对回报值的期望的值。要计算优势，只需（1）构建优势模块，该模块利用我们的值运算符，并且（2）在每个时期之前将每个数据批次通过它传递。GAE模块将使用新的“advantage”和“value_target”条目更新输入`tensordict`。
    “value_target”是一个无梯度的张量，表示值网络应该用输入观察值表示的经验值。这两者将被[`ClipPPOLoss`](https://pytorch.org/rl/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss
    "(在torchrl vmain (0.4.0)中")用于返回策略和值损失。
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Training loop
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练循环
- en: 'We now have all the pieces needed to code our training loop. The steps include:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了编写训练循环所需的所有要素。步骤包括：
- en: Collect data
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集数据
- en: Compute advantage
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算优势
- en: Loop over the collected to compute loss values
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环遍历收集的数据以计算损失值
- en: Back propagate
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Optimize
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化
- en: Repeat
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复
- en: Repeat
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复
- en: Repeat
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复
- en: '[PRE27]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Results
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: Before the 1M step cap is reached, the algorithm should have reached a max step
    count of 1000 steps, which is the maximum number of steps before the trajectory
    is truncated.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在达到 100 万步限制之前，算法应该已经达到了 1000 步的最大步数，这是轨迹被截断之前的最大步数。
- en: '[PRE29]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![training rewards (average), Max step count (training), Return (test), Max
    step count (test)](../Images/7dc745a74473740d696dfbc47563fb0b.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![训练奖励（平均值），最大步数（训练），回报（测试），最大步数（测试）](../Images/7dc745a74473740d696dfbc47563fb0b.png)'
- en: Conclusion and next steps
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论和下一步
- en: 'In this tutorial, we have learned:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们学到了：
- en: How to create and customize an environment with `torchrl`;
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用 `torchrl` 创建和自定义环境；
- en: How to write a model and a loss function;
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何编写模型和损失函数；
- en: How to set up a typical training loop.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何设置典型的训练循环。
- en: 'If you want to experiment with this tutorial a bit more, you can apply the
    following modifications:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想对本教程进行更多实验，可以应用以下修改：
- en: From an efficiency perspective, we could run several simulations in parallel
    to speed up data collection. Check [`ParallelEnv`](https://pytorch.org/rl/reference/generated/torchrl.envs.ParallelEnv.html#torchrl.envs.ParallelEnv
    "(in torchrl vmain (0.4.0 ))") for further information.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从效率的角度来看，我们可以并行运行多个模拟以加快数据收集速度。查看 [`ParallelEnv`](https://pytorch.org/rl/reference/generated/torchrl.envs.ParallelEnv.html#torchrl.envs.ParallelEnv
    "(在 torchrl vmain (0.4.0) 中)") 以获取更多信息。
- en: From a logging perspective, one could add a `torchrl.record.VideoRecorder` transform
    to the environment after asking for rendering to get a visual rendering of the
    inverted pendulum in action. Check `torchrl.record` to know more.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从记录的角度来看，可以在请求渲染后向环境添加 `torchrl.record.VideoRecorder` 转换，以获得倒立摆动作的视觉渲染。查看 `torchrl.record`
    以了解更多信息。
- en: '**Total running time of the script:** ( 4 minutes 50.072 seconds)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的总运行时间：（4分钟50.072秒）
- en: '[`Download Python source code: reinforcement_ppo.py`](../_downloads/7ed508ed54ec36ee5c1d3fa1e8ceede0/reinforcement_ppo.py)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载 Python 源代码：reinforcement_ppo.py`](../_downloads/7ed508ed54ec36ee5c1d3fa1e8ceede0/reinforcement_ppo.py)'
- en: '[`Download Jupyter notebook: reinforcement_ppo.ipynb`](../_downloads/4065a985b933a4377d3c7d93557e2282/reinforcement_ppo.ipynb)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载 Jupyter 笔记本：reinforcement_ppo.ipynb`](../_downloads/4065a985b933a4377d3c7d93557e2282/reinforcement_ppo.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery 生成的图库](https://sphinx-gallery.github.io)'
