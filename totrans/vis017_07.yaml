- en: Operators¶
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/vision/stable/ops.html](https://pytorch.org/vision/stable/ops.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`torchvision.ops` implements operators, losses and layers that are specific
    for Computer Vision.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All operators have native support for TorchScript.
  prefs: []
  type: TYPE_NORMAL
- en: Detection and Segmentation Operators[¶](#detection-and-segmentation-operators
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The below operators perform pre-processing as well as post-processing required
    in object detection and segmentation models.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`batched_nms`](generated/torchvision.ops.batched_nms.html#torchvision.ops.batched_nms
    "torchvision.ops.batched_nms")(boxes, scores, idxs, iou_threshold) | Performs
    non-maximum suppression in a batched fashion. |'
  prefs: []
  type: TYPE_TB
- en: '| [`masks_to_boxes`](generated/torchvision.ops.masks_to_boxes.html#torchvision.ops.masks_to_boxes
    "torchvision.ops.masks_to_boxes")(masks) | Compute the bounding boxes around the
    provided masks. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nms`](generated/torchvision.ops.nms.html#torchvision.ops.nms "torchvision.ops.nms")(boxes, scores, iou_threshold)
    | Performs non-maximum suppression (NMS) on the boxes according to their intersection-over-union
    (IoU). |'
  prefs: []
  type: TYPE_TB
- en: '| [`roi_align`](generated/torchvision.ops.roi_align.html#torchvision.ops.roi_align
    "torchvision.ops.roi_align")(input, boxes, output_size[, ...]) | Performs Region
    of Interest (RoI) Align operator with average pooling, as described in Mask R-CNN.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`roi_pool`](generated/torchvision.ops.roi_pool.html#torchvision.ops.roi_pool
    "torchvision.ops.roi_pool")(input, boxes, output_size[, ...]) | Performs Region
    of Interest (RoI) Pool operator described in Fast R-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| [`ps_roi_align`](generated/torchvision.ops.ps_roi_align.html#torchvision.ops.ps_roi_align
    "torchvision.ops.ps_roi_align")(input, boxes, output_size[, ...]) | Performs Position-Sensitive
    Region of Interest (RoI) Align operator mentioned in Light-Head R-CNN. |'
  prefs: []
  type: TYPE_TB
- en: '| [`ps_roi_pool`](generated/torchvision.ops.ps_roi_pool.html#torchvision.ops.ps_roi_pool
    "torchvision.ops.ps_roi_pool")(input, boxes, output_size[, ...]) | Performs Position-Sensitive
    Region of Interest (RoI) Pool operator described in R-FCN |'
  prefs: []
  type: TYPE_TB
- en: '| [`FeaturePyramidNetwork`](generated/torchvision.ops.FeaturePyramidNetwork.html#torchvision.ops.FeaturePyramidNetwork
    "torchvision.ops.FeaturePyramidNetwork")(in_channels_list, ...) | Module that
    adds a FPN from on top of a set of feature maps. |'
  prefs: []
  type: TYPE_TB
- en: '| [`MultiScaleRoIAlign`](generated/torchvision.ops.MultiScaleRoIAlign.html#torchvision.ops.MultiScaleRoIAlign
    "torchvision.ops.MultiScaleRoIAlign")(featmap_names, ...[, ...]) | Multi-scale
    RoIAlign pooling, which is useful for detection with or without FPN. |'
  prefs: []
  type: TYPE_TB
- en: '| [`RoIAlign`](generated/torchvision.ops.RoIAlign.html#torchvision.ops.RoIAlign
    "torchvision.ops.RoIAlign")(output_size, spatial_scale, ...[, ...]) | See [`roi_align()`](generated/torchvision.ops.roi_align.html#torchvision.ops.roi_align
    "torchvision.ops.roi_align"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`RoIPool`](generated/torchvision.ops.RoIPool.html#torchvision.ops.RoIPool
    "torchvision.ops.RoIPool")(output_size, spatial_scale) | See [`roi_pool()`](generated/torchvision.ops.roi_pool.html#torchvision.ops.roi_pool
    "torchvision.ops.roi_pool"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`PSRoIAlign`](generated/torchvision.ops.PSRoIAlign.html#torchvision.ops.PSRoIAlign
    "torchvision.ops.PSRoIAlign")(output_size, spatial_scale, ...) | See [`ps_roi_align()`](generated/torchvision.ops.ps_roi_align.html#torchvision.ops.ps_roi_align
    "torchvision.ops.ps_roi_align"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`PSRoIPool`](generated/torchvision.ops.PSRoIPool.html#torchvision.ops.PSRoIPool
    "torchvision.ops.PSRoIPool")(output_size, spatial_scale) | See [`ps_roi_pool()`](generated/torchvision.ops.ps_roi_pool.html#torchvision.ops.ps_roi_pool
    "torchvision.ops.ps_roi_pool"). |'
  prefs: []
  type: TYPE_TB
- en: Box Operators[¶](#box-operators "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These utility functions perform various operations on bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`box_area`](generated/torchvision.ops.box_area.html#torchvision.ops.box_area
    "torchvision.ops.box_area")(boxes) | Computes the area of a set of bounding boxes,
    which are specified by their (x1, y1, x2, y2) coordinates. |'
  prefs: []
  type: TYPE_TB
- en: '| [`box_convert`](generated/torchvision.ops.box_convert.html#torchvision.ops.box_convert
    "torchvision.ops.box_convert")(boxes, in_fmt, out_fmt) | Converts boxes from given
    in_fmt to out_fmt. |'
  prefs: []
  type: TYPE_TB
- en: '| [`box_iou`](generated/torchvision.ops.box_iou.html#torchvision.ops.box_iou
    "torchvision.ops.box_iou")(boxes1, boxes2) | Return intersection-over-union (Jaccard
    index) between two sets of boxes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`clip_boxes_to_image`](generated/torchvision.ops.clip_boxes_to_image.html#torchvision.ops.clip_boxes_to_image
    "torchvision.ops.clip_boxes_to_image")(boxes, size) | Clip boxes so that they
    lie inside an image of size size. |'
  prefs: []
  type: TYPE_TB
- en: '| [`complete_box_iou`](generated/torchvision.ops.complete_box_iou.html#torchvision.ops.complete_box_iou
    "torchvision.ops.complete_box_iou")(boxes1, boxes2[, eps]) | Return complete intersection-over-union
    (Jaccard index) between two sets of boxes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`distance_box_iou`](generated/torchvision.ops.distance_box_iou.html#torchvision.ops.distance_box_iou
    "torchvision.ops.distance_box_iou")(boxes1, boxes2[, eps]) | Return distance intersection-over-union
    (Jaccard index) between two sets of boxes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`generalized_box_iou`](generated/torchvision.ops.generalized_box_iou.html#torchvision.ops.generalized_box_iou
    "torchvision.ops.generalized_box_iou")(boxes1, boxes2) | Return generalized intersection-over-union
    (Jaccard index) between two sets of boxes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`remove_small_boxes`](generated/torchvision.ops.remove_small_boxes.html#torchvision.ops.remove_small_boxes
    "torchvision.ops.remove_small_boxes")(boxes, min_size) | Remove boxes which contains
    at least one side smaller than min_size. |'
  prefs: []
  type: TYPE_TB
- en: Losses[¶](#losses "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following vision-specific loss functions are implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`complete_box_iou_loss`](generated/torchvision.ops.complete_box_iou_loss.html#torchvision.ops.complete_box_iou_loss
    "torchvision.ops.complete_box_iou_loss")(boxes1, boxes2[, ...]) | Gradient-friendly
    IoU loss with an additional penalty that is non-zero when the boxes do not overlap.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`distance_box_iou_loss`](generated/torchvision.ops.distance_box_iou_loss.html#torchvision.ops.distance_box_iou_loss
    "torchvision.ops.distance_box_iou_loss")(boxes1, boxes2[, ...]) | Gradient-friendly
    IoU loss with an additional penalty that is non-zero when the distance between
    boxes'' centers isn''t zero. |'
  prefs: []
  type: TYPE_TB
- en: '| [`generalized_box_iou_loss`](generated/torchvision.ops.generalized_box_iou_loss.html#torchvision.ops.generalized_box_iou_loss
    "torchvision.ops.generalized_box_iou_loss")(boxes1, boxes2[, ...]) | Gradient-friendly
    IoU loss with an additional penalty that is non-zero when the boxes do not overlap
    and scales with the size of their smallest enclosing box. |'
  prefs: []
  type: TYPE_TB
- en: '| [`sigmoid_focal_loss`](generated/torchvision.ops.sigmoid_focal_loss.html#torchvision.ops.sigmoid_focal_loss
    "torchvision.ops.sigmoid_focal_loss")(inputs, targets[, alpha, ...]) | Loss used
    in RetinaNet for dense detection: [https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002).
    |'
  prefs: []
  type: TYPE_TB
- en: Layers[¶](#layers "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TorchVision provides commonly used building blocks as layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`Conv2dNormActivation`](generated/torchvision.ops.Conv2dNormActivation.html#torchvision.ops.Conv2dNormActivation
    "torchvision.ops.Conv2dNormActivation")(in_channels, ...) | Configurable block
    used for Convolution2d-Normalization-Activation blocks. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Conv3dNormActivation`](generated/torchvision.ops.Conv3dNormActivation.html#torchvision.ops.Conv3dNormActivation
    "torchvision.ops.Conv3dNormActivation")(in_channels, ...) | Configurable block
    used for Convolution3d-Normalization-Activation blocks. |'
  prefs: []
  type: TYPE_TB
- en: '| [`DeformConv2d`](generated/torchvision.ops.DeformConv2d.html#torchvision.ops.DeformConv2d
    "torchvision.ops.DeformConv2d")(in_channels, out_channels, ...) | See [`deform_conv2d()`](generated/torchvision.ops.deform_conv2d.html#torchvision.ops.deform_conv2d
    "torchvision.ops.deform_conv2d"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`DropBlock2d`](generated/torchvision.ops.DropBlock2d.html#torchvision.ops.DropBlock2d
    "torchvision.ops.DropBlock2d")(p, block_size[, inplace, eps]) | See [`drop_block2d()`](generated/torchvision.ops.drop_block2d.html#torchvision.ops.drop_block2d
    "torchvision.ops.drop_block2d"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`DropBlock3d`](generated/torchvision.ops.DropBlock3d.html#torchvision.ops.DropBlock3d
    "torchvision.ops.DropBlock3d")(p, block_size[, inplace, eps]) | See [`drop_block3d()`](generated/torchvision.ops.drop_block3d.html#torchvision.ops.drop_block3d
    "torchvision.ops.drop_block3d"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`FrozenBatchNorm2d`](generated/torchvision.ops.FrozenBatchNorm2d.html#torchvision.ops.FrozenBatchNorm2d
    "torchvision.ops.FrozenBatchNorm2d")(num_features[, eps]) | BatchNorm2d where
    the batch statistics and the affine parameters are fixed |'
  prefs: []
  type: TYPE_TB
- en: '| [`MLP`](generated/torchvision.ops.MLP.html#torchvision.ops.MLP "torchvision.ops.MLP")(in_channels, hidden_channels, ...)
    | This block implements the multi-layer perceptron (MLP) module. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Permute`](generated/torchvision.ops.Permute.html#torchvision.ops.Permute
    "torchvision.ops.Permute")(dims) | This module returns a view of the tensor input
    with its dimensions permuted. |'
  prefs: []
  type: TYPE_TB
- en: '| [`SqueezeExcitation`](generated/torchvision.ops.SqueezeExcitation.html#torchvision.ops.SqueezeExcitation
    "torchvision.ops.SqueezeExcitation")(input_channels, ...) | This block implements
    the Squeeze-and-Excitation block from [https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)
    (see Fig. |'
  prefs: []
  type: TYPE_TB
- en: '| [`StochasticDepth`](generated/torchvision.ops.StochasticDepth.html#torchvision.ops.StochasticDepth
    "torchvision.ops.StochasticDepth")(p, mode) | See [`stochastic_depth()`](generated/torchvision.ops.stochastic_depth.html#torchvision.ops.stochastic_depth
    "torchvision.ops.stochastic_depth"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`deform_conv2d`](generated/torchvision.ops.deform_conv2d.html#torchvision.ops.deform_conv2d
    "torchvision.ops.deform_conv2d")(input, offset, weight[, bias, ...]) | Performs
    Deformable Convolution v2, described in [Deformable ConvNets v2: More Deformable,
    Better Results](https://arxiv.org/abs/1811.11168) if `mask` is not `None` and
    Performs Deformable Convolution, described in [Deformable Convolutional Networks](https://arxiv.org/abs/1703.06211)
    if `mask` is `None`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`drop_block2d`](generated/torchvision.ops.drop_block2d.html#torchvision.ops.drop_block2d
    "torchvision.ops.drop_block2d")(input, p, block_size[, ...]) | Implements DropBlock2d
    from "DropBlock: A regularization method for convolutional networks" <https://arxiv.org/abs/1810.12890>.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`drop_block3d`](generated/torchvision.ops.drop_block3d.html#torchvision.ops.drop_block3d
    "torchvision.ops.drop_block3d")(input, p, block_size[, ...]) | Implements DropBlock3d
    from "DropBlock: A regularization method for convolutional networks" <https://arxiv.org/abs/1810.12890>.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`stochastic_depth`](generated/torchvision.ops.stochastic_depth.html#torchvision.ops.stochastic_depth
    "torchvision.ops.stochastic_depth")(input, p, mode[, training]) | Implements the
    Stochastic Depth from ["Deep Networks with Stochastic Depth"](https://arxiv.org/abs/1603.09382)
    used for randomly dropping residual branches of residual architectures. |'
  prefs: []
  type: TYPE_TB
