- en: ASR Inference with CTC Decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/audio/stable/tutorials/asr_inference_with_ctc_decoder_tutorial.html](https://pytorch.org/audio/stable/tutorials/asr_inference_with_ctc_decoder_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-tutorials-asr-inference-with-ctc-decoder-tutorial-py)
    to download the full example code
  prefs: []
  type: TYPE_NORMAL
- en: '**Author**: [Caroline Chen](mailto:carolinechen%40meta.com)'
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial shows how to perform speech recognition inference using a CTC
    beam search decoder with lexicon constraint and KenLM language model support.
    We demonstrate this on a pretrained wav2vec 2.0 model trained using CTC loss.
  prefs: []
  type: TYPE_NORMAL
- en: Overview[](#overview "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beam search decoding works by iteratively expanding text hypotheses (beams)
    with next possible characters, and maintaining only the hypotheses with the highest
    scores at each time step. A language model can be incorporated into the scoring
    computation, and adding a lexicon constraint restricts the next possible tokens
    for the hypotheses so that only words from the lexicon can be generated.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying implementation is ported from [Flashlight](https://arxiv.org/pdf/2201.12465.pdf)’s
    beam search decoder. A mathematical formula for the decoder optimization can be
    found in the [Wav2Letter paper](https://arxiv.org/pdf/1609.03193.pdf), and a more
    detailed algorithm can be found in this [blog](https://towardsdatascience.com/boosting-your-sequence-generation-performance-with-beam-search-language-model-decoding-74ee64de435a).
  prefs: []
  type: TYPE_NORMAL
- en: Running ASR inference using a CTC Beam Search decoder with a language model
    and lexicon constraint requires the following components
  prefs: []
  type: TYPE_NORMAL
- en: 'Acoustic Model: model predicting phonetics from audio waveforms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tokens: the possible predicted tokens from the acoustic model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lexicon: mapping between possible words and their corresponding tokens sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language Model (LM): n-gram language model trained with the [KenLM library](https://kheafield.com/code/kenlm/),
    or custom language model that inherits [`CTCDecoderLM`](../generated/torchaudio.models.decoder.CTCDecoder.html#torchaudio.models.decoder.CTCDecoderLM
    "torchaudio.models.decoder.CTCDecoderLM")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acoustic Model and Set Up[](#acoustic-model-and-set-up "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First we import the necessary utilities and fetch the data that we are working
    with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We use the pretrained [Wav2Vec 2.0](https://arxiv.org/abs/2006.11477) Base model
    that is finetuned on 10 min of the [LibriSpeech dataset](http://www.openslr.org/12),
    which can be loaded in using [`torchaudio.pipelines.WAV2VEC2_ASR_BASE_10M`](../generated/torchaudio.pipelines.WAV2VEC2_ASR_BASE_10M.html#torchaudio.pipelines.WAV2VEC2_ASR_BASE_10M
    "torchaudio.pipelines.WAV2VEC2_ASR_BASE_10M"). For more detail on running Wav2Vec
    2.0 speech recognition pipelines in torchaudio, please refer to [this tutorial](./speech_recognition_pipeline_tutorial.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We will load a sample from the LibriSpeech test-other dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: The transcript corresponding to this audio file is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Files and Data for Decoder[](#files-and-data-for-decoder "Permalink to this
    heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we load in our token, lexicon, and language model data, which are used
    by the decoder to predict words from the acoustic model output. Pretrained files
    for the LibriSpeech dataset can be downloaded through torchaudio, or the user
    can provide their own files.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens[](#tokens "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The tokens are the possible symbols that the acoustic model can predict, including
    the blank and silent symbols. It can either be passed in as a file, where each
    line consists of the tokens corresponding to the same index, or as a list of tokens,
    each mapping to a unique index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Lexicon[](#lexicon "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The lexicon is a mapping from words to their corresponding tokens sequence,
    and is used to restrict the search space of the decoder to only words from the
    lexicon. The expected format of the lexicon file is a line per word, with a word
    followed by its space-split tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Language Model[](#language-model "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A language model can be used in decoding to improve the results, by factoring
    in a language model score that represents the likelihood of the sequence into
    the beam search computation. Below, we outline the different forms of language
    models that are supported for decoding.
  prefs: []
  type: TYPE_NORMAL
- en: No Language Model[](#no-language-model "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To create a decoder instance without a language model, set lm=None when initializing
    the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: KenLM[](#kenlm "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is an n-gram language model trained with the [KenLM library](https://kheafield.com/code/kenlm/).
    Both the `.arpa` or the binarized `.bin` LM can be used, but the binary format
    is recommended for faster loading.
  prefs: []
  type: TYPE_NORMAL
- en: The language model used in this tutorial is a 4-gram KenLM trained using [LibriSpeech](http://www.openslr.org/11).
  prefs: []
  type: TYPE_NORMAL
- en: Custom Language Model[](#custom-language-model "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Users can define their own custom language model in Python, whether it be a
    statistical or neural network language model, using [`CTCDecoderLM`](../generated/torchaudio.models.decoder.CTCDecoder.html#torchaudio.models.decoder.CTCDecoderLM
    "torchaudio.models.decoder.CTCDecoderLM") and [`CTCDecoderLMState`](../generated/torchaudio.models.decoder.CTCDecoder.html#torchaudio.models.decoder.CTCDecoderLMState
    "torchaudio.models.decoder.CTCDecoderLMState").
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the following code creates a basic wrapper around a PyTorch `torch.nn.Module`
    language model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Downloading Pretrained Files[](#downloading-pretrained-files "Permalink to
    this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pretrained files for the LibriSpeech dataset can be downloaded using [`download_pretrained_files()`](../generated/torchaudio.models.decoder.download_pretrained_files.html#torchaudio.models.decoder.download_pretrained_files
    "torchaudio.models.decoder.download_pretrained_files").
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: this cell may take a couple of minutes to run, as the language model
    can be large'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Construct Decoders[](#construct-decoders "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we construct both a beam search decoder and a greedy decoder
    for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Beam Search Decoder[](#beam-search-decoder "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decoder can be constructed using the factory function [`ctc_decoder()`](../generated/torchaudio.models.decoder.ctc_decoder.html#torchaudio.models.decoder.ctc_decoder
    "torchaudio.models.decoder.ctc_decoder"). In addition to the previously mentioned
    components, it also takes in various beam search decoding parameters and token/word
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: This decoder can also be run without a language model by passing in None into
    the lm parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Greedy Decoder[](#greedy-decoder "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Run Inference[](#run-inference "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the data, acoustic model, and decoder, we can perform inference.
    The output of the beam search decoder is of type [`CTCHypothesis`](../generated/torchaudio.models.decoder.CTCDecoder.html#torchaudio.models.decoder.CTCHypothesis
    "torchaudio.models.decoder.CTCHypothesis"), consisting of the predicted token
    IDs, corresponding words (if a lexicon is provided), hypothesis score, and timesteps
    corresponding to the token IDs. Recall the transcript corresponding to the waveform
    is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The greedy decoder gives the following result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the beam search decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The [`words`](../generated/torchaudio.models.decoder.CTCDecoder.html#torchaudio.models.decoder.CTCHypothesis.words
    "torchaudio.models.decoder.CTCHypothesis.words") field of the output hypotheses
    will be empty if no lexicon is provided to the decoder. To retrieve a transcript
    with lexicon-free decoding, you can perform the following to retrieve the token
    indices, convert them to original tokens, then join them together.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We see that the transcript with the lexicon-constrained beam search decoder
    produces a more accurate result consisting of real words, while the greedy decoder
    can predict incorrectly spelled words like “affrayd” and “shoktd”.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental decoding[](#incremental-decoding "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the input speech is long, one can decode the emission in incremental manner.
  prefs: []
  type: TYPE_NORMAL
- en: You need to first initialize the internal state of the decoder with [`decode_begin()`](../generated/torchaudio.models.decoder.CTCDecoder.html#torchaudio.models.decoder.CTCDecoder.decode_begin
    "torchaudio.models.decoder.CTCDecoder.decode_begin").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Then, you can pass emissions to [`decode_begin()`](../generated/torchaudio.models.decoder.CTCDecoder.html#torchaudio.models.decoder.CTCDecoder.decode_begin
    "torchaudio.models.decoder.CTCDecoder.decode_begin"). Here we use the same emission
    but pass it to the decoder one frame at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Finally, finalize the internal state of the decoder, and retrieve the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The result of incremental decoding is identical to batch decoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Timestep Alignments[](#timestep-alignments "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that one of the components of the resulting Hypotheses is timesteps corresponding
    to the token IDs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Below, we visualize the token timestep alignments relative to the original waveform.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![asr inference with ctc decoder tutorial](../Images/e2abf68b7cace07964d5580316ac4575.png)'
  prefs: []
  type: TYPE_IMG
- en: Beam Search Decoder Parameters[](#beam-search-decoder-parameters "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we go a little bit more in depth about some different parameters
    and tradeoffs. For the full list of customizable parameters, please refer to the
    [`documentation`](../generated/torchaudio.models.decoder.ctc_decoder.html#torchaudio.models.decoder.ctc_decoder
    "torchaudio.models.decoder.ctc_decoder").
  prefs: []
  type: TYPE_NORMAL
- en: Helper Function[](#helper-function "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: nbest[](#nbest "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This parameter indicates the number of best hypotheses to return, which is a
    property that is not possible with the greedy decoder. For instance, by setting
    `nbest=3` when constructing the beam search decoder earlier, we can now access
    the hypotheses with the top 3 scores.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: beam size[](#beam-size "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `beam_size` parameter determines the maximum number of best hypotheses to
    hold after each decoding step. Using larger beam sizes allows for exploring a
    larger range of possible hypotheses which can produce hypotheses with higher scores,
    but it is computationally more expensive and does not provide additional gains
    beyond a certain point.
  prefs: []
  type: TYPE_NORMAL
- en: In the example below, we see improvement in decoding quality as we increase
    beam size from 1 to 5 to 50, but notice how using a beam size of 500 provides
    the same output as beam size 50 while increase the computation time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: beam size token[](#beam-size-token "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `beam_size_token` parameter corresponds to the number of tokens to consider
    for expanding each hypothesis at the decoding step. Exploring a larger number
    of next possible tokens increases the range of potential hypotheses at the cost
    of computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: beam threshold[](#beam-threshold "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `beam_threshold` parameter is used to prune the stored hypotheses set at
    each decoding step, removing hypotheses whose scores are greater than `beam_threshold`
    away from the highest scoring hypothesis. There is a balance between choosing
    smaller thresholds to prune more hypotheses and reduce the search space, and choosing
    a large enough threshold such that plausible hypotheses are not pruned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: language model weight[](#language-model-weight "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `lm_weight` parameter is the weight to assign to the language model score
    which to accumulate with the acoustic model score for determining the overall
    scores. Larger weights encourage the model to predict next words based on the
    language model, while smaller weights give more weight to the acoustic model score
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: additional parameters[](#additional-parameters "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Additional parameters that can be optimized include the following
  prefs: []
  type: TYPE_NORMAL
- en: '`word_score`: score to add when word finishes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_score`: unknown word appearance score to add'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sil_score`: silence appearance score to add'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_add`: whether to use log add for lexicon Trie smearing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 1 minutes 55.312 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: asr_inference_with_ctc_decoder_tutorial.py`](../_downloads/da151acc525ba1fb468e2a4904659af1/asr_inference_with_ctc_decoder_tutorial.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: asr_inference_with_ctc_decoder_tutorial.ipynb`](../_downloads/ade1a3c3b444796d2a34839c7ea75426/asr_inference_with_ctc_decoder_tutorial.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
