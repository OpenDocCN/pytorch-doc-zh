# 分布式优化器

> 原文：[https://pytorch.org/docs/stable/distributed.optim.html](https://pytorch.org/docs/stable/distributed.optim.html)

警告

在使用CUDA张量时，不支持分布式优化器

[`torch.distributed.optim`](#module-torch.distributed.optim "torch.distributed.optim") 暴露了DistributedOptimizer，它接受一个远程参数（`RRef`）的列表，并在参数所在的工作节点上本地运行优化器。分布式优化器可以使用任何本地优化器[基类](optim.html#optimizer-algorithms)来在每个工作节点上应用梯度。

```py
class torch.distributed.optim.DistributedOptimizer(optimizer_class, params_rref, *args, **kwargs)¶
```

DistributedOptimizer接受分布在工作节点上的参数的远程引用，并为每个参数本地应用给定的优化器。

这个类使用[`get_gradients()`](rpc.html#torch.distributed.autograd.get_gradients "torch.distributed.autograd.get_gradients")来检索特定参数的梯度。

来自同一客户端或不同客户端的对[`step()`](#torch.distributed.optim.DistributedOptimizer.step "torch.distributed.optim.DistributedOptimizer.step")的并发调用将在每个工作节点上串行化 - 因为每个工作节点的优化器一次只能处理一组梯度。但是，不能保证一个客户端一次只执行一个完整的前向-反向-优化器序列。这意味着应用的梯度可能不对应于给定工作节点上执行的最新前向传递。此外，工作节点之间也没有保证的顺序。

DistributedOptimizer默认启用了启用了TorchScript的本地优化器，以便在多线程训练（例如分布式模型并行）的情况下，优化器更新不会受到Python全局解释器锁（GIL）的阻塞。目前大多数优化器都启用了这一特性。您也可以按照[教程](https://github.com/pytorch/tutorials/pull/1465)中的步骤为自定义优化器启用TorchScript支持。

参数

+   **optimizer_class** ([*optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer")) – 在每个工作节点上实例化的优化器类。

+   **params_rref** ([*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)")*[**RRef**]*) – 用于优化的本地或远程参数的RRef列表。

+   **args** – 传递给每个工作节点上优化器构造函数的参数。

+   **kwargs** – 传递给每个工作节点上优化器构造函数的参数。

示例::

```py
>>> import torch.distributed.autograd as dist_autograd
>>> import torch.distributed.rpc as rpc
>>> from torch import optim
>>> from torch.distributed.optim import DistributedOptimizer
>>>
>>> with dist_autograd.context() as context_id:
>>>   # Forward pass.
>>>   rref1 = rpc.remote("worker1", torch.add, args=(torch.ones(2), 3))
>>>   rref2 = rpc.remote("worker1", torch.add, args=(torch.ones(2), 1))
>>>   loss = rref1.to_here() + rref2.to_here()
>>>
>>>   # Backward pass.
>>>   dist_autograd.backward(context_id, [loss.sum()])
>>>
>>>   # Optimizer.
>>>   dist_optim = DistributedOptimizer(
>>>      optim.SGD,
>>>      [rref1, rref2],
>>>      lr=0.05,
>>>   )
>>>   dist_optim.step(context_id) 
```

```py
step(context_id)¶
```

执行单个优化步骤。

这将在包含要优化的参数的每个工作节点上调用[`torch.optim.Optimizer.step()`](generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step "torch.optim.Optimizer.step")，并将阻塞直到所有工作节点返回。提供的`context_id`将用于检索包含应该应用于参数的梯度的相应[`context`](rpc.html#torch.distributed.autograd.context "torch.distributed.autograd.context")。

参数

**context_id** – 我们应该运行优化器步骤的自动求导上下文id。

```py
class torch.distributed.optim.PostLocalSGDOptimizer(optim, averager)¶
```

包装任意[`torch.optim.Optimizer`](optim.html#torch.optim.Optimizer "torch.optim.Optimizer")并运行[后局部SGD](https://arxiv.org/abs/1808.07217)，此优化器在每一步都运行本地优化器。在热身阶段之后，它会定期平均参数，然后应用本地优化器。

参数

+   **optim** ([*Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.optimizer.Optimizer")) – 本地优化器。

+   **averager** (*ModelAverager*) – 一个模型平均器实例，用于运行后局部SGD算法。

示例:

```py
>>> import torch
>>> import torch.distributed as dist
>>> import torch.distributed.algorithms.model_averaging.averagers as averagers
>>> import torch.nn as nn
>>> from torch.distributed.optim import PostLocalSGDOptimizer
>>> from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (
>>>   PostLocalSGDState,
>>>   post_localSGD_hook,
>>> )
>>>
>>> model = nn.parallel.DistributedDataParallel(
>>>    module, device_ids=[rank], output_device=rank
>>> )
>>>
>>> # Register a post-localSGD communication hook.
>>> state = PostLocalSGDState(process_group=None, subgroup=None, start_localSGD_iter=100)
>>> model.register_comm_hook(state, post_localSGD_hook)
>>>
>>> # Create a post-localSGD optimizer that wraps a local optimizer.
>>> # Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as
>>> # ``start_localSGD_iter`` used in ``PostLocalSGDState``.
>>> local_optim = torch.optim.SGD(params=model.parameters(), lr=0.01)
>>> opt = PostLocalSGDOptimizer(
>>>     optim=local_optim,
>>>     averager=averagers.PeriodicModelAverager(period=4, warmup_steps=100)
>>> )
>>>
>>> # In the first 100 steps, DDP runs global gradient averaging at every step.
>>> # After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),
>>> # and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.
>>> for step in range(0, 200):
>>>    opt.zero_grad()
>>>    loss = loss_fn(output, labels)
>>>    loss.backward()
>>>    opt.step() 
```

```py
load_state_dict(state_dict)¶
```

这与[`torch.optim.Optimizer`](optim.html#torch.optim.Optimizer "torch.optim.Optimizer") [`load_state_dict()`](#torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict "torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict")相同，但还将模型平均器的步骤值恢复为提供的`state_dict`中保存的值。

如果`state_dict`中没有`"step"`条目，它将引发警告并将模型平均器的步骤初始化为0。

```py
state_dict()¶
```

这与[`torch.optim.Optimizer`](optim.html#torch.optim.Optimizer "torch.optim.Optimizer") [`state_dict()`](#torch.distributed.optim.PostLocalSGDOptimizer.state_dict "torch.distributed.optim.PostLocalSGDOptimizer.state_dict")相同，但添加了一个额外的条目来记录模型平均器的步骤到检查点，以确保重新加载不会导致不必要的再次热身。

```py
step()¶
```

执行单个优化步骤（参数更新）。

```py
class torch.distributed.optim.ZeroRedundancyOptimizer(params, optimizer_class, process_group=None, parameters_as_bucket_view=False, overlap_with_ddp=False, **defaults)¶
```

将任意的[`optim.Optimizer`](optim.html#torch.optim.Optimizer "torch.optim.Optimizer")包装起来，并将其状态在组中分片。

共享的方式如[ZeRO](https://arxiv.org/abs/1910.02054)所描述。

每个rank中的本地优化器实例只负责更新大约`1 / world_size`的参数，因此只需要保留`1 / world_size`的优化器状态。在本地更新参数后，每个rank将向所有其他对等体广播其参数，以保持所有模型副本处于相同状态。`ZeroRedundancyOptimizer`可以与[`torch.nn.parallel.DistributedDataParallel`](generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel "torch.nn.parallel.DistributedDataParallel")一起使用，以减少每个rank的内存消耗峰值。

`ZeroRedundancyOptimizer`使用一种排序贪婪算法在每个rank上打包一定数量的参数。每个参数属于单个rank，不会在各个rank之间分割。分区是任意的，可能与参数注册或使用顺序不匹配。

参数

**params** (`Iterable`) – 一个包含所有参数的`Iterable`，这些参数将在各个rank之间进行分片。

关键字参数

+   **optimizer_class** (`torch.nn.Optimizer`) – 本地优化器的类。

+   **process_group** (`ProcessGroup`, optional) – `torch.distributed` `ProcessGroup`（默认值：由[`torch.distributed.init_process_group()`](distributed.html#torch.distributed.init_process_group "torch.distributed.init_process_group")初始化的`dist.group.WORLD`）。

+   **parameters_as_bucket_view** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")*,* *optional*) – 如果为`True`，参数将被打包成桶以加快通信速度，并且`param.data`字段指向不同偏移量的桶视图；如果为`False`，每个单独的参数将被单独传输，每个`params.data`保持不变（默认值：`False`）。

+   **overlap_with_ddp** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")*,* *optional*) – 如果为`True`，[`step()`](#torch.distributed.optim.ZeroRedundancyOptimizer.step "torch.distributed.optim.ZeroRedundancyOptimizer.step")将与`DistributedDataParallel`的梯度同步重叠；这需要（1）为`optimizer_class`参数提供一个功能性优化器，或者具有功能等效的优化器，并且（2）注册一个由`ddp_zero_hook.py`中的函数构建的DDP通信钩子；参数被打包成与`DistributedDataParallel`中相匹配的桶，这意味着`parameters_as_bucket_view`参数将被忽略。如果为`False`，[`step()`](#torch.distributed.optim.ZeroRedundancyOptimizer.step "torch.distributed.optim.ZeroRedundancyOptimizer.step")在反向传播后（正常情况下）独立运行。 （默认值：`False`）

+   ****defaults** – 任何后续参数，将被转发给本地优化器。

示例：

```py
>>> import torch.nn as nn
>>> from torch.distributed.optim import ZeroRedundancyOptimizer
>>> from torch.nn.parallel import DistributedDataParallel as DDP
>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])
>>> ddp = DDP(model, device_ids=[rank])
>>> opt = ZeroRedundancyOptimizer(
>>>     ddp.parameters(),
>>>     optimizer_class=torch.optim.Adam,
>>>     lr=0.01
>>> )
>>> ddp(inputs).sum().backward()
>>> opt.step() 
```

警告

目前，`ZeroRedundancyOptimizer`要求传入的所有参数都是相同的密集类型。

警告

如果传递`overlap_with_ddp=True`，请注意以下事项：鉴于当前实现了与[`ZeroRedundancyOptimizer`](#torch.distributed.optim.ZeroRedundancyOptimizer)重叠的`DistributedDataParallel`的方式，前两个或三个训练迭代不会在优化器步骤中执行参数更新，具体取决于`static_graph=False`或`static_graph=True`。这是因为它需要关于`DistributedDataParallel`使用的梯度桶策略的信息，该信息在第二次前向传递（如果`static_graph=False`）或第三次前向传递（如果`static_graph=True`）之前不会最终确定。为了调整此问题，一个选项是在前面添加虚拟输入。

警告

ZeroRedundancyOptimizer是实验性的，可能会发生变化。

```py
add_param_group(param_group)¶
```

向`Optimizer`的`param_groups`添加一个参数组。

在微调预训练网络时，这可能很有用，因为冻结的层可以在训练进行时变为可训练，并添加到`Optimizer`中。

参数

**param_group**（[*dict*](https://docs.python.org/3/library/stdtypes.html#dict)） - 指定要优化的参数和特定于组的优化选项。

警告

此方法处理所有分区上的碎片更新，但需要在所有等级上调用。在部分等级上调用此方法将导致训练挂起，因为通信原语是根据受管理的参数调用的，并且期望所有等级参与相同的参数集。

```py
consolidate_state_dict(to=0)¶
```

在目标等级上合并`state_dict`列表（每个等级一个）。

参数

**to**（[*int*](https://docs.python.org/3/library/functions.html#int)） - 接收优化器状态的等级（默认值：0）。

引发

[**RuntimeError**](https://docs.python.org/3/library/exceptions.html#RuntimeError) - 如果`overlap_with_ddp=True`，并且在此[`ZeroRedundancyOptimizer`](#torch.distributed.optim.ZeroRedundancyOptimizer)实例完全初始化之前调用此方法，这会在`DistributedDataParallel`梯度桶被重建后发生。

警告

这需要在所有等级上调用。

```py
property join_device: device¶
```

返回默认设备。

```py
join_hook(**kwargs)¶
```

返回ZeRO连接钩子。

它通过在优化器步骤中模拟集体通信来支持不均匀输入的训练。

在调用此钩子之前必须正确设置梯度。

参数

**kwargs**（[*dict*](https://docs.python.org/3/library/stdtypes.html#dict)） - 包含任何关键字参数以在运行时修改连接钩子行为的[`dict`](https://docs.python.org/3/library/stdtypes.html#dict)；所有共享相同连接上下文管理器的`Joinable`实例将在运行时转发相同的`kwargs`值。

此钩子不支持任何关键字参数；即`kwargs`未使用。

```py
property join_process_group: Any¶
```

返回进程组。

```py
load_state_dict(state_dict)¶
```

从输入`state_dict`中加载与给定等级相关的状态，根据需要更新本地优化器。

参数

**state_dict**（[*dict*](https://docs.python.org/3/library/stdtypes.html#dict)） - 优化器状态；应该是从调用[`state_dict()`](#torch.distributed.optim.ZeroRedundancyOptimizer.state_dict)返回的对象。

引发

[**RuntimeError**](https://docs.python.org/3/library/exceptions.html#RuntimeError) - 如果`overlap_with_ddp=True`，并且在此[`ZeroRedundancyOptimizer`](#torch.distributed.optim.ZeroRedundancyOptimizer)实例完全初始化之前调用此方法，这会在`DistributedDataParallel`梯度桶被重建后发生。

```py
state_dict()¶
```

返回此等级已知的最后一个全局优化器状态。

引发

[**RuntimeError**](https://docs.python.org/3/library/exceptions.html#RuntimeError "(在 Python v3.12 中)") - 如果 `overlap_with_ddp=True` 并且在此 [`ZeroRedundancyOptimizer`](#torch.distributed.optim.ZeroRedundancyOptimizer "torch.distributed.optim.ZeroRedundancyOptimizer") 实例完全初始化之前调用此方法，这会在 `DistributedDataParallel` 梯度桶重建后发生；或者如果在调用此方法之前没有调用 [`consolidate_state_dict()`](#torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict "torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict")。

返回类型

[*字典*](https://docs.python.org/3/library/typing.html#typing.Dict "(在 Python v3.12 中)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(在 Python v3.12 中)"), [*任意*](https://docs.python.org/3/library/typing.html#typing.Any "(在 Python v3.12 中)")]

```py
step(closure=None, **kwargs)¶
```

执行单个优化器步骤并同步所有排名的参数。

参数

**closure** (*Callable*) - 重新评估模型并返回损失的闭包；对大多数优化器来说是可选的。

返回

根据底层本地优化器而定的可选损失。

返回类型

[*可选*](https://docs.python.org/3/library/typing.html#typing.Optional "(在 Python v3.12 中)")[[float](https://docs.python.org/3/library/functions.html#float "(在 Python v3.12 中)")]
