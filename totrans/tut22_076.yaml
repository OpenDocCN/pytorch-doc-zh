- en: (beta) Channels Last Memory Format in PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: （beta）PyTorch中的通道最后内存格式
- en: 原文：[https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-intermediate-memory-format-tutorial-py) to download
    the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-intermediate-memory-format-tutorial-py)下载完整示例代码
- en: '**Author**: [Vitaly Fedyunin](https://github.com/VitalyFedyunin)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：[Vitaly Fedyunin](https://github.com/VitalyFedyunin)'
- en: What is Channels Last
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是通道最后
- en: Channels last memory format is an alternative way of ordering NCHW tensors in
    memory preserving dimensions ordering. Channels last tensors ordered in such a
    way that channels become the densest dimension (aka storing images pixel-per-pixel).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通道最后的内存格式是在保留维度顺序的同时对NCHW张量进行排序的另一种方式。通道最后的张量以通道成为最密集的维度（即按像素存储图像）的方式进行排序。
- en: 'For example, classic (contiguous) storage of NCHW tensor (in our case it is
    two 4x4 images with 3 color channels) look like this:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，NCHW张量的经典（连续）存储（在我们的情况下，是两个具有3个颜色通道的4x4图像）如下所示：
- en: '![classic_memory_format](../Images/77e0660b596f377125122a2409288181.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![classic_memory_format](../Images/77e0660b596f377125122a2409288181.png)'
- en: 'Channels last memory format orders data differently:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通道最后内存格式以不同的方式对数据进行排序：
- en: '![channels_last_memory_format](../Images/462373919a0dfe17cd816fa0d8af140c.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![channels_last_memory_format](../Images/462373919a0dfe17cd816fa0d8af140c.png)'
- en: Pytorch supports memory formats (and provides back compatibility with existing
    models including eager, JIT, and TorchScript) by utilizing existing strides structure.
    For example, 10x3x16x16 batch in Channels last format will have strides equal
    to (768, 1, 48, 3).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch通过利用现有的步幅结构来支持内存格式（并提供与现有模型（包括eager、JIT和TorchScript）的向后兼容性）。例如，通道最后格式中的10x3x16x16批次将具有等于（768，1，48，3）的步幅。
- en: Channels last memory format is implemented for 4D NCHW Tensors only.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通道最后内存格式仅适用于4D NCHW张量。
- en: Memory Format API
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存格式API
- en: Here is how to convert tensors between contiguous and channels last memory formats.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在连续和通道最后的内存格式之间转换张量的方法。
- en: Classic PyTorch contiguous tensor
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的PyTorch连续张量
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Conversion operator
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 转换运算符
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Back to contiguous
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 回到连续
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Alternative option
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 备选选项
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Format checks
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 格式检查
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There are minor difference between the two APIs `to` and `contiguous`. We suggest
    to stick with `to` when explicitly converting memory format of tensor.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`to`和`contiguous`这两个API之间存在一些细微差别。我们建议在明确转换张量的内存格式时坚持使用`to`。'
- en: 'For general cases the two APIs behave the same. However in special cases for
    a 4D tensor with size `NCHW` when either: `C==1` or `H==1 && W==1`, only `to`
    would generate a proper stride to represent channels last memory format.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般情况，这两个API的行为是相同的。然而，在特殊情况下，对于大小为`NCHW`的4D张量，当`C==1`或`H==1 && W==1`时，只有`to`会生成适当的步幅以表示通道最后的内存格式。
- en: This is because in either of the two cases above, the memory format of a tensor
    is ambiguous, i.e. a contiguous tensor with size `N1HW` is both `contiguous` and
    channels last in memory storage. Therefore, they are already considered as `is_contiguous`
    for the given memory format and hence `contiguous` call becomes a no-op and would
    not update the stride. On the contrary, `to` would restride tensor with a meaningful
    stride on dimensions whose sizes are 1 in order to properly represent the intended
    memory format
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为在上述两种情况中，张量的内存格式是模糊的，即大小为`N1HW`的连续张量在内存存储中既是`contiguous`又是通道最后的。因此，它们已被视为给定内存格式的`is_contiguous`，因此`contiguous`调用变为无操作，并且不会更新步幅。相反，`to`会在尺寸为1的维度上重新调整张量的步幅，以正确表示预期的内存格式。
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Same thing applies to explicit permutation API `permute`. In special case where
    ambiguity could occur, `permute` does not guarantee to produce a stride that properly
    carry the intended memory format. We suggest to use `to` with explicit memory
    format to avoid unintended behavior.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的情况也适用于显式置换API `permute`。在可能发生模糊的特殊情况下，`permute`不能保证生成适当携带预期内存格式的步幅。我们建议使用`to`并明确指定内存格式，以避免意外行为。
- en: And a side note that in the extreme case, where three non-batch dimensions are
    all equal to `1` (`C==1 && H==1 && W==1`), current implementation cannot mark
    a tensor as channels last memory format.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另外需要注意的是，在极端情况下，当三个非批量维度都等于`1`时（`C==1 && H==1 && W==1`），当前的实现无法将张量标记为通道最后的内存格式。
- en: Create as channels last
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 创建为通道最后
- en: '[PRE12]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`clone` preserves memory format'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`clone` 保留内存格式'
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`to`, `cuda`, `float` … preserves memory format'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`to`，`cuda`，`float` … 保留内存格式'
- en: '[PRE16]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`empty_like`, `*_like` operators preserves memory format'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`empty_like`，`*_like`运算符保留内存格式'
- en: '[PRE18]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Pointwise operators preserves memory format
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 逐点运算符保留内存格式
- en: '[PRE20]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`Conv`, `Batchnorm` modules using `cudnn` backends support channels last (only
    works for cuDNN >= 7.6). Convolution modules, unlike binary p-wise operator, have
    channels last as the dominating memory format. If all inputs are in contiguous
    memory format, the operator produces output in contiguous memory format. Otherwise,
    output will be in channels last memory format.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cudnn`后端的`Conv`，`Batchnorm`模块支持通道最后（仅适用于cuDNN >= 7.6）。卷积模块，与二进制逐点运算符不同，通道最后是主导的内存格式。如果所有输入都在连续的内存格式中，操作符将以连续的内存格式生成输出。否则，输出将以通道最后的内存格式生成。
- en: '[PRE22]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: When input tensor reaches a operator without channels last support, a permutation
    should automatically apply in the kernel to restore contiguous on input tensor.
    This introduces overhead and stops the channels last memory format propagation.
    Nevertheless, it guarantees correct output.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入张量到达不支持通道最后的操作符时，内核应自动应用置换以恢复输入张量上的连续性。这会引入开销并停止通道最后的内存格式传播。尽管如此，它保证了正确的输出。
- en: Performance Gains
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能收益
- en: Channels last memory format optimizations are available on both GPU and CPU.
    On GPU, the most significant performance gains are observed on NVIDIA’s hardware
    with Tensor Cores support running on reduced precision (`torch.float16`). We were
    able to archive over 22% performance gains with channels last comparing to contiguous
    format, both while utilizing ‘AMP (Automated Mixed Precision)’ training scripts.
    Our scripts uses AMP supplied by NVIDIA [https://github.com/NVIDIA/apex](https://github.com/NVIDIA/apex).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Channels last内存格式优化在GPU和CPU上都可用。在GPU上，观察到NVIDIA硬件上具有Tensor Cores支持的运行在降低精度（`torch.float16`）时，性能增益最显著。我们能够在使用‘AMP（自动混合精度）’训练脚本时，通过Channels
    last实现超过22%的性能增益，同时利用了由NVIDIA提供的AMP [https://github.com/NVIDIA/apex](https://github.com/NVIDIA/apex)。
- en: '`python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2  ./data`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2  ./data`'
- en: '[PRE24]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Passing `--channels-last true` allows running a model in Channels last format
    with observed 22% performance gain.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递`--channels-last true`允许在Channels last格式中运行模型，观察到22%的性能增益。
- en: '`python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 --channels-last
    true ./data`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 --channels-last
    true ./data`'
- en: '[PRE25]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following list of models has the full support of Channels last and showing
    8%-35% performance gains on Volta devices: `alexnet`, `mnasnet0_5`, `mnasnet0_75`,
    `mnasnet1_0`, `mnasnet1_3`, `mobilenet_v2`, `resnet101`, `resnet152`, `resnet18`,
    `resnet34`, `resnet50`, `resnext50_32x4d`, `shufflenet_v2_x0_5`, `shufflenet_v2_x1_0`,
    `shufflenet_v2_x1_5`, `shufflenet_v2_x2_0`, `squeezenet1_0`, `squeezenet1_1`,
    `vgg11`, `vgg11_bn`, `vgg13`, `vgg13_bn`, `vgg16`, `vgg16_bn`, `vgg19`, `vgg19_bn`,
    `wide_resnet101_2`, `wide_resnet50_2`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型列表完全支持Channels last，并在Volta设备上显示8%-35%的性能增益：`alexnet`，`mnasnet0_5`，`mnasnet0_75`，`mnasnet1_0`，`mnasnet1_3`，`mobilenet_v2`，`resnet101`，`resnet152`，`resnet18`，`resnet34`，`resnet50`，`resnext50_32x4d`，`shufflenet_v2_x0_5`，`shufflenet_v2_x1_0`，`shufflenet_v2_x1_5`，`shufflenet_v2_x2_0`，`squeezenet1_0`，`squeezenet1_1`，`vgg11`，`vgg11_bn`，`vgg13`，`vgg13_bn`，`vgg16`，`vgg16_bn`，`vgg19`，`vgg19_bn`，`wide_resnet101_2`，`wide_resnet50_2`
- en: 'The following list of models has the full support of Channels last and showing
    26%-76% performance gains on Intel(R) Xeon(R) Ice Lake (or newer) CPUs: `alexnet`,
    `densenet121`, `densenet161`, `densenet169`, `googlenet`, `inception_v3`, `mnasnet0_5`,
    `mnasnet1_0`, `resnet101`, `resnet152`, `resnet18`, `resnet34`, `resnet50`, `resnext101_32x8d`,
    `resnext50_32x4d`, `shufflenet_v2_x0_5`, `shufflenet_v2_x1_0`, `squeezenet1_0`,
    `squeezenet1_1`, `vgg11`, `vgg11_bn`, `vgg13`, `vgg13_bn`, `vgg16`, `vgg16_bn`,
    `vgg19`, `vgg19_bn`, `wide_resnet101_2`, `wide_resnet50_2`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型列表完全支持Channels last，并在Intel(R) Xeon(R) Ice Lake（或更新）CPU上显示26%-76%的性能增益：`alexnet`，`densenet121`，`densenet161`，`densenet169`，`googlenet`，`inception_v3`，`mnasnet0_5`，`mnasnet1_0`，`resnet101`，`resnet152`，`resnet18`，`resnet34`，`resnet50`，`resnext101_32x8d`，`resnext50_32x4d`，`shufflenet_v2_x0_5`，`shufflenet_v2_x1_0`，`squeezenet1_0`，`squeezenet1_1`，`vgg11`，`vgg11_bn`，`vgg13`，`vgg13_bn`，`vgg16`，`vgg16_bn`，`vgg19`，`vgg19_bn`，`wide_resnet101_2`，`wide_resnet50_2`
- en: Converting existing models
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换现有模型
- en: Channels last support is not limited by existing models, as any model can be
    converted to channels last and propagate format through the graph as soon as input
    (or certain weight) is formatted correctly.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Channels last支持不仅限于现有模型，因为任何模型都可以转换为Channels last并在输入（或某些权重）正确格式化后通过图形传播格式。
- en: '[PRE26]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: However, not all operators fully converted to support channels last (usually
    returning contiguous output instead). In the example posted above, layers that
    does not support channels last will stop the memory format propagation. In spite
    of that, as we have converted the model to channels last format, that means each
    convolution layer, which has its 4 dimensional weight in channels last memory
    format, will restore channels last memory format and benefit from faster kernels.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有运算符都完全转换为支持Channels last（通常返回连续的输出）。在上面发布的示例中，不支持Channels last的层将停止内存格式传播。尽管如此，由于我们已将模型转换为Channels
    last格式，这意味着每个卷积层，其4维权重在Channels last内存格式中，将恢复Channels last内存格式并从更快的内核中受益。
- en: But operators that does not support channels last does introduce overhead by
    permutation. Optionally, you can investigate and identify operators in your model
    that does not support channels last, if you want to improve the performance of
    converted model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，不支持Channels last的运算符会通过置换引入开销。可选地，您可以调查并识别模型中不支持Channels last的运算符，如果要改进转换模型的性能。
- en: That means you need to verify the list of used operators against supported operators
    list [https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support](https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support),
    or introduce memory format checks into eager execution mode and run your model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着您需要根据支持的运算符列表[https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support](https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support)验证所使用的运算符列表，或者在急切执行模式中引入内存格式检查并运行您的模型。
- en: After running the code below, operators will raise an exception if the output
    of the operator doesn’t match the memory format of the input.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行以下代码后，如果运算符的输出与输入的内存格式不匹配，运算符将引发异常。
- en: '[PRE27]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If you found an operator that doesn’t support channels last tensors and you
    want to contribute, feel free to use following developers guide [https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators](https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发现一个不支持Channels last张量的运算符，并且您想要贡献，可以随时使用以下开发者指南[https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators](https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators)。
- en: Code below is to recover the attributes of torch.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是为了恢复torch的属性。
- en: '[PRE28]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Work to do
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要做的工作
- en: 'There are still many things to do, such as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多事情要做，例如：
- en: Resolving ambiguity of `N1HW` and `NC11` Tensors;
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决`N1HW`和`NC11`张量的歧义；
- en: Testing of Distributed Training support;
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试分布式训练支持；
- en: Improving operators coverage.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高运算符覆盖率。
- en: If you have feedback and/or suggestions for improvement, please let us know
    by creating [an issue](https://github.com/pytorch/pytorch/issues).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有反馈和/或改进建议，请通过创建[一个问题](https://github.com/pytorch/pytorch/issues)让我们知道。
- en: '**Total running time of the script:** ( 0 minutes 0.038 seconds)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（0分钟0.038秒）'
- en: '[`Download Python source code: memory_format_tutorial.py`](../_downloads/591028d309d0401740cd71eb6b14bf93/memory_format_tutorial.py)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：memory_format_tutorial.py`](../_downloads/591028d309d0401740cd71eb6b14bf93/memory_format_tutorial.py)'
- en: '[`Download Jupyter notebook: memory_format_tutorial.ipynb`](../_downloads/f11c58c36c9b8a5daf09d3f9a792ef84/memory_format_tutorial.ipynb)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：memory_format_tutorial.ipynb`](../_downloads/f11c58c36c9b8a5daf09d3f9a792ef84/memory_format_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)'
