["```py\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr=0.0001) \n```", "```py\noptim.SGD([\n                {'params': model.base.parameters()},\n                {'params': model.classifier.parameters(), 'lr': 1e-3}\n            ], lr=1e-2, momentum=0.9) \n```", "```py\nfor input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step() \n```", "```py\nfor input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure) \n```", "```py\nclass torch.optim.Optimizer(params, defaults)\u00b6\n```", "```py\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = ExponentialLR(optimizer, gamma=0.9)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step() \n```", "```py\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler1 = ExponentialLR(optimizer, gamma=0.9)\nscheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler1.step()\n    scheduler2.step() \n```", "```py\n>>> scheduler = ...\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step() \n```", "```py\n>>> averaged_model = AveragedModel(model) \n```", "```py\n>>> decay = 0.999\n>>> averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay)) \n```", "```py\n>>> averaged_model.update_parameters(model) \n```", "```py\n>>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.9 * averaged_model_parameter + 0.1 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg) \n```", "```py\n>>> ema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9)) \n```", "```py\n>>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05) \n```", "```py\n>>> torch.optim.swa_utils.update_bn(loader, swa_model) \n```", "```py\n>>> loader, optimizer, model, loss_fn = ...\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n>>> swa_start = 160\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>       if epoch > swa_start:\n>>>           swa_model.update_parameters(model)\n>>>           swa_scheduler.step()\n>>>       else:\n>>>           scheduler.step()\n>>>\n>>> # Update bn statistics for the swa_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n>>> # Use swa_model to make predictions on test data\n>>> preds = swa_model(test_input) \n```", "```py\n>>> loader, optimizer, model, loss_fn = ...\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, \\\n>>>             multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>           ema_model.update_parameters(model)\n>>>\n>>> # Update bn statistics for the ema_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, ema_model)\n>>> # Use ema_model to make predictions on test data\n>>> preds = ema_model(test_input) \n```"]