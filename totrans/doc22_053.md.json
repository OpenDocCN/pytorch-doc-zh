["```py\nprobs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward() \n```", "```py\nparams = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward() \n```", "```py\nclass torch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)\u00b6\n```", "```py\nproperty arg_constraints: Dict[str, Constraint]\u00b6\n```", "```py\nproperty batch_shape: Size\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nenumerate_support(expand=True)\u00b6\n```", "```py\nproperty event_shape: Size\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nicdf(value)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean: Tensor\u00b6\n```", "```py\nproperty mode: Tensor\u00b6\n```", "```py\nperplexity()\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsample_n(n)\u00b6\n```", "```py\nstatic set_default_validate_args(value)\u00b6\n```", "```py\nproperty stddev: Tensor\u00b6\n```", "```py\nproperty support: Optional[Any]\u00b6\n```", "```py\nproperty variance: Tensor\u00b6\n```", "```py\nclass torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nclass torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None)\u00b6\n```", "```py\n>>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.]) \n```", "```py\narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nenumerate_support(expand=True)\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_enumerate_support = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty param_shape\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = Boolean()\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None)\u00b6\n```", "```py\n>>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046]) \n```", "```py\narg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nproperty concentration0\u00b6\n```", "```py\nproperty concentration1\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=())\u00b6\n```", "```py\nsupport = Interval(lower_bound=0.0, upper_bound=1.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None)\u00b6\n```", "```py\n>>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n [ 7.,  6.]]) \n```", "```py\narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nenumerate_support(expand=True)\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_enumerate_support = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty param_shape\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty support\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None)\u00b6\n```", "```py\n>>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3) \n```", "```py\narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nenumerate_support(expand=True)\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_enumerate_support = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty param_shape\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty support\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None)\u00b6\n```", "```py\n>>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214]) \n```", "```py\narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nicdf(value)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = Real()\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.chi2.Chi2(df, validate_args=None)\u00b6\n```", "```py\n>>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046]) \n```", "```py\narg_constraints = {'df': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nproperty df\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nclass torch.distributions.continuous_bernoulli.ContinuousBernoulli(probs=None, logits=None, lims=(0.499, 0.501), validate_args=None)\u00b6\n```", "```py\n>>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538]) \n```", "```py\narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nicdf(value)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty param_shape\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty stddev\u00b6\n```", "```py\nsupport = Interval(lower_bound=0.0, upper_bound=1.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None)\u00b6\n```", "```py\n>>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentration [0.5, 0.5]\ntensor([ 0.1046,  0.8954]) \n```", "```py\narg_constraints = {'concentration': IndependentConstraint(GreaterThan(lower_bound=0.0), 1)}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=())\u00b6\n```", "```py\nsupport = Simplex()\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.exponential.Exponential(rate, validate_args=None)\u00b6\n```", "```py\n>>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046]) \n```", "```py\narg_constraints = {'rate': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nicdf(value)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty stddev\u00b6\n```", "```py\nsupport = GreaterThanEq(lower_bound=0.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None)\u00b6\n```", "```py\n>>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453]) \n```", "```py\narg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = GreaterThan(lower_bound=0.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.gamma.Gamma(concentration, rate, validate_args=None)\u00b6\n```", "```py\n>>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046]) \n```", "```py\narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = GreaterThanEq(lower_bound=0.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None)\u00b6\n```", "```py\n>>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.]) \n```", "```py\narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = IntegerGreaterThan(lower_bound=0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None)\u00b6\n```", "```py\n>>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124]) \n```", "```py\narg_constraints: Dict[str, Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty stddev\u00b6\n```", "```py\nsupport = Real()\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None)\u00b6\n```", "```py\nX ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale) \n```", "```py\n>>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214]) \n```", "```py\narg_constraints: Dict[str, Constraint] = {'scale': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nicdf(prob)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty scale\u00b6\n```", "```py\nsupport = GreaterThanEq(lower_bound=0.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.half_normal.HalfNormal(scale, validate_args=None)\u00b6\n```", "```py\nX ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale) \n```", "```py\n>>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046]) \n```", "```py\narg_constraints: Dict[str, Constraint] = {'scale': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nicdf(prob)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty scale\u00b6\n```", "```py\nsupport = GreaterThanEq(lower_bound=0.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None)\u00b6\n```", "```py\n>>> from torch.distributions.multivariate_normal import MultivariateNormal\n>>> from torch.distributions.normal import Normal\n>>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size([]), torch.Size([3])]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size([3]), torch.Size([])]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size([]), torch.Size([3])] \n```", "```py\narg_constraints: Dict[str, Constraint] = {}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nenumerate_support(expand=True)\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nproperty has_enumerate_support\u00b6\n```", "```py\nproperty has_rsample\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty support\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.inverse_gamma.InverseGamma(concentration, rate, validate_args=None)\u00b6\n```", "```py\nX ~ Gamma(concentration, rate)\nY = 1 / X ~ InverseGamma(concentration, rate) \n```", "```py\n>>> m = InverseGamma(torch.tensor([2.0]), torch.tensor([3.0]))\n>>> m.sample()\ntensor([ 1.2953]) \n```", "```py\narg_constraints: Dict[str, Constraint] = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nproperty concentration\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty rate\u00b6\n```", "```py\nsupport = GreaterThan(lower_bound=0.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.kumaraswamy.Kumaraswamy(concentration1, concentration0, validate_args=None)\u00b6\n```", "```py\n>>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729]) \n```", "```py\narg_constraints: Dict[str, Constraint] = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nsupport = Interval(lower_bound=0.0, upper_bound=1.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.lkj_cholesky.LKJCholesky(dim, concentration=1.0, validate_args=None)\u00b6\n```", "```py\nL ~ LKJCholesky(dim, concentration)\nX = L @ L' ~ LKJCorr(dim, concentration) \n```", "```py\n>>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n [ 0.3516,  0.9361,  0.0000],\n [-0.1899,  0.4748,  0.8593]]) \n```", "```py\narg_constraints = {'concentration': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = CorrCholesky()\u00b6\n```", "```py\nclass torch.distributions.laplace.Laplace(loc, scale, validate_args=None)\u00b6\n```", "```py\n>>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046]) \n```", "```py\narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nicdf(value)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty stddev\u00b6\n```", "```py\nsupport = Real()\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None)\u00b6\n```", "```py\nX ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale) \n```", "```py\n>>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046]) \n```", "```py\narg_constraints: Dict[str, Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nproperty loc\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty scale\u00b6\n```", "```py\nsupport = GreaterThan(lower_bound=0.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None)\u00b6\n```", "```py\ncovariance_matrix = cov_factor @ cov_factor.T + cov_diag \n```", "```py\n>>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429]) \n```", "```py\ncapacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor \n```", "```py\narg_constraints = {'cov_diag': IndependentConstraint(GreaterThan(lower_bound=0.0), 1), 'cov_factor': IndependentConstraint(Real(), 2), 'loc': IndependentConstraint(Real(), 1)}\u00b6\n```", "```py\nproperty covariance_matrix\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty precision_matrix\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty scale_tril\u00b6\n```", "```py\nsupport = IndependentConstraint(Real(), 1)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.mixture_same_family.MixtureSameFamily(mixture_distribution, component_distribution, validate_args=None)\u00b6\n```", "```py\n>>> # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n>>> # weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n>>> # Construct Gaussian Mixture Model in 2D consisting of 5 equally\n>>> # weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n...          torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n>>> # Construct a batch of 3 Gaussian Mixture Models in 2D each\n>>> # consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n...         torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp) \n```", "```py\narg_constraints: Dict[str, Constraint] = {}\u00b6\n```", "```py\ncdf(x)\u00b6\n```", "```py\nproperty component_distribution\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = False\u00b6\n```", "```py\nlog_prob(x)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mixture_distribution\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty support\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None)\u00b6\n```", "```py\n>>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338]) \n```", "```py\narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty param_shape\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty support\u00b6\n```", "```py\ntotal_count: int\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)\u00b6\n```", "```py\n>>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429]) \n```", "```py\narg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': IndependentConstraint(Real(), 1), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()}\u00b6\n```", "```py\nproperty covariance_matrix\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty precision_matrix\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty scale_tril\u00b6\n```", "```py\nsupport = IndependentConstraint(Real(), 1)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None)\u00b6\n```", "```py\narg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)}\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty param_shape\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = IntegerGreaterThan(lower_bound=0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.normal.Normal(loc, scale, validate_args=None)\u00b6\n```", "```py\n>>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046]) \n```", "```py\narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nicdf(value)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty stddev\u00b6\n```", "```py\nsupport = Real()\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None)\u00b6\n```", "```py\n>>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.]) \n```", "```py\narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nenumerate_support(expand=True)\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_enumerate_support = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty param_shape\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = OneHot()\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.pareto.Pareto(scale, alpha, validate_args=None)\u00b6\n```", "```py\n>>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623]) \n```", "```py\narg_constraints: Dict[str, Constraint] = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty support\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.poisson.Poisson(rate, validate_args=None)\u00b6\n```", "```py\n>>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.]) \n```", "```py\narg_constraints = {'rate': GreaterThanEq(lower_bound=0.0)}\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = IntegerGreaterThan(lower_bound=0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)\u00b6\n```", "```py\n>>> m = RelaxedBernoulli(torch.tensor([2.2]),\n...                      torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021]) \n```", "```py\narg_constraints: Dict[str, Constraint] = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nsupport = Interval(lower_bound=0.0, upper_bound=1.0)\u00b6\n```", "```py\nproperty temperature\u00b6\n```", "```py\nclass torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)\u00b6\n```", "```py\narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty param_shape\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = Real()\u00b6\n```", "```py\nclass torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None)\u00b6\n```", "```py\n>>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n...                              torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523]) \n```", "```py\narg_constraints: Dict[str, Constraint] = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nproperty logits\u00b6\n```", "```py\nproperty probs\u00b6\n```", "```py\nsupport = Simplex()\u00b6\n```", "```py\nproperty temperature\u00b6\n```", "```py\nclass torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None)\u00b6\n```", "```py\n>>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046]) \n```", "```py\narg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = Real()\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None)\u00b6\n```", "```py\nX ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)| \n```", "```py\n# Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms) \n```", "```py\narg_constraints: Dict[str, Constraint] = {}\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nproperty has_rsample\u00b6\n```", "```py\nicdf(value)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty support\u00b6\n```", "```py\nclass torch.distributions.uniform.Uniform(low, high, validate_args=None)\u00b6\n```", "```py\n>>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418]) \n```", "```py\narg_constraints = {'high': Dependent(), 'low': Dependent()}\u00b6\n```", "```py\ncdf(value)\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nicdf(value)\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nproperty stddev\u00b6\n```", "```py\nproperty support\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.von_mises.VonMises(loc, concentration, validate_args=None)\u00b6\n```", "```py\n>>> m = VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777]) \n```", "```py\narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'loc': Real()}\u00b6\n```", "```py\nexpand(batch_shape)\u00b6\n```", "```py\nhas_rsample = False\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nsample(sample_shape=torch.Size([]))\u00b6\n```", "```py\nsupport = Real()\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.weibull.Weibull(scale, concentration, validate_args=None)\u00b6\n```", "```py\n>>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784]) \n```", "```py\narg_constraints: Dict[str, Constraint] = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nsupport = GreaterThan(lower_bound=0.0)\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\nclass torch.distributions.wishart.Wishart(df, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)\u00b6\n```", "```py\n>>> m = Wishart(torch.Tensor([2]), covariance_matrix=torch.eye(2))\n>>> m.sample()  # Wishart distributed with mean=`df * I` and\n>>>             # variance(x_ij)=`df` for i != j and variance(x_ij)=`2 * df` for i == j \n```", "```py\narg_constraints = {'covariance_matrix': PositiveDefinite(), 'df': GreaterThan(lower_bound=0), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()}\u00b6\n```", "```py\nproperty covariance_matrix\u00b6\n```", "```py\nentropy()\u00b6\n```", "```py\nexpand(batch_shape, _instance=None)\u00b6\n```", "```py\nhas_rsample = True\u00b6\n```", "```py\nlog_prob(value)\u00b6\n```", "```py\nproperty mean\u00b6\n```", "```py\nproperty mode\u00b6\n```", "```py\nproperty precision_matrix\u00b6\n```", "```py\nrsample(sample_shape=torch.Size([]), max_try_correction=None)\u00b6\n```", "```py\nproperty scale_tril\u00b6\n```", "```py\nsupport = PositiveDefinite()\u00b6\n```", "```py\nproperty variance\u00b6\n```", "```py\ntorch.distributions.kl.kl_divergence(p, q)\u00b6\n```", "```py\ntorch.distributions.kl.register_kl(type_p, type_q)\u00b6\n```", "```py\n@register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here \n```", "```py\n@register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ... \n```", "```py\nregister_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie. \n```", "```py\nclass torch.distributions.transforms.AbsTransform(cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.CatTransform(tseq, dim=0, lengths=None, cache_size=0)\u00b6\n```", "```py\nx0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0)\nx = torch.cat([x0, x0], dim=0)\nt0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10])\nt = CatTransform([t0, t0], dim=0, lengths=[20, 20])\ny = t(x) \n```", "```py\nclass torch.distributions.transforms.ComposeTransform(parts, cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.CorrCholeskyTransform(cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.CumulativeDistributionTransform(distribution, cache_size=0)\u00b6\n```", "```py\n# Construct a Gaussian copula from a multivariate normal.\nbase_dist = MultivariateNormal(\n    loc=torch.zeros(2),\n    scale_tril=LKJCholesky(2).sample(),\n)\ntransform = CumulativeDistributionTransform(Normal(0, 1))\ncopula = TransformedDistribution(base_dist, [transform]) \n```", "```py\nclass torch.distributions.transforms.ExpTransform(cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.LowerCholeskyTransform(cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.PositiveDefiniteTransform(cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.PowerTransform(exponent, cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.ReshapeTransform(in_shape, out_shape, cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.SigmoidTransform(cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.SoftplusTransform(cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.TanhTransform(cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.SoftmaxTransform(cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.StackTransform(tseq, dim=0, cache_size=0)\u00b6\n```", "```py\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1)\nt = StackTransform([ExpTransform(), identity_transform], dim=1)\ny = t(x) \n```", "```py\nclass torch.distributions.transforms.StickBreakingTransform(cache_size=0)\u00b6\n```", "```py\nclass torch.distributions.transforms.Transform(cache_size=0)\u00b6\n```", "```py\ny = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients. \n```", "```py\ny = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x \n```", "```py\nproperty inv\u00b6\n```", "```py\nproperty sign\u00b6\n```", "```py\nlog_abs_det_jacobian(x, y)\u00b6\n```", "```py\nforward_shape(shape)\u00b6\n```", "```py\ninverse_shape(shape)\u00b6\n```", "```py\nclass torch.distributions.constraints.Constraint\u00b6\n```", "```py\ncheck(value)\u00b6\n```", "```py\ntorch.distributions.constraints.cat\u00b6\n```", "```py\ntorch.distributions.constraints.dependent_property\u00b6\n```", "```py\ntorch.distributions.constraints.greater_than\u00b6\n```", "```py\ntorch.distributions.constraints.greater_than_eq\u00b6\n```", "```py\ntorch.distributions.constraints.independent\u00b6\n```", "```py\ntorch.distributions.constraints.integer_interval\u00b6\n```", "```py\ntorch.distributions.constraints.interval\u00b6\n```", "```py\ntorch.distributions.constraints.half_open_interval\u00b6\n```", "```py\ntorch.distributions.constraints.less_than\u00b6\n```", "```py\ntorch.distributions.constraints.multinomial\u00b6\n```", "```py\ntorch.distributions.constraints.stack\u00b6\n```", "```py\nloc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints['scale'])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum() \n```", "```py\ndist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum() \n```", "```py\ntransform_to.register(my_constraint, my_transform) \n```", "```py\n@transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2) \n```", "```py\nclass torch.distributions.constraint_registry.ConstraintRegistry\u00b6\n```", "```py\nregister(constraint, factory=None)\u00b6\n```", "```py\n@my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints) \n```"]