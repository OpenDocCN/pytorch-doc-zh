- en: Gradcheck mechanics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/notes/gradcheck.html](https://pytorch.org/docs/stable/notes/gradcheck.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This note presents an overview of how the [`gradcheck()`](../autograd.html#module-torch.autograd.gradcheck
    "torch.autograd.gradcheck") and `gradgradcheck()` functions work.
  prefs: []
  type: TYPE_NORMAL
- en: It will cover both forward and backward mode AD for both real and complex-valued
    functions as well as higher-order derivatives. This note also covers both the
    default behavior of gradcheck as well as the case where `fast_mode=True` argument
    is passed (referred to as fast gradcheck below).
  prefs: []
  type: TYPE_NORMAL
- en: '[Notations and background information](#notations-and-background-information)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Default backward mode gradcheck behavior](#default-backward-mode-gradcheck-behavior)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Real-to-real functions](#real-to-real-functions)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Complex-to-real functions](#complex-to-real-functions)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Functions with complex outputs](#functions-with-complex-outputs)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fast backward mode gradcheck](#fast-backward-mode-gradcheck)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fast gradcheck for real-to-real functions](#fast-gradcheck-for-real-to-real-functions)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fast gradcheck for complex-to-real functions](#fast-gradcheck-for-complex-to-real-functions)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fast gradcheck for functions with complex outputs](#fast-gradcheck-for-functions-with-complex-outputs)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gradgradcheck implementation](#gradgradcheck-implementation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Notations and background information](#id2)[](#notations-and-background-information
    "Permalink to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Throughout this note, we will use the following convention:'
  prefs: []
  type: TYPE_NORMAL
- en: $x$x,
    $y$y,
    $a$a,
    $b$b,
    $v$v,
    $u$u,
    $ur$ur
    and $ui$ui
    are real-valued vectors and $z$z is a complex-valued
    vector that can be rewritten in terms of two real-valued vectors as $z = a + i b$z=a+ib.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $N$N
    and $M$M
    are two integers that we will use for the dimension of the input and output space
    respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '$f:
    \mathcal{R}^N \to \mathcal{R}^M$f:RN→RM is our
    basic real-to-real function such that $y = f(x)$y=f(x).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '$g:
    \mathcal{C}^N \to \mathcal{R}^M$g:CN→RM is our
    basic complex-to-real function such that $y = g(z)$y=g(z).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the simple real-to-real case, we write as $J_f$Jf​ the Jacobian
    matrix associated with $f$f
    of size $M \times N$M×N. This
    matrix contains all the partial derivatives such that the entry at position $(i, j)$(i,j) contains
    $\frac{\partial y_i}{\partial x_j}$∂xj​∂yi​​.
    Backward mode AD is then computing, for a given vector $v$v of size $M$M, the quantity
    $v^T J_f$vTJf​. Forward
    mode AD on the other hand is computing, for a given vector $u$u of size $N$N, the quantity
    $J_f u$Jf​u.
  prefs: []
  type: TYPE_NORMAL
- en: For functions that contain complex values, the story is a lot more complex.
    We only provide the gist here and the full description can be found at [Autograd
    for Complex Numbers](autograd.html#complex-autograd-doc).
  prefs: []
  type: TYPE_NORMAL
- en: The constraints to satisfy complex differentiability (Cauchy-Riemann equations)
    are too restrictive for all real-valued loss functions, so we instead opted to
    use Wirtinger calculus. In a basic setting of Wirtinger calculus, the chain rule
    requires access to both the Wirtinger derivative (called $W$W below) and the
    Conjugate Wirtinger derivative (called $CW$CW below). Both
    $W$W
    and $CW$CW
    need to be propagated because in general, despite their name, one is not the complex
    conjugate of the other.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid having to propagate both values, for backward mode AD, we always work
    under the assumption that the function whose derivative is being calculated is
    either a real-valued function or is part of a bigger real-valued function. This
    assumption means that all the intermediary gradients we compute during the backward
    pass are also associated with real-valued functions. In practice, this assumption
    is not restrictive when doing optimization as such problem require real-valued
    objectives (as there is no natural ordering of the complex numbers).
  prefs: []
  type: TYPE_NORMAL
- en: Under this assumption, using $W$W and $CW$CW definitions,
    we can show that $W = CW^*$W=CW∗ (we
    use $*$∗
    to denote complex conjugation here) and so only one of the two values actually
    need to be “backwarded through the graph” as the other one can easily be recovered.
    To simplify internal computations, PyTorch uses $2 * CW$2∗CW as the
    value it backwards and returns when the user asks for gradients. Similarly to
    the real case, when the output is actually in $\mathcal{R}^M$RM,
    backward mode AD does not compute $2 * CW$2∗CW but only
    $v^T (2
    * CW)$vT(2∗CW) for a given vector $v
    \in \mathcal{R}^M$v∈RM.
  prefs: []
  type: TYPE_NORMAL
- en: For forward mode AD, we use a similar logic, in this case, assuming that the
    function is part of a larger function whose input is in $\mathcal{R}$R.
    Under this assumption, we can make a similar claim that every intermediary result
    corresponds to a function whose input is in $\mathcal{R}$R and
    in this case, using $W$W
    and $CW$CW
    definitions, we can show that $W = CW$W=CW for the
    intermediary functions. To make sure the forward and backward mode compute the
    same quantities in the elementary case of a one dimensional function, the forward
    mode also computes $2 * CW$2∗CW. Similarly
    to the real case, when the input is actually in $\mathcal{R}^N$RN,
    forward mode AD does not compute $2 * CW$2∗CW but only
    $(2
    * CW) u$(2∗CW)u for a given vector $u
    \in \mathcal{R}^N$u∈RN.
  prefs: []
  type: TYPE_NORMAL
- en: '[Default backward mode gradcheck behavior](#id3)[](#default-backward-mode-gradcheck-behavior
    "Permalink to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Real-to-real functions](#id4)[](#real-to-real-functions "Permalink to this
    heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To test a function $f: \mathcal{R}^N \to \mathcal{R}^M, x \to y$f:RN→RM,x→y,
    we reconstruct the full Jacobian matrix $J_f$Jf​ of size $M \times N$M×N in
    two ways: analytically and numerically. The analytical version uses our backward
    mode AD while the numerical version uses finite difference. The two reconstructed
    Jacobian matrices are then compared elementwise for equality.'
  prefs: []
  type: TYPE_NORMAL
- en: Default real input numerical evaluation[](#default-real-input-numerical-evaluation
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If we consider the elementary case of a one-dimensional function ($N = M = 1$N=M=1),
    then we can use the basic finite difference formula from [the wikipedia article](https://en.wikipedia.org/wiki/Finite_difference).
    We use the “central difference” for better numerical properties:'
  prefs: []
  type: TYPE_NORMAL
- en: $\frac{\partial y}{\partial x} \approx \frac{f(x +
    eps) - f(x - eps)}{2 * eps}$ ∂x∂y​≈2∗epsf(x+eps)−f(x−eps)​
  prefs: []
  type: TYPE_NORMAL
- en: This formula easily generalizes for multiple outputs ($M \gt 1$M>1) by having
    $\frac{\partial
    y}{\partial x}$∂x∂y​ be a column vector of size
    $M
    \times 1$M×1 like $f(x + eps)$f(x+eps).
    In that case, the above formula can be re-used as-is and approximates the full
    Jacobian matrix with only two evaluations of the user function (namely $f(x + eps)$f(x+eps)
    and $f(x - eps)$f(x−eps)).
  prefs: []
  type: TYPE_NORMAL
- en: It is more computationally expensive to handle the case with multiple inputs
    ($N
    \gt 1$N>1). In this scenario, we loop over all
    the inputs one after the other and apply the $eps$eps perturbation
    for each element of $x$x
    one after the other. This allows us to reconstruct the $J_f$Jf​ matrix column
    by column.
  prefs: []
  type: TYPE_NORMAL
- en: Default real input analytical evaluation[](#default-real-input-analytical-evaluation
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the analytical evaluation, we use the fact, as described above, that backward
    mode AD computes $v^T J_f$vTJf​. For
    functions with a single output, we simply use $v = 1$v=1 to recover
    the full Jacobian matrix with a single backward pass.
  prefs: []
  type: TYPE_NORMAL
- en: For functions with more than one output, we resort to a for-loop which iterates
    over the outputs where each $v$v is a one-hot vector
    corresponding to each output one after the other. This allows to reconstruct the
    $J_f$Jf​
    matrix row by row.
  prefs: []
  type: TYPE_NORMAL
- en: '[Complex-to-real functions](#id5)[](#complex-to-real-functions "Permalink
    to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To test a function $g: \mathcal{C}^N \to \mathcal{R}^M, z \to y$g:CN→RM,z→y
    with $z = a + i b$z=a+ib,
    we reconstruct the (complex-valued) matrix that contains $2 * CW$2∗CW.'
  prefs: []
  type: TYPE_NORMAL
- en: Default complex input numerical evaluation[](#default-complex-input-numerical-evaluation
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider the elementary case where $N = M = 1$N=M=1 first.
    We know from (chapter 3 of) [this research paper](https://arxiv.org/pdf/1701.00392.pdf)
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: $CW := \frac{\partial
    y}{\partial z^*} = \frac{1}{2} * (\frac{\partial y}{\partial a} + i \frac{\partial
    y}{\partial b})$ CW:=∂z∗∂y​=21​∗(∂a∂y​+i∂b∂y​)
  prefs: []
  type: TYPE_NORMAL
- en: Note that $\frac{\partial
    y}{\partial a}$∂a∂y​ and $\frac{\partial y}{\partial b}$∂b∂y​,
    in the above equation, are $\mathcal{R}
    \to \mathcal{R}$R→R derivatives. To evaluate these
    numerically, we use the method described above for the real-to-real case. This
    allows us to compute the $CW$CW matrix and then
    multiply it by $2$2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the code, as of time of writing, computes this value in a slightly
    convoluted way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Default complex input analytical evaluation[](#default-complex-input-analytical-evaluation
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since backward mode AD computes exactly twice the $CW$CW derivative already,
    we simply use the same trick as for the real-to-real case here and reconstruct
    the matrix row by row when there are multiple real outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Functions with complex outputs](#id6)[](#functions-with-complex-outputs "Permalink
    to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this case, the user-provided function does not follow the assumption from
    the autograd that the function we compute backward AD for is real-valued. This
    means that using autograd directly on this function is not well defined. To solve
    this, we will replace the test of the function $h: \mathcal{P}^N \to \mathcal{C}^M$h:PN→CM
    (where $\mathcal{P}$P can
    be either $\mathcal{R}$R or $\mathcal{C}$C),
    with two functions: $hr$hr and $hi$hi such that:'
  prefs: []
  type: TYPE_NORMAL
- en: $\begin{aligned}
    hr(q) &:= real(f(q)) \\ hi(q) &:= imag(f(q)) \end{aligned}$
    hr(q)hi(q)​:=real(f(q)):=imag(f(q))​
  prefs: []
  type: TYPE_NORMAL
- en: where $q \in \mathcal{P}$q∈P.
    We then do a basic gradcheck for both $hr$hr and $hi$hi using either
    the real-to-real or complex-to-real case described above, depending on $\mathcal{P}$P.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, the code, as of time of writing, does not create these functions
    explicitly but perform the chain rule with the $real$real or $imag$imag functions
    manually by passing the $\text{grad\_out}$grad_out
    arguments to the different functions. When $\text{grad\_out} = 1$grad_out=1,
    then we are considering $hr$hr. When $\text{grad\_out} = 1j$grad_out=1j,
    then we are considering $hi$hi.
  prefs: []
  type: TYPE_NORMAL
- en: '[Fast backward mode gradcheck](#id7)[](#fast-backward-mode-gradcheck "Permalink
    to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the above formulation of gradcheck is great, both, to ensure correctness
    and debuggability, it is very slow because it reconstructs the full Jacobian matrices.
    This section presents a way to perform gradcheck in a faster way without affecting
    its correctness. The debuggability can be recovered by adding special logic when
    we detect an error. In that case, we can run the default version that reconstructs
    the full matrix to give full details to the user.
  prefs: []
  type: TYPE_NORMAL
- en: The high level strategy here is to find a scalar quantity that can be computed
    efficiently by both the numerical and analytical methods and that represents the
    full matrix computed by the slow gradcheck well enough to ensure that it will
    catch any discrepancy in the Jacobians.
  prefs: []
  type: TYPE_NORMAL
- en: '[Fast gradcheck for real-to-real functions](#id8)[](#fast-gradcheck-for-real-to-real-functions
    "Permalink to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The scalar quantity that we want to compute here is $v^T J_f u$vTJf​u for
    a given random vector $v \in \mathcal{R}^M$v∈RM
    and a random unit norm vector $u
    \in \mathcal{R}^N$u∈RN.
  prefs: []
  type: TYPE_NORMAL
- en: For the numerical evaluation, we can efficiently compute
  prefs: []
  type: TYPE_NORMAL
- en: $J_f
    u \approx \frac{f(x + u * eps) - f(x - u * eps)}{2 * eps}.$
    Jf​u≈2∗epsf(x+u∗eps)−f(x−u∗eps)​.
  prefs: []
  type: TYPE_NORMAL
- en: We then perform the dot product between this vector and $v$v to get the scalar
    value of interest.
  prefs: []
  type: TYPE_NORMAL
- en: For the analytical version, we can use backward mode AD to compute $v^T J_f$vTJf​ directly.
    We then perform the dot product with $u$u to get the expected
    value.
  prefs: []
  type: TYPE_NORMAL
- en: '[Fast gradcheck for complex-to-real functions](#id9)[](#fast-gradcheck-for-complex-to-real-functions
    "Permalink to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to the real-to-real case, we want to perform a reduction of the full
    matrix. But the $2 * CW$2∗CW matrix
    is complex-valued and so in this case, we will compare to complex scalars.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to some constraints on what we can compute efficiently in the numerical
    case and to keep the number of numerical evaluations to a minimum, we compute
    the following (albeit surprising) scalar value:'
  prefs: []
  type: TYPE_NORMAL
- en: $s := 2
    * v^T (real(CW) ur + i * imag(CW) ui)$ s:=2∗vT(real(CW)ur+i∗imag(CW)ui)
  prefs: []
  type: TYPE_NORMAL
- en: where $v \in \mathcal{R}^M$v∈RM,
    $ur \in \mathcal{R}^N$ur∈RN
    and $ui \in \mathcal{R}^N$ui∈RN.
  prefs: []
  type: TYPE_NORMAL
- en: Fast complex input numerical evaluation[](#fast-complex-input-numerical-evaluation
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We first consider how to compute $s$s with a numerical
    method. To do so, keeping in mind that we’re considering $g:
    \mathcal{C}^N \to \mathcal{R}^M, z \to y$g:CN→RM,z→y
    with $z = a + i b$z=a+ib,
    and that $CW = \frac{1}{2}
    * (\frac{\partial y}{\partial a} + i \frac{\partial y}{\partial b})$CW=21​∗(∂a∂y​+i∂b∂y​),
    we rewrite it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: $\begin{aligned}
    s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\ &= 2 * v^T (\frac{1}{2} * \frac{\partial
    y}{\partial a} ur + i * \frac{1}{2} * \frac{\partial y}{\partial b} ui) \\ &=
    v^T (\frac{\partial y}{\partial a} ur + i * \frac{\partial y}{\partial b} ui)
    \\ &= v^T ((\frac{\partial y}{\partial a} ur) + i * (\frac{\partial y}{\partial
    b} ui)) \end{aligned}$ s​=2∗vT(real(CW)ur+i∗imag(CW)ui)=2∗vT(21​∗∂a∂y​ur+i∗21​∗∂b∂y​ui)=vT(∂a∂y​ur+i∗∂b∂y​ui)=vT((∂a∂y​ur)+i∗(∂b∂y​ui))​
  prefs: []
  type: TYPE_NORMAL
- en: In this formula, we can see that $\frac{\partial y}{\partial a} ur$∂a∂y​ur
    and $\frac{\partial y}{\partial b} ui$∂b∂y​ui
    can be evaluated the same way as the fast version for the real-to-real case. Once
    these real-valued quantities have been computed, we can reconstruct the complex
    vector on the right side and do a dot product with the real-valued $v$v vector.
  prefs: []
  type: TYPE_NORMAL
- en: Fast complex input analytical evaluation[](#fast-complex-input-analytical-evaluation
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the analytical case, things are simpler and we rewrite the formula as:'
  prefs: []
  type: TYPE_NORMAL
- en: $\begin{aligned} s &= 2 * v^T (real(CW) ur + i * imag(CW)
    ui) \\ &= v^T real(2 * CW) ur + i * v^T imag(2 * CW) ui) \\ &= real(v^T (2 * CW))
    ur + i * imag(v^T (2 * CW)) ui \end{aligned}$ s​=2∗vT(real(CW)ur+i∗imag(CW)ui)=vTreal(2∗CW)ur+i∗vTimag(2∗CW)ui)=real(vT(2∗CW))ur+i∗imag(vT(2∗CW))ui​
  prefs: []
  type: TYPE_NORMAL
- en: We can thus use the fact that the backward mode AD provides us with an efficient
    way to compute $v^T (2
    * CW)$vT(2∗CW) and then perform a dot product of
    the real part with $ur$ur and the imaginary
    part with $ui$ui
    before reconstructing the final complex scalar $s$s.
  prefs: []
  type: TYPE_NORMAL
- en: Why not use a complex $u$u[](#why-not-use-a-complex-u
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At this point, you might be wondering why we did not select a complex $u$u and just performed
    the reduction $2 * v^T CW u''$2∗vTCWu′.
    To dive into this, in this paragraph, we will use the complex version of $u$u noted $u'' = ur'' + i ui''$u′=ur′+iui′.
    Using such complex $u''$u′,
    the problem is that when doing the numerical evaluation, we would need to compute:'
  prefs: []
  type: TYPE_NORMAL
- en: $\begin{aligned} 2*CW u' &= (\frac{\partial y}{\partial
    a} + i \frac{\partial y}{\partial b})(ur' + i ui') \\ &= \frac{\partial y}{\partial
    a} ur' + i \frac{\partial y}{\partial a} ui' + i \frac{\partial y}{\partial b}
    ur' - \frac{\partial y}{\partial b} ui' \end{aligned}$
    2∗CWu′​=(∂a∂y​+i∂b∂y​)(ur′+iui′)=∂a∂y​ur′+i∂a∂y​ui′+i∂b∂y​ur′−∂b∂y​ui′​
  prefs: []
  type: TYPE_NORMAL
- en: Which would require four evaluations of real-to-real finite difference (twice
    as much compared to the approached proposed above). Since this approach does not
    have more degrees of freedom (same number of real valued variables) and we try
    to get the fastest possible evaluation here, we use the other formulation above.
  prefs: []
  type: TYPE_NORMAL
- en: '[Fast gradcheck for functions with complex outputs](#id10)[](#fast-gradcheck-for-functions-with-complex-outputs
    "Permalink to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just like in the slow case, we consider two real-valued functions and use the
    appropriate rule from above for each function.
  prefs: []
  type: TYPE_NORMAL
- en: '[Gradgradcheck implementation](#id11)[](#gradgradcheck-implementation "Permalink
    to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch also provide a utility to verify second order gradients. The goal here
    is to make sure that the backward implementation is also properly differentiable
    and computes the right thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This feature is implemented by considering the function $F: x, v \to v^T J_f$F:x,v→vTJf​
    and use the gradcheck defined above on this function. Note that $v$v in this case is
    just a random vector with the same type as $f(x)$f(x).'
  prefs: []
  type: TYPE_NORMAL
- en: The fast version of gradgradcheck is implemented by using the fast version of
    gradcheck on that same function $F$F.
  prefs: []
  type: TYPE_NORMAL
