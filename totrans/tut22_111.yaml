- en: Distributed and Parallel Training Tutorials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/distributed/home.html](https://pytorch.org/tutorials/distributed/home.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Distributed training is a model training paradigm that involves spreading training
    workload across multiple worker nodes, therefore significantly improving the speed
    of training and model accuracy. While distributed training can be used for any
    type of ML model training, it is most beneficial to use it for large models and
    compute demanding tasks as deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few ways you can perform distributed training in PyTorch with each
    method having their advantages in certain use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[DistributedDataParallel (DDP)](#learn-ddp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fully Sharded Data Parallel (FSDP)](#learn-fsdp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Device Mesh](#device-mesh)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Remote Procedure Call (RPC) distributed training](#learn-rpc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Custom Extensions](#custom-extensions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read more about these options in [Distributed Overview](../beginner/dist_overview.html).
  prefs: []
  type: TYPE_NORMAL
- en: '## Learn DDP'
  prefs: []
  type: TYPE_NORMAL
- en: DDP Intro Video Tutorials
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step video series on how to get started with DistributedDataParallel
    and advance to more complex topics
  prefs: []
  type: TYPE_NORMAL
- en: Code Video
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Distributed Data Parallel
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial provides a short and gentle intro to the PyTorch DistributedData
    Parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Training with Uneven Inputs Using the Join Context Manager
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial describes the Join context manager and demonstrates it’s use with
    DistributedData Parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code  ## Learn FSDP'
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with FSDP
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial demonstrates how you can perform distributed training with FSDP
    on a MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs: []
  type: TYPE_NORMAL
- en: FSDP Advanced
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, you will learn how to fine-tune a HuggingFace (HF) T5 model
    with FSDP for text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code  ## Learn DeviceMesh'
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with DeviceMesh
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial you will learn about DeviceMesh and how it can help with distributed
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code  ## Learn RPC'
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Distributed RPC Framework
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial demonstrates how to get started with RPC-based distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Parameter Server Using Distributed RPC Framework
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial walks you through a simple example of implementing a parameter
    server using PyTorch’s Distributed RPC framework.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Batch RPC Processing Using Asynchronous Executions
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial you will build batch-processing RPC applications with the @rpc.functions.async_execution
    decorator.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs: []
  type: TYPE_NORMAL
- en: Combining Distributed DataParallel with Distributed RPC Framework
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial you will learn how to combine distributed data parallelism
    with distributed model parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code  ## Custom Extensions'
  prefs: []
  type: TYPE_NORMAL
- en: Customize Process Group Backends Using Cpp Extensions
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial you will learn to implement a custom ProcessGroup backend and
    plug that into PyTorch distributed package using cpp extensions.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs: []
  type: TYPE_NORMAL
