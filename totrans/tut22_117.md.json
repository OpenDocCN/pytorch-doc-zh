["```py\npip3  install  --pre  torch  torchvision  torchaudio  -f  https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html \n```", "```py\n# Based on: https://github.com/pytorch/examples/blob/master/mnist/main.py\nimport os\nimport argparse\nimport functools\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\nfrom torch.optim.lr_scheduler import StepLR\n\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import (\n    CPUOffload,\n    BackwardPrefetch,\n)\nfrom torch.distributed.fsdp.wrap import (\n    size_based_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n) \n```", "```py\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group() \n```", "```py\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output \n```", "```py\ndef train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    ddp_loss = torch.zeros(2).to(rank)\n    if sampler:\n        sampler.set_epoch(epoch)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(rank), target.to(rank)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target, reduction='sum')\n        loss.backward()\n        optimizer.step()\n        ddp_loss[0] += loss.item()\n        ddp_loss[1] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n    if rank == 0:\n        print('Train Epoch: {}  \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1])) \n```", "```py\ndef test(model, rank, world_size, test_loader):\n    model.eval()\n    correct = 0\n    ddp_loss = torch.zeros(3).to(rank)\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(rank), target.to(rank)\n            output = model(data)\n            ddp_loss[0] += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            ddp_loss[1] += pred.eq(target.view_as(pred)).sum().item()\n            ddp_loss[2] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n\n    if rank == 0:\n        test_loss = ddp_loss[0] / ddp_loss[2]\n        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n            test_loss, int(ddp_loss[1]), int(ddp_loss[2]),\n            100. * ddp_loss[1] / ddp_loss[2])) \n```", "```py\ndef fsdp_main(rank, world_size, args):\n    setup(rank, world_size)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    dataset1 = datasets.MNIST('../data', train=True, download=True,\n                        transform=transform)\n    dataset2 = datasets.MNIST('../data', train=False,\n                        transform=transform)\n\n    sampler1 = DistributedSampler(dataset1, rank=rank, num_replicas=world_size, shuffle=True)\n    sampler2 = DistributedSampler(dataset2, rank=rank, num_replicas=world_size)\n\n    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n    cuda_kwargs = {'num_workers': 2,\n                    'pin_memory': True,\n                    'shuffle': False}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n    my_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=100\n    )\n    torch.cuda.set_device(rank)\n\n    init_start_event = torch.cuda.Event(enable_timing=True)\n    init_end_event = torch.cuda.Event(enable_timing=True)\n\n    model = Net().to(rank)\n\n    model = FSDP(model)\n\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    init_start_event.record()\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n        test(model, rank, world_size, test_loader)\n        scheduler.step()\n\n    init_end_event.record()\n\n    if rank == 0:\n        print(f\"CUDA event elapsed time: {init_start_event.elapsed_time(init_end_event)  /  1000}sec\")\n        print(f\"{model}\")\n\n    if args.save_model:\n        # use a barrier to make sure training is done on all ranks\n        dist.barrier()\n        states = model.state_dict()\n        if rank == 0:\n            torch.save(states, \"mnist_cnn.pt\")\n\n    cleanup() \n```", "```py\nif __name__ == '__main__':\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    WORLD_SIZE = torch.cuda.device_count()\n    mp.spawn(fsdp_main,\n        args=(WORLD_SIZE, args),\n        nprocs=WORLD_SIZE,\n        join=True) \n```", "```py\npython  FSDP_mnist.py\n\nCUDA  event  elapsed  time  on  training  loop  40.67462890625sec \n```", "```py\n FullyShardedDataParallel(\n  (_fsdp_wrapped_module):  FlattenParamsWrapper(\n  (_fpw_module):  Net(\n  (conv1):  Conv2d(1,  32,  kernel_size=(3,  3),  stride=(1,  1))\n  (conv2):  Conv2d(32,  64,  kernel_size=(3,  3),  stride=(1,  1))\n  (dropout1):  Dropout(p=0.25,  inplace=False)\n  (dropout2):  Dropout(p=0.5,  inplace=False)\n  (fc1):  Linear(in_features=9216,  out_features=128,  bias=True)\n  (fc2):  Linear(in_features=128,  out_features=10,  bias=True)\n  )\n  )\n) \n```", "```py\nmy_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=20000\n    )\ntorch.cuda.set_device(rank)\nmodel = Net().to(rank)\n\nmodel = FSDP(model,\n    fsdp_auto_wrap_policy=my_auto_wrap_policy) \n```", "```py\n FullyShardedDataParallel(\n(_fsdp_wrapped_module):  FlattenParamsWrapper(\n  (_fpw_module):  Net(\n  (conv1):  Conv2d(1,  32,  kernel_size=(3,  3),  stride=(1,  1))\n  (conv2):  Conv2d(32,  64,  kernel_size=(3,  3),  stride=(1,  1))\n  (dropout1):  Dropout(p=0.25,  inplace=False)\n  (dropout2):  Dropout(p=0.5,  inplace=False)\n  (fc1):  FullyShardedDataParallel(\n  (_fsdp_wrapped_module):  FlattenParamsWrapper(\n  (_fpw_module):  Linear(in_features=9216,  out_features=128,  bias=True)\n  )\n  )\n  (fc2):  Linear(in_features=128,  out_features=10,  bias=True)\n  )\n) \n```", "```py\npython  FSDP_mnist.py\n\nCUDA  event  elapsed  time  on  training  loop  41.89130859375sec \n```", "```py\nmodel = FSDP(model,\n    fsdp_auto_wrap_policy=my_auto_wrap_policy,\n    cpu_offload=CPUOffload(offload_params=True)) \n```", "```py\nmodel = Net().to(rank)\nmodel = DDP(model) \n```", "```py\npython  DDP_mnist.py\n\nCUDA  event  elapsed  time  on  training  loop  39.77766015625sec \n```"]