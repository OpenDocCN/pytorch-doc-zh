["```py\nx = torch.randn(5, requires_grad=True)\ny = x.pow(2)\nprint(x.equal(y.grad_fn._saved_self))  # True\nprint(x is y.grad_fn._saved_self)  # True \n```", "```py\nx = torch.randn(5, requires_grad=True)\ny = x.exp()\nprint(y.equal(y.grad_fn._saved_result))  # True\nprint(y is y.grad_fn._saved_result)  # False \n```", "```py\n# Define a train function to be used in different threads\ndef train_fn():\n    x = torch.ones(5, 5, requires_grad=True)\n    # forward\n    y = (x + 3) * (x + 4) * 0.5\n    # backward\n    y.sum().backward()\n    # potential optimizer update\n\n# User write their own threading code to drive the train_fn\nthreads = []\nfor _ in range(10):\n    p = threading.Thread(target=train_fn, args=())\n    p.start()\n    threads.append(p)\n\nfor p in threads:\n    p.join() \n```", "```py\nclass SelfDeletingTempFile():\n    def __init__(self):\n        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n\n    def __del__(self):\n        os.remove(self.name)\n\ndef pack_hook(tensor):\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(temp_file):\n    return torch.load(temp_file.name) \n```", "```py\nx = torch.randn(5, requires_grad=True)\ny = x.pow(2)\ny.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook) \n```", "```py\n# Only save on disk tensors that have size >= 1000\nSAVE_ON_DISK_THRESHOLD = 1000\n\ndef pack_hook(x):\n    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n        return x\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(tensor_or_sctf):\n    if isinstance(tensor_or_sctf, torch.Tensor):\n        return tensor_or_sctf\n    return torch.load(tensor_or_sctf.name)\n\nclass Model(nn.Module):\n    def forward(self, x):\n        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n          # ... compute output\n          output = x\n        return output\n\nmodel = Model()\nnet = nn.DataParallel(model) \n```", "```py\n# Example what NOT to do\n\nnet = nn.DataParallel(model)\nwith torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n    output = net(input) \n```", "```py\nwith torch.autograd.graph.saved_tensors_hooks(lambda x: x, lambda x: x):\n    x = torch.randn(5, requires_grad=True)\n    y = x * x \n```", "```py\nt = torch.tensor(1., requires_grad=True).sin()\nt.cos_()\nt.register_hook(fn)\nt.backward() \n```"]