- en: torchaudio.models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/audio/stable/models.html](https://pytorch.org/audio/stable/models.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The `torchaudio.models` subpackage contains definitions of models for addressing
    common audio tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For models with pre-trained parameters, please refer to [`torchaudio.pipelines`](pipelines.html#module-torchaudio.pipelines
    "torchaudio.pipelines") module.
  prefs: []
  type: TYPE_NORMAL
- en: Model defintions are responsible for constructing computation graphs and executing
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Some models have complex structure and variations. For such models, factory
    functions are provided.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`Conformer`](generated/torchaudio.models.Conformer.html#torchaudio.models.Conformer
    "torchaudio.models.Conformer") | Conformer architecture introduced in *Conformer:
    Convolution-augmented Transformer for Speech Recognition* [[Gulati *et al.*, 2020](references.html#id21
    "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu,
    Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer:
    convolution-augmented transformer for speech recognition. 2020\. arXiv:2005.08100.")].
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ConvTasNet`](generated/torchaudio.models.ConvTasNet.html#torchaudio.models.ConvTasNet
    "torchaudio.models.ConvTasNet") | Conv-TasNet architecture introduced in *Conv-TasNet:
    Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation* [[Luo
    and Mesgarani, 2019](references.html#id22 "Yi Luo and Nima Mesgarani. Conv-tasnet:
    surpassing ideal time–frequency magnitude masking for speech separation. IEEE/ACM
    Transactions on Audio, Speech, and Language Processing, 27(8):1256–1266, Aug 2019\.
    URL: http://dx.doi.org/10.1109/TASLP.2019.2915167, doi:10.1109/taslp.2019.2915167.")].
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`DeepSpeech`](generated/torchaudio.models.DeepSpeech.html#torchaudio.models.DeepSpeech
    "torchaudio.models.DeepSpeech") | DeepSpeech architecture introduced in *Deep
    Speech: Scaling up end-to-end speech recognition* [[Hannun *et al.*, 2014](references.html#id17
    "Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen,
    Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y. Ng.
    Deep speech: scaling up end-to-end speech recognition. 2014\. arXiv:1412.5567.")].
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Emformer`](generated/torchaudio.models.Emformer.html#torchaudio.models.Emformer
    "torchaudio.models.Emformer") | Emformer architecture introduced in *Emformer:
    Efficient Memory Transformer Based Acoustic Model for Low Latency Streaming Speech
    Recognition* [[Shi *et al.*, 2021](references.html#id30 "Yangyang Shi, Yongqiang
    Wang, Chunyang Wu, Ching-Feng Yeh, Julian Chan, Frank Zhang, Duc Le, and Mike
    Seltzer. Emformer: efficient memory transformer based acoustic model for low latency
    streaming speech recognition. In ICASSP 2021 - 2021 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP), 6783-6787\. 2021.")]. |'
  prefs: []
  type: TYPE_TB
- en: '| [`HDemucs`](generated/torchaudio.models.HDemucs.html#torchaudio.models.HDemucs
    "torchaudio.models.HDemucs") | Hybrid Demucs model from *Hybrid Spectrogram and
    Waveform Source Separation* [[Défossez, 2021](references.html#id50 "Alexandre
    Défossez. Hybrid spectrogram and waveform source separation. In Proceedings of
    the ISMIR 2021 Workshop on Music Source Separation. 2021.")]. |'
  prefs: []
  type: TYPE_TB
- en: '| [`HuBERTPretrainModel`](generated/torchaudio.models.HuBERTPretrainModel.html#torchaudio.models.HuBERTPretrainModel
    "torchaudio.models.HuBERTPretrainModel") | HuBERT model used for pretraining in
    *HuBERT* [[Hsu *et al.*, 2021](references.html#id16 "Wei-Ning Hsu, Benjamin Bolte,
    Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed.
    Hubert: self-supervised speech representation learning by masked prediction of
    hidden units. 2021\. arXiv:2106.07447.")]. |'
  prefs: []
  type: TYPE_TB
- en: '| [`RNNT`](generated/torchaudio.models.RNNT.html#torchaudio.models.RNNT "torchaudio.models.RNNT")
    | Recurrent neural network transducer (RNN-T) model. |'
  prefs: []
  type: TYPE_TB
- en: '| [`RNNTBeamSearch`](generated/torchaudio.models.RNNTBeamSearch.html#torchaudio.models.RNNTBeamSearch
    "torchaudio.models.RNNTBeamSearch") | Beam search decoder for RNN-T model. |'
  prefs: []
  type: TYPE_TB
- en: '| [`SquimObjective`](generated/torchaudio.models.SquimObjective.html#torchaudio.models.SquimObjective
    "torchaudio.models.SquimObjective") | Speech Quality and Intelligibility Measures
    (SQUIM) model that predicts **objective** metric scores for speech enhancement
    (e.g., STOI, PESQ, and SI-SDR). |'
  prefs: []
  type: TYPE_TB
- en: '| [`SquimSubjective`](generated/torchaudio.models.SquimSubjective.html#torchaudio.models.SquimSubjective
    "torchaudio.models.SquimSubjective") | Speech Quality and Intelligibility Measures
    (SQUIM) model that predicts **subjective** metric scores for speech enhancement
    (e.g., Mean Opinion Score (MOS)). |'
  prefs: []
  type: TYPE_TB
- en: '| [`Tacotron2`](generated/torchaudio.models.Tacotron2.html#torchaudio.models.Tacotron2
    "torchaudio.models.Tacotron2") | Tacotron2 model from *Natural TTS Synthesis by
    Conditioning WaveNet on Mel Spectrogram Predictions* [[Shen *et al.*, 2018](references.html#id27
    "Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng
    Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, and others. Natural
    tts synthesis by conditioning wavenet on mel spectrogram predictions. In 2018
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    4779–4783\. IEEE, 2018.")] based on the implementation from [Nvidia Deep Learning
    Examples](https://github.com/NVIDIA/DeepLearningExamples/). |'
  prefs: []
  type: TYPE_TB
- en: '| [`Wav2Letter`](generated/torchaudio.models.Wav2Letter.html#torchaudio.models.Wav2Letter
    "torchaudio.models.Wav2Letter") | Wav2Letter model architecture from *Wav2Letter:
    an End-to-End ConvNet-based Speech Recognition System* [[Collobert *et al.*, 2016](references.html#id19
    "Ronan Collobert, Christian Puhrsch, and Gabriel Synnaeve. Wav2letter: an end-to-end
    convnet-based speech recognition system. 2016\. arXiv:1609.03193.")]. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Wav2Vec2Model`](generated/torchaudio.models.Wav2Vec2Model.html#torchaudio.models.Wav2Vec2Model
    "torchaudio.models.Wav2Vec2Model") | Acoustic model used in *wav2vec 2.0* [[Baevski
    *et al.*, 2020](references.html#id15 "Alexei Baevski, Henry Zhou, Abdelrahman
    Mohamed, and Michael Auli. Wav2vec 2.0: a framework for self-supervised learning
    of speech representations. 2020\. arXiv:2006.11477.")]. |'
  prefs: []
  type: TYPE_TB
- en: '| [`WaveRNN`](generated/torchaudio.models.WaveRNN.html#torchaudio.models.WaveRNN
    "torchaudio.models.WaveRNN") | WaveRNN model from *Efficient Neural Audio Synthesis*
    [[Kalchbrenner *et al.*, 2018](references.html#id3 "Nal Kalchbrenner, Erich Elsen,
    Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg,
    Aäron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio
    synthesis. CoRR, 2018\. URL: http://arxiv.org/abs/1802.08435, arXiv:1802.08435.")]
    based on the implementation from [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN).
    |'
  prefs: []
  type: TYPE_TB
