- en: ASR Inference with CUDA CTC Decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/audio/stable/tutorials/asr_inference_with_cuda_ctc_decoder_tutorial.html](https://pytorch.org/audio/stable/tutorials/asr_inference_with_cuda_ctc_decoder_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-tutorials-asr-inference-with-cuda-ctc-decoder-tutorial-py)
    to download the full example code
  prefs: []
  type: TYPE_NORMAL
- en: '**Author**: [Yuekai Zhang](mailto:yuekaiz%40nvidia.com)'
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial shows how to perform speech recognition inference using a CUDA-based
    CTC beam search decoder. We demonstrate this on a pretrained [Zipformer](https://github.com/k2-fsa/icefall/tree/master/egs/librispeech/ASR/pruned_transducer_stateless7_ctc)
    model from [Next-gen Kaldi](https://nadirapovey.com/next-gen-kaldi-what-is-it)
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Overview[](#overview "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beam search decoding works by iteratively expanding text hypotheses (beams)
    with next possible characters, and maintaining only the hypotheses with the highest
    scores at each time step.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying implementation uses cuda to acclerate the whole decoding process
  prefs: []
  type: TYPE_NORMAL
- en: A mathematical formula for the decoder can be
  prefs: []
  type: TYPE_NORMAL
- en: found in the [paper](https://arxiv.org/pdf/1408.2873.pdf), and a more detailed
    algorithm can be found in this [blog](https://distill.pub/2017/ctc/).
  prefs: []
  type: TYPE_NORMAL
- en: Running ASR inference using a CUDA CTC Beam Search decoder requires the following
    components
  prefs: []
  type: TYPE_NORMAL
- en: 'Acoustic Model: model predicting modeling units (BPE in this tutorial) from
    acoustic features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BPE Model: the byte-pair encoding (BPE) tokenizer file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acoustic Model and Set Up[](#acoustic-model-and-set-up "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First we import the necessary utilities and fetch the data that we are working
    with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We use the pretrained [Zipformer](https://huggingface.co/Zengwei/icefall-asr-librispeech-pruned-transducer-stateless7-ctc-2022-12-01)
    model that is trained on the [LibriSpeech dataset](http://www.openslr.org/12).
    The model is jointly trained with CTC and Transducer loss functions. In this tutorial,
    we only use CTC head of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We will load a sample from the LibriSpeech test-other dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: The transcript corresponding to this audio file is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Files and Data for Decoder[](#files-and-data-for-decoder "Permalink to this
    heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we load in our token from BPE model, which is the tokenizer for decoding.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens[](#tokens "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The tokens are the possible symbols that the acoustic model can predict, including
    the blank symbol in CTC. In this tutorial, it includes 500 BPE tokens. It can
    either be passed in as a file, where each line consists of the tokens corresponding
    to the same index, or as a list of tokens, each mapping to a unique index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Construct CUDA Decoder[](#construct-cuda-decoder "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we will construct a CUDA beam search decoder. The decoder
    can be constructed using the factory function [`cuda_ctc_decoder()`](../generated/torchaudio.models.decoder.cuda_ctc_decoder.html#torchaudio.models.decoder.cuda_ctc_decoder
    "torchaudio.models.decoder.cuda_ctc_decoder").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Run Inference[](#run-inference "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the data, acoustic model, and decoder, we can perform inference.
    The output of the beam search decoder is of type [`CUCTCHypothesis`](../generated/torchaudio.models.decoder.CUCTCDecoder.html#torchaudio.models.decoder.CUCTCHypothesis
    "torchaudio.models.decoder.CUCTCHypothesis"), consisting of the predicted token
    IDs, words (symbols corresponding to the token IDs), and hypothesis scores. Recall
    the transcript corresponding to the waveform is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The cuda ctc decoder gives the following result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Beam Search Decoder Parameters[](#beam-search-decoder-parameters "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we go a little bit more in depth about some different parameters
    and tradeoffs. For the full list of customizable parameters, please refer to the
    [`documentation`](../generated/torchaudio.models.decoder.cuda_ctc_decoder.html#torchaudio.models.decoder.cuda_ctc_decoder
    "torchaudio.models.decoder.cuda_ctc_decoder").
  prefs: []
  type: TYPE_NORMAL
- en: Helper Function[](#helper-function "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: nbest[](#nbest "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This parameter indicates the number of best hypotheses to return. For instance,
    by setting `nbest=10` when constructing the beam search decoder earlier, we can
    now access the hypotheses with the top 10 scores.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: beam size[](#beam-size "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `beam_size` parameter determines the maximum number of best hypotheses to
    hold after each decoding step. Using larger beam sizes allows for exploring a
    larger range of possible hypotheses which can produce hypotheses with higher scores,
    but it does not provide additional gains beyond a certain point. We recommend
    to set beam_size=10 for cuda beam search decoder.
  prefs: []
  type: TYPE_NORMAL
- en: In the example below, we see improvement in decoding quality as we increase
    beam size from 1 to 3, but notice how using a beam size of 3 provides the same
    output as beam size 10.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: blank skip threshold[](#blank-skip-threshold "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `blank_skip_threshold` parameter is used to prune the frames which have
    large blank probability. Pruning these frames with a good blank_skip_threshold
    could speed up decoding process a lot while no accuracy drop. Since the rule of
    CTC, we would keep at least one blank frame between two non-blank frames to avoid
    mistakenly merge two consecutive identical symbols. We recommend to set blank_skip_threshold=0.95
    for cuda beam search decoder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Benchmark with flashlight CPU decoder[](#benchmark-with-flashlight-cpu-decoder
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We benchmark the throughput and accuracy between CUDA decoder and CPU decoder
    using librispeech test_other set. To reproduce below benchmark results, you may
    refer [here](https://github.com/pytorch/audio/tree/main/examples/asr/librispeech_cuda_ctc_decoder).
  prefs: []
  type: TYPE_NORMAL
- en: '| Decoder | Setting | WER (%) | N-Best Oracle WER (%) | Decoder Cost Time (seconds)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA decoder | blank_skip_threshold 0.95 | 5.81 | 4.11 | 2.57 |'
  prefs: []
  type: TYPE_TB
- en: '| CUDA decoder | blank_skip_threshold 1.0 (no frame-skip) | 5.81 | 4.09 | 6.24
    |'
  prefs: []
  type: TYPE_TB
- en: '| CPU decoder | beam_size_token 10 | 5.86 | 4.30 | 28.61 |'
  prefs: []
  type: TYPE_TB
- en: '| CPU decoder | beam_size_token 500 | 5.86 | 4.30 | 791.80 |'
  prefs: []
  type: TYPE_TB
- en: From the above table, CUDA decoder could give a slight improvement in WER and
    a significant increase in throughput.
  prefs: []
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 0 minutes 8.752 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: asr_inference_with_cuda_ctc_decoder_tutorial.py`](../_downloads/3956cf493d21711e687e9610c91f9cd1/asr_inference_with_cuda_ctc_decoder_tutorial.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: asr_inference_with_cuda_ctc_decoder_tutorial.ipynb`](../_downloads/96982138e59c541534342222a3f5c69e/asr_inference_with_cuda_ctc_decoder_tutorial.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
