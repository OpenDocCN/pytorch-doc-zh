["```py\nimport torch\n\nclass Square(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        # Because we are saving one of the inputs use `save_for_backward`\n        # Save non-tensors and non-inputs/non-outputs directly on ctx\n        ctx.save_for_backward(x)\n        return x**2\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        # A function support double backward automatically if autograd\n        # is able to record the computations performed in backward\n        x, = ctx.saved_tensors\n        return grad_out * 2 * x\n\n# Use double precision because finite differencing method magnifies errors\nx = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(Square.apply, x)\n# Use gradcheck to verify second-order derivatives\ntorch.autograd.gradgradcheck(Square.apply, x) \n```", "```py\nimport torchviz\n\nx = torch.tensor(1., requires_grad=True).clone()\nout = Square.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), {\"grad_x\": grad_x, \"x\": x, \"out\": out}) \n```", "```py\nclass Exp(torch.autograd.Function):\n    # Simple case where everything goes well\n    @staticmethod\n    def forward(ctx, x):\n        # This time we save the output\n        result = torch.exp(x)\n        # Note that we should use `save_for_backward` here when\n        # the tensor saved is an ouptut (or an input).\n        ctx.save_for_backward(result)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        result, = ctx.saved_tensors\n        return result * grad_out\n\nx = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n# Validate our gradients using gradcheck\ntorch.autograd.gradcheck(Exp.apply, x)\ntorch.autograd.gradgradcheck(Exp.apply, x) \n```", "```py\nout = Exp.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), {\"grad_x\": grad_x, \"x\": x, \"out\": out}) \n```", "```py\nclass Sinh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        expx = torch.exp(x)\n        expnegx = torch.exp(-x)\n        ctx.save_for_backward(expx, expnegx)\n        # In order to be able to save the intermediate results, a trick is to\n        # include them as our outputs, so that the backward graph is constructed\n        return (expx - expnegx) / 2, expx, expnegx\n\n    @staticmethod\n    def backward(ctx, grad_out, _grad_out_exp, _grad_out_negexp):\n        expx, expnegx = ctx.saved_tensors\n        grad_input = grad_out * (expx + expnegx) / 2\n        # We cannot skip accumulating these even though we won't use the outputs\n        # directly. They will be used later in the second backward.\n        grad_input += _grad_out_exp * expx\n        grad_input -= _grad_out_negexp * expnegx\n        return grad_input\n\ndef sinh(x):\n    # Create a wrapper that only returns the first output\n    return Sinh.apply(x)[0]\n\nx = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(sinh, x)\ntorch.autograd.gradgradcheck(sinh, x) \n```", "```py\nout = sinh(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out}) \n```", "```py\nclass SinhBad(torch.autograd.Function):\n    # This is an example of what NOT to do!\n    @staticmethod\n    def forward(ctx, x):\n        expx = torch.exp(x)\n        expnegx = torch.exp(-x)\n        ctx.expx = expx\n        ctx.expnegx = expnegx\n        return (expx - expnegx) / 2\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        expx = ctx.expx\n        expnegx = ctx.expnegx\n        grad_input = grad_out * (expx + expnegx) / 2\n        return grad_input \n```", "```py\nout = SinhBad.apply(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out}) \n```", "```py\ndef cube_forward(x):\n    return x**3\n\ndef cube_backward(grad_out, x):\n    return grad_out * 3 * x**2\n\ndef cube_backward_backward(grad_out, sav_grad_out, x):\n    return grad_out * sav_grad_out * 6 * x\n\ndef cube_backward_backward_grad_out(grad_out, x):\n    return grad_out * 3 * x**2\n\nclass Cube(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return cube_forward(x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, = ctx.saved_tensors\n        return CubeBackward.apply(grad_out, x)\n\nclass CubeBackward(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, grad_out, x):\n        ctx.save_for_backward(x, grad_out)\n        return cube_backward(grad_out, x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, sav_grad_out = ctx.saved_tensors\n        dx = cube_backward_backward(grad_out, sav_grad_out, x)\n        dgrad_out = cube_backward_backward_grad_out(grad_out, x)\n        return dgrad_out, dx\n\nx = torch.tensor(2., requires_grad=True, dtype=torch.double)\n\ntorch.autograd.gradcheck(Cube.apply, x)\ntorch.autograd.gradgradcheck(Cube.apply, x) \n```", "```py\nout = Cube.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out}) \n```"]