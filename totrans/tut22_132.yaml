- en: Introduction to TorchRec
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TorchRec简介
- en: 原文：[https://pytorch.org/tutorials/intermediate/torchrec_tutorial.html](https://pytorch.org/tutorials/intermediate/torchrec_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/intermediate/torchrec_tutorial.html](https://pytorch.org/tutorials/intermediate/torchrec_tutorial.html)
- en: Tip
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: To get the most of this tutorial, we suggest using this [Colab Version](https://colab.research.google.com/github/pytorch/torchrec/blob/main/Torchrec_Introduction.ipynb).
    This will allow you to experiment with the information presented below.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用本教程，我们建议使用这个[Colab版本](https://colab.research.google.com/github/pytorch/torchrec/blob/main/Torchrec_Introduction.ipynb)。这将使您能够尝试下面提供的信息。
- en: Follow along with the video below or on [youtube](https://www.youtube.com/watch?v=cjgj41dvSeQ).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请跟随下面的视频或在[youtube](https://www.youtube.com/watch?v=cjgj41dvSeQ)上观看。
- en: '[https://www.youtube.com/embed/cjgj41dvSeQ](https://www.youtube.com/embed/cjgj41dvSeQ)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/cjgj41dvSeQ](https://www.youtube.com/embed/cjgj41dvSeQ)'
- en: When building recommendation systems, we frequently want to represent entities
    like products or pages with embeddings. For example, see Meta AI’s [Deep learning
    recommendation model](https://arxiv.org/abs/1906.00091), or DLRM. As the number
    of entities grow, the size of the embedding tables can exceed a single GPU’s memory.
    A common practice is to shard the embedding table across devices, a type of model
    parallelism. To that end, TorchRec introduces its primary API called [`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel),
    or DMP. Like PyTorch’s DistributedDataParallel, DMP wraps a model to enable distributed
    training.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建推荐系统时，我们经常希望用嵌入来表示产品或页面等实体。例如，参见Meta AI的[深度学习推荐模型](https://arxiv.org/abs/1906.00091)，或DLRM。随着实体数量的增长，嵌入表的大小可能超过单个GPU的内存。一种常见做法是将嵌入表分片到不同设备上，这是一种模型并行的类型。为此，TorchRec引入了其主要API称为[`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel)，或DMP。与PyTorch的DistributedDataParallel类似，DMP包装了一个模型以实现分布式训练。
- en: Installation
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'Requirements: python >= 3.7'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要求：python >= 3.7
- en: 'We highly recommend CUDA when using TorchRec (If using CUDA: cuda >= 11.0).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用TorchRec时，我们强烈建议使用CUDA（如果使用CUDA：cuda >= 11.0）。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Overview
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'This tutorial will cover three pieces of TorchRec: the `nn.module` [`EmbeddingBagCollection`](https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection),
    the [`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel)
    API, and the datastructure [`KeyedJaggedTensor`](https://pytorch.org/torchrec/torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将涵盖TorchRec的三个部分：`nn.module` [`EmbeddingBagCollection`](https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection)，[`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel)
    API和数据结构[`KeyedJaggedTensor`](https://pytorch.org/torchrec/torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor)。
- en: Distributed Setup
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式设置
- en: We setup our environment with torch.distributed. For more info on distributed,
    see this [tutorial](https://pytorch.org/tutorials/beginner/dist_overview.html).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用torch.distributed设置我们的环境。有关分布式的更多信息，请参见此[tutorial](https://pytorch.org/tutorials/beginner/dist_overview.html)。
- en: Here, we use one rank (the colab process) corresponding to our 1 colab GPU.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用一个rank（colab进程）对应于我们的1个colab GPU。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: From EmbeddingBag to EmbeddingBagCollection
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从EmbeddingBag到EmbeddingBagCollection
- en: PyTorch represents embeddings through [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)
    and [`torch.nn.EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html).
    EmbeddingBag is a pooled version of Embedding.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch通过[`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)和[`torch.nn.EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html)来表示嵌入。EmbeddingBag是Embedding的池化版本。
- en: TorchRec extends these modules by creating collections of embeddings. We will
    use [`EmbeddingBagCollection`](https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection)
    to represent a group of EmbeddingBags.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: TorchRec通过创建嵌入的集合来扩展这些模块。我们将使用[`EmbeddingBagCollection`](https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection)来表示一组EmbeddingBags。
- en: Here, we create an EmbeddingBagCollection (EBC) with two embedding bags. Each
    table, `product_table` and `user_table`, is represented by a 64 dimension embedding
    of size 4096\. Note how we initially allocate the EBC on device “meta”. This will
    tell EBC to not allocate memory yet.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个包含两个嵌入包的EmbeddingBagCollection（EBC）。每个表，`product_table`和`user_table`，由大小为4096的64维嵌入表示。请注意，我们最初将EBC分配到设备“meta”。这将告诉EBC暂时不要分配内存。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: DistributedModelParallel
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DistributedModelParallel
- en: 'Now, we’re ready to wrap our model with [`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel)
    (DMP). Instantiating DMP will:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备用[`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel)
    (DMP)包装我们的模型。实例化DMP将：
- en: Decide how to shard the model. DMP will collect the available ‘sharders’ and
    come up with a ‘plan’ of the optimal way to shard the embedding table(s) (i.e.,
    the EmbeddingBagCollection).
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定如何分片模型。DMP将收集可用的“分片器”并提出一种最佳方式来分片嵌入表（即EmbeddingBagCollection）的“计划”。
- en: Actually shard the model. This includes allocating memory for each embedding
    table on the appropriate device(s).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际分片模型。这包括为每个嵌入表在适当设备上分配内存。
- en: In this toy example, since we have two EmbeddingTables and one GPU, TorchRec
    will place both on the single GPU.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，由于我们有两个EmbeddingTables和一个GPU，TorchRec将两者都放在单个GPU上。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Query vanilla nn.EmbeddingBag with input and offsets
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用输入和偏移查询普通的nn.EmbeddingBag
- en: We query [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)
    and [`nn.EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html)
    with `input` and `offsets`. Input is a 1-D tensor containing the lookup values.
    Offsets is a 1-D tensor where the sequence is a cumulative sum of the number of
    values to pool per example.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`input`和`offsets`查询[`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)和[`nn.EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html)。Input是包含查找值的1-D张量。Offsets是一个1-D张量，其中序列是每个示例要汇总的值的累积和。
- en: 'Let’s look at an example, recreating the product EmbeddingBag above:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，重新创建上面的产品EmbeddingBag：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Representing minibatches with KeyedJaggedTensor
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用KeyedJaggedTensor表示小批量
- en: We need an efficient representation of multiple examples of an arbitrary number
    of entity IDs per feature per example. In order to enable this “jagged” representation,
    we use the TorchRec datastructure [`KeyedJaggedTensor`](https://pytorch.org/torchrec/torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor)
    (KJT).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个有效的表示，每个示例的每个特征中有任意数量的实体ID的多个示例。为了实现这种“不规则”表示，我们使用TorchRec数据结构[`KeyedJaggedTensor`](https://pytorch.org/torchrec/torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor)（KJT）。
- en: Let’s take a look at how to lookup a collection of two embedding bags, “product”
    and “user”. Assume the minibatch is made up of three examples for three users.
    The first of which has two product IDs, the second with none, and the third with
    one product ID.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何查找两个嵌入包“product”和“user”的集合。假设小批量由三个用户的三个示例组成。第一个示例有两个产品ID，第二个没有，第三个有一个产品ID。
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The query should be:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 查询应该是：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that the KJT batch size is `batch_size = len(lengths)//len(keys)`. In the
    above example, batch_size is 3.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，KJT批量大小为`batch_size = len(lengths)//len(keys)`。在上面的例子中，batch_size为3。
- en: Putting it all together, querying our distributed model with a KJT minibatch
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起，使用KJT小批量查询我们的分布式模型
- en: Finally, we can query our model using our minibatch of products and users.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用我们的产品和用户的小批量查询我们的模型。
- en: The resulting lookup will contain a KeyedTensor, where each key (or feature)
    contains a 2D tensor of size 3x64 (batch_size x embedding_dim).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结果查找将包含一个KeyedTensor，其中每个键（或特征）包含一个大小为3x64（batch_size x embedding_dim）的2D张量。
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: More resources
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多资源
- en: For more information, please see our [dlrm](https://github.com/pytorch/torchrec/tree/main/examples/dlrm)
    example, which includes multinode training on the criteo terabyte dataset, using
    Meta’s [DLRM](https://arxiv.org/abs/1906.00091).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅我们的[dlrm](https://github.com/pytorch/torchrec/tree/main/examples/dlrm)示例，其中包括在criteo
    terabyte数据集上进行多节点训练，使用Meta的[DLRM](https://arxiv.org/abs/1906.00091)。
