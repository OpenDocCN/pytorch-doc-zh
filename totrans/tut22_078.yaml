- en: 'Jacobians, Hessians, hvp, vhp, and more: composing function transforms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/intermediate/jacobians_hessians.html](https://pytorch.org/tutorials/intermediate/jacobians_hessians.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-intermediate-jacobians-hessians-py) to download
    the full example code
  prefs: []
  type: TYPE_NORMAL
- en: Computing jacobians or hessians are useful in a number of non-traditional deep
    learning models. It is difficult (or annoying) to compute these quantities efficiently
    using PyTorch’s regular autodiff APIs (`Tensor.backward()`, `torch.autograd.grad`).
    PyTorch’s [JAX-inspired](https://github.com/google/jax) [function transforms API](https://pytorch.org/docs/master/func.html)
    provides ways of computing various higher-order autodiff quantities efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial requires PyTorch 2.0.0 or later.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the Jacobian
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s start with a function that we’d like to compute the jacobian of. This
    is a simple linear function with non-linear activation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s add some dummy data: a weight, a bias, and a feature vector x.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s think of `predict` as a function that maps the input `x` from \(R^D \to
    R^D\). PyTorch Autograd computes vector-Jacobian products. In order to compute
    the full Jacobian of this \(R^D \to R^D\) function, we would have to compute it
    row-by-row by using a different unit vector each time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of computing the jacobian row-by-row, we can use PyTorch’s `torch.vmap`
    function transform to get rid of the for-loop and vectorize the computation. We
    can’t directly apply `vmap` to `torch.autograd.grad`; instead, PyTorch provides
    a `torch.func.vjp` transform that composes with `torch.vmap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In a later tutorial a composition of reverse-mode AD and `vmap` will give us
    per-sample-gradients. In this tutorial, composing reverse-mode AD and `vmap` gives
    us Jacobian computation! Various compositions of `vmap` and autodiff transforms
    can give us different interesting quantities.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch provides `torch.func.jacrev` as a convenience function that performs
    the `vmap-vjp` composition to compute jacobians. `jacrev` accepts an `argnums`
    argument that says which argument we would like to compute Jacobians with respect
    to.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let’s compare the performance of the two ways to compute the jacobian. The function
    transform version is much faster (and becomes even faster the more outputs there
    are).
  prefs: []
  type: TYPE_NORMAL
- en: In general, we expect that vectorization via `vmap` can help eliminate overhead
    and give better utilization of your hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '`vmap` does this magic by pushing the outer loop down into the function’s primitive
    operations in order to obtain better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make a quick function to evaluate performance and deal with microseconds
    and milliseconds measurements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And then run the performance comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s do a relative performance comparison of the above with our `get_perf`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Furthermore, it’s pretty easy to flip the problem around and say we want to
    compute Jacobians of the parameters to our model (weight, bias) instead of the
    input
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Reverse-mode Jacobian (`jacrev`) vs forward-mode Jacobian (`jacfwd`)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We offer two APIs to compute jacobians: `jacrev` and `jacfwd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`jacrev` uses reverse-mode AD. As you saw above it is a composition of our
    `vjp` and `vmap` transforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jacfwd` uses forward-mode AD. It is implemented as a composition of our `jvp`
    and `vmap` transforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jacfwd` and `jacrev` can be substituted for each other but they have different
    performance characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a general rule of thumb, if you’re computing the jacobian of an \(R^N \to
    R^M\) function, and there are many more outputs than inputs (for example, \(M
    > N\)) then `jacfwd` is preferred, otherwise use `jacrev`. There are exceptions
    to this rule, but a non-rigorous argument for this follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In reverse-mode AD, we are computing the jacobian row-by-row, while in forward-mode
    AD (which computes Jacobian-vector products), we are computing it column-by-column.
    The Jacobian matrix has M rows and N columns, so if it is taller or wider one
    way we may prefer the method that deals with fewer rows or columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let’s benchmark with more inputs than outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'and then do a relative benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'and now the reverse - more outputs (M) than inputs (N):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'and a relative performance comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Hessian computation with functorch.hessian
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We offer a convenience API to compute hessians: `torch.func.hessiani`. Hessians
    are the jacobian of the jacobian (or the partial derivative of the partial derivative,
    aka second order).'
  prefs: []
  type: TYPE_NORMAL
- en: This suggests that one can just compose functorch jacobian transforms to compute
    the Hessian. Indeed, under the hood, `hessian(f)` is simply `jacfwd(jacrev(f))`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: to boost performance: depending on your model, you may also want to use
    `jacfwd(jacfwd(f))` or `jacrev(jacrev(f))` instead to compute hessians leveraging
    the rule of thumb above regarding wider vs taller matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Let’s verify we have the same result regardless of using hessian API or using
    `jacfwd(jacfwd())`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Batch Jacobian and Batch Hessian
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the above examples we’ve been operating with a single feature vector. In
    some cases you might want to take the Jacobian of a batch of outputs with respect
    to a batch of inputs. That is, given a batch of inputs of shape `(B, N)` and a
    function that goes from \(R^N \to R^M\), we would like a Jacobian of shape `(B,
    M, N)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to do this is to use `vmap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have a function that goes from (B, N) -> (B, M) instead and are certain
    that each input produces an independent output, then it’s also sometimes possible
    to do this without using `vmap` by summing the outputs and then computing the
    Jacobian of that function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If you instead have a function that goes from \(R^N \to R^M\) but inputs that
    are batched, you compose `vmap` with `jacrev` to compute batched jacobians:'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, batch hessians can be computed similarly. It’s easiest to think about
    them by using `vmap` to batch over hessian computation, but in some cases the
    sum trick also works.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Computing Hessian-vector products
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The naive way to compute a Hessian-vector product (hvp) is to materialize the
    full Hessian and perform a dot-product with a vector. We can do better: it turns
    out we don’t need to materialize the full Hessian to do this. We’ll go through
    two (of many) different strategies to compute Hessian-vector products: - composing
    reverse-mode AD with reverse-mode AD - composing reverse-mode AD with forward-mode
    AD'
  prefs: []
  type: TYPE_NORMAL
- en: 'Composing reverse-mode AD with forward-mode AD (as opposed to reverse-mode
    with reverse-mode) is generally the more memory efficient way to compute a hvp
    because forward-mode AD doesn’t need to construct an Autograd graph and save intermediates
    for backward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here’s some sample usage.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If PyTorch forward-AD does not have coverage for your operations, then we can
    instead compose reverse-mode AD with reverse-mode AD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '**Total running time of the script:** ( 0 minutes 10.644 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: jacobians_hessians.py`](../_downloads/089b69a49b6eb4080d35c4b983b939a5/jacobians_hessians.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: jacobians_hessians.ipynb`](../_downloads/748f25c58a5ac0f57235c618e51c869b/jacobians_hessians.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
