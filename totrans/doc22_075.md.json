["```py\ntorch.distributed.rpc.init_rpc(name, backend=None, rank=-1, world_size=None, rpc_backend_options=None)\u00b6\n```", "```py\ntorch.distributed.rpc.rpc_sync(to, func, args=None, kwargs=None, timeout=-1.0)\u00b6\n```", "```py\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rpc.shutdown() \n```", "```py\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown() \n```", "```py\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar) \n```", "```py\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rpc.shutdown() \n```", "```py\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown() \n```", "```py\ntorch.distributed.rpc.rpc_async(to, func, args=None, kwargs=None, timeout=-1.0)\u00b6\n```", "```py\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut1 = rpc.rpc_async(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> fut2 = rpc.rpc_async(\"worker1\", min, args=(1, 2))\n>>> result = fut1.wait() + fut2.wait()\n>>> rpc.shutdown() \n```", "```py\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown() \n```", "```py\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar) \n```", "```py\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut = rpc.rpc_async(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> ret = fut.wait()\n>>> rpc.shutdown() \n```", "```py\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown() \n```", "```py\ntorch.distributed.rpc.remote(to, func, args=None, kwargs=None, timeout=-1.0)\u00b6\n```", "```py\nMake sure that ``MASTER_ADDR`` and ``MASTER_PORT`` are set properly\non both workers. Refer to :meth:`~torch.distributed.init_process_group`\nAPI for more details. For example,\n\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\n\nThen run the following code in two different processes:\n\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>> x = rref1.to_here() + rref2.to_here()\n>>> rpc.shutdown()\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\nBelow is an example of running a TorchScript function using RPC.\n\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar)\n\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> rref = rpc.remote(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rref.to_here()\n>>> rpc.shutdown()\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown() \n```", "```py\ntorch.distributed.rpc.get_worker_info(worker_name=None)\u00b6\n```", "```py\ntorch.distributed.rpc.shutdown(graceful=True, timeout=0)\u00b6\n```", "```py\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> # do some work\n>>> result = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(1), 1))\n>>> # ready to shutdown\n>>> rpc.shutdown() \n```", "```py\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> # wait for worker 0 to finish work, and then shutdown.\n>>> rpc.shutdown() \n```", "```py\nclass torch.distributed.rpc.WorkerInfo\u00b6\n```", "```py\nproperty id\u00b6\n```", "```py\nproperty name\u00b6\n```", "```py\ntorch.distributed.rpc.functions.async_execution(fn)\u00b6\n```", "```py\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @rpc.functions.async_execution\n>>> def async_add_chained(to, x, y, z):\n>>>     # This function runs on \"worker1\" and returns immediately when\n>>>     # the callback is installed through the `then(cb)` API. In the\n>>>     # mean time, the `rpc_async` to \"worker2\" can run concurrently.\n>>>     # When the return value of that `rpc_async` arrives at\n>>>     # \"worker1\", \"worker1\" will run the lambda function accordingly\n>>>     # and set the value for the previously returned `Future`, which\n>>>     # will then trigger RPC to send the result back to \"worker0\".\n>>>     return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>         lambda fut: fut.wait() + z\n>>>     )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add_chained,\n>>>     args=(\"worker2\", torch.ones(2), 1, 1)\n>>> )\n>>> print(ret)  # prints tensor([3., 3.]) \n```", "```py\n>>> from torch import Tensor\n>>> from torch.futures import Future\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @torch.jit.script\n>>> def script_add(x: Tensor, y: Tensor) -> Tensor:\n>>>     return x + y\n>>>\n>>> @rpc.functions.async_execution\n>>> @torch.jit.script\n>>> def async_add(to: str, x: Tensor, y: Tensor) -> Future[Tensor]:\n>>>     return rpc.rpc_async(to, script_add, (x, y))\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1)\n>>> )\n>>> print(ret)  # prints tensor([2., 2.]) \n```", "```py\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> class AsyncExecutionClass:\n>>>\n>>>     @staticmethod\n>>>     @rpc.functions.async_execution\n>>>     def static_async_add(to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>>     @classmethod\n>>>     @rpc.functions.async_execution\n>>>     def class_async_add(cls, to, x, y, z):\n>>>         ret_fut = torch.futures.Future()\n>>>         rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: ret_fut.set_result(fut.wait() + z)\n>>>         )\n>>>         return ret_fut\n>>>\n>>>     @rpc.functions.async_execution\n>>>     def bound_async_add(self, to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.static_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.class_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.]) \n```", "```py\n>>> from torch.distributed import rpc\n>>>\n>>> # reuse the AsyncExecutionClass class above\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_sync().static_async_add(\"worker2\", torch.ones(2), 1, 2)\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_async().static_async_add(\"worker2\", torch.ones(2), 1, 2).wait()\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.remote().static_async_add(\"worker2\", torch.ones(2), 1, 2).to_here()\n>>> print(ret)  # prints tensor([4., 4.]) \n```", "```py\nclass torch.distributed.rpc.BackendType(value)\u00b6\n```", "```py\nclass torch.distributed.rpc.RpcBackendOptions\u00b6\n```", "```py\nproperty init_method\u00b6\n```", "```py\nproperty rpc_timeout\u00b6\n```", "```py\n>>> import os\n>>> from torch.distributed import rpc\n>>> os.environ['MASTER_ADDR'] = 'localhost'\n>>> os.environ['MASTER_PORT'] = '29500'\n>>>\n>>> rpc.init_rpc(\n>>>     \"worker1\",\n>>>     rank=0,\n>>>     world_size=2,\n>>>     rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\n>>>         num_worker_threads=8,\n>>>         rpc_timeout=20 # 20 second timeout\n>>>     )\n>>> )\n>>>\n>>> # omitting init_rpc invocation on worker2 \n```", "```py\nclass torch.distributed.rpc.TensorPipeRpcBackendOptions(*, num_worker_threads=16, rpc_timeout=60.0, init_method='env://', device_maps=None, devices=None, _transports=None, _channels=None)\u00b6\n```", "```py\nproperty device_maps\u00b6\n```", "```py\nproperty devices\u00b6\n```", "```py\nproperty init_method\u00b6\n```", "```py\nproperty num_worker_threads\u00b6\n```", "```py\nproperty rpc_timeout\u00b6\n```", "```py\nset_device_map(to, device_map)\u00b6\n```", "```py\n>>> # both workers\n>>> def add(x, y):\n>>>     print(x)  # tensor([1., 1.], device='cuda:1')\n>>>     return x + y, (x + y).to(2)\n>>>\n>>> # on worker 0\n>>> options = TensorPipeRpcBackendOptions(\n>>>     num_worker_threads=8,\n>>>     device_maps={\"worker1\": {0: 1}}\n>>>     # maps worker0's cuda:0 to worker1's cuda:1\n>>> )\n>>> options.set_device_map(\"worker1\", {1: 2})\n>>> # maps worker0's cuda:1 to worker1's cuda:2\n>>>\n>>> rpc.init_rpc(\n>>>     \"worker0\",\n>>>     rank=0,\n>>>     world_size=2,\n>>>     backend=rpc.BackendType.TENSORPIPE,\n>>>     rpc_backend_options=options\n>>> )\n>>>\n>>> x = torch.ones(2)\n>>> rets = rpc.rpc_sync(\"worker1\", add, args=(x.to(0), 1))\n>>> # The first argument will be moved to cuda:1 on worker1\\. When\n>>> # sending the return value back, it will follow the invert of\n>>> # the device map, and hence will be moved back to cuda:0 and\n>>> # cuda:1 on worker0\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:0')\n>>> print(rets[1])  # tensor([2., 2.], device='cuda:1') \n```", "```py\nset_devices(devices)\u00b6\n```", "```py\nclass torch.distributed.rpc.PyRRef(RRef)\u00b6\n```", "```py\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> # get a copy of value from the RRef\n>>> x = rref.to_here() \n```", "```py\n>>> import torch\n>>> from torch.distributed.rpc import RRef\n>>> x = torch.zeros(2, 2)\n>>> rref = RRef(x) \n```", "```py\n>>> # On both worker0 and worker1:\n>>> def f(rref):\n>>>   return rref.to_here() + 1 \n```", "```py\n>>> # On worker0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> from torch.distributed.rpc import RRef\n>>> rref = RRef(torch.zeros(2, 2))\n>>> # the following RPC shares the rref with worker1, reference\n>>> # count is automatically updated.\n>>> rpc.rpc_sync(\"worker1\", f, args=(rref,)) \n```", "```py\nbackward(self: torch._C._distributed_rpc.PyRRef, dist_autograd_ctx_id: int = -1, retain_graph: bool = False) \u2192 None\u00b6\n```", "```py\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id) \n```", "```py\nconfirmed_by_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool\u00b6\n```", "```py\nis_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool\u00b6\n```", "```py\nlocal_value(self: torch._C._distributed_rpc.PyRRef) \u2192 object\u00b6\n```", "```py\nowner(self: torch._C._distributed_rpc.PyRRef) \u2192 torch._C._distributed_rpc.WorkerInfo\u00b6\n```", "```py\nowner_name(self: torch._C._distributed_rpc.PyRRef) \u2192 str\u00b6\n```", "```py\nremote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object\u00b6\n```", "```py\n>>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs)) \n```", "```py\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]]) \n```", "```py\nrpc_async(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object\u00b6\n```", "```py\n>>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs)) \n```", "```py\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]]) \n```", "```py\nrpc_sync(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object\u00b6\n```", "```py\n>>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs)) \n```", "```py\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]]) \n```", "```py\nto_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object\u00b6\n```", "```py\nclass torch.distributed.nn.api.remote_module.RemoteModule(*args, **kwargs)\u00b6\n```", "```py\n    >>> class MyModule(nn.Module):\n    >>>     def forward(input):\n    >>>         return input + 1\n    >>>\n    >>> module_cls = MyModule \n    ```", "```py\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> from torch import nn, Tensor\n>>> from torch.distributed.nn.api.remote_module import RemoteModule\n>>>\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> remote_linear_module = RemoteModule(\n>>>     \"worker1/cpu\", nn.Linear, args=(20, 30),\n>>> )\n>>> input = torch.randn(128, 20)\n>>> ret_fut = remote_linear_module.forward_async(input)\n>>> ret = ret_fut.wait()\n>>> rpc.shutdown() \n```", "```py\n>>> # On worker 1:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>>\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown() \n```", "```py\nget_module_rref()\u00b6\n```", "```py\nremote_parameters(recurse=True)\u00b6\n```", "```py\ntorch.distributed.autograd.backward(context_id: int, roots: List[Tensor], retain_graph=False) \u2192 None\u00b6\n```", "```py\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     pred = model.forward()\n>>>     loss = loss_func(pred, loss)\n>>>     dist_autograd.backward(context_id, loss) \n```", "```py\nclass torch.distributed.autograd.context\u00b6\n```", "```py\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = rpc.rpc_sync(\"worker1\", torch.add, args=(t1, t2)).sum()\n>>>     dist_autograd.backward(context_id, [loss]) \n```", "```py\ntorch.distributed.autograd.get_gradients(context_id: int) \u2192 Dict[Tensor, Tensor]\u00b6\n```", "```py\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = t1 + t2\n>>>     dist_autograd.backward(context_id, [loss.sum()])\n>>>     grads = dist_autograd.get_gradients(context_id)\n>>>     print(grads[t1])\n>>>     print(grads[t2]) \n```"]