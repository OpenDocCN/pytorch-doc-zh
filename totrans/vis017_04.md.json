["```py\nfrom torchvision.models import resnet50, ResNet50_Weights\n\n# Old weights with accuracy 76.130%\nresnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n\n# New weights with accuracy 80.858%\nresnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n\n# Best available weights (currently alias for IMAGENET1K_V2)\n# Note that these weights may change across versions\nresnet50(weights=ResNet50_Weights.DEFAULT)\n\n# Strings are also supported\nresnet50(weights=\"IMAGENET1K_V2\")\n\n# No weights - random initialization\nresnet50(weights=None) \n```", "```py\nfrom torchvision.models import resnet50, ResNet50_Weights\n\n# Using pretrained weights:\nresnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\nresnet50(weights=\"IMAGENET1K_V1\")\nresnet50(pretrained=True)  # deprecated\nresnet50(True)  # deprecated\n\n# Using no weights:\nresnet50(weights=None)\nresnet50()\nresnet50(pretrained=False)  # deprecated\nresnet50(False)  # deprecated \n```", "```py\n# Initialize the Weight Transforms\nweights = ResNet50_Weights.DEFAULT\npreprocess = weights.transforms()\n\n# Apply it to the input image\nimg_transformed = preprocess(img) \n```", "```py\n# Initialize model\nweights = ResNet50_Weights.DEFAULT\nmodel = resnet50(weights=weights)\n\n# Set model to eval mode\nmodel.eval() \n```", "```py\n# List available models\nall_models = list_models()\nclassification_models = list_models(module=torchvision.models)\n\n# Initialize models\nm1 = get_model(\"mobilenet_v3_large\", weights=None)\nm2 = get_model(\"quantized_mobilenet_v3_large\", weights=\"DEFAULT\")\n\n# Fetch weights\nweights = get_weight(\"MobileNet_V3_Large_QuantizedWeights.DEFAULT\")\nassert weights == MobileNet_V3_Large_QuantizedWeights.DEFAULT\n\nweights_enum = get_model_weights(\"quantized_mobilenet_v3_large\")\nassert weights_enum == MobileNet_V3_Large_QuantizedWeights\n\nweights_enum2 = get_model_weights(torchvision.models.quantization.mobilenet_v3_large)\nassert weights_enum == weights_enum2 \n```", "```py\nimport torch\n\n# Option 1: passing weights param as string\nmodel = torch.hub.load(\"pytorch/vision\", \"resnet50\", weights=\"IMAGENET1K_V2\")\n\n# Option 2: passing weights param as enum\nweights = torch.hub.load(\"pytorch/vision\", \"get_weight\", weights=\"ResNet50_Weights.IMAGENET1K_V2\")\nmodel = torch.hub.load(\"pytorch/vision\", \"resnet50\", weights=weights) \n```", "```py\nimport torch\n\nweight_enum = torch.hub.load(\"pytorch/vision\", \"get_model_weights\", name=\"resnet50\")\nprint([weight for weight in weight_enum]) \n```", "```py\nfrom torchvision.io import read_image\nfrom torchvision.models import resnet50, ResNet50_Weights\n\nimg = read_image(\"test/assets/encode_jpeg/grace_hopper_517x606.jpg\")\n\n# Step 1: Initialize model with the best available weights\nweights = ResNet50_Weights.DEFAULT\nmodel = resnet50(weights=weights)\nmodel.eval()\n\n# Step 2: Initialize the inference transforms\npreprocess = weights.transforms()\n\n# Step 3: Apply inference preprocessing transforms\nbatch = preprocess(img).unsqueeze(0)\n\n# Step 4: Use the model and print the predicted category\nprediction = model(batch).squeeze(0).softmax(0)\nclass_id = prediction.argmax().item()\nscore = prediction[class_id].item()\ncategory_name = weights.meta[\"categories\"][class_id]\nprint(f\"{category_name}: {100  *  score:.1f}%\") \n```", "```py\nfrom torchvision.io import read_image\nfrom torchvision.models.quantization import resnet50, ResNet50_QuantizedWeights\n\nimg = read_image(\"test/assets/encode_jpeg/grace_hopper_517x606.jpg\")\n\n# Step 1: Initialize model with the best available weights\nweights = ResNet50_QuantizedWeights.DEFAULT\nmodel = resnet50(weights=weights, quantize=True)\nmodel.eval()\n\n# Step 2: Initialize the inference transforms\npreprocess = weights.transforms()\n\n# Step 3: Apply inference preprocessing transforms\nbatch = preprocess(img).unsqueeze(0)\n\n# Step 4: Use the model and print the predicted category\nprediction = model(batch).squeeze(0).softmax(0)\nclass_id = prediction.argmax().item()\nscore = prediction[class_id].item()\ncategory_name = weights.meta[\"categories\"][class_id]\nprint(f\"{category_name}: {100  *  score}%\") \n```", "```py\nfrom torchvision.io.image import read_image\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\nfrom torchvision.transforms.functional import to_pil_image\n\nimg = read_image(\"gallery/assets/dog1.jpg\")\n\n# Step 1: Initialize model with the best available weights\nweights = FCN_ResNet50_Weights.DEFAULT\nmodel = fcn_resnet50(weights=weights)\nmodel.eval()\n\n# Step 2: Initialize the inference transforms\npreprocess = weights.transforms()\n\n# Step 3: Apply inference preprocessing transforms\nbatch = preprocess(img).unsqueeze(0)\n\n# Step 4: Use the model and visualize the prediction\nprediction = model(batch)[\"out\"]\nnormalized_masks = prediction.softmax(dim=1)\nclass_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\nmask = normalized_masks[0, class_to_idx[\"dog\"]]\nto_pil_image(mask).show() \n```", "```py\nfrom torchvision.io.image import read_image\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\nfrom torchvision.utils import draw_bounding_boxes\nfrom torchvision.transforms.functional import to_pil_image\n\nimg = read_image(\"test/assets/encode_jpeg/grace_hopper_517x606.jpg\")\n\n# Step 1: Initialize model with the best available weights\nweights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\nmodel = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\nmodel.eval()\n\n# Step 2: Initialize the inference transforms\npreprocess = weights.transforms()\n\n# Step 3: Apply inference preprocessing transforms\nbatch = [preprocess(img)]\n\n# Step 4: Use the model and visualize the prediction\nprediction = model(batch)[0]\nlabels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\nbox = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n                          labels=labels,\n                          colors=\"red\",\n                          width=4, font_size=30)\nim = to_pil_image(box.detach())\nim.show() \n```", "```py\nfrom torchvision.io.video import read_video\nfrom torchvision.models.video import r3d_18, R3D_18_Weights\n\nvid, _, _ = read_video(\"test/assets/videos/v_SoccerJuggling_g23_c01.avi\", output_format=\"TCHW\")\nvid = vid[:32]  # optionally shorten duration\n\n# Step 1: Initialize model with the best available weights\nweights = R3D_18_Weights.DEFAULT\nmodel = r3d_18(weights=weights)\nmodel.eval()\n\n# Step 2: Initialize the inference transforms\npreprocess = weights.transforms()\n\n# Step 3: Apply inference preprocessing transforms\nbatch = preprocess(vid).unsqueeze(0)\n\n# Step 4: Use the model and print the predicted category\nprediction = model(batch).squeeze(0).softmax(0)\nlabel = prediction.argmax().item()\nscore = prediction[label].item()\ncategory_name = weights.meta[\"categories\"][label]\nprint(f\"{category_name}: {100  *  score}%\") \n```"]