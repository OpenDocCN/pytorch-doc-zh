# HIP（ROCm）语义

> 原文：[https://pytorch.org/docs/stable/notes/hip.html](https://pytorch.org/docs/stable/notes/hip.html)

ROCm™是AMD的开源软件平台，用于GPU加速的高性能计算和机器学习。HIP是ROCm的C++方言，旨在简化将CUDA应用程序转换为可移植的C++代码。在将现有CUDA应用程序（如PyTorch）转换为可移植的C++以及需要在AMD和NVIDIA之间实现可移植性的新项目中使用HIP。

## HIP接口重用CUDA接口

PyTorch for HIP有意重用现有的[`torch.cuda`](../cuda.html#module-torch.cuda)接口。这有助于加速现有PyTorch代码和模型的移植，因为几乎不需要进行任何代码更改。

来自[CUDA语义](cuda.html#cuda-semantics)的示例将在HIP上完全相同：

```py
cuda = torch.device('cuda')     # Default HIP device
cuda0 = torch.device('cuda:0')  # 'rocm' or 'hip' are not valid, use 'cuda'
cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)

x = torch.tensor([1., 2.], device=cuda0)
# x.device is device(type='cuda', index=0)
y = torch.tensor([1., 2.]).cuda()
# y.device is device(type='cuda', index=0)

with torch.cuda.device(1):
    # allocates a tensor on GPU 1
    a = torch.tensor([1., 2.], device=cuda)

    # transfers a tensor from CPU to GPU 1
    b = torch.tensor([1., 2.]).cuda()
    # a.device and b.device are device(type='cuda', index=1)

    # You can also use ``Tensor.to`` to transfer a tensor:
    b2 = torch.tensor([1., 2.]).to(device=cuda)
    # b.device and b2.device are device(type='cuda', index=1)

    c = a + b
    # c.device is device(type='cuda', index=1)

    z = x + y
    # z.device is device(type='cuda', index=0)

    # even within a context, you can specify the device
    # (or give a GPU index to the .cuda call)
    d = torch.randn(2, device=cuda2)
    e = torch.randn(2).to(cuda2)
    f = torch.randn(2).cuda(cuda2)
    # d.device, e.device, and f.device are all device(type='cuda', index=2) 
```## 检查HIP

无论您是在CUDA还是HIP上使用PyTorch，调用[`is_available()`](../generated/torch.cuda.is_available.html#torch.cuda.is_available)的结果都将是相同的。如果您使用已构建有GPU支持的PyTorch，它将返回True。如果您必须检查正在使用的PyTorch版本，请参考下面的示例：

```py
if torch.cuda.is_available() and torch.version.hip:
    # do something specific for HIP
elif torch.cuda.is_available() and torch.version.cuda:
    # do something specific for CUDA 
```## ROCm上的TensorFloat-32(TF32)

ROCm上不支持TF32。## 内存管理

PyTorch使用缓存内存分配器来加速内存分配。这允许快速的内存释放而无需设备同步。然而，分配器管理的未使用内存仍会显示为在`rocm-smi`中使用。您可以使用[`memory_allocated()`](../generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated)和[`max_memory_allocated()`](../generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated)来监视张量占用的内存，并使用[`memory_reserved()`](../generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved)和[`max_memory_reserved()`](../generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved)来监视缓存分配器管理的总内存量。调用[`empty_cache()`](../generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache)会释放PyTorch中所有**未使用**的缓存内存，以便其他GPU应用程序可以使用。然而，张量占用的GPU内存不会被释放，因此不能增加供PyTorch使用的GPU内存量。

对于更高级的用户，我们通过[`memory_stats()`](../generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats)提供更全面的内存基准测试。我们还提供通过[`memory_snapshot()`](../generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot)捕获内存分配器状态的完整快照的能力，这可以帮助您了解代码产生的底层分配模式。

要调试内存错误，请在环境中设置`PYTORCH_NO_CUDA_MEMORY_CACHING=1`以禁用缓存。## hipFFT/rocFFT计划缓存

不支持设置hipFFT/rocFFT计划的缓存大小。## torch.distributed后端

目前，仅支持“nccl”和“gloo”后端的torch.distributed在ROCm上。## C++中的CUDA API到HIP API映射

请参考：[https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP_API_Guide.html](https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP_API_Guide.html)

注意：CUDA_VERSION 宏、cudaRuntimeGetVersion 和 cudaDriverGetVersion API 的语义映射与 HIP_VERSION 宏、hipRuntimeGetVersion 和 hipDriverGetVersion API 的值不同。在进行版本检查时，请不要混用它们。

例如：不要使用

`#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000` 以隐式排除 ROCm/HIP，

使用以下内容来避免进入 ROCm/HIP 的代码路径：

`#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(USE_ROCM)`

或者，如果希望进入 ROCm/HIP 的代码路径：

`#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || defined(USE_ROCM)`

或者如果只想针对特定的 HIP 版本进入 ROCm/HIP 的代码路径：

`#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || (defined(USE_ROCM) && ROCM_VERSION >= 40300)`

## 参考 CUDA 语义文档[](#refer-to-cuda-semantics-doc "跳转到此标题")

对于此处未列出的任何部分，请参考 CUDA 语义文档：[CUDA 语义](cuda.html#cuda-semantics)

## 启用内核断言

ROCm 支持内核断言，但由于性能开销而被禁用。可以通过重新编译 PyTorch 源代码来启用它。

请将以下行作为参数添加到 cmake 命令参数中：

```py
-DROCM_FORCE_ENABLE_GPU_ASSERTS:BOOL=ON 
```
