- en: torchrec.distributed.sharding¶
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/torchrec/torchrec.distributed.sharding.html](https://pytorch.org/torchrec/torchrec.distributed.sharding.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '## torchrec.distributed.sharding.cw_sharding[¶](#module-torchrec.distributed.sharding.cw_sharding
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseTwEmbeddingSharding`](#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding
    "torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding")[`C`, `F`,
    `T`, `W`]'
  prefs: []
  type: TYPE_NORMAL
- en: Base class for column-wise sharding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseCwEmbeddingSharding`](#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding
    "torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Shards embedding bags column-wise, i.e.. a given embedding table is partitioned
    along its columns and placed on specified ranks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseCwEmbeddingSharding`](#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding
    "torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), [`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]  ## torchrec.distributed.dist_data[¶](#module-torchrec.distributed.dist_data
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Merges the pooled/sequence embedding tensor on each device into single tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which buffer will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cat_dim** (*int*) – which dimension you would like to concatenate on. For
    pooled embedding it is 1; for sequence embedding it is 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoOne operation on pooled/sequence embeddings tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of embedding tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of the merged embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Merges the pooled embedding tensor on each device into single tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which buffer will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoOne operation with Reduce on pooled embeddings tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of embedding tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of the reduced embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes KeyedJaggedTensor to a ProcessGroup according to splits.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation utilizes AlltoAll collective as part of torch.distributed.
  prefs: []
  type: TYPE_NORMAL
- en: The input provides the necessary tensors and input splits to distribute. The
    first collective call in KJTAllToAllSplitsAwaitable will transmit output splits
    (to allocate correct space for tensors) and batch size per rank. The following
    collective calls in KJTAllToAllTensorsAwaitable will transmit the actual tensors
    asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**splits** (*List**[**int**]*) – List of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stagger** (*int*) – stagger value to apply to recat tensor, see _get_recat
    function for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Sends input to relevant ProcessGroup ranks.
  prefs: []
  type: TYPE_NORMAL
- en: The first wait will get the output splits for the provided tensors and issue
    tensors AlltoAll. The second wait will get the tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – KeyedJaggedTensor of values
    to distribute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of a KJTAllToAllTensorsAwaitable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KJTAllToAllTensorsAwaitable](#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`KJTAllToAllTensorsAwaitable`](#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable for KJT tensors splits AlltoAll.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – input KJT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**splits** (*List**[**int**]*) – list of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tensor_splits** (*Dict**[**str**,* *List**[**int**]**]*) – tensor splits
    provided by input KJT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – provided KJT tensors (ie.
    lengths, values) to redistribute according to splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keys** (*List**[**str**]*) – KJT keys after AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which buffers will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stagger** (*int*) – stagger value to apply to recat tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable for KJT tensors AlltoAll.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – input KJT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**splits** (*List**[**int**]*) – list of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_splits** (*List**[**List**[**int**]**]*) – input splits (number of
    values each rank will get) for each tensor in AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_splits** (*List**[**List**[**int**]**]*) – output splits (number of
    values per rank in output) for each tensor in AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – provided KJT tensors (ie.
    lengths, values) to redistribute according to splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (*List**[**str**]*) – labels for each provided tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keys** (*List**[**str**]*) – KJT keys after AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which buffers will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stagger** (*int*) – stagger value to apply to recat tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stride_per_rank** (*Optional**[**List**[**int**]**]*) – stride per rank in
    the non variable batch per feature case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes KeyedJaggedTensor to all devices.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation utilizes OnetoAll function, which essentially P2P copies the
    feature to the devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**splits** (*List**[**int**]*) – lengths of features to split the KeyJaggedTensor
    features into before copying them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – the device on which the KJTs will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Splits features first and then sends the slices to the corresponding devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kjt** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – the input features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of KeyedJaggedTensor splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[List[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The module class that wraps the all-gather communication primitive for pooled
    embedding communication.
  prefs: []
  type: TYPE_NORMAL
- en: Provided a local input tensor with a layout of [batch_size, dimension], we want
    to gather input tensors from all ranks into a flattened output tensor.
  prefs: []
  type: TYPE_NORMAL
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    all-gather is only available for NCCL backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – the process group that the all-gather communication
    happens within.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce scatter operation on pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_emb** (*torch.Tensor*) – tensor of shape [num_buckets x batch_size,
    dimension].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Shards batches and collects keys of tensor with a ProcessGroup according to
    dim_sum_per_rank.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation utilizes alltoall_pooled operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dim_sum_per_rank** (*List**[**int**]*) – number of features (sum of dimensions)
    of the embedding in each rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    – callback functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoAll pooled operation on pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_rank** (*Optional**[**List**[**int**]**]*) – batch size per
    rank, to support variable batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[`Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable for pooled embeddings after collective operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor_awaitable** ([*Awaitable*](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*) – awaitable of concatenated
    tensors from all the processes in the group after collective.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The module class that wraps reduce-scatter communication primitives for pooled
    embedding communication in row-wise and twrw sharding.
  prefs: []
  type: TYPE_NORMAL
- en: For pooled embeddings, we have a local model-parallel output tensor with a layout
    of [num_buckets x batch_size, dimension]. We need to sum over num_buckets dimension
    across batches. We split the tensor along the first dimension into unequal chunks
    (tensor slices of different buckets) according to input_splits and reduce them
    into the output tensor and scatter the results for corresponding ranks.
  prefs: []
  type: TYPE_NORMAL
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    reduce-scatter-v operation is only available for NCCL backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – the process group that the reduce-scatter communication
    happens within.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** – quantized communication codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce scatter operation on pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – tensor of shape [num_buckets * batch_size,
    dimension].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_splits** (*Optional**[**List**[**int**]**]*) – list of splits for local_embs
    dim 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Merges the pooled/sequence embedding tensor on each device into single tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which buffer will be allocated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cat_dim** (*int*) – which dimension you like to concate on. For pooled embedding
    it is 1; for sequence embedding it is 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoOne operation on pooled embeddings tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of pooled embedding tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of the merged pooled embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes sequence embedding to a ProcessGroup according to splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – the process group that the AlltoAll communication
    happens within.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**features_per_rank** (*List**[**int**]*) – list of number of features per
    rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoAll operation on sequence embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – input embeddings tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lengths** (*torch.Tensor*) – lengths of sparse features after AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_splits** (*List**[**int**]*) – input splits of AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_splits** (*List**[**int**]*) – output splits of AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unbucketize_permute_tensor** (*Optional**[**torch.Tensor**]*) – stores the
    permute order of the KJT bucketize (for row-wise sharding only).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_rank** – (Optional[List[int]]): batch size per rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sparse_features_recat** (*Optional**[**torch.Tensor**]*) – recat tensor used
    for sparse feature input dist. Must be provided if using variable batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of sequence embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[SequenceEmbeddingsAwaitable](#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable
    "torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[`Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable for sequence embeddings after collective operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor_awaitable** ([*Awaitable*](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*) – awaitable of concatenated
    tensors from all the processes in the group after collective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unbucketize_permute_tensor** (*Optional**[**torch.Tensor**]*) – stores the
    permute order of KJT bucketize (for row-wise sharding only).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**embedding_dim** (*int*) – embedding dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[`List`[`List`[`int`]]]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable for splits AlltoAll.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – tensor of splits to redistribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Shards batches and collects keys of tensor with a ProcessGroup according to
    dim_sum_per_rank.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation utilizes variable_batch_alltoall_pooled operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**emb_dim_per_rank_per_feature** (*List**[**List**[**int**]**]*) – embedding
    dimensions per rank per feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    – callback functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoAll pooled operation with variable batch size per feature on a
    pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – batch
    size per rank per feature, post a2a. Used to get the input splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_feature_pre_a2a** (*List**[**int**]*) – local batch size before
    scattering, used to get the output splits. Ordered by rank_0 feature, rank_1 feature,
    …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The module class that wraps reduce-scatter communication primitives for pooled
    embedding communication of variable batch in rw and twrw sharding.
  prefs: []
  type: TYPE_NORMAL
- en: For variable batch per feature pooled embeddings, we have a local model-parallel
    output tensor with a 1d layout of the total sum of batch sizes per rank per feature
    multiplied by corresponding embedding dim [batch_size_r0_f0 * emb_dim_f0 + …)].
    We split the tensor into unequal chunks by rank according to batch_size_per_rank_per_feature
    and corresponding embedding_dims and reduce them into the output tensor and scatter
    the results for corresponding ranks.
  prefs: []
  type: TYPE_NORMAL
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    reduce-scatter-v operation is only available for NCCL backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – the process group that the reduce-scatter communication
    happens within.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** – quantized communication codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce scatter operation on pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – tensor of shape [num_buckets * batch_size,
    dimension].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – batch
    size per rank per feature used to determine input splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**embedding_dims** (*List**[**int**]*) – embedding dimensions per feature used
    to determine input splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]  ## torchrec.distributed.sharding.dp_sharding[¶](#module-torchrec.distributed.sharding.dp_sharding
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]'
  prefs: []
  type: TYPE_NORMAL
- en: Base class for data-parallel sharding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Distributes pooled embeddings to be data-parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: No-op as pooled embeddings are already distributed in data-parallel fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – output sequence embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseDpEmbeddingSharding`](#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding
    "torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Shards embedding bags data-parallel, with no table sharding i.e.. a given embedding
    table is replicated across all ranks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  prefs: []
  type: TYPE_NORMAL
- en: Distributes sparse features (input) to be data-parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: No-op as sparse features are already distributed in data-parallel fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**sparse_features** (*SparseFeatures*) – input sparse features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of awaitable of SparseFeatures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[SparseFeatures]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]  ## torchrec.distributed.sharding.rw_sharding[¶](#module-torchrec.distributed.sharding.rw_sharding
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]'
  prefs: []
  type: TYPE_NORMAL
- en: Base class for row-wise sharding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes pooled embedding tensor in RW fashion with an AlltoOne operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which the tensors will be communicated
    to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoOne operation on sequence embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of sequence embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseRwEmbeddingSharding`](#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding
    "torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), [`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes pooled embedding tensor in RW fashion by performing a reduce-scatter
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for reduce-scatter communication.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce-scatter pooled operation on pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – pooled embeddings tensor to distribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sharding_ctx** (*Optional**[*[*EmbeddingShardingContext*](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext")*]*) – shared
    context from KJTAllToAll operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseRwEmbeddingSharding`](#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding
    "torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Shards embedding bags row-wise, i.e.. a given embedding table is evenly distributed
    by rows and table slices are placed on all ranks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  prefs: []
  type: TYPE_NORMAL
- en: Bucketizes sparse features in RW fashion and then redistributes with an AlltoAll
    collective operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**intra_pg** (*dist.ProcessGroup*) – ProcessGroup within single host group
    for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_features** (*int*) – total number of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**feature_hash_sizes** (*List**[**int**]*) – hash sizes of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**is_sequence** (*bool*) – if this is for a sequence embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**has_feature_processor** (*bool*) – existence of feature processor (ie. position
    weighted features).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: Bucketizes sparse feature values into world size number of buckets and then
    performs AlltoAll operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**sparse_features** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – sparse features to bucketize
    and redistribute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of awaitable of KeyedJaggedTensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]  ## torchrec.distributed.sharding.tw_sharding[¶](#module-torchrec.distributed.sharding.tw_sharding
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]'
  prefs: []
  type: TYPE_NORMAL
- en: Base class for table wise sharding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseTwEmbeddingSharding`](#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding
    "torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), [`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Shards embedding bags table-wise for inference
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Merges pooled embedding tensor from each device for inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffer will be
    allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoOne operation on pooled embedding tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*List**[**torch.Tensor**]*) – pooled embedding tensors with
    len(local_embs) == world_size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of merged pooled embedding tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes sparse features to all devices for inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**features_per_rank** (*List**[**int**]*) – number of features to send to each
    rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fused_params** (*Dict**[**str**,* *Any**]*) – fused parameters of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: Performs OnetoAll operation on sparse features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**sparse_features** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – sparse features to redistribute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of awaitable of KeyedJaggedTensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes pooled embedding tensor with an AlltoAll collective operation
    for table wise sharding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dim_sum_per_rank** (*List**[**int**]*) – number of features (sum of dimensions)
    of the embedding in each rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**emb_dim_per_rank_per_feature** (*List**[**List**[**int**]**]*) – embedding
    dimension per rank per feature, used for variable batch per feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qcomm_codecs_registry** (*Optional**[**Dict**[**str**,* [*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]**]*) –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoAll operation on pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sharding_ctx** (*Optional**[*[*EmbeddingShardingContext*](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext")*]*) – shared
    context from KJTAllToAll operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseTwEmbeddingSharding`](#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding
    "torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Shards embedding bags table-wise, i.e.. a given embedding table is entirely
    placed on a selected rank.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes sparse features with an AlltoAll collective operation for table
    wise sharding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**features_per_rank** (*List**[**int**]*) – number of features to send to each
    rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoAll operation on sparse features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**sparse_features** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – sparse features to redistribute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of awaitable of KeyedJaggedTensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]  ## torchrec.distributed.sharding.twcw_sharding[¶](#module-torchrec.distributed.sharding.twcw_sharding
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`CwPooledEmbeddingSharding`](#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding
    "torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Shards embedding bags table-wise column-wise, i.e.. a given embedding table
    is partitioned along its columns and the table slices are placed on all ranks
    within a host group.  ## torchrec.distributed.sharding.twrw_sharding[¶](#module-torchrec.distributed.sharding.twrw_sharding
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]'
  prefs: []
  type: TYPE_NORMAL
- en: Base class for table wise row wise sharding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes pooled embedding tensor in TWRW fashion by performing a reduce-scatter
    operation row wise on the host level and then an AlltoAll operation table wise
    on the global level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**cross_pg** (*dist.ProcessGroup*) – global level ProcessGroup for AlltoAll
    communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**intra_pg** (*dist.ProcessGroup*) – host level ProcessGroup for reduce-scatter
    communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dim_sum_per_node** (*List**[**int**]*) – number of features (sum of dimensions)
    of the embedding for each host.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**emb_dim_per_node_per_feature** (*List**[**List**[**int**]**]*) –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qcomm_codecs_registry** (*Optional**[**Dict**[**str**,* [*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]**]*) –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce-scatter pooled operation on pooled embeddings tensor followed
    by AlltoAll pooled operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – pooled embeddings tensor to distribute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseTwRwEmbeddingSharding`](#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding
    "torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Shards embedding bags table-wise then row-wise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  prefs: []
  type: TYPE_NORMAL
- en: Bucketizes sparse features in TWRW fashion and then redistributes with an AlltoAll
    collective operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**intra_pg** (*dist.ProcessGroup*) – ProcessGroup within single host group
    for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**id_list_features_per_rank** (*List**[**int**]*) – number of id list features
    to send to each rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**id_score_list_features_per_rank** (*List**[**int**]*) – number of id score
    list features to send to each rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**id_list_feature_hash_sizes** (*List**[**int**]*) – hash sizes of id list
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**id_score_list_feature_hash_sizes** (*List**[**int**]*) – hash sizes of id
    score list features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**has_feature_processor** (*bool*) – existence of a feature processor (ie.
    position weighted features).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: Bucketizes sparse feature values into local world size number of buckets, performs
    staggered shuffle on the sparse features, and then performs AlltoAll operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**sparse_features** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – sparse features to bucketize
    and redistribute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of KeyedJaggedTensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
