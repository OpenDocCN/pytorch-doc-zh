# torch.optim

> 原文：[`pytorch.org/docs/stable/optim.html`](https://pytorch.org/docs/stable/optim.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)


`torch.optim`是一个实现各种优化算法的包。

最常用的方法已经得到支持，接口足够通用，以便将来也可以轻松集成更复杂的方法。

## 如何使用优化器

要使用`torch.optim`，您必须构建一个优化器对象，该对象将保存当前状态，并根据计算出的梯度更新参数。

### 构建它

要构建一个`Optimizer`，您必须给它一个包含要优化的参数（都应该是`Variable`）的可迭代对象。然后，您可以指定特定于优化器的选项，如学习率、权重衰减等。

示例：

```py
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001) 
```

### 每个参数的选项

`Optimizer`还支持指定每个参数的选项。为此，不要传递`Variable`的可迭代对象，而是传递一个[`dict`](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)")的可迭代对象。每个字典将定义一个单独的参数组，并应包含一个`params`键，其中包含属于该组的参数列表。其他键应与优化器接受的关键字参数匹配，并将用作该组的优化选项。

注意

您仍然可以将选项作为关键字参数传递。它们将被用作默认值，在未覆盖它们的组中。当您只想要改变单个选项，同时保持所有其他参数组之间一致时，这是很有用的。

例如，当想要指定每层的学习率时，这是非常有用的：

```py
optim.SGD([
                {'params': model.base.parameters()},
                {'params': model.classifier.parameters(), 'lr': 1e-3}
            ], lr=1e-2, momentum=0.9) 
```

这意味着`model.base`的参数将使用学习率的默认值`1e-2`，`model.classifier`的参数将使用学习率`1e-3`，并且所有参数将使用动量`0.9`。

### 执行优化步骤[](#taking-an-optimization-step "Permalink to this heading")

所有优化器都实现了一个`step()`方法，用于更新参数。它可以以两种方式使用：

#### `optimizer.step()`

这是大多数优化器支持的简化版本。一旦使用`backward()`计算出梯度，就可以调用该函数。

示例：

```py
for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step() 
```

#### `optimizer.step(closure)`

一些优化算法，如共轭梯度和 LBFGS，需要多次重新评估函数，因此您必须传递一个闭包，允许它们重新计算您的模型。闭包应清除梯度，计算损失并返回它。

示例：

```py
for input, target in dataset:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss
    optimizer.step(closure) 
```

## 基类

```py
class torch.optim.Optimizer(params, defaults)
```

所有优化器的基类。

警告

需要将参数指定为具有确定性顺序的集合，该顺序在运行之间保持一致。不满足这些属性的对象的示例包括集合和字典值的迭代器。

参数

+   **params**（*可迭代对象*）- 一个包含`torch.Tensor`或[`dict`](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)")的可迭代对象。指定应该优化哪些张量。

+   **defaults**（[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)")*[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)")*,* [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)")*]*) - （字典）：包含优化选项的默认值的字典（当参数组未指定它们时使用）。

| `Optimizer.add_param_group` | 向`Optimizer`的 param_groups 中添加一个参数组。 |
| --- | --- |
| `Optimizer.load_state_dict` | 加载优化器状态。 |
| `Optimizer.state_dict` | 返回优化器状态的[`dict`](https://docs.python.org/3/library/stdtypes.html#dict "(在 Python v3.12 中)")。 |
| `Optimizer.step` | 执行单个优化步骤（参数更新）。 |
| `Optimizer.zero_grad` | 重置所有优化过的`torch.Tensor`的梯度。 |

## 算法

| `Adadelta` | 实现了 Adadelta 算法。 |
| --- | --- |
| `Adagrad` | 实现了 Adagrad 算法。 |
| `Adam` | 实现了 Adam 算法。 |
| `AdamW` | 实现了 AdamW 算法。 |
| `SparseAdam` | SparseAdam 实现了适用于稀疏梯度的掩码版本的 Adam 算法。 |
| `Adamax` | 实现了 Adamax 算法（基于无穷范数的 Adam 变体）。 |
| `ASGD` | 实现了平均随机梯度下降算法。 |
| `LBFGS` | 实现了 L-BFGS 算法。 |
| `NAdam` | 实现了 NAdam 算法。 |
| `RAdam` | 实现了 RAdam 算法。 |
| `RMSprop` | 实现了 RMSprop 算法。 |
| `Rprop` | 实现了弹性反向传播算法。 |
| `SGD` | 实现了随机梯度下降（可选带有动量）。 |

我们的许多算法都有各种实现，针对性能、可读性和/或通用性进行了优化，因此如果用户没有指定特定的实现，我们会尝试默认选择当前设备上通常最快的实现。

我们有 3 个主要类别的实现：for 循环、foreach（多张量）和融合。最直接的实现是对参数进行 for 循环，并进行大量计算。for 循环通常比我们的 foreach 实现慢，后者将参数组合成多个张量，并一次性运行大量计算，从而节省许多顺序内核调用。我们的一些优化器甚至有更快的融合实现，将大量计算融合成一个内核。我们可以将 foreach 实现看作是在水平方向融合，将融合实现看作是在此基础上在垂直方向融合。

一般来说，这 3 种实现的性能排序是融合 > foreach > for-loop。因此，在适用的情况下，我们默认选择 foreach 而不是 for-loop。适用的意思是 foreach 实现可用，用户没有指定任何特定于实现的 kwargs（例如，融合，foreach，可微分），并且所有张量都是本地的且在 CUDA 上。请注意，尽管融合应该比 foreach 更快，但这些实现是较新的，我们希望在完全切换之前给它们更多的时间来烘烤。您可以尝试它们！

下面是一个表格，显示了每种算法的可用和默认实现：

| 算法 | 默认 | 有 foreach？ | 有融合？ |
| --- | --- | --- | --- |
| `Adadelta` | foreach | yes | no |
| `Adagrad` | foreach | yes | no |
| `Adam` | foreach | yes | yes |
| `AdamW` | foreach | yes | yes |
| `SparseAdam` | for-loop | no | no |
| `Adamax` | foreach | yes | no |
| `ASGD` | foreach | yes | no |
| `LBFGS` | for-loop | no | no |
| `NAdam` | foreach | yes | no |
| `RAdam` | foreach | yes | no |
| `RMSprop` | foreach | yes | no |
| `Rprop` | foreach | yes | no |
| `SGD` | foreach | yes | no |

## 如何调整学习率[](#how-to-adjust-learning-rate "Permalink to this heading")

`torch.optim.lr_scheduler` 提供了几种根据周期数调整学习率的方法。`torch.optim.lr_scheduler.ReduceLROnPlateau` 允许根据一些验证测量动态减少学习率。

学习率调度应该在优化器更新之后应用；例如，您应该按照以下方式编写代码：

示例：

```py
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler = ExponentialLR(optimizer, gamma=0.9)

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler.step() 
```

大多数学习率调度器可以被连续调用（也称为链接调度器）。结果是，每个调度器都会在前一个调度器获得的学习率上依次应用。

示例：

```py
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler1.step()
    scheduler2.step() 
```

在文档的许多地方，我们将使用以下模板来引用调度器算法。

```py
>>> scheduler = ...
>>> for epoch in range(100):
>>>     train(...)
>>>     validate(...)
>>>     scheduler.step() 
```

警告

在 PyTorch 1.1.0 之前，预期应在优化器更新之前调用学习率调度器；1.1.0 以一种破坏向后兼容的方式改变了这种行为。如果您在优化器更新之前（调用`scheduler.step()`）使用学习率调度器（调用`optimizer.step()`），这将跳过学习率调度表的第一个值。如果您在升级到 PyTorch 1.1.0 后无法复现结果，请检查是否在错误的时间调用了`scheduler.step()`。

| `lr_scheduler.LambdaLR` | 将每个参数组的学习率设置为初始学习率乘以给定的函数。 |
| --- | --- |
| `lr_scheduler.MultiplicativeLR` | 将每个参数组的学习率乘以指定函数中给定的因子。 |
| `lr_scheduler.StepLR` | 每隔 step_size 个 epoch 通过 gamma 减少每个参数组的学习率。 |
| `lr_scheduler.MultiStepLR` | 当 epoch 数达到里程碑之一时，通过 gamma 减少每个参数组的学习率。 |
| `lr_scheduler.ConstantLR` | 通过一个小的常数因子逐渐减少每个参数组的学习率，直到 epoch 数达到预定义的里程碑：total_iters。 |
| `lr_scheduler.LinearLR` | 通过线性改变小的乘法因子逐渐减少每个参数组的学习率，直到 epoch 数达到预定义的里程碑：total_iters。 |
| `lr_scheduler.ExponentialLR` | 每个 epoch 通过 gamma 减少每个参数组的学习率。 |
| `lr_scheduler.PolynomialLR` | 使用给定的 total_iters 中的多项式函数逐渐减少每个参数组的学习率。 |
| `lr_scheduler.CosineAnnealingLR` | 使用余弦退火调度设置每个参数组的学习率，其中$ \eta_{max} $设置为初始 lr，$ T_{cur} $是自上次 SGDR 重新启动以来的 epoch 数： |
| `lr_scheduler.ChainedScheduler` | 链接学习率调度器列表。 |
| `lr_scheduler.SequentialLR` | 接收预期在优化过程中按顺序调用的调度器列表和提供确切间隔的里程碑点，以反映在给定 epoch 时应调用哪个调度器。 |
| `lr_scheduler.ReduceLROnPlateau` | 当指标停止改善时减少学习率。 |
| `lr_scheduler.CyclicLR` | 根据循环学习率策略（CLR）设置每个参数组的学习率。 |
| `lr_scheduler.OneCycleLR` | 根据 1cycle 学习率策略设置每个参数组的学习率。 |
| `lr_scheduler.CosineAnnealingWarmRestarts` | 使用余弦退火调度设置每个参数组的学习率，其中$\eta_{max}$设置为初始 lr，$T_{cur}$是自上次重启以来的时代数，$T_{i}$是 SGDR 中两次热重启之间的时代数： |

## 权重平均（SWA 和 EMA）[](#weight-averaging-swa-and-ema "跳转到此标题的永久链接")

`torch.optim.swa_utils`实现了随机权重平均（SWA）和指数移动平均（EMA）。特别是，`torch.optim.swa_utils.AveragedModel`类实现了 SWA 和 EMA 模型，`torch.optim.swa_utils.SWALR`实现了 SWA 学习率调度程序，`torch.optim.swa_utils.update_bn()`是一个实用函数，用于在训练结束时更新 SWA/EMA 批归一化统计数据。

SWA 已经在[Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407)中提出。

EMA 是一种广泛知晓的技术，通过减少所需的权重更新次数来减少训练时间。它是[Polyak 平均](https://paperswithcode.com/method/polyak-averaging)的一种变体，但是使用指数权重而不是在迭代中使用相等权重。

### 构建平均模型[](#constructing-averaged-models "跳转到此标题的永久链接")

AveragedModel 类用于计算 SWA 或 EMA 模型的权重。

您可以通过运行以下命令创建一个 SWA 平均模型：

```py
>>> averaged_model = AveragedModel(model) 
```

通过指定`multi_avg_fn`参数来构建 EMA 模型，如下所示：

```py
>>> decay = 0.999
>>> averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay)) 
```

衰减是一个介于 0 和 1 之间的参数，控制平均参数衰减的速度。如果未提供给`get_ema_multi_avg_fn`，则默认值为 0.999。

`get_ema_multi_avg_fn`返回一个函数，该函数将以下 EMA 方程应用于权重：

$W^\textrm{EMA}_{t+1} = \alpha W^\textrm{EMA}_{t} + (1 - \alpha) W^\textrm{model}_t$

其中 alpha 是 EMA 衰减。

在这里，模型`model`可以是任意`torch.nn.Module`对象。`averaged_model`将跟踪`model`的参数的运行平均值。要更新这些平均值，您应该在 optimizer.step()之后使用`update_parameters()`函数：

```py
>>> averaged_model.update_parameters(model) 
```

对于 SWA 和 EMA，这个调用通常在 optimizer `step()`之后立即执行。在 SWA 的情况下，通常在训练开始时跳过一些步骤。

### 自定义平均策略[](#custom-averaging-strategies "跳转到此标题的永久链接")

默认情况下，`torch.optim.swa_utils.AveragedModel`计算您提供的参数的运行平均值，但您也可以使用`avg_fn`或`multi_avg_fn`参数使用自定义平均函数：

+   `avg_fn`允许定义一个操作在每个参数元组（平均参数，模型参数）上，并应返回新的平均参数。

+   `multi_avg_fn`允许定义更高效的操作，同时作用于参数列表的元组（平均参数列表，模型参数列表），例如使用`torch._foreach*`函数。此函数必须原地更新平均参数。

在以下示例中，`ema_model`使用`avg_fn`参数计算指数移动平均值：

```py
>>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\
>>>         0.9 * averaged_model_parameter + 0.1 * model_parameter
>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg) 
```

在以下示例中，`ema_model`使用更高效的`multi_avg_fn`参数计算指数移动平均值：

```py
>>> ema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9)) 
```

### SWA 学习率调度[](#swa-learning-rate-schedules "跳转到此标题的永久链接")

通常，在 SWA 中，学习率设置为一个较高的恒定值。`SWALR`是一个学习率调度程序，它将学习率退火到一个固定值，然后保持恒定。例如，以下代码创建一个调度程序，它在每个参数组内将学习率从初始值线性退火到 0.05，共 5 个时期：

```py
>>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \
>>>         anneal_strategy="linear", anneal_epochs=5, swa_lr=0.05) 
```

您还可以通过设置`anneal_strategy="cos"`来使用余弦退火到固定值，而不是线性退火。

### 处理批归一化[](#taking-care-of-batch-normalization "Permalink to this heading")

`update_bn()`是一个实用函数，允许在训练结束时计算给定数据加载器`loader`上 SWA 模型的批归一化统计信息：

```py
>>> torch.optim.swa_utils.update_bn(loader, swa_model) 
```

`update_bn()`将`swa_model`应用于数据加载器中的每个元素，并计算模型中每个批归一化层的激活统计信息。

警告

`update_bn()`假设数据加载器`loader`中的每个批次都是张量或张量列表，其中第一个元素是应用于网络`swa_model`的张量。如果您的数据加载器具有不同的结构，您可以通过在数据集的每个元素上使用`swa_model`进行前向传递来更新`swa_model`的批归一化统计信息。

### 将所有内容放在一起：SWA[](#putting-it-all-together-swa "Permalink to this heading")

在下面的示例中，`swa_model`是累积权重平均值的 SWA 模型。我们总共训练模型 300 个时期，并切换到 SWA 学习率计划，并开始在第 160 个时期收集参数的 SWA 平均值：

```py
>>> loader, optimizer, model, loss_fn = ...
>>> swa_model = torch.optim.swa_utils.AveragedModel(model)
>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)
>>> swa_start = 160
>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)
>>>
>>> for epoch in range(300):
>>>       for input, target in loader:
>>>           optimizer.zero_grad()
>>>           loss_fn(model(input), target).backward()
>>>           optimizer.step()
>>>       if epoch > swa_start:
>>>           swa_model.update_parameters(model)
>>>           swa_scheduler.step()
>>>       else:
>>>           scheduler.step()
>>>
>>> # Update bn statistics for the swa_model at the end
>>> torch.optim.swa_utils.update_bn(loader, swa_model)
>>> # Use swa_model to make predictions on test data
>>> preds = swa_model(test_input) 
```

### 将所有内容放在一起：EMA[](#putting-it-all-together-ema "Permalink to this heading")

在下面的示例中，`ema_model`是 EMA 模型，它累积权重的指数衰减平均值，衰减率为 0.999。我们总共训练模型 300 个时期，并立即开始收集 EMA 平均值。

```py
>>> loader, optimizer, model, loss_fn = ...
>>> ema_model = torch.optim.swa_utils.AveragedModel(model, \
>>>             multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))
>>>
>>> for epoch in range(300):
>>>       for input, target in loader:
>>>           optimizer.zero_grad()
>>>           loss_fn(model(input), target).backward()
>>>           optimizer.step()
>>>           ema_model.update_parameters(model)
>>>
>>> # Update bn statistics for the ema_model at the end
>>> torch.optim.swa_utils.update_bn(loader, ema_model)
>>> # Use ema_model to make predictions on test data
>>> preds = ema_model(test_input) 
```
