["```py\nimport torch\n\ndef foo1([x1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [x2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n    a = [torch.neg](https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg \"torch.neg\")([x1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    b = [torch.maximum](https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum \"torch.maximum\")([x2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), a)\n    y = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")([b], dim=0)\n    return y\n\n[x1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randint](https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint \"torch.randint\")(256, (1, 8), dtype=[torch.uint8](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n[x2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randint](https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint \"torch.randint\")(256, (8390, 8), dtype=[torch.uint8](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n\ncompiled_foo1 = [torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile \"torch.compile\")(foo1)\n[result](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = compiled_foo1([x1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [x2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) \n```", "```py\ndef neg1(x):\n    return f\"decltype({x})(-{x})\" \n```", "```py\nTORCH_COMPILE_DEBUG=1  python  xx.py \n```", "```py\ntorch._inductor.debug:  [WARNING]  model___20  debug  trace:  /tmp/torchinductor_root/rx/crxfi2ybd7yp5sbj2pnhw33wfhtdw7wumvrobyp5sjvdui5ktjc2.debug \n```", "```py\ndef forward1(self, arg0_1, arg1_1):\n    neg = torch.ops.aten.neg.default(arg0_1);  arg0_1 = None\n    maximum = torch.ops.aten.maximum.default(arg1_1, neg);  arg1_1 = neg = None\n    clone = torch.ops.aten.clone.default(maximum);  maximum = None\n    return (clone,) \n```", "```py\nfrom torch._inductor.codecache import AsyncCompile\nasync_compile = AsyncCompile()\n\ncpp_fused_cat_maximum_neg_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_root/gv/cgv6n5aotqjo5w4vknjibhengeycuattfto532hkxpozszcgxr3x.h\"\nextern \"C\" void kernel(const unsigned char* in_ptr0,\n const unsigned char* in_ptr1,\n unsigned char* out_ptr0)\n{\n {\n #pragma GCC ivdep\n for(long i0=static_cast<long>(0L); i0<static_cast<long>(8390L); i0+=static_cast<long>(1L))\n {\n #pragma GCC ivdep\n for(long i1=static_cast<long>(0L); i1<static_cast<long>(8L); i1+=static_cast<long>(1L))\n {\n auto tmp0 = in_ptr0[static_cast<long>(i1 + (8L*i0))];\n auto tmp1 = in_ptr1[static_cast<long>(i1)];\n // Corresponding FX code line: neg = torch.ops.aten.neg.default(arg0_1);  arg0_1 = None\n auto tmp2 = decltype(tmp1)(-tmp1);\n // Corresponding FX code line: maximum = torch.ops.aten.maximum.default(arg1_1, neg);  arg1_1 = neg = None\n auto tmp3 = max_propagate_nan(tmp0, tmp2);\n // Corresponding FX code line: clone = torch.ops.aten.clone.default(maximum);  maximum = None\n out_ptr0[static_cast<long>(i1 + (8L*i0))] = tmp3;\n }\n }\n }\n}''') \n```", "```py\ntorch.neg  (Python)  ->  torch.ops.aten.neg.default  (within  FX  graph)  ->  ops.neg  (within  IR  node)  ->  tmp2  =  -tmp1  (within  C++  kernel) \n```", "```py\ndef neg2(x):\n    return f\"-{x}\" \n```", "```py\n torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n CppCompileError: C++ compile error\n /tmp/torchinductor_root/xg/cxga5tk3b4lkwoxyigrtocjp5s7vc5cg2ikuscf6bk6pjqip2bhx.cpp: In function \u2018void kernel(const unsigned char*, const unsigned char*, unsigned char*)\u2019:\n /tmp/torchinductor_root/xg/cxga5tk3b4lkwoxyigrtocjp5s7vc5cg2ikuscf6bk6pjqip2bhx.cpp:17:57: error: no matching function for call to \u2018max_propagate_nan(unsigned char&, int&)\u2019\n   17 |                 auto tmp3 = max_propagate_nan(tmp0, tmp2);\n        |                                                         ^\n In file included from /tmp/torchinductor_root/xg/cxga5tk3b4lkwoxyigrtocjp5s7vc5cg2ikuscf6bk6pjqip2bhx.cpp:2:\n /tmp/torchinductor_root/gv/cgv6n5aotqjo5w4vknjibhengeycuattfto532hkxpozszcgxr3x.h:27:17: note: candidate: \u2018template<class scalar_t> scalar_t max_propagate_nan(scalar_t, scalar_t)\u2019\n 27 | inline scalar_t max_propagate_nan(scalar_t a, scalar_t b) {\n      |                 ^~~~~~~~~~~~~~~~~\n /tmp/torchinductor_root/gv/cgv6n5aotqjo5w4vknjibhengeycuattfto532hkxpozszcgxr3x.h:27:17: note:   template argument deduction/substitution failed:\n/tmp/torchinductor_root/xg/cxga5tk3b4lkwoxyigrtocjp5s7vc5cg2ikuscf6bk6pjqip2bhx.cpp:17:57: note:   deduced conflicting types for parameter \u2018scalar_t\u2019 (\u2018unsigned char\u2019 and \u2018int\u2019)\n 17 |                 auto tmp3 = max_propagate_nan(tmp0, tmp2);\n      |                                                         ^ \n```", "```py\ninclude  \"/tmp/torchinductor_root/gv/cgv6n5aotqjo5w4vknjibhengeycuattfto532hkxpozszcgxr3x.h\"\nextern  \"C\"  void  kernel(const  unsigned  char*  in_ptr0,\n  const  unsigned  char*  in_ptr1,\n  unsigned  char*  out_ptr0)\n{\n  {\n  #pragma GCC ivdep\n  for(long  i0=static_cast<long>(0L);  i0<static_cast<long>(8390L);  i0+=static_cast<long>(1L))\n  {\n  #pragma GCC ivdep\n  for(long  i1=static_cast<long>(0L);  i1<static_cast<long>(8L);  i1+=static_cast<long>(1L))\n  {\n  auto  tmp0  =  in_ptr0[static_cast<long>(i1  +  (8L*i0))];\n  auto  tmp1  =  in_ptr1[static_cast<long>(i1)];\n  auto  tmp2  =  -tmp1;\n  auto  tmp3  =  max_propagate_nan(tmp0,  tmp2);\n  out_ptr0[static_cast<long>(i1  +  (8L*i0))]  =  tmp3;\n  }\n  }\n  }\n} \n```", "```py\nbuf0:  SchedulerNode(ComputedBuffer)\nbuf0.writes  =  [MemoryDep('buf0',  c0,  {c0:  67120})]\nbuf0.unmet_dependencies  =  []\nbuf0.met_dependencies  =\n  [  MemoryDep('arg0_1',  c1,  {c0:  8390,  c1:  8}),\n  MemoryDep('arg1_1',  c0,  {c0:  67120})]\nbuf0.users  =  [NodeUser(node=OUTPUT,  can_inplace=False)]\nbuf0.group.device  =  cpu\nbuf0.group.iteration  =  ((8390,  8),  ())\nbuf0.sizes  =  ([8390,  8],  [])\nclass  buf0_loop_body:\n  var_ranges  =  {z0:  8390,  z1:  8}\n  index0  =  8*z0  +  z1\n  index1  =  z1\n  def  body(self,  ops):\n  get_index  =  self.get_index('index0')\n  load  =  ops.load('arg1_1',  get_index)\n  get_index_1  =  self.get_index('index1')\n  load_1  =  ops.load('arg0_1',  get_index_1)\n  neg  =  ops.neg(load_1)\n  maximum  =  ops.maximum(load,  neg)\n  get_index_2  =  self.get_index('index0')\n  store  =  ops.store('buf0',  get_index_2,  maximum,  None)\n  return  store \n```", "```py\nfrom torch._dynamo.utils import same\n\ndef foo2([x1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [x2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n    a = [torch.neg](https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg \"torch.neg\")([x1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    b = [torch.maximum](https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum \"torch.maximum\")([x2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), a)\n    y = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")([b], dim=0)\n    return y\n\n[x1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")((1, 8), dtype=[torch.float32](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n[x2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")((8390, 8), dtype=[torch.float32](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n\n[expected_result](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = foo2([x1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [x2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\ncompiled_foo2 = [torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile \"torch.compile\")(foo2)\n[actual_result](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = compiled_foo2([x1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [x2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\nassert same([expected_result](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [actual_result](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) == True \n```", "```py\ndef neg3(x):\n    return f\"decltype({x})(2 * {x})\" \n```", "```py\ntorch._dynamo.utils:  [ERROR]  Accuracy  failed:  allclose  not  within  tol=0.0001\nTraceback  (most  recent  call  last):\n  File  \"test_script.py\",  line  18,  in  <module>\n  assert  same(expected_result,  actual_result)  ==  True\nAssertionError \n```", "```py\nTORCHDYNAMO_REPRO_AFTER=\"aot\"  TORCHDYNAMO_REPRO_LEVEL=4  python  xx.py \n```", "```py\nStarted  off  with  6  nodes\n\nTrying  granularity  2\nStrategy:  Truncate  suffix  (G:  2)  (6  nodes,  2  inputs)\nSUCCESS:  Went  from  6  to  4  nodes\n\nTrying  granularity  4\nStrategy:  Remove  unused  inputs  (G:  4)  (4  nodes,  2  inputs)\nSUCCESS:  Went  from  4  to  3  nodes \n```", "```py\ndef forward2(self, arg0_1):\n    neg = torch.ops.aten.neg.default(arg0_1);  arg0_1 = None\n    return (neg,) \n```", "```py\nexport  KMP_BLOCKTIME=1\nexport  KMP_SETTINGS=1\nexport  KMP_AFFINITY=granularity=fine,compact,1,0\nexport  LD_PRELOAD=${CONDA_PREFIX:-\"$(dirname  $(which  conda))/../\"}/lib/libiomp5.so:${CONDA_PREFIX:-\"$(dirname  $(which  conda))/../\"}/lib/libjemalloc.so\nexport  MALLOC_CONF=\"oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:-1,muzzy_decay_ms:-1\"\nnumactl  -C  0-31  -m  0  python  bench.py \n```", "```py\n# bench.py\nfrom transformers import MobileBertForQuestionAnswering\n# Initialize an eager model\nmodel = MobileBertForQuestionAnswering.from_pretrained(\"csarron/mobilebert-uncased-squad-v2\")\nseq_length = 128\nbs = 128\nvocab_size = model.config.vocab_size\ninput = [torch.randint](https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint \"torch.randint\")(0, vocab_size, (bs, seq_length), dtype=[torch.int64](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\ninput_dict = {\"input_ids\": input}\n\n# Initialize the inductor model\ncompiled_model = [torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile \"torch.compile\")(model)\nwith [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n    compiled_model(**input_dict)\n\nNUM_ITERS=50\nimport timeit\nwith [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n    # warmup\n    for _ in range(10):\n        model(**input_dict)\n    eager_t = timeit.timeit(\"model(**input_dict)\", number=NUM_ITERS, globals=globals())\n\nwith [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n    # warmup\n    for _ in range(10):\n        compiled_model(**input_dict)\n    inductor_t = timeit.timeit(\"compiled_model(**input_dict)\", number=NUM_ITERS, globals=globals())\n# print(f\"eager use: {eager_t * 1000 / NUM_ITERS} ms/iter\")\n# print(f\"inductor use: {inductor_t * 1000 / NUM_ITERS} ms/iter\")\n# print(f\"speed up ratio: {eager_t / inductor_t}\") \n```", "```py\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning:\n\ntorch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n\nDownloading config.json:   0%|          | 0.00/765 [00:00<?, ?B/s]\nDownloading config.json: 100%|##########| 765/765 [00:00<00:00, 4.90MB/s]\n\nDownloading model.safetensors:   0%|          | 0.00/98.5M [00:00<?, ?B/s]\nDownloading model.safetensors:  32%|###1      | 31.5M/98.5M [00:00<00:00, 293MB/s]\nDownloading model.safetensors:  75%|#######4  | 73.4M/98.5M [00:00<00:00, 342MB/s]\nDownloading model.safetensors: 100%|##########| 98.5M/98.5M [00:00<00:00, 346MB/s] \n```", "```py\neager  use:  802.1023553796113  ms/iter\ninductor  use:  339.95180135127157  ms/iter\nspeed  up  ratio:  2.359459053287382 \n```", "```py\nfrom torch._inductor import config\nconfig.cpp.enable_kernel_profile = True \n```", "```py\n# bench.py\nfrom torch.profiler import [profile](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile \"torch.profiler.profile\"), [schedule](https://pytorch.org/docs/stable/profiler.html#torch.profiler.schedule \"torch.profiler.schedule\"), ProfilerActivity\nRESULT_DIR = \"./prof_trace\"\nmy_schedule = [schedule](https://pytorch.org/docs/stable/profiler.html#torch.profiler.schedule \"torch.profiler.schedule\")(\n    skip_first=10,\n    wait=5,\n    warmup=5,\n    active=1,\n    repeat=5)\n\ndef trace_handler([p](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile \"torch.profiler.profile\")):\n    output = [p.key_averages](https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.key_averages \"torch.profiler._KinetoProfile.key_averages\")().table(sort_by=\"self_cpu_time_total\", row_limit=20)\n    # print(output)\n    [p.export_chrome_trace](https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.export_chrome_trace \"torch.profiler._KinetoProfile.export_chrome_trace\")(f\"{RESULT_DIR}/{[p](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile \"torch.profiler.profile\").step_num}.json\")\n\nfor _ in range(10):\n    model(**input_dict)  # compiled_model(**input_dict) to get inductor model profiling\n\ntotal = 0\nwith [profile](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile \"torch.profiler.profile\")(\n    activities=[[ProfilerActivity.CPU](https://pytorch.org/docs/stable/profiler.html#torch.profiler.ProfilerActivity \"torch._C._profiler.ProfilerActivity\")],\n    [schedule](https://pytorch.org/docs/stable/profiler.html#torch.profiler.schedule \"torch.profiler.schedule\")=my_schedule,\n    on_trace_ready=trace_handler\n) as [p](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile \"torch.profiler.profile\"):\n    for _ in range(50):\n        model(**input_dict)  # compiled_model(**input_dict) to get inductor model profiling\n        [p.step](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile.step \"torch.profiler.profile.step\")() \n```", "```py\n-------------------------  ------------  ------------  ------------\n  Name  CPU  total  %  CPU  total  # of Calls\n-------------------------  ------------  ------------  ------------\n  aten::addmm  45.73%  370.814ms  362\n  aten::add  19.89%  161.276ms  363\n  aten::copy_  14.97%  121.416ms  488\n  aten::mul  9.02%  73.154ms  194\n  aten::clamp_min  8.81%  71.444ms  96\n  aten::bmm  5.46%  44.258ms  48\n  ProfilerStep*  100.00%  810.920ms  1\n  aten::div  2.89%  23.447ms  24\n  aten::_softmax  1.00%  8.087ms  24\n  aten::linear  46.48%  376.888ms  362\n  aten::clone  2.77%  22.430ms  98\n  aten::t  0.31%  2.502ms  362\n  aten::view  0.14%  1.161ms  850\n  aten::transpose  0.17%  1.377ms  386\n  aten::index_select  0.12%  952.000us  3\n  aten::expand  0.12%  986.000us  458\n  aten::matmul  8.31%  67.420ms  48\n  aten::cat  0.09%  703.000us  1\n  aten::as_strided  0.08%  656.000us  963\n  aten::relu  8.86%  71.864ms  96\n-------------------------  ------------  ------------  ------------\nSelf  CPU  time  total:  810.920ms \n```", "```py\n-----------------------------------------------  ------------  ------------  ------------\n  Name  CPU  total  %  CPU  total  # of Calls\n-----------------------------------------------  ------------  ------------  ------------\n  mkl::_mkl_linear  68.79%  231.573ms  362\n  aten::bmm  8.02%  26.992ms  48\n  ProfilerStep*  100.00%  336.642ms  1\n  graph_0_cpp_fused_constant_pad_nd_embedding_0  0.27%  915.000us  1\n  aten::empty  0.27%  911.000us  362\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_151  0.27%  901.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_226  0.27%  899.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_361  0.27%  898.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_121  0.27%  895.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_31  0.27%  893.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_76  0.26%  892.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_256  0.26%  892.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_346  0.26%  892.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_241  0.26%  891.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_316  0.26%  891.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_91  0.26%  890.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_106  0.26%  890.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_211  0.26%  890.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_61  0.26%  889.000us  1\n  graph_0_cpp_fused__mkl_linear_add_mul_relu_286  0.26%  889.000us  1\n-----------------------------------------------  ------------  ------------  ------------\nSelf  CPU  time  total:  336.642ms \n```", "```py\ncpp_fused__mkl_linear_add_mul_relu_151 = async_compile.cpp('''\n#include <ATen/record_function.h>\n#include \"/tmp/torchinductor_root/lr/clrlgu27q4ggd472umdzwsu6qcpqxcuusjxqvx2hwitjbujiiz7z.h\"\nextern \"C\" void kernel(float* in_out_ptr0,\n const float* in_ptr0,\n const float* in_ptr1,\n const float* in_ptr2,\n const float* in_ptr3)\n{\n RECORD_FUNCTION(\"graph_0_cpp_fused__mkl_linear_add_mul_relu_151\", c10::ArrayRef<c10::IValue>({}));\n #pragma omp parallel num_threads(32)\n {\n {\n #pragma omp for\n for(long i0=static_cast<long>(0L); i0<static_cast<long>(16384L); i0+=static_cast<long>(1L))\n {\n for(long i1=static_cast<long>(0L); i1<static_cast<long>(512L); i1+=static_cast<long>(8L))\n {\n auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<long>(i1 + (512L*i0)));\n auto tmp1 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(i1));\n auto tmp3 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + static_cast<long>(i1 + (512L*i0)));\n auto tmp5 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(i1));\n auto tmp7 = at::vec::Vectorized<float>::loadu(in_ptr3 + static_cast<long>(i1));\n auto tmp2 = tmp0 + tmp1;\n auto tmp4 = tmp2 + tmp3;\n auto tmp6 = tmp4 * tmp5;\n auto tmp8 = tmp6 + tmp7;\n tmp8.store(in_out_ptr0 + static_cast<long>(i1 + (512L*i0)));\n }\n }\n }\n }\n}''') \n```", "```py\n# bench.py\ndef func([arg_0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [arg_1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [arg_2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [arg_3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [arg_4](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n    add_0 = [arg_0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") + [arg_1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\n    add_1 = add_0 + [arg_2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\n    mul_1 = add_1 * [arg_3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\n    add_2 = mul_1 + [arg_4](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\n    [arg_2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = add_2\n    return [arg_2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\n\n[arg_0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(16384, 512)\n[arg_1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(1, 512)\n[arg_2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros \"torch.zeros\")(16384, 512)\n[arg_3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(1, 512)\n[arg_4](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(1, 512)\n\ninput = ([arg_0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [arg_1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [arg_2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [arg_3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [arg_4](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\ninductor_func = [torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile \"torch.compile\")(func)\nwith [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n    inductor_func(*input)\n\nimport timeit\nNUM_ITERS=100\nwith [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n    # warmup\n    for _ in range(10):\n        func(*input)\n    eager_t = timeit.timeit(\"func(*input)\", number=NUM_ITERS, globals=globals())\n\nwith [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n    # warmup\n    for _ in range(10):\n        inductor_func(*input)\n    inductor_t = timeit.timeit(\"inductor_func(*input)\", number=NUM_ITERS, globals=globals())\n# print(f\"eager use: {eager_t * 1000 / NUM_ITERS} ms/iter\")\n# print(f\"inductor use: {inductor_t * 1000 / NUM_ITERS} ms/iter\")\n# print(f\"speed up ratio: {eager_t / inductor_t}\") \n```", "```py\neager  use:  5.780875144992024  ms/iter\ninductor  use:  0.9588955780491233  ms/iter\nspeed  up  ratio:  6.0286805751604735 \n```"]