# Tensor Views

> 原文：[https://pytorch.org/docs/stable/tensor_view.html](https://pytorch.org/docs/stable/tensor_view.html)

PyTorch 允许一个张量是现有张量的 `View`。视图张量与其基本张量共享相同的基础数据。支持 `View` 避免了显式数据复制，因此允许我们进行快速和内存高效的重塑、切片和逐元素操作。

例如，要获取现有张量 `t` 的视图，可以调用 `t.view(...)`。

```py
>>> t = torch.rand(4, 4)
>>> b = t.view(2, 8)
>>> t.storage().data_ptr() == b.storage().data_ptr()  # `t` and `b` share the same underlying data.
True
# Modifying view tensor changes base tensor as well.
>>> b[0][0] = 3.14
>>> t[0][0]
tensor(3.14) 
```

由于视图与其基本张量共享基础数据，如果在视图中编辑数据，将会反映在基本张量中。

通常，PyTorch 操作会返回一个新的张量作为输出，例如 [`add()`](generated/torch.Tensor.add.html#torch.Tensor.add "torch.Tensor.add")。但是在视图操作中，输出是输入张量的视图，以避免不必要的数据复制。创建视图时不会发生数据移动，视图张量只是改变了解释相同数据的方式。对连续张量进行视图操作可能会产生非连续张量。用户应额外注意，因为连续性可能会对性能产生隐含影响。[`transpose()`](generated/torch.Tensor.transpose.html#torch.Tensor.transpose "torch.Tensor.transpose") 是一个常见示例。

```py
>>> base = torch.tensor([[0, 1],[2, 3]])
>>> base.is_contiguous()
True
>>> t = base.transpose(0, 1)  # `t` is a view of `base`. No data movement happened here.
# View tensors might be non-contiguous.
>>> t.is_contiguous()
False
# To get a contiguous tensor, call `.contiguous()` to enforce
# copying data when `t` is not contiguous.
>>> c = t.contiguous() 
```

作为参考，以下是 PyTorch 中所有视图操作的完整列表：

+   基本的切片和索引操作，例如 `tensor[0, 2:, 1:7:2]` 返回基本 `tensor` 的视图，请参见下面的说明。

+   [`adjoint()`](generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint "torch.Tensor.adjoint")

+   [`as_strided()`](generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided "torch.Tensor.as_strided")

+   [`detach()`](generated/torch.Tensor.detach.html#torch.Tensor.detach "torch.Tensor.detach")

+   [`diagonal()`](generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal "torch.Tensor.diagonal")

+   [`expand()`](generated/torch.Tensor.expand.html#torch.Tensor.expand "torch.Tensor.expand")

+   [`expand_as()`](generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as "torch.Tensor.expand_as")

+   [`movedim()`](generated/torch.Tensor.movedim.html#torch.Tensor.movedim "torch.Tensor.movedim")

+   [`narrow()`](generated/torch.Tensor.narrow.html#torch.Tensor.narrow "torch.Tensor.narrow")

+   [`permute()`](generated/torch.Tensor.permute.html#torch.Tensor.permute "torch.Tensor.permute")

+   [`select()`](generated/torch.Tensor.select.html#torch.Tensor.select "torch.Tensor.select")

+   [`squeeze()`](generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze "torch.Tensor.squeeze")

+   [`transpose()`](generated/torch.Tensor.transpose.html#torch.Tensor.transpose "torch.Tensor.transpose")

+   [`t()`](generated/torch.Tensor.t.html#torch.Tensor.t "torch.Tensor.t")

+   [`T`](tensors.html#torch.Tensor.T "torch.Tensor.T")

+   [`H`](tensors.html#torch.Tensor.H "torch.Tensor.H")

+   [`mT`](tensors.html#torch.Tensor.mT "torch.Tensor.mT")

+   [`mH`](tensors.html#torch.Tensor.mH "torch.Tensor.mH")

+   [`real`](generated/torch.Tensor.real.html#torch.Tensor.real "torch.Tensor.real")

+   [`imag`](generated/torch.Tensor.imag.html#torch.Tensor.imag "torch.Tensor.imag")

+   `view_as_real()`

+   [`unflatten()`](generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten "torch.Tensor.unflatten")

+   [`unfold()`](generated/torch.Tensor.unfold.html#torch.Tensor.unfold "torch.Tensor.unfold")

+   [`unsqueeze()`](generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze "torch.Tensor.unsqueeze")

+   [`view()`](generated/torch.Tensor.view.html#torch.Tensor.view "torch.Tensor.view")

+   [`view_as()`](generated/torch.Tensor.view_as.html#torch.Tensor.view_as "torch.Tensor.view_as")

+   [`unbind()`](generated/torch.Tensor.unbind.html#torch.Tensor.unbind "torch.Tensor.unbind")

+   [`split()`](generated/torch.Tensor.split.html#torch.Tensor.split "torch.Tensor.split")

+   [`hsplit()`](generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit "torch.Tensor.hsplit")

+   [`vsplit()`](generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit "torch.Tensor.vsplit")

+   [`tensor_split()`](generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split "torch.Tensor.tensor_split")

+   `split_with_sizes()`

+   [`swapaxes()`](generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes "torch.Tensor.swapaxes")

+   [`swapdims()`](generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims "torch.Tensor.swapdims")

+   [`chunk()`](generated/torch.Tensor.chunk.html#torch.Tensor.chunk "torch.Tensor.chunk")

+   [`indices()`](generated/torch.Tensor.indices.html#torch.Tensor.indices "torch.Tensor.indices")（仅适用于稀疏张量）

+   [`values()`](generated/torch.Tensor.values.html#torch.Tensor.values "torch.Tensor.values")（仅适用于稀疏张量）

注意

当通过索引访问张量的内容时，PyTorch遵循Numpy的行为，基本索引返回视图，而高级索引返回副本。通过基本或高级索引进行赋值是原地的。在[Numpy索引文档](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)中查看更多示例。

还值得一提的是一些具有特殊行为的操作：

+   [`reshape()`](generated/torch.Tensor.reshape.html#torch.Tensor.reshape "torch.Tensor.reshape")、[`reshape_as()`](generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as "torch.Tensor.reshape_as")和[`flatten()`](generated/torch.Tensor.flatten.html#torch.Tensor.flatten "torch.Tensor.flatten")可能返回视图或新张量，用户代码不应该依赖于它是视图还是不是。

+   [`contiguous()`](generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous "torch.Tensor.contiguous")如果输入张量已经是连续的，则返回**自身**，否则通过复制数据返回一个新的连续张量。

有关PyTorch内部实现的更详细介绍，请参考[ezyang关于PyTorch内部的博文](http://blog.ezyang.com/2019/05/pytorch-internals/)。
