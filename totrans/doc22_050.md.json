["```py\nclass torch.distributed.optim.DistributedOptimizer(optimizer_class, params_rref, *args, **kwargs)\u00b6\n```", "```py\n>>> import torch.distributed.autograd as dist_autograd\n>>> import torch.distributed.rpc as rpc\n>>> from torch import optim\n>>> from torch.distributed.optim import DistributedOptimizer\n>>>\n>>> with dist_autograd.context() as context_id:\n>>>   # Forward pass.\n>>>   rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>>   rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>>   loss = rref1.to_here() + rref2.to_here()\n>>>\n>>>   # Backward pass.\n>>>   dist_autograd.backward(context_id, [loss.sum()])\n>>>\n>>>   # Optimizer.\n>>>   dist_optim = DistributedOptimizer(\n>>>      optim.SGD,\n>>>      [rref1, rref2],\n>>>      lr=0.05,\n>>>   )\n>>>   dist_optim.step(context_id) \n```", "```py\nstep(context_id)\u00b6\n```", "```py\nclass torch.distributed.optim.PostLocalSGDOptimizer(optim, averager)\u00b6\n```", "```py\n>>> import torch\n>>> import torch.distributed as dist\n>>> import torch.distributed.algorithms.model_averaging.averagers as averagers\n>>> import torch.nn as nn\n>>> from torch.distributed.optim import PostLocalSGDOptimizer\n>>> from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (\n>>>   PostLocalSGDState,\n>>>   post_localSGD_hook,\n>>> )\n>>>\n>>> model = nn.parallel.DistributedDataParallel(\n>>>    module, device_ids=[rank], output_device=rank\n>>> )\n>>>\n>>> # Register a post-localSGD communication hook.\n>>> state = PostLocalSGDState(process_group=None, subgroup=None, start_localSGD_iter=100)\n>>> model.register_comm_hook(state, post_localSGD_hook)\n>>>\n>>> # Create a post-localSGD optimizer that wraps a local optimizer.\n>>> # Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as\n>>> # ``start_localSGD_iter`` used in ``PostLocalSGDState``.\n>>> local_optim = torch.optim.SGD(params=model.parameters(), lr=0.01)\n>>> opt = PostLocalSGDOptimizer(\n>>>     optim=local_optim,\n>>>     averager=averagers.PeriodicModelAverager(period=4, warmup_steps=100)\n>>> )\n>>>\n>>> # In the first 100 steps, DDP runs global gradient averaging at every step.\n>>> # After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),\n>>> # and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.\n>>> for step in range(0, 200):\n>>>    opt.zero_grad()\n>>>    loss = loss_fn(output, labels)\n>>>    loss.backward()\n>>>    opt.step() \n```", "```py\nload_state_dict(state_dict)\u00b6\n```", "```py\nstate_dict()\u00b6\n```", "```py\nstep()\u00b6\n```", "```py\nclass torch.distributed.optim.ZeroRedundancyOptimizer(params, optimizer_class, process_group=None, parameters_as_bucket_view=False, overlap_with_ddp=False, **defaults)\u00b6\n```", "```py\n>>> import torch.nn as nn\n>>> from torch.distributed.optim import ZeroRedundancyOptimizer\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n>>> ddp = DDP(model, device_ids=[rank])\n>>> opt = ZeroRedundancyOptimizer(\n>>>     ddp.parameters(),\n>>>     optimizer_class=torch.optim.Adam,\n>>>     lr=0.01\n>>> )\n>>> ddp(inputs).sum().backward()\n>>> opt.step() \n```", "```py\nadd_param_group(param_group)\u00b6\n```", "```py\nconsolidate_state_dict(to=0)\u00b6\n```", "```py\nproperty join_device: device\u00b6\n```", "```py\njoin_hook(**kwargs)\u00b6\n```", "```py\nproperty join_process_group: Any\u00b6\n```", "```py\nload_state_dict(state_dict)\u00b6\n```", "```py\nstate_dict()\u00b6\n```", "```py\nstep(closure=None, **kwargs)\u00b6\n```"]