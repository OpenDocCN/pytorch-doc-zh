- en: Extending torch.func with autograd.Function
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用autograd.Function扩展torch.func
- en: 原文：[https://pytorch.org/docs/stable/notes/extending.func.html](https://pytorch.org/docs/stable/notes/extending.func.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/notes/extending.func.html](https://pytorch.org/docs/stable/notes/extending.func.html)
- en: So you’d like to use [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") with the [`torch.func`](../func.api.html#module-torch.func
    "torch.func") transforms like [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap"), [`torch.func.grad()`](../generated/torch.func.grad.html#torch.func.grad
    "torch.func.grad"), etc.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您希望使用[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")与[`torch.func`](../func.api.html#module-torch.func "torch.func")转换，如[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")、[`torch.func.grad()`](../generated/torch.func.grad.html#torch.func.grad
    "torch.func.grad")等。
- en: 'There are two main use cases:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种主要用例：
- en: you wish to call code that does not contain PyTorch operations and have it work
    with function transforms. That is, the [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")’s forward/backward/etc calls into functions from other
    systems like C++, CUDA, numpy.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望调用不包含PyTorch操作的代码，并使其与函数转换一起工作。也就是说，[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")的forward/backward等调用其他系统（如C++、CUDA、numpy）的函数。
- en: you wish to specify custom gradient rules, like JAX’s [custom_vjp/custom_jvp](https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望指定自定义梯度规则，类似于JAX的[custom_vjp/custom_jvp](https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html)
- en: PyTorch combines both of these concepts into [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function").
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch将这两个概念结合到[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")中。
- en: Basic Usage
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本用法
- en: This guide assumes you are familiar with [Extending torch.autograd](extending.html#extending-autograd),
    which explains how to use [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function").
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南假定您熟悉[扩展torch.autograd](extending.html#extending-autograd)，该指南解释了如何使用[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")。
- en: '[`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")
    can either have a [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") that accepts a ctx object, or it can have separate
    [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") (that does not accept `ctx`) and a `setup_context()`
    staticmethod that modifies the `ctx` object.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")可以有一个接受ctx对象的[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")，或者可以有单独的不接受`ctx`的[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")和一个修改`ctx`对象的`setup_context()`静态方法。'
- en: 'Only the latter is supported with function transforms:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 只有后者支持函数转换：
- en: '[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") is the code that performs the operation and
    it should not accept a `ctx` object.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")是执行操作的代码，不应接受`ctx`对象。'
- en: '`setup_context(ctx, inputs, output)` is the code where you can call methods
    on `ctx`. Here is where you should save Tensors for backward (by calling `ctx.save_for_backward(*tensors)`),
    or save non-Tensors (by assigning them to the `ctx` object).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setup_context(ctx, inputs, output)`是您可以在其中调用`ctx`上的方法的代码。在这里，您应该保存张量以进行反向传播（通过调用`ctx.save_for_backward(*tensors)`），或保存非张量（通过将它们分配给`ctx`对象）。'
- en: Because `setup_context()` accepts only `inputs` and `output`, the only quantities
    that can be saved are either objects (such as Tensors) in the inputs or outputs
    or quantities (like `Tensor.shape`) derived from them. If you wish to save a non-input
    intermediate activation from [`Function.forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") for backward, then you’ll need to return it
    as an output from [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") so that it gets passed to `setup_context()`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`setup_context()`只接受`inputs`和`output`，所以只能保存输入或输出中的对象（如张量）或从中派生的量（如`Tensor.shape`）。如果您希望保存来自[`Function.forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")的非输入中间激活以进行反向传播，则需要将其作为输出从[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")返回，以便它传递给`setup_context()`。
- en: Depending on the transform,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 根据转换，
- en: to support reverse-mode AD ([`torch.func.grad()`](../generated/torch.func.grad.html#torch.func.grad
    "torch.func.grad"), [`torch.func.vjp()`](../generated/torch.func.vjp.html#torch.func.vjp
    "torch.func.vjp")), the [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") needs a [`backward()`](../generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward") staticmethod.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要支持反向模式自动微分（[`torch.func.grad()`](../generated/torch.func.grad.html#torch.func.grad
    "torch.func.grad")、[`torch.func.vjp()`](../generated/torch.func.vjp.html#torch.func.vjp
    "torch.func.vjp"))，[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")需要一个[`backward()`](../generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward")静态方法。
- en: to support [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap "torch.vmap"),
    the [`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")
    needs a [`vmap()`](../generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap") staticmethod.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要支持[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap "torch.vmap")，[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")需要一个[`vmap()`](../generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap")静态方法。
- en: to support [`torch.func.jvp()`](../generated/torch.func.jvp.html#torch.func.jvp
    "torch.func.jvp"), the [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") needs a [`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") staticmethod.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了支持[`torch.func.jvp()`](../generated/torch.func.jvp.html#torch.func.jvp "torch.func.jvp")，[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")需要一个[`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") staticmethod。
- en: to support compositions of transforms (like [`torch.func.jacrev()`](../generated/torch.func.jacrev.html#torch.func.jacrev
    "torch.func.jacrev"), [`torch.func.jacfwd()`](../generated/torch.func.jacfwd.html#torch.func.jacfwd
    "torch.func.jacfwd"), [`torch.func.hessian()`](../generated/torch.func.hessian.html#torch.func.hessian
    "torch.func.hessian")) – you may need multiple of the above.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持变换的组合（例如[`torch.func.jacrev()`](../generated/torch.func.jacrev.html#torch.func.jacrev
    "torch.func.jacrev")，[`torch.func.jacfwd()`](../generated/torch.func.jacfwd.html#torch.func.jacfwd
    "torch.func.jacfwd")，[`torch.func.hessian()`](../generated/torch.func.hessian.html#torch.func.hessian
    "torch.func.hessian")) - 您可能需要上述多个。
- en: 'In order for the [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") to be arbitrarily composable with function transforms,
    we recommend that all other staticmethods other than [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") and `setup_context()` must be transformable:
    that is, they must consist of only PyTorch operators or call other [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") (that may call into C++/CUDA/etc).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使[`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")能够任意与函数变换组合，我们建议除了[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")和`setup_context()`之外的所有其他staticmethod必须是可转换的：也就是说，它们必须仅由PyTorch操作符组成或调用其他[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")（可能调用C++/CUDA等）。
- en: Let’s go over some examples of common use cases.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些常见用例的例子。
- en: 'Example 1: autograd.Function calls into another system[](#example-1-autograd-function-calls-into-another-system
    "Permalink to this heading")'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例1：autograd.Function调用另一个系统[](#example-1-autograd-function-calls-into-another-system
    "Permalink to this heading")
- en: A common case is a [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") with both forward() and backward() calling into another
    system (like C++, CUDA, numpy, triton).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见情况是一个同时调用另一个系统（如C++，CUDA，numpy，triton）的[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")，同时具有forward()和backward()。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, to make it easier to use `NumpySort` (to hide away the intermediates we
    returned as outputs, as well as allow default args and kwargs), we create a new
    function that invokes it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了更容易使用`NumpySort`（隐藏我们返回的中间结果，以及允许默认参数和关键字参数），我们创建一个调用它的新函数：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And here’s a sanity check:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个健全性检查：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Example 2: autograd.Function specifies custom gradient rules[](#example-2-autograd-function-specifies-custom-gradient-rules
    "Permalink to this heading")'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例2：autograd.Function指定自定义梯度规则[](#example-2-autograd-function-specifies-custom-gradient-rules
    "Permalink to this heading")
- en: 'Another common case is an [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") that is implemented with PyTorch operations. PyTorch
    is able to compute gradients for PyTorch operations automatically, but perhaps
    we wish to customize how the gradients are computed. Some reasons why we may want
    a custom backward different from the one PyTorch gives us are:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见情况是一个使用PyTorch操作实现的[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")。PyTorch能够自动为PyTorch操作计算梯度，但也许我们希望自定义梯度的计算方式。我们可能希望自定义反向传递与PyTorch给出的不同的原因有：
- en: improving numeric stability
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高数值稳定性
- en: changing the performance characteristics of the backward
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改反向传递的性能特征
- en: changing how edge cases are handled (e.g. nans, inf)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改如何处理边缘情况（例如nans，inf）
- en: modifying the gradient (e.g. gradient clipping)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改梯度（例如梯度裁剪）
- en: Here’s an example of an [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") for the function `y = x ** 3` where we change the performance
    characteristics (some computation that would normally happen during the backward
    pass, computing dx, happens in the forward pass).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个关于函数`y = x ** 3`的[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")的示例，我们在其中改变了性能特征（一些在反向传递期间通常会发生的计算，计算dx，现在发生在正向传递中）。
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, to make it easier to use `NumpySort` (and hide away the intermediates
    we returned as outputs) we create a new function that invokes it:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了更容易使用`NumpySort`（并隐藏我们返回的中间结果），我们创建一个调用它的新函数：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here’s a sanity check computing the second-order gradients:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个检查计算二阶梯度的健全性检查：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Limitations and gotchas
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制和注意事项
- en: Warning
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Please read these limitations of [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") with torch.func transforms carefully. We are not able
    to catch many of these situations and error out gracefully so they will lead to
    undefined behavior.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请仔细阅读这些关于[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")的限制与torch.func变换。我们无法捕捉许多这些情况并优雅地报错，因此它们将导致未定义的行为。
- en: Please do not capture Tensors that are being transformed over, have requires_grad=True,
    or are dual tensors, into the methods of the [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function"). The way to be completely safe is to ensure that the
    only Tensors being used inside any method of the [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") must be directly passed as inputs (or via the ctx object)
    rather than come from outside the [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function").
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要在[`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")的方法中捕获正在转换的张量，这些张量具有requires_grad=True或是双重张量。完全安全的方法是确保[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")的任何方法中使用的唯一张量必须直接作为输入（或通过ctx对象）传递，而不是来自[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")外部的。
- en: '[`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")
    does not handle Tensors in pytrees (arbitrary nested Python data structures that
    may or may not contain Tensors). For those Tensors to be tracked by autograd,
    they must be passed directly as an argument to [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function"). This is in contrast to jax.{custom_vjp, custom_jvp},
    which do accept pytrees.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")不处理pytrees中的张量（可能包含或不包含张量的任意嵌套Python数据结构）。为了让这些张量被autograd跟踪，它们必须直接作为参数传递给[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")。这与jax.{custom_vjp, custom_jvp}相反，后者接受pytrees。'
- en: Please only use [`save_for_backward()`](../generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward
    "torch.autograd.function.FunctionCtx.save_for_backward") or `save_for_forward()`
    to save Tensors. Please do not assign Tensors or collections of Tensors directly
    onto the ctx object - these Tensors will not get tracked
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请只使用[`save_for_backward()`](../generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward
    "torch.autograd.function.FunctionCtx.save_for_backward")或`save_for_forward()`来保存张量。请不要直接将张量或张量集合分配到ctx对象上
    - 这些张量将不会被跟踪
- en: '[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap "torch.vmap") Support[](#torch-vmap-support
    "Permalink to this heading")'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap "torch.vmap")支持[](#torch-vmap-support
    "Permalink to this heading")'
- en: 'To use an [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") with [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap"), you must either:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用[`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")与[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")，您必须：
- en: provide a [`vmap()`](../generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap") staticmethod that tells us the behavior of the
    [`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")
    under [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap "torch.vmap")
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供一个[`vmap()`](../generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap")静态方法，告诉我们[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")在[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")下的行为
- en: ask us to autogenerate it by setting `generate_vmap_rule=True`.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置`generate_vmap_rule=True`来要求我们自动生成它。
- en: Automatically generate a vmap rule[](#automatically-generate-a-vmap-rule "Permalink
    to this heading")
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动生成一个vmap规则[](#automatically-generate-a-vmap-rule "Permalink to this heading")
- en: If your [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") fulfills the following additional constraints, then
    we are able to generate a vmap rule for it. If it doesn’t fulfill the constraints
    or if you want custom behavior under vmap, please manually define a vmap staticmethod
    (see next section).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的[`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")满足以下额外约束条件，则我们可以为其生成一个vmap规则。如果不满足约束条件或者希望在vmap下自定义行为，请手动定义一个vmap静态方法（请参见下一节）。
- en: Warning
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: We are not easily able to check for the following constraints and error out
    gracefully. Violation of the constraints may lead to undefined behavior.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法轻松检查以下约束条件并优雅地报错。违反约束条件可能导致未定义的行为。
- en: The [`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")’s
    [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward"), [`backward()`](../generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward") (if it exists) and [`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") (if it exists) staticmethods must be transformable
    via [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap "torch.vmap"). That
    is, they must consist of only PyTorch operations (as opposed to e.g. NumPy or
    custom CUDA kernels).
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")的[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")、[`backward()`](../generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward")（如果存在）和[`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp")（如果存在）静态方法必须通过[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")进行转换。也就是说，它们必须仅包含PyTorch操作（而不是例如NumPy或自定义CUDA内核）。'
- en: 'Example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Defining the vmap staticmethod[](#defining-the-vmap-staticmethod "Permalink
    to this heading")
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义vmap静态方法[](#defining-the-vmap-staticmethod "Permalink to this heading")
- en: If your [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") calls into another system (like NumPy, C++, CUDA, triton),
    then to get it to work with [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap") or transforms that use it, you’ll need to manually define a [`vmap()`](../generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap") staticmethod.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的[`torch.autograd.Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")调用另一个系统（如NumPy、C++、CUDA、triton），那么为了使其与[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")或使用它的转换一起工作，您需要手动定义一个[`vmap()`](../generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap")静态方法。
- en: 'Depending on what transforms you want to use and your use case, you may not
    need to add a [`vmap()`](../generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap") staticmethod to all of your [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function"):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您想要使用的转换和用例，您可能不需要为所有的[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")添加一个[`vmap()`](../generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap")静态方法：
- en: For example, [`torch.func.jacrev()`](../generated/torch.func.jacrev.html#torch.func.jacrev
    "torch.func.jacrev") performs [`vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap") over the backward pass. So if you’re only interested in using [`torch.func.jacrev()`](../generated/torch.func.jacrev.html#torch.func.jacrev
    "torch.func.jacrev"), only the [`backward()`](../generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward") staticmethod needs to be vmappable.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，[`torch.func.jacrev()`](../generated/torch.func.jacrev.html#torch.func.jacrev
    "torch.func.jacrev")在反向传播中执行[`vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")。因此，如果您只对使用[`torch.func.jacrev()`](../generated/torch.func.jacrev.html#torch.func.jacrev
    "torch.func.jacrev")感兴趣，则只需要将[`backward()`](../generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward")静态方法设置为可vmapped。
- en: We do recommend ensuring all of your [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") have support for [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap") though, especially if you are writing a third-party library and
    you want your [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") to work with all combinations of [`torch.func()`](../func.api.html#module-torch.func
    "torch.func") transforms.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议确保所有的[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")都支持[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")，尤其是如果您正在编写第三方库，并且希望您的[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")能够与所有组合的[`torch.func()`](../func.api.html#module-torch.func
    "torch.func")转换一起使用。
- en: 'Conceptually, the vmap staticmethod is responsible for defining how the [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") should behave under [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap"). That is, it defines how to transform the [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") to run over inputs with an additional dimension
    (the dimension being vmapped over). This is similar to how [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap") is implemented over PyTorch operations: for each operation, we define
    a vmap rule (sometimes also referred to as a “batching rule”).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，vmap静态方法负责定义[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")在[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")下应该如何行为。也就是说，它定义了如何将[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")转换为在具有额外维度（正在被vmapped覆盖的维度）的输入上运行。这类似于PyTorch操作上实现[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")的方式：对于每个操作，我们定义一个vmap规则（有时也称为“批处理规则”）。
- en: 'Here’s how to define the [`vmap()`](../generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap") staticmethod:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何定义[`vmap()`](../generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap")静态方法的：
- en: 'the signature is `vmap(info, in_dims: Tuple[Optional[int]], *args)`, where
    `*args` is the same as the args to [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward").'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '签名是`vmap(info, in_dims: Tuple[Optional[int]], *args)`，其中`*args`与[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")的参数相同。'
- en: The vmap staticmethod is responsible for defining how the [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") should behave under [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap"). That is, given inputs with an additional dimension (specified by
    `in_dims`), how do we compute the batched version of [`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")?
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vmap静态方法负责定义[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")在[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")下应该如何行为。也就是说，给定具有额外维度（由`in_dims`指定）的输入，我们如何计算[`forward()`](../generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")的批处理版本？
- en: For each arg in `args`, `in_dims` has a corresponding `Optional[int]`. It is
    `None` if the arg is not a Tensor or if the arg is not being vmapped over, otherwise,
    it is an integer specifying what dimension of the Tensor is being vmapped over.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`args`中的每个参数，`in_dims`都有一个相应的`Optional[int]`。如果参数不是张量或参数不是被vmapped覆盖的，则为`None`，否则，它是一个整数，指定正在被vmapped覆盖的张量的维度。
- en: '`info` is a collection of additional metadata that may be helpful: `info.batch_size`
    specifies the size of the dimension being vmapped over, while `info.randomness`
    is the `randomness` option that was passed to [`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap").'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info`是一组额外的元数据，可能会有所帮助：`info.batch_size`指定了正在进行vmapped的维度的大小，而`info.randomness`是传递给[`torch.vmap()`](../generated/torch.vmap.html#torch.vmap
    "torch.vmap")的`randomness`选项。'
- en: The return of the vmap staticmethod is a tuple of `(output, out_dims)`. Similar
    to `in_dims`, `out_dims` should be of the same structure as `output` and contain
    one `out_dim` per output that specifies if the output has the vmapped dimension
    and what index it is in.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vmap静态方法的返回值是一个元组`(output, out_dims)`。与`in_dims`类似，`out_dims`应该与`output`的结构相同，并且包含一个`out_dim`，指定输出是否具有vmapped维度以及其索引。
- en: 'Example:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The vmap staticmethod should aim to preserve the semantics of the entire [`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function"). That is, (pseudocode) `grad(vmap(MyFunc))` should
    be replaceable with a `grad(map(MyFunc))`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: vmap静态方法应该旨在保留整个[`Function`](../autograd.html#torch.autograd.Function "torch.autograd.Function")的语义。也就是说，（伪代码）`grad(vmap(MyFunc))`应该可以替换为`grad(map(MyFunc))`。
- en: If your autograd.Function has any custom behavior in the backward pass, please
    keep this in mind.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的autograd.Function在反向传播中具有任何自定义行为，请记住这一点。
- en: Note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is a legitimate use case to write a custom vmap staticmethod for a [`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") that PyTorch is able to generate a vmap rule for via
    `generate_vmap_rule=True`. You may wish to do this if the generated vmap rule
    doesn’t have the semantics you’re looking for.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为一个PyTorch能够通过`generate_vmap_rule=True`生成vmap规则的[`Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")编写自定义vmap静态方法是一个合法的用例。如果生成的vmap规则不符合您的预期语义，您可能希望这样做。
- en: '[`torch.func.jvp()`](../generated/torch.func.jvp.html#torch.func.jvp "torch.func.jvp")
    Support'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[`torch.func.jvp()`](../generated/torch.func.jvp.html#torch.func.jvp "torch.func.jvp")
    支持'
- en: To support forward-mode AD, a [`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function") must have a [`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") staticmethod. Please see [Forward mode AD](extending.html#forward-ad-autograd-function)
    for details.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持正向模式自动微分，一个[`torch.autograd.Function`](../autograd.html#torch.autograd.Function
    "torch.autograd.Function")必须有一个[`jvp()`](../generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp")静态方法。请参阅[正向模式自动微分](extending.html#forward-ad-autograd-function)获取详细信息。
