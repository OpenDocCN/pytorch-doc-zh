# DDP通信钩子

> 原文：[https://pytorch.org/docs/stable/ddp_comm_hooks.html](https://pytorch.org/docs/stable/ddp_comm_hooks.html)

DDP通信钩子是一个通用接口，通过覆盖[DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.)中的基本allreduce来控制如何在工作进程之间通信梯度。提供了一些内置的通信钩子，用户可以轻松应用其中任何一个来优化通信。此外，该钩子接口还可以支持用户定义的通信策略，以满足更高级的用例需求。

## 如何使用通信钩子？[](#how-to-use-a-communication-hook "跳转到此标题的永久链接")

要使用通信钩子，用户只需在训练循环之前让DDP模型注册钩子，如下所示。

[`torch.nn.parallel.DistributedDataParallel.register_comm_hook()`](generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.register_comm_hook "torch.nn.parallel.DistributedDataParallel.register_comm_hook")

## 通信钩子操作的是什么？[](#what-does-a-communication-hook-operate-on "跳转到此标题的永久链接")

通信钩子提供了一种灵活的方式来allreduce梯度。因此，它主要在每个副本上的梯度上操作，然后进行allreduce，这些梯度被分桶以增加通信和计算之间的重叠。特别地，[`torch.distributed.GradBucket`](#torch.distributed.GradBucket "torch.distributed.GradBucket")表示要进行allreduce的梯度张量的一个桶。

```py
class torch.distributed.GradBucket¶
```

这个类主要将一个扁平化的梯度张量（由[`buffer()`](#torch.distributed.GradBucket.buffer "torch.distributed.GradBucket.buffer")返回）传递给DDP通信钩子。这个张量可以进一步分解为此桶中每个参数张量的列表（由`get_per_parameter_tensors()`返回），以应用逐层操作。

```py
torch.distributed.GradBucket.index(self: torch._C._distributed_c10d.GradBucket) → int¶
```

警告

由于桶在第一次迭代后被重建，因此不应依赖于训练开始时的索引。

返回值

存储少数连续层梯度的桶的索引。所有梯度都被分桶。

```py
torch.distributed.GradBucket.buffer(self: torch._C._distributed_c10d.GradBucket) → torch.Tensor¶
```

返回值

一个扁平化的1D `torch.Tensor` 缓冲区，可以进一步分解为此桶中每个参数张量的列表。

```py
torch.distributed.GradBucket.gradients(self: torch._C._distributed_c10d.GradBucket) → List[torch.Tensor]¶
```

返回值

一个`torch.Tensor`列表。列表中的每个张量对应一个梯度。

```py
torch.distributed.GradBucket.is_last(self: torch._C._distributed_c10d.GradBucket) → bool¶
```

返回值

这个桶是否是迭代中最后一个要进行allreduce的桶。这也意味着这个桶对应于前向传播中的前几层。

```py
torch.distributed.GradBucket.set_buffer(self: torch._C._distributed_c10d.GradBucket, buffer: torch.Tensor) → None¶
```

用输入张量缓冲区替换桶中的张量。

```py
torch.distributed.GradBucket.parameters(self: torch._C._distributed_c10d.GradBucket) → List[torch.Tensor]¶
```

返回值

一个`torch.Tensor`列表。列表中的每个张量对应一个模型参数。

## 默认通信钩子[](#default-communication-hooks "跳转到此标题的永久链接")

默认通信钩子是简单的**无状态**钩子，因此`register_comm_hook`中的输入状态要么是一个进程组，要么是`None`。输入`bucket`是一个[`torch.distributed.GradBucket`](#torch.distributed.GradBucket "torch.distributed.GradBucket")对象。

```py
torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook(process_group, bucket)¶
```

这个DDP通信钩子只是使用`GradBucket`张量调用`allreduce`。一旦梯度张量在所有工作进程中聚合，它的`then`回调会取平均值并返回结果。如果用户注册了这个钩子，DDP的结果预计与未注册钩子的情况相同。因此，这不会改变DDP的行为，用户可以将其用作参考或修改此钩子以记录有用信息或其他目的，同时不影响DDP的行为。

示例::

```py
>>> ddp_model.register_comm_hook(process_group, allreduce_hook) 
```

返回类型

[*Future*](futures.html#torch.futures.Future "torch.jit.Future")[[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")]

```py
torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook(process_group, bucket)¶
```

这个DDP通信钩子实现了一种简单的梯度压缩方法，将`GradBucket`张量转换为半精度浮点格式（`torch.float16`），然后将其除以进程组大小。它对这些`float16`梯度张量进行全局归约。一旦压缩的梯度张量全部归约，链式回调`decompress`将其转换回输入数据类型（如`float32`）。

示例::

```py
>>> ddp_model.register_comm_hook(process_group, fp16_compress_hook) 
```

返回类型

[*Future*](futures.html#torch.futures.Future "torch.jit.Future")[[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")]

```py
torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook(process_group, bucket)¶
```

警告：此API是实验性的，需要NCCL版本高于2.9.6。

这个DDP通信钩子实现了一种简单的梯度压缩方法，将`GradBucket`张量转换为半精度[Brain浮点格式](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format) (`torch.bfloat16`)，然后将其除以进程组大小。它对这些`bfloat16`梯度张量进行全局归约。一旦压缩的梯度张量全部归约，链式回调`decompress`将其转换回输入数据类型（如`float32`）。

示例::

```py
>>> ddp_model.register_comm_hook(process_group, bf16_compress_hook) 
```

返回类型

[*Future*](futures.html#torch.futures.Future "torch.jit.Future")[[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")]

此外，提供了一个通信钩子包装器，支持[`fp16_compress_hook()`](#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook")或[`bf16_compress_hook()`](#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook")作为一个包装器，可以与其他通信钩子组合使用。

```py
torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper(hook)¶
```

这个包装器将给定DDP通信钩子的输入梯度张量转换为半精度浮点格式（`torch.float16`），并将给定钩子的结果张量转换回输入数据类型，如`float32`。

因此，`fp16_compress_hook`等同于`fp16_compress_wrapper(allreduce_hook)`。

示例::

```py
>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)
>>> ddp_model.register_comm_hook(state, fp16_compress_wrapper(powerSGD_hook)) 
```

返回类型

[*Callable*](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)")[[[*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"), [*GradBucket*](#torch.distributed.GradBucket "torch._C._distributed_c10d.GradBucket")], [*Future*](futures.html#torch.futures.Future "torch.jit.Future")[[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")]]

```py
torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper(hook)¶
```

警告：此API是实验性的，需要NCCL版本高于2.9.6。

这个包装器将给定DDP通信钩子的输入梯度张量转换为半精度Brain浮点格式<https://en.wikipedia.org/wiki/Bfloat16_floating-point_format> `_（`torch.bfloat16`），并将给定钩子的结果张量转换回输入数据类型，如`float32`。

因此，`bf16_compress_hook`等同于`bf16_compress_wrapper(allreduce_hook)`。

示例::

```py
>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)
>>> ddp_model.register_comm_hook(state, bf16_compress_wrapper(powerSGD_hook)) 
```

返回类型

[*Callable*](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)")[[[*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"), [*GradBucket*](#torch.distributed.GradBucket "torch._C._distributed_c10d.GradBucket")], [*Future*](futures.html#torch.futures.Future "torch.jit.Future")[[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")]]

## PowerSGD通信钩子[](#powersgd-communication-hook "跳转到此标题")

PowerSGD（[Vogels等人，NeurIPS 2019](https://arxiv.org/abs/1905.13727)）是一种梯度压缩算法，可以提供非常高的压缩率，并加速带宽受限的分布式训练。该算法需要维护一些超参数和内部状态。因此，PowerSGD通信钩子是一个**有状态**的钩子，用户需要提供以下定义的状态对象。

### PowerSGD状态

```py
class torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState(process_group, matrix_approximation_rank=1, start_powerSGD_iter=1000, min_compression_rate=2, use_error_feedback=True, warm_start=True, orthogonalization_epsilon=0, random_seed=0, compression_stats_logging_frequency=10000, batch_tensors_with_same_shape=False)¶
```

在训练期间存储算法的超参数和所有梯度的内部状态。特别是，`matrix_approximation_rank` 和 `start_powerSGD_iter` 是用户应该调整的主要超参数。为了性能，建议保持二进制超参数 `use_error_feedback` 和 `warm_start` 打开。

1.  `matrix_approximation_rank` 控制压缩的低秩张量的大小，从而确定压缩率。低秩越低，压缩越强。

    > 1.1\. 如果 `matrix_approximation_rank` 太低，完整模型质量将需要更多的训练步骤才能达到，或者永远无法达到，并且会导致准确性下降。
    > 
    > 1.2\. 增加 `matrix_approximation_rank` 可大幅增加压缩的计算成本，而准确性可能不会在某个特定的 `matrix_approximation_rank` 阈值之上进一步提高。

要调整 `matrix_approximation_rank`，建议从 1 开始，按 2 的倍数递增（如指数网格搜索，1、2、4、...），直到达到满意的准确性。通常只使用一个小值 1-4。对于一些 NLP 任务（如原始论文附录 D 中所示），这个值已增加到 32。

1.  `start_powerSGD_iter` 推迟 PowerSGD 压缩直到步骤 `start_powerSGD_iter`，并且在步骤 `start_powerSGD_iter` 之前运行普通的 allreduce。这种 **普通 allreduce + PowerSGD** 的混合方案可以有效提高准确性，即使使用相对较小的 `matrix_approximation_rank`。这是因为训练阶段的开始通常对不准确的梯度非常敏感，而过早压缩梯度可能会使训练迅速走向次优轨迹，这可能会对准确性产生不可挽回的影响。

要调整 `start_powerSGD_iter`，建议从总训练步骤的 10% 开始，并逐渐增加，直到达到满意的准确性。如果训练中有热身阶段，则 `start_powerSGD_iter` 通常不应少于热身步数。

1.  `min_compression_rate` 是在压缩层时所需的最小压缩率。由于压缩带来的计算开销，只有当在带宽上可以节省足够的内容时，张量才值得压缩，其中 `(num_rows + num_cols) * matrix_approximation_rank * min_compression_rate < num_rows * num_cols`。如果无法满足指定的压缩率阈值，则张量将直接进行无压缩的 allreduce。

一旦开始 PowerSGD 压缩，每隔 `compression_stats_logging_frequency` 次迭代记录一次压缩统计信息。

1.  `orthogonalization_epsilon` 可以是一个非常小的值（例如，1e-8），添加到正交化步骤中每个归一化矩阵列中，以防止除零错误，如果任何列都是全 0。如果这已经可以被防止（例如，通过批量归一化），则建议将 epsilon 设置为 0 以提高准确性。

1.  `batch_tensors_with_same_shape` 控制是否在批处理操作中压缩和解压具有相同形状的张量，以实现更高的并行性。请注意，您还应增加桶大小（即 DDP 构造函数中的 `bucket_cap_mb` 参数），以使更多具有相同形状的张量出现在同一个桶中，但这可能会减少计算和通信之间的重叠，并增加内存占用量，因为需要堆叠相同形状的张量。如果压缩/解压计算是瓶颈，请将其设置为 `True`。

警告

如果启用了误差反馈或热身阶段，DDP 中允许的 `start_powerSGD_iter` 的最小值为 2。这是因为在 DDP 中的第 1 次迭代中重新构建桶的另一个内部优化，这可能会与重建过程之前记忆的任何张量发生冲突。

### PowerSGD 钩子

警告

PowerSGD 通常需要额外的内存，大小与模型梯度相同，以启用误差反馈，这可以补偿有偏压缩通信并提高准确性。

警告

PowerSGD钩子可能与[Apex自动混合精度包](https://github.com/NVIDIA/apex)冲突。请改用PyTorch的[本机自动混合精度包](https://pytorch.org/docs/stable/amp.html)。

```py
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook(state, bucket)¶
```

这个DDP通信钩子实现了[论文](https://arxiv.org/abs/1905.13727)中描述的PowerSGD梯度压缩算法。一旦梯度张量在所有工作节点上聚合，此钩子将按以下方式应用压缩：

1.  将输入扁平化的1D梯度张量视为每个参数张量的列表，并将所有张量分为两组：

    > 1.1 应在allreduce之前压缩的张量，因为压缩可以在带宽上节省足够的空间。
    > 
    > 1.2 其余张量将直接进行allreduce而不进行压缩，包括所有的向量张量（用于偏置）。

1.  处理未压缩的张量：

    > 2.1\. 为这些未压缩的张量分配连续内存，并将所有未压缩的张量作为一个批次进行allreduce，不进行压缩；
    > 
    > 2.2\. 将单个未压缩的张量从连续内存复制回输入张量。

1.  处理应通过PowerSGD压缩进行压缩的张量：

    > 3.1\. 对于每个张量M，创建两个低秩张量P和Q来分解M，使得M = PQ^T，其中Q从标准正态分布初始化并正交化；
    > 
    > 3.2\. 计算Ps中的每个P，等于MQ；
    > 
    > 3.3\. 将Ps作为一个批次进行allreduce；
    > 
    > 3.4\. 对Ps中的每个P进行正交化；
    > 
    > 3.5\. 计算Qs中的每个Q，大致等于M^TP；
    > 
    > 3.6\. 所有的Q作为一个批次进行allreduce；
    > 
    > 3.7\. 计算所有压缩张量中的每个M，大致等于PQ^T。

请注意，此通信钩子在前`state.start_powerSGD_iter`次迭代中强制使用普通allreduce。这不仅使用户能够更好地控制速度和准确性之间的权衡，还有助于将DDP的内部优化复杂性抽象化为未来通信钩子开发者。

参数

+   **state**（[*PowerSGDState*](#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState")）- 用于配置压缩率和支持错误反馈、热启动等的状态信息。要调整压缩配置，主要需要调整`matrix_approximation_rank`、`start_powerSGD_iter`和`min_compression_rate`。

+   **bucket**（*dist.GradBucket*）- 存储批处理多个每个变量张量的扁平化梯度张量的桶。请注意，由于DDP通信钩子仅支持单进程单设备模式，因此此桶中仅存储一个张量。

返回

未来处理通信的处理程序，可以就地更新梯度。

返回类型

[*未来*](futures.html#torch.futures.Future "torch.jit.Future")[[*张量*](tensors.html#torch.Tensor "torch.Tensor")]

示例::

```py
>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1,
 start_powerSGD_iter=10, min_compression_rate=0.5)
>>> ddp_model.register_comm_hook(state, powerSGD_hook) 
```

```py
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook(state, bucket)¶
```

这个DDP通信钩子实现了一个简化的PowerSGD梯度压缩算法，描述在[论文](https://arxiv.org/abs/1905.13727)中。这个变体不是逐层压缩梯度，而是压缩批处理所有梯度的扁平输入张量。因此，它比[`powerSGD_hook()`](#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook")**更快**，但通常会导致**更低的准确性**，除非`matrix_approximation_rank`为1。

警告

在这里增加`matrix_approximation_rank`可能不一定会增加准确性，因为对于没有列/行对齐的每个参数张量进行批处理可能会破坏低秩结构。因此，用户应始终首先考虑[`powerSGD_hook()`](#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook")，仅在`matrix_approximation_rank`为1时可以实现令人满意的准确性时才考虑此变体。

一旦梯度张量在所有工作进程中聚合，此挂钩将应用压缩如下：

1.  将输入扁平化的1D梯度张量视为带有0填充的方形张量M;

1.  创建两个低秩张量P和Q以分解M，使得M = PQ^T，其中Q从标准正态分布初始化并正交化;

1.  计算P，它等于MQ；

1.  全局归约P;

1.  正交化P;

1.  计算Q，它大约等于M^TP;

1.  全局归约Q;

1.  计算M，它大约等于PQ^T。

1.  将输入张量截断到原始长度。

请注意，此通信挂钩在前`state.start_powerSGD_iter`次迭代中强制执行基本全局归约。这不仅使用户能够更好地控制速度和准确性之间的权衡，还有助于为未来通信挂钩开发人员抽象出DDP内部优化的一些复杂性。

参数

+   **state** ([*PowerSGDState*](#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState")) – 用于配置压缩率和支持错误反馈、热启动等的状态信息。要调整压缩配置，主要需要调整`matrix_approximation_rank`和`start_powerSGD_iter`。

+   **bucket** (*dist.GradBucket*) – 存储批处理多个每个变量张量的扁平化梯度张量的桶。请注意，由于DDP通信挂钩仅支持单进程单设备模式，因此此桶中仅存储一个张量。

返回

通信的未来处理程序，它在原地更新梯度。

返回类型

[*Future*](futures.html#torch.futures.Future "torch.jit.Future")[[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")]

示例::

```py
>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1)
>>> ddp_model.register_comm_hook(state, batched_powerSGD_hook) 
```

## 调试通信挂钩[](#debugging-communication-hooks "跳转到此标题")

顾名思义，调试通信挂钩仅用于调试和性能优化目的。

警告

调试通信挂钩不一定会输出正确的结果。

```py
torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook(_, bucket)¶
```

此DDP通信挂钩返回一个包装输入的未来，因此它是一个不会产生任何通信开销的空操作。

此挂钩应**仅**用于全局归约优化的headroom分析，而不是正常的梯度同步。例如，如果在注册此挂钩后只能观察到训练时间少于10%的加速，通常意味着对于这种情况，全局归约不是性能瓶颈。如果GPU跟踪不能轻松检索或跟踪分析受到某些因素的复杂影响，例如全局归约和计算之间的重叠或跨等级的不同步，这种仪器化可能特别有用。

示例::

```py
>>> ddp_model.register_comm_hook(None, noop_hook) 
```

返回类型

[*Future*](futures.html#torch.futures.Future "torch.jit.Future")[[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")]

## 通信挂钩的检查点[](#checkpointing-of-communication-hooks "跳转到此标题")

作为模型检查点的一部分，可以将有状态的通信挂钩保存以启用训练器重新启动。要使挂钩可序列化，应定义`__setstate__`和`__getstate__`。

警告

`__getstate__`应从返回的字典中排除非可序列化属性。

警告

`__setstate__`应正确初始化非可序列化属性，从提供的`state`中排除。

[`PowerSGDState`](#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState")已实现`__setstate__`和`__getstate__`，可用作参考。

```py
class torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState
```

```py
__getstate__()¶
```

返回一个`Dict[str, Any]`，将被pickle化并保存。`process_group`不可序列化并从返回的状态中排除。

```py
__setstate__(state)¶
```

接受一个提供的`state`并检索`PowerSGDState`。`process_group`设置为默认值。

这里是一个简单的端到端示例，演示了如何保存和重新加载PowerSGD状态和钩子。

```py
import os
import sys
import tempfile
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp

from torch.nn.parallel import DistributedDataParallel
from torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook as powerSGD

class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(24,24)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(24,12)

    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # initialize the process group
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

def run_demo(demo_fn, world_size):
    mp.spawn(
        demo_fn,
        args=(world_size,),
        nprocs=world_size,
        join=True)

def demo_serialization(rank, world_size):
    setup(rank, world_size)

    CHECKPOINT = tempfile.gettempdir() + "/checkpoint.pt"

    model = SimpleModel().to(rank)
    ddp_model = DistributedDataParallel(model, device_ids=[rank])

    powersgd_hook = powerSGD.powerSGD_hook
    powersgd_state = powerSGD.PowerSGDState(process_group=None)

    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)
    ddp_model.register_comm_hook(powersgd_state, powersgd_hook)

    state = {
        'state_dict': ddp_model.state_dict(),
        'comm_hook': powersgd_hook,
        'comm_hook_state': powersgd_state}

    if rank == 0:
        torch.save(state, CHECKPOINT)

    dist.barrier()
    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
    checkpoint = torch.load(CHECKPOINT, map_location=map_location)

    new_ddp_model = DistributedDataParallel(SimpleModel().to(rank), device_ids=[rank])
    new_ddp_model.load_state_dict(checkpoint['state_dict'])
    powersgd_hook = checkpoint['comm_hook']
    powersgd_state = checkpoint['comm_hook_state']

    new_ddp_model.register_comm_hook(powersgd_state, powersgd_hook)

    if rank == 0:
        os.remove(CHECKPOINT)

    cleanup()

if __name__ == "__main__":
    n_gpus = torch.cuda.device_count()
    assert n_gpus >= 2, f"Requires at least 2 GPUs to run, but got {n_gpus}"
    world_size = n_gpus
    run_demo(demo_serialization, world_size) 
```

## 致谢

非常感谢PowerSGD论文作者**Thijs Vogels**对PowerSGD通信钩子的代码审查，以及[比较实验](https://observablehq.com/@tvogels/powersgd-benchmark)，显示PowerSGD通信钩子的性能与原始[论文](https://arxiv.org/abs/1905.13727)中的实现相当。
