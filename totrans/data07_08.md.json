["```py\n@functional_datapipe(\"parse_csv\")\nclass CSVParserIterDataPipe(IterDataPipe):\n    def __init__(self, dp, **fmtparams) -> None:\n        self.dp = dp\n        self.fmtparams = fmtparams\n\n    def __iter__(self) -> Iterator[Union[Str_Or_Bytes, Tuple[str, Str_Or_Bytes]]]:\n        for path, file in self.source_datapipe:\n            stream = self._helper.skip_lines(file)\n            stream = self._helper.strip_newline(stream)\n            stream = self._helper.decode(stream)\n            yield from self._helper.return_path(stream, path=path)  # Returns 1 line at a time as List[str or bytes] \n```", "```py\nimport torchdata.datapipes as dp\n\nFOLDER = 'path/2/csv/folder'\ndatapipe = dp.iter.FileLister([FOLDER]).filter(filter_fn=lambda filename: filename.endswith('.csv'))\ndatapipe = dp.iter.FileOpener(datapipe, mode='rt')\ndatapipe = datapipe.parse_csv(delimiter=',')\nN_ROWS = 10000  # total number of rows of data\ntrain, valid = datapipe.random_split(total_length=N_ROWS, weights={\"train\": 0.5, \"valid\": 0.5}, seed=0)\n\nfor x in train:  # Iterating through the training dataset\n    pass\n\nfor y in valid:  # Iterating through the validation dataset\n    pass \n```", "```py\nimport csv\nimport random\n\ndef generate_csv(file_label, num_rows: int = 5000, num_features: int = 20) -> None:\n    fieldnames = ['label'] + [f'c{i}' for i in range(num_features)]\n    writer = csv.DictWriter(open(f\"sample_data{file_label}.csv\", \"w\", newline=''), fieldnames=fieldnames)\n    writer.writeheader()\n    for i in range(num_rows):\n        row_data = {col: random.random() for col in fieldnames}\n        row_data['label'] = random.randint(0, 9)\n        writer.writerow(row_data) \n```", "```py\nimport numpy as np\nimport torchdata.datapipes as dp\n\ndef filter_for_data(filename):\n    return \"sample_data\" in filename and filename.endswith(\".csv\")\n\ndef row_processor(row):\n    return {\"label\": np.array(row[0], np.int32), \"data\": np.array(row[1:], dtype=np.float64)}\n\ndef build_datapipes(root_dir=\".\"):\n    datapipe = dp.iter.FileLister(root_dir)\n    datapipe = datapipe.filter(filter_fn=filter_for_data)\n    datapipe = datapipe.open_files(mode='rt')\n    datapipe = datapipe.parse_csv(delimiter=\",\", skip_lines=1)\n    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader\n    datapipe = datapipe.shuffle()\n    datapipe = datapipe.map(row_processor)\n    return datapipe \n```", "```py\nfrom torch.utils.data import DataLoader\n\nif __name__ == '__main__':\n    num_files_to_generate = 3\n    for i in range(num_files_to_generate):\n        generate_csv(file_label=i, num_rows=10, num_features=3)\n    datapipe = build_datapipes()\n    dl = DataLoader(dataset=datapipe, batch_size=5, num_workers=2)\n    first = next(iter(dl))\n    labels, features = first['label'], first['data']\n    print(f\"Labels batch shape: {labels.size()}\")\n    print(f\"Feature batch shape: {features.size()}\")\n    print(f\"{labels  = }\\n{features  = }\")\n    n_sample = 0\n    for row in iter(dl):\n        n_sample += 1\n    print(f\"{n_sample  = }\") \n```", "```py\nLabels batch shape: torch.Size([5])\nFeature batch shape: torch.Size([5, 3])\nlabels = tensor([8, 9, 5, 9, 7], dtype=torch.int32)\nfeatures = tensor([[0.2867, 0.5973, 0.0730],\n        [0.7890, 0.9279, 0.7392],\n        [0.8930, 0.7434, 0.0780],\n        [0.8225, 0.4047, 0.0800],\n        [0.1655, 0.0323, 0.5561]], dtype=torch.float64)\nn_sample = 12 \n```", "```py\ndef build_datapipes(root_dir=\".\"):\n    datapipe = ...\n    # Add the following line to `build_datapipes`\n    # Note that it is somewhere after `Shuffler` in the DataPipe line, but before expensive operations\n    datapipe = datapipe.sharding_filter()\n    return datapipe \n```", "```py\n...\nn_sample = 6 \n```", "```py\nfrom torchdata.datapipes.iter import IterDataPipe\n\nclass MapperIterDataPipe(IterDataPipe):\n    def __init__(self, source_dp: IterDataPipe, fn) -> None:\n        super().__init__()\n        self.source_dp = source_dp\n        self.fn = fn \n```", "```py\nclass MapperIterDataPipe(IterDataPipe):\n    # ... See __init__() defined above\n\n    def __iter__(self):\n        for d in self.dp:\n            yield self.fn(d) \n```", "```py\nclass MapperIterDataPipe(IterDataPipe):\n    # ... See __iter__() defined above\n\n    def __len__(self):\n        return len(self.dp) \n```", "```py\n@functional_datapipe(\"map\")\nclass MapperIterDataPipe(IterDataPipe):\n   # ... \n```", "```py\nimport torchdata.datapipes as dp\n\n# Using functional form (recommended)\ndatapipes1 = dp.iter.FileOpener(['a.file', 'b.file']).map(fn=decoder).shuffle().batch(2)\n# Using class constructors\ndatapipes2 = dp.iter.FileOpener(['a.file', 'b.file'])\ndatapipes2 = dp.iter.Mapper(datapipes2, fn=decoder)\ndatapipes2 = dp.iter.Shuffler(datapipes2)\ndatapipes2 = dp.iter.Batcher(datapipes2, 2) \n```", "```py\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndp = IterableWrapper([\"s3://BUCKET_NAME\"]).list_files_by_fsspec() \n```", "```py\nfrom torchdata.datapipes.iter import IterableWrapper\ndp = IterableWrapper([\"s3://BUCKET_NAME/DIRECTORY/1.tar\"])\ndp = dp.open_files_by_fsspec(mode=\"rb\", anon=True).load_from_tar(mode=\"r|\") # Streaming version\n# The rest of data processing logic goes here \n```", "```py\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndp = IterableWrapper([\"gcs://uspto-pair/\"]).list_files_by_fsspec()\nprint(list(dp))\n# ['gcs://uspto-pair/applications', 'gcs://uspto-pair/docs', 'gcs://uspto-pair/prosecution-history-docs'] \n```", "```py\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndp = IterableWrapper([\"gcs://uspto-pair/applications/05900035.zip\"]) \\\n        .open_files_by_fsspec(mode=\"rb\") \\\n        .load_from_zip()\n# Logic to process those archive files comes after\nfor path, filestream in dp:\n    print(path, filestream)\n# gcs:/uspto-pair/applications/05900035.zip/05900035/README.txt, StreamWrapper<...>\n# gcs:/uspto-pair/applications/05900035.zip/05900035/05900035-address_and_attorney_agent.tsv, StreamWrapper<...>\n# gcs:/uspto-pair/applications/05900035.zip/05900035/05900035-application_data.tsv, StreamWrapper<...>\n# gcs:/uspto-pair/applications/05900035.zip/05900035/05900035-continuity_data.tsv, StreamWrapper<...>\n# gcs:/uspto-pair/applications/05900035.zip/05900035/05900035-transaction_history.tsv, StreamWrapper<...> \n```", "```py\nfrom torchdata.datapipes.iter import IterableWrapper\n\nstorage_options={'account_name': ACCOUNT_NAME, 'account_key': ACCOUNT_KEY}\ndp = IterableWrapper(['abfs://CONTAINER/DIRECTORY']).list_files_by_fsspec(**storage_options)\nprint(list(dp))\n# ['abfs://container/directory/file1.txt', 'abfs://container/directory/file2.txt', ...] \n```", "```py\nfrom torchdata.datapipes.iter import IterableWrapper\ndp = IterableWrapper(['abfs://public/curated/covid-19/ecdc_cases/latest/ecdc_cases.csv']) \\\n        .open_files_by_fsspec(account_name='pandemicdatalake') \\\n        .parse_csv()\nprint(list(dp)[:3])\n# [['date_rep', 'day', ..., 'iso_country', 'daterep'],\n# ['2020-12-14', '14', ..., 'AF', '2020-12-14'],\n# ['2020-12-13', '13', ..., 'AF', '2020-12-13']] \n```", "```py\nfrom torchdata.datapipes.iter import IterableWrapper\n\n# set the subscription_id, resource_group, and AzureML workspace_name\nsubscription_id = \"<subscription_id>\"\nresource_group = \"<resource_group>\"\nworkspace_name = \"<workspace_name>\"\n\n# set the datastore name and path on the datastore\ndatastore_name = \"<datastore_name>\"\npath_on_datastore = \"<path_on_datastore>\"\n\nuri = f\"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace_name}/datastores/{datastore_name}/paths/{path_on_datastore}\"\n\ndp = IterableWrapper([uri]).list_files_by_fsspec()\nprint(list(dp))\n# ['azureml:///<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/<folder>/file1.txt',\n# 'azureml:///<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/<folder>/file2.txt', ...] \n```", "```py\nfrom torchdata.datapipes.iter import IterableWrapper\n\n# set the subscription_id, resource_group, and AzureML workspace_name\nsubscription_id = \"<subscription_id>\"\nresource_group = \"<resource_group>\"\nworkspace_name = \"<workspace_name>\"\n\n# set the datastore name and path on the datastore\ndatastore_name = \"workspaceblobstore\"\npath_on_datastore = \"cifar-10-python.tar.gz\"\n\nuri = f\"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace_name}/datastores/{datastore_name}/paths/{path_on_datastore}\"\n\ndp = IterableWrapper([uri]) \\\n        .open_files_by_fsspec(mode=\"rb\") \\\n        .load_from_tar()\n\nfor path, filestream in dp:\n    print(path)\n# ['azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/data_batch_4',\n#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/readme.html',\n#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/test_batch',\n#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/data_batch_3',\n#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/batches.meta',\n#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/data_batch_2',\n#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/data_batch_5',\n#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/data_batch_1] \n```", "```py\nfrom torchdata.datapipes.iter import IterableWrapper\n\n# set the subscription_id, resource_group, and AzureML workspace_name\nsubscription_id = \"<subscription_id>\"\nresource_group = \"<resource_group>\"\nworkspace_name = \"<workspace_name>\"\n\n# set the datastore name and path on the datastore\ndatastore_name = \"workspaceblobstore\"\npath_on_datastore = \"titanic.csv\"\n\nuri = f\"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace_name}/datastores/{datastore_name}/paths/{path_on_datastore}\"\n\ndef row_processer(row):\n    # return the label and data (the class and age of the passenger)\n    # if missing age, set to 50\n    if row[5] == \"\":\n        row[5] = 50.0\n    return {\"label\": np.array(row[1], np.int32), \"data\": np.array([row[2],row[5]], dtype=np.float32)}\n\ndp = IterableWrapper([uri]) \\\n        .open_files_by_fsspec() \\\n        .parse_csv(delimiter=\",\", skip_lines=1) \\\n        .map(row_processer)\n\nprint(list(dp)[:3])\n# [{'label': array(0, dtype=int32), 'data': array([ 3., 22.], dtype=float32)},\n#  {'label': array(1, dtype=int32), 'data': array([ 1., 38.], dtype=float32)},\n#  {'label': array(1, dtype=int32), 'data': array([ 3., 26.], dtype=float32)}] \n```"]