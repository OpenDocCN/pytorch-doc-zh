["```py\n# This enables the extended features such as the camera.\nstart_x=1\n\n# This needs to be at least 128M for the camera processing, if it's bigger you can just leave it as is.\ngpu_mem=128\n\n# You need to commment/remove the existing camera_auto_detect line since this causes issues with OpenCV/V4L2 capture.\n#camera_auto_detect=1 \n```", "```py\n$  pip  install  torch  torchvision  torchaudio\n$  pip  install  opencv-python\n$  pip  install  numpy  --upgrade \n```", "```py\n$  python  -c  \"import torch; print(torch.__version__)\" \n```", "```py\nimport cv2\nfrom PIL import Image\n\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36) \n```", "```py\nret, image = cap.read()\n# convert opencv output from BGR to RGB\nimage = image[:, :, [2, 1, 0]] \n```", "```py\nfrom torchvision import transforms\n\npreprocess = transforms.Compose([\n    # convert the frame to a CHW torch tensor for training\n    transforms.ToTensor(),\n    # normalize the colors to the range that mobilenet_v2/3 expect\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(image)\n# The model can handle multiple images simultaneously so we need to add an\n# empty dimension for the batch.\n# [3, 224, 224] -> [1, 3, 224, 224]\ninput_batch = input_tensor.unsqueeze(0) \n```", "```py\nimport torch\ntorch.backends.quantized.engine = 'qnnpack' \n```", "```py\nfrom torchvision import models\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True) \n```", "```py\nnet = torch.jit.script(net) \n```", "```py\nimport time\n\nimport torch\nimport numpy as np\nfrom torchvision import models, transforms\n\nimport cv2\nfrom PIL import Image\n\ntorch.backends.quantized.engine = 'qnnpack'\n\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36)\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n# jit model to take it from ~20fps to ~30fps\nnet = torch.jit.script(net)\n\nstarted = time.time()\nlast_logged = time.time()\nframe_count = 0\n\nwith torch.no_grad():\n    while True:\n        # read frame\n        ret, image = cap.read()\n        if not ret:\n            raise RuntimeError(\"failed to read frame\")\n\n        # convert opencv output from BGR to RGB\n        image = image[:, :, [2, 1, 0]]\n        permuted = image\n\n        # preprocess\n        input_tensor = preprocess(image)\n\n        # create a mini-batch as expected by the model\n        input_batch = input_tensor.unsqueeze(0)\n\n        # run model\n        output = net(input_batch)\n        # do something with output ...\n\n        # log model performance\n        frame_count += 1\n        now = time.time()\n        if now - last_logged > 1:\n            print(f\"{frame_count  /  (now-last_logged)} fps\")\n            last_logged = now\n            frame_count = 0 \n```", "```py\ntop = list(enumerate(output[0].softmax(dim=0)))\ntop.sort(key=lambda x: x[1], reverse=True)\nfor idx, val in top[:10]:\n    print(f\"{val.item()*100:.2f}% {classes[idx]}\") \n```", "```py\ntorch.set_num_threads(2) \n```"]