- en: Registering a Dispatched Operator in C++
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在C++中注册一个分发的运算符
- en: 原文：[https://pytorch.org/tutorials/advanced/dispatcher.html](https://pytorch.org/tutorials/advanced/dispatcher.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/tutorials/advanced/dispatcher.html](https://pytorch.org/tutorials/advanced/dispatcher.html)'
- en: 'The dispatcher is an internal component of PyTorch which is responsible for
    figuring out what code should actually get run when you call a function like `torch::add`.
    This can be nontrivial, because PyTorch operations need to handle a lot of cross-cutting
    concerns that are “layered” on top of one of another. Here is a sampling of some
    of the things it handles:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 分发器是PyTorch的一个内部组件，负责确定在调用诸如`torch::add`这样的函数时实际运行哪些代码。这可能并不简单，因为PyTorch操作需要处理许多“层叠”在彼此之上的交叉关注点。以下是它处理的一些事项的示例：
- en: Switching between the CPU and CUDA implementations of an operator, depending
    on the devices of the input tensors.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据输入张量的设备在CPU和CUDA实现之间切换运算符。
- en: Switching between the autograd and backend implementations of an operator, depending
    on whether or not autograd handling is necessary.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在是否需要自动微分处理的情况下，在运算符的自动微分和后端实现之间切换。
- en: Applying autocasting when necessary for automatic mixed precision.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在需要自动混合精度时应用自动转换。
- en: Applying batching rules when an operator is run under a `vmap` call.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`vmap`调用下运行运算符时应用批处理规则。
- en: Tracing execution of operations, if you are tracing a model for export.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪操作的执行，如果您正在跟踪一个模型以进行导出。
- en: If in your [custom operator code](torch_script_custom_ops) you find yourself
    manually writing if statements to handle these cases, the dispatcher APIs can
    help organize your code. (Conversely, if your custom operator is very simple and
    is only for CPU inference, you probably don’t need to use the dispatcher, just
    use the basic API.)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在您的[自定义运算符代码](torch_script_custom_ops)中发现自己手动编写if语句来处理这些情况，分发器API可以帮助组织您的代码。（相反，如果您的自定义运算符非常简单且仅用于CPU推断，则可能不需要使用分发器，只需使用基本API。）
- en: In this tutorial, we will describe how to structure a custom operator registration
    to use the dispatcher to organize various components. We’ll assume that you are
    familiar with how to [register an operator](torch_script_custom_ops) and how to
    write a [custom autograd function](cpp_autograd).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将描述如何结构化自定义运算符注册以使用分发器来组织各种组件。我们假设您已经熟悉如何[注册运算符](torch_script_custom_ops)以及如何编写[自定义自动微分函数](cpp_autograd)。
- en: Defining schema and backend implementations
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义模式和后端实现
- en: The general principle behind the dispatcher is that it divides the implementation
    of an operator into multiple kernels, each of which implements functionality for
    a specific *dispatch key*, e.g. CPU, CUDA. The dispatcher determines what the
    highest priority dispatch key is at the time you call an operator (this is done
    by looking at both the tensor arguments as well as some thread local state), and
    transfers control to the kernel for that dispatch key. The end effect is that
    when you call an operator, we first execute the Autograd kernel, and then we redispatch
    to the backend kernel depending on the device types of the passed in tensors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 分发器背后的一般原则是将运算符的实现分成多个内核，每个内核为特定的*分发键*实现功能，例如CPU、CUDA。分发器确定在调用运算符时最高优先级的分发键是什么（这是通过查看张量参数以及一些线程本地状态来完成的），并将控制权转移到该分发键的内核。最终效果是当您调用一个运算符时，我们首先执行自动微分内核，然后根据传入张量的设备类型重新分发到后端内核。
- en: 'Let’s take a look at the various parts involved in making this happen. First,
    we must define the schema for the operator in question. Unlike simple pybind11-style
    operator registration, we don’t actually provide an implementation of our operator
    at this point; we just provide a schema string specifying the type signature of
    the operator that all of our other kernels will abide by:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使这一切发生所涉及的各个部分。首先，我们必须为所讨论的运算符定义模式。与简单的pybind11风格的运算符注册不同，我们此时实际上并没有提供我们运算符的实现；我们只提供一个模式字符串，指定所有其他内核将遵守的运算符类型签名：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we need to actually provide some implementations of this operator. For
    concreteness, here is a really simple implementation of addition on CPU:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要实际提供一些这个运算符的实现。具体来说，这是一个在CPU上进行加法的非常简单的实现：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We’d like to register this function as an implementation of `myops::myadd`.
    However, the simple way of registering it (`def("myadd", myadd_cpu)`) would register
    the kernel to run in all cases, even if the tensor is not a CPU tensor! (Internally,
    we refer to these as “catch-all” kernels, since they catch all cases.) To ensure
    that `myadd_cpu` is only run for CPU tensors, we can use the `TORCH_LIBRARY_IMPL`
    macro:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要将这个函数注册为`myops::myadd`的实现。然而，简单的注册方式（`def("myadd", myadd_cpu)`）会注册内核在所有情况下运行，即使张量不是CPU张量！（在内部，我们将这些称为“全能”内核，因为它们涵盖所有情况。）为了确保`myadd_cpu`仅在CPU张量上运行，我们可以使用`TORCH_LIBRARY_IMPL`宏：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `TORCH_LIBRARY_IMPL` lets us register implementations for operators on
    a specific dispatch key (in this case, CPU). Each call to `impl` associates a
    CPU kernel with the corresponding operator (which we previously defined in the
    `TORCH_LIBRARY` block). If we also have a CUDA implementation `myadd_cuda`, we
    can register it in a separate `TORCH_LIBRARY_IMPL` block:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`TORCH_LIBRARY_IMPL`让我们为特定分发键（在本例中为CPU）上的运算符注册实现。每次调用`impl`都会将CPU内核与相应的运算符关联起来（我们之前在`TORCH_LIBRARY`块中定义）。如果我们还有一个CUDA实现`myadd_cuda`，我们可以在单独的`TORCH_LIBRARY_IMPL`块中注册它：'
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'These registrations can be split across files or even across library boundaries;
    so for example, you could have these two `TORCH_LIBRARY_IMPL` blocks compiled
    into a separate `myops_cpu` and `myops_cuda` dynamic libraries. Generally, speaking,
    the structure of your registrations will look like this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些注册可以跨文件或甚至跨库边界拆分；例如，您可以将这两个`TORCH_LIBRARY_IMPL`块编译到单独的`myops_cpu`和`myops_cuda`动态库中。一般来说，您的注册结构将如下所示：
- en: A single `TORCH_LIBRARY` that lists every custom operator in your namespace
    in a centralized place.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个单独的`TORCH_LIBRARY`，列出您命名空间中的每个自定义操作符，集中在一个地方。
- en: A `TORCH_LIBRARY_IMPL` per dispatch key that registers implementations for that
    key (e.g., CPU or CUDA). If you like, you can further subdivide `TORCH_LIBRARY_IMPL`
    blocks into a block per operator. This is convenient if you have a separate file
    per operator implementation, but don’t want to expose the operators in a header;
    you can just put the registration in the cpp file that defines your operator.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个调度键注册一个`TORCH_LIBRARY_IMPL`，为该键（例如，CPU或CUDA）注册实现。如果愿意，您还可以将`TORCH_LIBRARY_IMPL`块进一步细分为每个操作符的块。如果您有一个单独的文件用于每个操作符的实现，但不想在头文件中公开这些操作符，您可以将注册放在定义操作符的cpp文件中。
- en: Note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Did you know that you can also write `TORCH_LIBRARY_IMPL` blocks for existing
    core operators in PyTorch? This is how XLA support for PyTorch is implemented:
    the `torch_xla` library contains a `TORCH_LIBRARY_IMPL` that provides implementations
    for all basic operators on the XLA dispatch key.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您知道吗，您还可以为PyTorch中现有核心操作符编写`TORCH_LIBRARY_IMPL`块吗？这就是PyTorch对XLA的支持是如何实现的：`torch_xla`库包含一个`TORCH_LIBRARY_IMPL`，为XLA调度键上的所有基本操作符提供实现。
- en: For operators that do not need autograd
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对于不需要自动求导的操作符
- en: 'Note: This section only applies to versions of PyTorch `>= 1.10`.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此部分仅适用于PyTorch版本`>= 1.10`。
- en: In the next section, we will discuss how to add autograd support to an operator.
    But for the ops that do not need autograd support, the following kernel should
    be registered improve useability and make your op behave like PyTorch’s built-in
    operators.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何为操作符添加自动求导支持。但对于不需要自动求导支持的操作符，应注册以下内核以提高可用性，并使您的操作符的行为类似于PyTorch的内置操作符。
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The above lines registers an `Autograd` kernel that appends a dummy `NotImplemented`
    node on forward (preserving the `require_grad`-ness of the inputs). On backward,
    the `NotImplemented` node raises an error. This can be helpful for debugging in
    larger models where previously it can be hard to pin-point exactly where the `requires_grad`-ness
    is lost during the forward pass.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码注册了一个`Autograd`内核，该内核在前向传播时附加一个虚拟的`NotImplemented`节点（保留输入的`require_grad`属性）。在反向传播中，`NotImplemented`节点会引发错误。在较大模型中进行调试时，这可能有助于确定在前向传播过程中确切丢失`requires_grad`属性的位置。
- en: In-place or view ops
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原地或视图操作
- en: 'To ensure correctness and best possible performance, if your op mutates an
    input in-place or returns a tensor that aliases with one of the inputs, two additional
    steps should be taken:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保正确性和最佳性能，如果您的操作在原地更改输入或返回一个与输入之一别名的张量，则应采取两个额外步骤：
- en: Register an `ADInplaceOrView` kernel in addition to the `Autograd` kernel above.
    This kernel handles the necessary bookkeeping to ensure the correctness of in-place
    or view operations. It is important to note that this ADInplaceOrView kernel should
    only be used with `autogradNotImplementedFallback`.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了上面的`Autograd`内核外，还注册一个`ADInplaceOrView`内核。该内核处理必要的记录工作，以确保原地或视图操作的正确性。重要的是要注意，此ADInplaceOrView内核应仅与`autogradNotImplementedFallback`一起使用。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `Autograd` or `ADInplaceOrView` boxed kernels registered above rely on operator
    schema information in their logi. If your op mutates an input in-place or returns
    a tensor that aliases with one of the inputs it is important to ensure that your
    schema properly reflects this. See [here](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md)
    for more information on how to annotate the schema.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上面注册的`Autograd`或`ADInplaceOrView`封装的内核依赖于其逻辑中的运算符模式信息。如果您的操作在原地对输入进行了更改，或者返回一个与输入之一别名的张量，那么确保您的模式正确反映这一点非常重要。请参阅[此处](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md)以获取有关如何注释模式的更多信息。
- en: '## Adding autograd support'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '## 添加自动求导支持'
- en: 'At this point, we have an operator with both CPU and CUDA implementations.
    How can we add autograd support to it? As you might guess, we will register an
    autograd kernel (similar to what’s described in the [custom autograd function](cpp_autograd)
    tutorial)! However, there is a twist: unlike the CPU and CUDA kernels, the autograd
    kernel needs to *redispatch*: it needs to call back into the dispatcher to get
    to the inference kernels, e.g. CPU or CUDA implementations.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一个既有CPU实现又有CUDA实现的操作符。我们如何为其添加自动求导支持？正如您可能猜到的那样，我们将注册一个自动求导内核（类似于[自定义自动求导函数](cpp_autograd)教程中描述的内容）！但是，有一个转折：与CPU和CUDA内核不同，自动求导内核需要*重新调度*：它需要回调到调度程序以获取推断内核，例如CPU或CUDA实现。
- en: 'Thus, before we write the autograd kernel, let’s write a *dispatching function*
    which calls into the dispatcher to find the right kernel for your operator. This
    function constitutes the public C++ API for your operators–in fact, all of the
    tensor functions in PyTorch’s C++ API all call the dispatcher in the same way
    under the hood. Here’s what the dispatching function looks like:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在编写自动求导内核之前，让我们编写一个*调度函数*，该函数调用调度程序以找到适合您操作符的正确内核。这个函数构成了您操作符的公共C++ API -
    实际上，PyTorch的C++ API中的所有张量函数都在底层以相同的方式调用调度程序。调度函数如下所示：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s break it down:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来详细了解一下：
- en: 'In the first line, we look up a typed operator handle from the dispatcher corresponding
    to the operator that we are going to dispatch to. `findSchemaOrThrow` takes two
    arguments: the (namespace qualified) name of the operator, and the overload name
    of the operator (typically just the empty string). `typed` casts the dynamically
    typed handle into a statically typed handle (doing a runtime test to make sure
    you’ve given the correct C++ type), so that we can do a normal C++ call on it.
    We pass it `decltype(myadd)` since the type of the dispatching function is the
    same as the type of the underlying kernels registered to the dispatcher.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一行中，我们从调度程序中查找与我们要分派的运算符对应的类型化运算符句柄。`findSchemaOrThrow`接受两个参数：运算符的（命名空间限定的）名称和运算符的重载名称（通常为空字符串）。`typed`将动态类型的句柄转换为静态类型的句柄（进行运行时测试以确保您提供了正确的C++类型），以便我们可以对其进行正常的C++调用。我们传递`decltype(myadd)`，因为分派函数的类型与注册到调度程序的基础内核的类型相同。
- en: For performance, this computation is done in a static variable, so that we only
    need to do the (slow) lookup once. If you typoed the name of the operator you
    want to call, this lookup will error the first time you call this function.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了性能，此计算是在静态变量中完成的，因此我们只需要进行一次（慢速）查找。如果您拼错了要调用的运算符的名称，那么在第一次调用此函数时，此查找将出错。
- en: In the second line, we simply `call` the operator handle with all of the arguments
    passed into the dispatching function. This will actually invoke the dispatcher
    and in the end control will be transferred to whatever kernel is appropriate for
    this call.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二行中，我们简单地使用传递给分派函数的所有参数“调用”运算符句柄。这将实际调用调度程序，最终控制将转移到适用于此调用的任何内核。
- en: 'With the dispatch function in hand, we can now write the autograd kernel:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有了分派函数，我们现在可以编写自动微分内核了：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The autograd function is written as normal using `torch::autograd::Function`,
    except that instead of directly writing the implementation in `forward()`, we:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分函数是使用`torch::autograd::Function`正常编写的，只是在`forward()`中不直接编写实现，而是：
- en: Turn off autograd handling with the `at::AutoNonVariableTypeMode` RAII guard,
    and then
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`at::AutoNonVariableTypeMode` RAII保护关闭自动微分处理，然后
- en: Call the dispatch function `myadd` to call back into the dispatcher.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用分派函数`myadd`以回调到调度程序。
- en: Without (1), your calls will infinite loop (and stack overflow), because `myadd`
    will send you back to this function (as the highest priority dispatch key would
    still be autograd.) With (1), autograd is excluded from the set of dispatch keys
    under consideration, and we will go to the next handlers, which will either be
    CPU and CUDA.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 没有（1），您的调用将无限循环（并堆栈溢出），因为`myadd`将将您发送回此函数（因为最高优先级的调度键仍然是自动微分）。有了（1），自动微分将从考虑的调度键集合中排除，我们将转到下一个处理程序，这将是CPU和CUDA。
- en: 'We can now register this function in the same way we registered the CPU/CUDA
    functions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以以与注册CPU/CUDA函数相同的方式注册此函数：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this example we register the kernel to `Autograd`, which installs it as the
    autograd kernel for all backends. You can also register optimized kernels for
    specific backends by using the corresponding backend-specific dispatch key - for
    example, `AutogradCPU` or `AutogradCUDA`. To explore these and other dispatch
    key options in more detail, check out the `PythonDispatcher` tool provided in
    [torch/_python_dispatcher.py](https://github.com/pytorch/pytorch/blob/master/torch/_python_dispatcher.py).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将内核注册到`Autograd`，这将将其安装为所有后端的自动微分内核。您还可以通过使用相应的特定于后端的调度键（例如`AutogradCPU`或`AutogradCUDA`）为特定后端注册优化内核。要更详细地探索这些和其他调度键选项，请查看[torch/_python_dispatcher.py](https://github.com/pytorch/pytorch/blob/master/torch/_python_dispatcher.py)中提供的`PythonDispatcher`工具。
- en: Going beyond autograd
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越自动微分
- en: 'In some sense, the dispatcher isn’t doing all that much: all it does is implement
    a glorified if-statement, along the lines of this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种意义上，调度程序并没有做太多事情：它只是实现了一个类似于这样的if语句：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So why use the dispatcher? There are a few reasons:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要使用调度程序？有几个原因：
- en: It is decentralized. You can assemble all of the pieces of an operator (CPU,
    CUDA, Autograd) without having to write a single, centralized if statement that
    refers to all of them. Importantly, third parties can register extra implementations
    for other aspects without having to patch the original definition of an operator.
    We’ll talk more about extending the dispatcher in [extending dispatcher for a
    new backend](extend_dispatcher).
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它是分散的。您可以组装运算符的所有部分（CPU、CUDA、Autograd）而无需编写一个引用所有这些部分的单个集中if语句。重要的是，第三方可以注册其他方面的额外实现，而无需修补运算符的原始定义。我们将在[扩展调度程序以支持新后端](extend_dispatcher)中更多地讨论扩展调度程序。
- en: It supports more dispatch keys than CPU, CUDA and Autograd. You can see a full
    list of dispatch keys that are currently implemented in PyTorch in `c10/core/DispatchKey.h`.
    These dispatch keys implement a variety of optional functionality for operators,
    and if you decide you want your custom operator to support this functionality,
    all you have to register a kernel for the appropriate key.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它支持比CPU、CUDA和Autograd更多的调度键。您可以在PyTorch中当前实现的`c10/core/DispatchKey.h`中看到当前实现的所有调度键的完整列表。这些调度键为运算符实现了各种可选功能，如果您决定希望您的自定义运算符支持此功能，您只需为适当的键注册一个内核。
- en: The dispatcher implements support for boxed fallback functions, which are functions
    that can be implemented once and apply to all operators in the system. Boxed fallbacks
    can be used to provide default behavior for a dispatch key; if you use the dispatcher
    to implement your operator, you also opt into the fallbacks for all of these operations.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调度程序实现了对装箱回退函数的支持，这些函数可以一次实现并应用于系统中的所有运算符。装箱回退可用于为调度键提供默认行为；如果您使用调度程序来实现您的运算符，您还可以选择为所有这些操作启用回退。
- en: Here are some particular dispatch keys which you may need to define an operator
    for.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些您可能需要为其定义运算符的特定调度键。
- en: Autocast
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动转换
- en: The Autocast dispatch key implements support for [automatic mixed precision
    (AMP)](https://pytorch.org/docs/stable/amp.html). An autocast wrapper kernel typically
    casts incoming `float16` or `float32` CUDA tensors to some preferred precision
    before running the op. For example, matmuls and convolutions on floating-point
    CUDA tensors usually run faster and use less memory in `float16` without impairing
    convergence. Autocast wrappers only have an effect in [autocast-enabled contexts](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Autocast分派键实现了对[自动混合精度（AMP）](https://pytorch.org/docs/stable/amp.html)的支持。自动转换包装器内核通常会将传入的`float16`或`float32`
    CUDA张量转换为某种首选精度，然后运行操作。例如，在浮点CUDA张量上运行的矩阵乘法和卷积通常在`float16`中运行更快，使用更少的内存，而不会影响收敛。自动转换包装器仅在[启用自动转换的上下文](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast)中起作用。
- en: 'Here’s an autocast wrapper for a hypothetical custom matmul, along with its
    registration:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个假设的自定义矩阵乘法的自动转换包装器，以及其注册：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`cached_cast(kHalf, tensor)` casts `tensor` to `float16` if `tensor` is CUDA
    and `float32`, otherwise, it leaves `tensor` unchanged (c.f. the [eligibility
    policy](https://pytorch.org/docs/stable/amp.html#op-eligibility) for natively
    autocasted ops). This ensures if the network calls `mymatmul` on any mixture of
    `float16` and `float32` CUDA tensors, `mymatmul` runs in `float16`. Meanwhile,
    calls to `mymatmul` with non-CUDA, integer-type, or `float64` inputs are unaffected.
    Using `cached_cast` to follow the native eligibility policy in your own autocast
    wrapper is recommended, but not required. For example, if you wanted to force
    `float16` execution for all input types, you could `return mymatmul(self.half(),
    other.half());` instead of using `cached_cast`.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`cached_cast(kHalf, tensor)`将`tensor`转换为`float16`，如果`tensor`是CUDA且为`float32`，否则将`tensor`保持不变（参见[natively
    autocasted ops的资格政策](https://pytorch.org/docs/stable/amp.html#op-eligibility)）。这确保了如果网络在任何混合`float16`和`float32`
    CUDA张量上调用`mymatmul`，`mymatmul`将以`float16`运行。同时，对于非CUDA、整数类型或`float64`输入的`mymatmul`调用不受影响。建议在自己的自动转换包装器中使用`cached_cast`遵循本机资格政策，但不是必需的。例如，如果您想要强制所有输入类型执行`float16`，您可以使用`return
    mymatmul(self.half(), other.half());`而不是使用`cached_cast`。'
- en: Notice that, like our autograd kernels, we exclude the `Autocast` key from dispatch
    before redispatching.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与我们的自动求导内核一样，在重新分派之前，我们将`Autocast`键排除在分派之外。
- en: By default, if no autocast wrapper is provided, we fallthrough directly to the
    regular operator implementation (no autocasting occurs). (We didn’t use `myadd`
    for this example, since pointwise addition doesn’t need autocasting and should
    just fall through.)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，如果没有提供自动转换包装器，我们将直接转到常规操作员实现（不会发生自动转换）。（我们没有在此示例中使用`myadd`，因为逐点加法不需要自动转换，应该直接通过。）
- en: 'When should an autocast wrapper be registered? Unfortunately, there aren’t
    cut-and-dried rules for an op’s preferred precision. You can get a sense for some
    native ops’ preferred precisions by looking at the [cast lists](https://pytorch.org/docs/master/amp.html#op-specific-behavior).
    General guidance:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 何时应注册自动转换包装器？不幸的是，没有关于操作首选精度的明确规则。您可以通过查看[cast lists](https://pytorch.org/docs/master/amp.html#op-specific-behavior)来了解一些本机操作的首选精度。一般指导：
- en: Ops that do reductions should probably execute in `float32`,
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行减少的操作可能应该以`float32`执行，
- en: Any op that does a convolution or gemm under the hood should probably execute
    in `float16`, and
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在底层执行卷积或gemm的任何操作可能应该以`float16`执行，
- en: Other ops with multiple floating-point tensor inputs should standardize them
    to a common precision (unless the implementation supports inputs with different
    precisions).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有多个浮点张量输入的其他操作应将它们标准化为公共精度（除非实现支持具有不同精度的输入）。
- en: 'If your custom op falls into the third category, the `promote_type` template
    helps figure out the widest floating-point type present among input tensors, which
    is the safest choice for the execution type:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的自定义操作属于第三类别，则`promote_type`模板有助于确定输入张量中存在的最宽浮点类型，这是执行类型的最安全选择：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If your custom op is [autograd-enabled](#autograd-support), you only need to
    write and register an autocast wrapper for the same name onto which the autograd
    wrapper is registered. For example, if you wanted an autocast wrapper for the
    `myadd` function shown in the autograd section, all you’d need is
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的自定义操作是[自动求导启用的](#autograd-support)，您只需要为与自动求导包装器注册的相同名称编写并注册一个自动转换包装器。例如，如果您想要一个`myadd`函数的自动转换包装器，只需
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There are no separate gymnastics to make the backward method autocast compatible.
    However, the backward method defined in your custom autograd function will run
    in the same dtype as autocast sets for the forward method, so you should choose
    a `<desired dtype>` suitable for both your forward and backward methods.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 没有单独的技巧使得反向方法与自动转换兼容。但是，您自定义的自动求导函数中定义的反向方法将以与自动转换为前向方法设置的相同dtype运行，因此您应该选择一个适合您的前向和反向方法的`<desired
    dtype>`。
- en: Batched
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批处理
- en: Batched tensors allow you to write your code in a per-example manner, and then
    have them be automatically batched when run under a `vmap` invocation. The API
    for writing batching rules is currently under development, but once it is stabilized,
    you can add support for `vmap` for your operators by registering a kernel at the
    Batched dispatch key.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理张量允许您以每个示例的方式编写代码，然后在`vmap`调用下运行时自动批处理它们。编写批处理规则的API目前正在开发中，但一旦稳定下来，您可以通过在批处理分派键上注册一个内核来为您的操作添加对`vmap`的支持。
- en: Tracer
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 追踪器
- en: 'The Tracer dispatch key implements support for recording invocations of operators
    into a trace when you run `torch.jit.trace`. We intend to provide a boxed fallback
    that will implement tracing for arbitrary operations, see [issue #41478](https://github.com/pytorch/pytorch/issues/41478)
    to track progress.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 追踪器分派键实现了在运行`torch.jit.trace`时将操作调用记录到跟踪中的支持。我们打算提供一个包装回退，用于实现任意操作的跟踪，参见[issue＃41478](https://github.com/pytorch/pytorch/issues/41478)以跟踪进展。
