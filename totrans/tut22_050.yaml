- en: 'NLP From Scratch: Generating Names with a Character-Level RNN'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始的NLP：使用字符级RNN生成名字
- en: 原文：[https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-intermediate-char-rnn-generation-tutorial-py)
    to download the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-intermediate-char-rnn-generation-tutorial-py)下载完整的示例代码
- en: '**Author**: [Sean Robertson](https://github.com/spro)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：[Sean Robertson](https://github.com/spro)'
- en: This is our second of three tutorials on “NLP From Scratch”. In the [first tutorial](/intermediate/char_rnn_classification_tutorial)
    we used a RNN to classify names into their language of origin. This time we’ll
    turn around and generate names from languages.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们关于“从零开始的NLP”的三个教程中的第二个。在[第一个教程](/intermediate/char_rnn_classification_tutorial)中，我们使用RNN将名字分类到其语言来源。这一次我们将转而生成不同语言的名字。
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are still hand-crafting a small RNN with a few linear layers. The big difference
    is instead of predicting a category after reading in all the letters of a name,
    we input a category and output one letter at a time. Recurrently predicting characters
    to form language (this could also be done with words or other higher order constructs)
    is often referred to as a “language model”.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然手工制作一个小型RNN，其中包含几个线性层。最大的区别是，我们不是在读取名字的所有字母后预测类别，而是输入一个类别，并逐个输出一个字母。循环地预测字符以形成语言（这也可以用单词或其他更高级别的结构来完成）通常被称为“语言模型”。
- en: '**Recommended Reading:**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**推荐阅读：**'
- en: 'I assume you have at least installed PyTorch, know Python, and understand Tensors:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您至少已经安装了PyTorch，了解Python，并理解张量：
- en: '[https://pytorch.org/](https://pytorch.org/) For installation instructions'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/](https://pytorch.org/) 安装说明'
- en: '[Deep Learning with PyTorch: A 60 Minute Blitz](../beginner/deep_learning_60min_blitz.html)
    to get started with PyTorch in general'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyTorch进行深度学习：60分钟入门](../beginner/deep_learning_60min_blitz.html) 以一般性的PyTorch开始'
- en: '[Learning PyTorch with Examples](../beginner/pytorch_with_examples.html) for
    a wide and deep overview'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用示例学习PyTorch](../beginner/pytorch_with_examples.html) 进行广泛和深入的概述'
- en: '[PyTorch for Former Torch Users](../beginner/former_torchies_tutorial.html)
    if you are former Lua Torch user'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch for Former Torch Users](../beginner/former_torchies_tutorial.html)
    如果您以前是Lua Torch用户'
- en: 'It would also be useful to know about RNNs and how they work:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 了解RNN以及它们的工作原理也会很有用：
- en: '[The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    shows a bunch of real life examples'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[循环神经网络的非凡有效性](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)展示了一堆真实生活中的例子'
- en: '[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    is about LSTMs specifically but also informative about RNNs in general'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解LSTM网络](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) 是关于LSTMs的，但也对RNNs有一般性的信息'
- en: 'I also suggest the previous tutorial, [NLP From Scratch: Classifying Names
    with a Character-Level RNN](char_rnn_classification_tutorial.html)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我还建议查看之前的教程，[从零开始的NLP：使用字符级RNN对名字进行分类](char_rnn_classification_tutorial.html)
- en: Preparing the Data
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Download the data from [here](https://download.pytorch.org/tutorial/data.zip)
    and extract it to the current directory.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从[这里](https://download.pytorch.org/tutorial/data.zip)下载数据并将其解压到当前目录。
- en: 'See the last tutorial for more detail of this process. In short, there are
    a bunch of plain text files `data/names/[Language].txt` with a name per line.
    We split lines into an array, convert Unicode to ASCII, and end up with a dictionary
    `{language: [names ...]}`.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '有关此过程的更多详细信息，请参阅上一个教程。简而言之，有一堆纯文本文件`data/names/[Language].txt`，每行一个名字。我们将行拆分为数组，将Unicode转换为ASCII，最终得到一个字典`{language:
    [names ...]}`。'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Creating the Network
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建网络
- en: This network extends [the last tutorial’s RNN](#Creating-the-Network) with an
    extra argument for the category tensor, which is concatenated along with the others.
    The category tensor is a one-hot vector just like the letter input.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络扩展了[上一个教程的RNN](#Creating-the-Network)，增加了一个额外的参数用于类别张量，该参数与其他参数一起连接。类别张量是一个独热向量，就像字母输入一样。
- en: We will interpret the output as the probability of the next letter. When sampling,
    the most likely output letter is used as the next input letter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解释输出为下一个字母的概率。在采样时，最有可能的输出字母将被用作下一个输入字母。
- en: I added a second linear layer `o2o` (after combining hidden and output) to give
    it more muscle to work with. There’s also a dropout layer, which [randomly zeros
    parts of its input](https://arxiv.org/abs/1207.0580) with a given probability
    (here 0.1) and is usually used to fuzz inputs to prevent overfitting. Here we’re
    using it towards the end of the network to purposely add some chaos and increase
    sampling variety.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加了第二个线性层`o2o`（在隐藏和输出合并后）以增加其处理能力。还有一个dropout层，它会[随机将其输入的部分置零](https://arxiv.org/abs/1207.0580)以给定的概率（这里是0.1），通常用于模糊输入以防止过拟合。在网络末尾使用它是为了故意增加一些混乱并增加采样的多样性。
- en: '![](../Images/28a4f1426695fb55f1f6bc86278f6547.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28a4f1426695fb55f1f6bc86278f6547.png)'
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Training
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Preparing for Training
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备训练
- en: 'First of all, helper functions to get random pairs of (category, line):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，编写帮助函数以获取随机的（类别，行）对：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For each timestep (that is, for each letter in a training word) the inputs of
    the network will be `(category, current letter, hidden state)` and the outputs
    will be `(next letter, next hidden state)`. So for each training set, we’ll need
    the category, a set of input letters, and a set of output/target letters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个时间步（即训练单词中的每个字母），网络的输入将是`(类别，当前字母，隐藏状态)`，输出将是`(下一个字母，下一个隐藏状态)`。因此，对于每个训练集，我们需要类别、一组输入字母和一组输出/目标字母。
- en: Since we are predicting the next letter from the current letter for each timestep,
    the letter pairs are groups of consecutive letters from the line - e.g. for `"ABCD<EOS>"`
    we would create (“A”, “B”), (“B”, “C”), (“C”, “D”), (“D”, “EOS”).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在预测每个时间步的下一个字母，所以字母对是来自行中连续字母的组 - 例如对于`"ABCD<EOS>"`，我们将创建(“A”, “B”), (“B”,
    “C”), (“C”, “D”), (“D”, “EOS”)。
- en: '![](../Images/3fae03d85aed3a2237fd4b2f7fb7b480.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fae03d85aed3a2237fd4b2f7fb7b480.png)'
- en: The category tensor is a [one-hot tensor](https://en.wikipedia.org/wiki/One-hot)
    of size `<1 x n_categories>`. When training we feed it to the network at every
    timestep - this is a design choice, it could have been included as part of initial
    hidden state or some other strategy.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 类别张量是一个大小为`<1 x n_categories>`的[独热张量](https://en.wikipedia.org/wiki/One-hot)。在训练时，我们在每个时间步将其馈送到网络中
    - 这是一个设计选择，它可以作为初始隐藏状态的一部分或其他策略的一部分。
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For convenience during training we’ll make a `randomTrainingExample` function
    that fetches a random (category, line) pair and turns them into the required (category,
    input, target) tensors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便训练，我们将创建一个`randomTrainingExample`函数，该函数获取一个随机的（类别，行）对，并将它们转换为所需的（类别，输入，目标）张量。
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Training the Network
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练网络
- en: In contrast to classification, where only the last output is used, we are making
    a prediction at every step, so we are calculating loss at every step.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类不同，分类只使用最后一个输出，我们在每一步都在做预测，因此我们在每一步都在计算损失。
- en: The magic of autograd allows you to simply sum these losses at each step and
    call backward at the end.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分的魔力使您可以简单地在每一步总结这些损失，并在最后调用反向传播。
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To keep track of how long training takes I am adding a `timeSince(timestamp)`
    function which returns a human readable string:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪训练需要多长时间，我添加了一个`timeSince(timestamp)`函数，它返回一个可读的字符串：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training is business as usual - call train a bunch of times and wait a few minutes,
    printing the current time and loss every `print_every` examples, and keeping store
    of an average loss per `plot_every` examples in `all_losses` for plotting later.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 训练就像往常一样 - 多次调用train并等待几分钟，每`print_every`个示例打印当前时间和损失，并在`all_losses`中保留每`plot_every`个示例的平均损失以供稍后绘图。
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Plotting the Losses
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绘制损失
- en: 'Plotting the historical loss from all_losses shows the network learning:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从`all_losses`中绘制历史损失显示网络学习：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![char rnn generation tutorial](../Images/5ad82e2b23a82287af2caa2fe4b316b3.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![char rnn generation tutorial](../Images/5ad82e2b23a82287af2caa2fe4b316b3.png)'
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Sampling the Network
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对网络进行采样
- en: To sample we give the network a letter and ask what the next one is, feed that
    in as the next letter, and repeat until the EOS token.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了采样，我们给网络一个字母，并询问下一个是什么，将其作为下一个字母馈送进去，并重复，直到EOS令牌。
- en: Create tensors for input category, starting letter, and empty hidden state
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为输入类别、起始字母和空隐藏状态创建张量
- en: Create a string `output_name` with the starting letter
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个字符串`output_name`，以起始字母开始
- en: Up to a maximum output length,
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最大输出长度之内，
- en: Feed the current letter to the network
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将当前字母馈送到网络
- en: Get the next letter from highest output, and next hidden state
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从最高输出中获取下一个字母，并获取下一个隐藏状态
- en: If the letter is EOS, stop here
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果字母是EOS，则在此停止
- en: If a regular letter, add to `output_name` and continue
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是常规字母，则添加到`output_name`并继续
- en: Return the final name
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回最终名称
- en: Note
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Rather than having to give it a starting letter, another strategy would have
    been to include a “start of string” token in training and have the network choose
    its own starting letter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与其必须给出一个起始字母，另一种策略是在训练中包含一个“字符串开始”标记，并让网络选择自己的起始字母。
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Exercises
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: 'Try with a different dataset of category -> line, for example:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用不同的类别 -> 行数据集，例如：
- en: Fictional series -> Character name
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚构系列 -> 角色名称
- en: Part of speech -> Word
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性 -> 单词
- en: Country -> City
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国家 -> 城市
- en: Use a “start of sentence” token so that sampling can be done without choosing
    a start letter
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用“句子开始”标记，以便可以进行采样而无需选择起始字母
- en: Get better results with a bigger and/or better shaped network
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过更大和/或更好形状的网络获得更好的结果
- en: Try the `nn.LSTM` and `nn.GRU` layers
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用`nn.LSTM`和`nn.GRU`层
- en: Combine multiple of these RNNs as a higher level network
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多个这些RNN组合为更高级别的网络
- en: '**Total running time of the script:** ( 3 minutes 7.253 seconds)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（3分钟7.253秒）'
- en: '[`Download Python source code: char_rnn_generation_tutorial.py`](../_downloads/322506af160d5e2056afd75de1fd34ee/char_rnn_generation_tutorial.py)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：char_rnn_generation_tutorial.py`](../_downloads/322506af160d5e2056afd75de1fd34ee/char_rnn_generation_tutorial.py)'
- en: '[`Download Jupyter notebook: char_rnn_generation_tutorial.ipynb`](../_downloads/a75cfadf4fa84dd594874d4c53b62820/char_rnn_generation_tutorial.ipynb)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：char_rnn_generation_tutorial.ipynb`](../_downloads/a75cfadf4fa84dd594874d4c53b62820/char_rnn_generation_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery生成的画廊](https://sphinx-gallery.github.io)'
