- en: 'NLP From Scratch: Generating Names with a Character-Level RNN'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-intermediate-char-rnn-generation-tutorial-py)
    to download the full example code
  prefs: []
  type: TYPE_NORMAL
- en: '**Author**: [Sean Robertson](https://github.com/spro)'
  prefs: []
  type: TYPE_NORMAL
- en: This is our second of three tutorials on “NLP From Scratch”. In the [first tutorial](/intermediate/char_rnn_classification_tutorial)
    we used a RNN to classify names into their language of origin. This time we’ll
    turn around and generate names from languages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We are still hand-crafting a small RNN with a few linear layers. The big difference
    is instead of predicting a category after reading in all the letters of a name,
    we input a category and output one letter at a time. Recurrently predicting characters
    to form language (this could also be done with words or other higher order constructs)
    is often referred to as a “language model”.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recommended Reading:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'I assume you have at least installed PyTorch, know Python, and understand Tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pytorch.org/](https://pytorch.org/) For installation instructions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Learning with PyTorch: A 60 Minute Blitz](../beginner/deep_learning_60min_blitz.html)
    to get started with PyTorch in general'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learning PyTorch with Examples](../beginner/pytorch_with_examples.html) for
    a wide and deep overview'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyTorch for Former Torch Users](../beginner/former_torchies_tutorial.html)
    if you are former Lua Torch user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It would also be useful to know about RNNs and how they work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    shows a bunch of real life examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    is about LSTMs specifically but also informative about RNNs in general'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I also suggest the previous tutorial, [NLP From Scratch: Classifying Names
    with a Character-Level RNN](char_rnn_classification_tutorial.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Download the data from [here](https://download.pytorch.org/tutorial/data.zip)
    and extract it to the current directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the last tutorial for more detail of this process. In short, there are
    a bunch of plain text files `data/names/[Language].txt` with a name per line.
    We split lines into an array, convert Unicode to ASCII, and end up with a dictionary
    `{language: [names ...]}`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Creating the Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This network extends [the last tutorial’s RNN](#Creating-the-Network) with an
    extra argument for the category tensor, which is concatenated along with the others.
    The category tensor is a one-hot vector just like the letter input.
  prefs: []
  type: TYPE_NORMAL
- en: We will interpret the output as the probability of the next letter. When sampling,
    the most likely output letter is used as the next input letter.
  prefs: []
  type: TYPE_NORMAL
- en: I added a second linear layer `o2o` (after combining hidden and output) to give
    it more muscle to work with. There’s also a dropout layer, which [randomly zeros
    parts of its input](https://arxiv.org/abs/1207.0580) with a given probability
    (here 0.1) and is usually used to fuzz inputs to prevent overfitting. Here we’re
    using it towards the end of the network to purposely add some chaos and increase
    sampling variety.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28a4f1426695fb55f1f6bc86278f6547.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Preparing for Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First of all, helper functions to get random pairs of (category, line):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For each timestep (that is, for each letter in a training word) the inputs of
    the network will be `(category, current letter, hidden state)` and the outputs
    will be `(next letter, next hidden state)`. So for each training set, we’ll need
    the category, a set of input letters, and a set of output/target letters.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are predicting the next letter from the current letter for each timestep,
    the letter pairs are groups of consecutive letters from the line - e.g. for `"ABCD<EOS>"`
    we would create (“A”, “B”), (“B”, “C”), (“C”, “D”), (“D”, “EOS”).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fae03d85aed3a2237fd4b2f7fb7b480.png)'
  prefs: []
  type: TYPE_IMG
- en: The category tensor is a [one-hot tensor](https://en.wikipedia.org/wiki/One-hot)
    of size `<1 x n_categories>`. When training we feed it to the network at every
    timestep - this is a design choice, it could have been included as part of initial
    hidden state or some other strategy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For convenience during training we’ll make a `randomTrainingExample` function
    that fetches a random (category, line) pair and turns them into the required (category,
    input, target) tensors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Training the Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In contrast to classification, where only the last output is used, we are making
    a prediction at every step, so we are calculating loss at every step.
  prefs: []
  type: TYPE_NORMAL
- en: The magic of autograd allows you to simply sum these losses at each step and
    call backward at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To keep track of how long training takes I am adding a `timeSince(timestamp)`
    function which returns a human readable string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Training is business as usual - call train a bunch of times and wait a few minutes,
    printing the current time and loss every `print_every` examples, and keeping store
    of an average loss per `plot_every` examples in `all_losses` for plotting later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Plotting the Losses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Plotting the historical loss from all_losses shows the network learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![char rnn generation tutorial](../Images/5ad82e2b23a82287af2caa2fe4b316b3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Sampling the Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To sample we give the network a letter and ask what the next one is, feed that
    in as the next letter, and repeat until the EOS token.
  prefs: []
  type: TYPE_NORMAL
- en: Create tensors for input category, starting letter, and empty hidden state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a string `output_name` with the starting letter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to a maximum output length,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed the current letter to the network
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the next letter from highest output, and next hidden state
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the letter is EOS, stop here
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If a regular letter, add to `output_name` and continue
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the final name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Rather than having to give it a starting letter, another strategy would have
    been to include a “start of string” token in training and have the network choose
    its own starting letter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Try with a different dataset of category -> line, for example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fictional series -> Character name
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Part of speech -> Word
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Country -> City
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a “start of sentence” token so that sampling can be done without choosing
    a start letter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get better results with a bigger and/or better shaped network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try the `nn.LSTM` and `nn.GRU` layers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine multiple of these RNNs as a higher level network
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 3 minutes 7.253 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: char_rnn_generation_tutorial.py`](../_downloads/322506af160d5e2056afd75de1fd34ee/char_rnn_generation_tutorial.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: char_rnn_generation_tutorial.ipynb`](../_downloads/a75cfadf4fa84dd594874d4c53b62820/char_rnn_generation_tutorial.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
