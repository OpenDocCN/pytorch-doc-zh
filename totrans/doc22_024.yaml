- en: Reproducibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Completely reproducible results are not guaranteed across PyTorch releases,
    individual commits, or different platforms. Furthermore, results may not be reproducible
    between CPU and GPU executions, even when using identical seeds.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some steps you can take to limit the number of sources of
    nondeterministic behavior for a specific platform, device, and PyTorch release.
    First, you can control sources of randomness that can cause multiple executions
    of your application to behave differently. Second, you can configure PyTorch to
    avoid using nondeterministic algorithms for some operations, so that multiple
    calls to those operations, given the same inputs, will produce the same result.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic operations are often slower than nondeterministic operations,
    so single-run performance may decrease for your model. However, determinism may
    save time in development by facilitating experimentation, debugging, and regression
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling sources of randomness[](#controlling-sources-of-randomness "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch random number generator[](#pytorch-random-number-generator "Permalink
    to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can use [`torch.manual_seed()`](../generated/torch.manual_seed.html#torch.manual_seed
    "torch.manual_seed") to seed the RNG for all devices (both CPU and CUDA):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Some PyTorch operations may use random numbers internally. [`torch.svd_lowrank()`](../generated/torch.svd_lowrank.html#torch.svd_lowrank
    "torch.svd_lowrank") does this, for instance. Consequently, calling it multiple
    times back-to-back with the same input arguments may give different results. However,
    as long as [`torch.manual_seed()`](../generated/torch.manual_seed.html#torch.manual_seed
    "torch.manual_seed") is set to a constant at the beginning of an application and
    all other sources of nondeterminism have been eliminated, the same series of random
    numbers will be generated each time the application is run in the same environment.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to obtain identical results from an operation that uses
    random numbers by setting [`torch.manual_seed()`](../generated/torch.manual_seed.html#torch.manual_seed
    "torch.manual_seed") to the same value between subsequent calls.
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For custom operators, you might need to set python seed as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Random number generators in other libraries[](#random-number-generators-in-other-libraries
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you or any of the libraries you are using rely on NumPy, you can seed the
    global NumPy RNG with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: However, some applications and libraries may use NumPy Random Generator objects,
    not the global RNG ([https://numpy.org/doc/stable/reference/random/generator.html](https://numpy.org/doc/stable/reference/random/generator.html)),
    and those will need to be seeded consistently as well.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using any other libraries that use random number generators, refer
    to the documentation for those libraries to see how to set consistent seeds for
    them.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA convolution benchmarking[](#cuda-convolution-benchmarking "Permalink to
    this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism
    across multiple executions of an application. When a cuDNN convolution is called
    with a new set of size parameters, an optional feature can run multiple convolution
    algorithms, benchmarking them to find the fastest one. Then, the fastest algorithm
    will be used consistently during the rest of the process for the corresponding
    set of size parameters. Due to benchmarking noise and different hardware, the
    benchmark may select different algorithms on subsequent runs, even on the same
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: Disabling the benchmarking feature with `torch.backends.cudnn.benchmark = False`
    causes cuDNN to deterministically select an algorithm, possibly at the cost of
    reduced performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you do not need reproducibility across multiple executions of your
    application, then performance might improve if the benchmarking feature is enabled
    with `torch.backends.cudnn.benchmark = True`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this setting is different from the `torch.backends.cudnn.deterministic`
    setting discussed below.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding nondeterministic algorithms[](#avoiding-nondeterministic-algorithms
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`torch.use_deterministic_algorithms()`](../generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms
    "torch.use_deterministic_algorithms") lets you configure PyTorch to use deterministic
    algorithms instead of nondeterministic ones where available, and to throw an error
    if an operation is known to be nondeterministic (and without a deterministic alternative).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please check the documentation for [`torch.use_deterministic_algorithms()`](../generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms
    "torch.use_deterministic_algorithms") for a full list of affected operations.
    If an operation does not act correctly according to the documentation, or if you
    need a deterministic implementation of an operation that does not have one, please
    submit an issue: [https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22](https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, running the nondeterministic CUDA implementation of [`torch.Tensor.index_add_()`](../generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_
    "torch.Tensor.index_add_") will throw an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When [`torch.bmm()`](../generated/torch.bmm.html#torch.bmm "torch.bmm") is
    called with sparse-dense CUDA tensors it typically uses a nondeterministic algorithm,
    but when the deterministic flag is turned on, its alternate deterministic implementation
    will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or
    greater, you should set the environment variable CUBLAS_WORKSPACE_CONFIG according
    to CUDA documentation: [https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility](https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility)'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA convolution determinism[](#cuda-convolution-determinism "Permalink to
    this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While disabling CUDA convolution benchmarking (discussed above) ensures that
    CUDA selects the same algorithm each time an application is run, that algorithm
    itself may be nondeterministic, unless either `torch.use_deterministic_algorithms(True)`
    or `torch.backends.cudnn.deterministic = True` is set. The latter setting controls
    only this behavior, unlike [`torch.use_deterministic_algorithms()`](../generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms
    "torch.use_deterministic_algorithms") which will make other PyTorch operations
    behave deterministically, too.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA RNN and LSTM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In some versions of CUDA, RNNs and LSTM networks may have non-deterministic
    behavior. See [`torch.nn.RNN()`](../generated/torch.nn.RNN.html#torch.nn.RNN "torch.nn.RNN")
    and [`torch.nn.LSTM()`](../generated/torch.nn.LSTM.html#torch.nn.LSTM "torch.nn.LSTM")
    for details and workarounds.
  prefs: []
  type: TYPE_NORMAL
- en: Filling uninitialized memory[](#filling-uninitialized-memory "Permalink to
    this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Operations like [`torch.empty()`](../generated/torch.empty.html#torch.empty
    "torch.empty") and [`torch.Tensor.resize_()`](../generated/torch.Tensor.resize_.html#torch.Tensor.resize_
    "torch.Tensor.resize_") can return tensors with uninitialized memory that contain
    undefined values. Using such a tensor as an input to another operation is invalid
    if determinism is required, because the output will be nondeterministic. But there
    is nothing to actually prevent such invalid code from being run. So for safety,
    [`torch.utils.deterministic.fill_uninitialized_memory`](../deterministic.html#torch.utils.deterministic.fill_uninitialized_memory
    "torch.utils.deterministic.fill_uninitialized_memory") is set to `True` by default,
    which will fill the uninitialized memory with a known value if `torch.use_deterministic_algorithms(True)`
    is set. This will to prevent the possibility of this kind of nondeterministic
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: However, filling uninitialized memory is detrimental to performance. So if your
    program is valid and does not use uninitialized memory as the input to an operation,
    then this setting can be turned off for better performance.
  prefs: []
  type: TYPE_NORMAL
- en: DataLoader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DataLoader will reseed workers following [Randomness in multi-process data
    loading](../data.html#data-loading-randomness) algorithm. Use `worker_init_fn()`
    and generator to preserve reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
