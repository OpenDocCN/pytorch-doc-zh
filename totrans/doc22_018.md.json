["```py\ncuda = torch.device('cuda')     # Default HIP device\ncuda0 = torch.device('cuda:0')  # 'rocm' or 'hip' are not valid, use 'cuda'\ncuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n\nwith torch.cuda.device(1):\n    # allocates a tensor on GPU 1\n    a = torch.tensor([1., 2.], device=cuda)\n\n    # transfers a tensor from CPU to GPU 1\n    b = torch.tensor([1., 2.]).cuda()\n    # a.device and b.device are device(type='cuda', index=1)\n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n    b2 = torch.tensor([1., 2.]).to(device=cuda)\n    # b.device and b2.device are device(type='cuda', index=1)\n\n    c = a + b\n    # c.device is device(type='cuda', index=1)\n\n    z = x + y\n    # z.device is device(type='cuda', index=0)\n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n    d = torch.randn(2, device=cuda2)\n    e = torch.randn(2).to(cuda2)\n    f = torch.randn(2).cuda(cuda2)\n    # d.device, e.device, and f.device are all device(type='cuda', index=2) \n```", "```py\nif torch.cuda.is_available() and torch.version.hip:\n    # do something specific for HIP\nelif torch.cuda.is_available() and torch.version.cuda:\n    # do something specific for CUDA \n```", "```py\n-DROCM_FORCE_ENABLE_GPU_ASSERTS:BOOL=ON \n```"]