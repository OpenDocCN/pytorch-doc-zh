["```py\nimport torch\nfrom torch.export import export\n\ndef f(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    a = torch.sin(x)\n    b = torch.cos(y)\n    return a + b\n\nexample_args = (torch.randn(10, 10), torch.randn(10, 10))\n\nexported_program: torch.export.ExportedProgram = export(\n    f, args=example_args\n)\nprint(exported_program) \n```", "```py\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[10, 10], arg1_1: f32[10, 10]):\n            # code: a = torch.sin(x)\n            sin: f32[10, 10] = torch.ops.aten.sin.default(arg0_1);\n\n            # code: b = torch.cos(y)\n            cos: f32[10, 10] = torch.ops.aten.cos.default(arg1_1);\n\n            # code: return a + b\n            add: f32[10, 10] = torch.ops.aten.add.Tensor(sin, cos);\n            return (add,)\n\n    Graph signature: ExportGraphSignature(\n        parameters=[],\n        buffers=[],\n        user_inputs=['arg0_1', 'arg1_1'],\n        user_outputs=['add'],\n        inputs_to_parameters={},\n        inputs_to_buffers={},\n        buffers_to_mutate={},\n        backward_signature=None,\n        assertion_dep_token=None,\n    )\n    Range constraints: {}\n    Equality constraints: [] \n```", "```py\nimport torch\nfrom torch.export import export\n\n# Simple module for demonstration\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels=3, out_channels=16, kernel_size=3, padding=1\n        )\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=3)\n\n    def forward(self, x: torch.Tensor, *, constant=None) -> torch.Tensor:\n        a = self.conv(x)\n        a.add_(constant)\n        return self.maxpool(self.relu(a))\n\nexample_args = (torch.randn(1, 3, 256, 256),)\nexample_kwargs = {\"constant\": torch.ones(1, 16, 256, 256)}\n\nexported_program: torch.export.ExportedProgram = export(\n    M(), args=example_args, kwargs=example_kwargs\n)\nprint(exported_program) \n```", "```py\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[16, 3, 3, 3], arg1_1: f32[16], arg2_1: f32[1, 3, 256, 256], arg3_1: f32[1, 16, 256, 256]):\n\n            # code: a = self.conv(x)\n            convolution: f32[1, 16, 256, 256] = torch.ops.aten.convolution.default(\n                arg2_1, arg0_1, arg1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1\n            );\n\n            # code: a.add_(constant)\n            add: f32[1, 16, 256, 256] = torch.ops.aten.add.Tensor(convolution, arg3_1);\n\n            # code: return self.maxpool(self.relu(a))\n            relu: f32[1, 16, 256, 256] = torch.ops.aten.relu.default(add);\n            max_pool2d_with_indices = torch.ops.aten.max_pool2d_with_indices.default(\n                relu, [3, 3], [3, 3]\n            );\n            getitem: f32[1, 16, 85, 85] = max_pool2d_with_indices[0];\n            return (getitem,)\n\n    Graph signature: ExportGraphSignature(\n        parameters=['L__self___conv.weight', 'L__self___conv.bias'],\n        buffers=[],\n        user_inputs=['arg2_1', 'arg3_1'],\n        user_outputs=['getitem'],\n        inputs_to_parameters={\n            'arg0_1': 'L__self___conv.weight',\n            'arg1_1': 'L__self___conv.bias',\n        },\n        inputs_to_buffers={},\n        buffers_to_mutate={},\n        backward_signature=None,\n        assertion_dep_token=None,\n    )\n    Range constraints: {}\n    Equality constraints: [] \n```", "```py\nimport torch\nfrom torch.export import Dim, export\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.branch1 = torch.nn.Sequential(\n            torch.nn.Linear(64, 32), torch.nn.ReLU()\n        )\n        self.branch2 = torch.nn.Sequential(\n            torch.nn.Linear(128, 64), torch.nn.ReLU()\n        )\n        self.buffer = torch.ones(32)\n\n    def forward(self, x1, x2):\n        out1 = self.branch1(x1)\n        out2 = self.branch2(x2)\n        return (out1 + self.buffer, out2)\n\nexample_args = (torch.randn(32, 64), torch.randn(32, 128))\n\n# Create a dynamic batch size\nbatch = Dim(\"batch\")\n# Specify that the first dimension of each input is that batch size\ndynamic_shapes = {\"x1\": {0: batch}, \"x2\": {0: batch}}\n\nexported_program: torch.export.ExportedProgram = export(\n    M(), args=example_args, dynamic_shapes=dynamic_shapes\n)\nprint(exported_program) \n```", "```py\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[32, 64], arg1_1: f32[32], arg2_1: f32[64, 128], arg3_1: f32[64], arg4_1: f32[32], arg5_1: f32[s0, 64], arg6_1: f32[s0, 128]):\n\n            # code: out1 = self.branch1(x1)\n            permute: f32[64, 32] = torch.ops.aten.permute.default(arg0_1, [1, 0]);\n            addmm: f32[s0, 32] = torch.ops.aten.addmm.default(arg1_1, arg5_1, permute);\n            relu: f32[s0, 32] = torch.ops.aten.relu.default(addmm);\n\n            # code: out2 = self.branch2(x2)\n            permute_1: f32[128, 64] = torch.ops.aten.permute.default(arg2_1, [1, 0]);\n            addmm_1: f32[s0, 64] = torch.ops.aten.addmm.default(arg3_1, arg6_1, permute_1);\n            relu_1: f32[s0, 64] = torch.ops.aten.relu.default(addmm_1);  addmm_1 = None\n\n            # code: return (out1 + self.buffer, out2)\n            add: f32[s0, 32] = torch.ops.aten.add.Tensor(relu, arg4_1);\n            return (add, relu_1)\n\n    Graph signature: ExportGraphSignature(\n        parameters=[\n            'branch1.0.weight',\n            'branch1.0.bias',\n            'branch2.0.weight',\n            'branch2.0.bias',\n        ],\n        buffers=['L__self___buffer'],\n        user_inputs=['arg5_1', 'arg6_1'],\n        user_outputs=['add', 'relu_1'],\n        inputs_to_parameters={\n            'arg0_1': 'branch1.0.weight',\n            'arg1_1': 'branch1.0.bias',\n            'arg2_1': 'branch2.0.weight',\n            'arg3_1': 'branch2.0.bias',\n        },\n        inputs_to_buffers={'arg4_1': 'L__self___buffer'},\n        buffers_to_mutate={},\n        backward_signature=None,\n        assertion_dep_token=None,\n    )\n    Range constraints: {s0: RangeConstraint(min_val=2, max_val=9223372036854775806)}\n    Equality constraints: [(InputDim(input_name='arg5_1', dim=0), InputDim(input_name='arg6_1', dim=0))] \n```", "```py\nimport torch\nimport io\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nexported_program = torch.export.export(MyModule(), torch.randn(5))\n\ntorch.export.save(exported_program, 'exported_program.pt2')\nsaved_exported_program = torch.export.load('exported_program.pt2') \n```", "```py\nimport torch\nfrom torch.export import export\n\ndef fn(x):\n    if x.shape[0] > 5:\n        return x + 1\n    else:\n        return x - 1\n\nexample_inputs = (torch.rand(10, 2),)\nexported_program = export(fn, example_inputs)\nprint(exported_program) \n```", "```py\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[10, 2]):\n            add: f32[10, 2] = torch.ops.aten.add.Tensor(arg0_1, 1);\n            return (add,) \n```", "```py\nimport torch\nfrom torch.export import export\n\ndef fn(x: torch.Tensor, const: int, times: int):\n    for i in range(times):\n        x = x + const\n    return x\n\nexample_inputs = (torch.rand(2, 2), 1, 3)\nexported_program = export(fn, example_inputs)\nprint(exported_program) \n```", "```py\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[2, 2], arg1_1, arg2_1):\n            add: f32[2, 2] = torch.ops.aten.add.Tensor(arg0_1, 1);\n            add_1: f32[2, 2] = torch.ops.aten.add.Tensor(add, 1);\n            add_2: f32[2, 2] = torch.ops.aten.add.Tensor(add_1, 1);\n            return (add_2,) \n```", "```py\ntorch.export.export(f, args, kwargs=None, *, constraints=None, dynamic_shapes=None, strict=True, preserve_module_call_signature=())\u00b6\n```", "```py\ndim = Dim(\"dim0_x\", max=5) \n```", "```py\ntorch.export.dynamic_dim(t, index)\u00b6\n```", "```py\n    t0 = torch.rand(2, 3)\n    t1 = torch.rand(3, 4)\n\n    # First dimension of t0 can be dynamic size rather than always being static size 2\n    constraints = [dynamic_dim(t0, 0)]\n    ep = export(fn, (t0, t1), constraints=constraints) \n    ```", "```py\n    t0 = torch.rand(10, 3)\n    t1 = torch.rand(3, 4)\n\n    # First dimension of t0 can be dynamic size with a lower bound of 5 (inclusive)\n    # Second dimension of t1 can be dynamic size with a lower bound of 2 (exclusive)\n    constraints = [\n        dynamic_dim(t0, 0) >= 5,\n        dynamic_dim(t1, 1) > 2,\n    ]\n    ep = export(fn, (t0, t1), constraints=constraints) \n    ```", "```py\n    t0 = torch.rand(10, 3)\n    t1 = torch.rand(3, 4)\n\n    # First dimension of t0 can be dynamic size with a upper bound of 16 (inclusive)\n    # Second dimension of t1 can be dynamic size with a upper bound of 8 (exclusive)\n    constraints = [\n        dynamic_dim(t0, 0) <= 16,\n        dynamic_dim(t1, 1) < 8,\n    ]\n    ep = export(fn, (t0, t1), constraints=constraints) \n    ```", "```py\n    t0 = torch.rand(10, 3)\n    t1 = torch.rand(3, 4)\n\n    # Sizes of second dimension of t0 and first dimension are always equal\n    constraints = [\n        dynamic_dim(t0, 1) == dynamic_dim(t1, 0),\n    ]\n    ep = export(fn, (t0, t1), constraints=constraints) \n    ```", "```py\ntorch.export.save(ep, f, *, extra_files=None, opset_version=None)\u00b6\n```", "```py\nimport torch\nimport io\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nep = torch.export.export(MyModule(), (torch.randn(5),))\n\n# Save to file\ntorch.export.save(ep, 'exported_program.pt2')\n\n# Save to io.BytesIO buffer\nbuffer = io.BytesIO()\ntorch.export.save(ep, buffer)\n\n# Save with extra files\nextra_files = {'foo.txt': b'bar'.decode('utf-8')}\ntorch.export.save(ep, 'exported_program.pt2', extra_files=extra_files) \n```", "```py\ntorch.export.load(f, *, extra_files=None, expected_opset_version=None)\u00b6\n```", "```py\nimport torch\nimport io\n\n# Load ExportedProgram from file\nep = torch.export.load('exported_program.pt2')\n\n# Load ExportedProgram from io.BytesIO object\nwith open('exported_program.pt2', 'rb') as f:\n    buffer = io.BytesIO(f.read())\nbuffer.seek(0)\nep = torch.export.load(buffer)\n\n# Load with extra files.\nextra_files = {'foo.txt': ''}  # values will be replaced with data\nep = torch.export.load('exported_program.pt2', extra_files=extra_files)\nprint(extra_files['foo.txt'])\nprint(ep(torch.randn(5))) \n```", "```py\ntorch.export.register_dataclass(cls)\u00b6\n```", "```py\n@dataclass\nclass InputDataClass:\n    feature: torch.Tensor\n    bias: int\n\nclass OutputDataClass:\n    res: torch.Tensor\n\ntorch.export.register_dataclass(InputDataClass)\ntorch.export.register_dataclass(OutputDataClass)\n\ndef fn(o: InputDataClass) -> torch.Tensor:\n    res = res=o.feature + o.bias\n    return OutputDataClass(res=res)\n\nep = torch.export.export(fn, (InputDataClass(torch.ones(2, 2), 1), ))\nprint(ep) \n```", "```py\ntorch.export.Dim(name, *, min=None, max=None)\u00b6\n```", "```py\ntorch.export.dims(*names, min=None, max=None)\u00b6\n```", "```py\nclass torch.export.Constraint(*args, **kwargs)\u00b6\n```", "```py\nclass torch.export.ExportedProgram(root, graph, graph_signature, state_dict, range_constraints, equality_constraints, module_call_graph, example_inputs=None, verifier=None, tensor_constants=None)\u00b6\n```", "```py\nmodule(*, flat=True)\u00b6\n```", "```py\nbuffers()\u00b6\n```", "```py\nnamed_buffers()\u00b6\n```", "```py\nparameters()\u00b6\n```", "```py\nnamed_parameters()\u00b6\n```", "```py\nclass torch.export.ExportBackwardSignature(gradients_to_parameters: Dict[str, str], gradients_to_user_inputs: Dict[str, str], loss_output: str)\u00b6\n```", "```py\nclass torch.export.ExportGraphSignature(input_specs, output_specs)\u00b6\n```", "```py\nInputs = [*parameters_buffers_constant_tensors, *flattened_user_inputs]\nOutputs = [*mutated_inputs, *flattened_user_outputs] \n```", "```py\nclass CustomModule(nn.Module):\n    def __init__(self):\n        super(CustomModule, self).__init__()\n\n        # Define a parameter\n        self.my_parameter = nn.Parameter(torch.tensor(2.0))\n\n        # Define two buffers\n        self.register_buffer('my_buffer1', torch.tensor(3.0))\n        self.register_buffer('my_buffer2', torch.tensor(4.0))\n\n    def forward(self, x1, x2):\n        # Use the parameter, buffers, and both inputs in the forward method\n        output = (x1 + self.my_parameter) * self.my_buffer1 + x2 * self.my_buffer2\n\n        # Mutate one of the buffers (e.g., increment it by 1)\n        self.my_buffer2.add_(1.0) # In-place addition\n\n        return output \n```", "```py\ngraph():\n    %arg0_1 := placeholder[target=arg0_1]\n    %arg1_1 := placeholder[target=arg1_1]\n    %arg2_1 := placeholder[target=arg2_1]\n    %arg3_1 := placeholder[target=arg3_1]\n    %arg4_1 := placeholder[target=arg4_1]\n    %add_tensor := call_function[target=torch.ops.aten.add.Tensor](args = (%arg3_1, %arg0_1), kwargs = {})\n    %mul_tensor := call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor, %arg1_1), kwargs = {})\n    %mul_tensor_1 := call_function[target=torch.ops.aten.mul.Tensor](args = (%arg4_1, %arg2_1), kwargs = {})\n    %add_tensor_1 := call_function[target=torch.ops.aten.add.Tensor](args = (%mul_tensor, %mul_tensor_1), kwargs = {})\n    %add_tensor_2 := call_function[target=torch.ops.aten.add.Tensor](args = (%arg2_1, 1.0), kwargs = {})\n    return (add_tensor_2, add_tensor_1) \n```", "```py\nExportGraphSignature(\n    input_specs=[\n        InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='arg0_1'), target='my_parameter'),\n        InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='arg1_1'), target='my_buffer1'),\n        InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='arg2_1'), target='my_buffer2'),\n        InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='arg3_1'), target=None),\n        InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='arg4_1'), target=None)\n    ],\n    output_specs=[\n        OutputSpec(kind=<OutputKind.BUFFER_MUTATION: 3>, arg=TensorArgument(name='add_2'), target='my_buffer2'),\n        OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add_1'), target=None)\n    ]\n) \n```", "```py\nclass torch.export.ModuleCallSignature(inputs: List[Union[torch.export.graph_signature.TensorArgument, torch.export.graph_signature.SymIntArgument, torch.export.graph_signature.ConstantArgument]], outputs: List[Union[torch.export.graph_signature.TensorArgument, torch.export.graph_signature.SymIntArgument, torch.export.graph_signature.ConstantArgument]], in_spec: torch.utils._pytree.TreeSpec, out_spec: torch.utils._pytree.TreeSpec)\u00b6\n```", "```py\nclass torch.export.ModuleCallEntry(fqn: str, signature: Union[torch.export.exported_program.ModuleCallSignature, NoneType] = None)\u00b6\n```", "```py\nclass torch.export.graph_signature.InputKind(value)\u00b6\n```", "```py\nclass torch.export.graph_signature.InputSpec(kind: torch.export.graph_signature.InputKind, arg: Union[torch.export.graph_signature.TensorArgument, torch.export.graph_signature.SymIntArgument, torch.export.graph_signature.ConstantArgument], target: Union[str, NoneType])\u00b6\n```", "```py\nclass torch.export.graph_signature.OutputKind(value)\u00b6\n```", "```py\nclass torch.export.graph_signature.OutputSpec(kind: torch.export.graph_signature.OutputKind, arg: Union[torch.export.graph_signature.TensorArgument, torch.export.graph_signature.SymIntArgument, torch.export.graph_signature.ConstantArgument], target: Union[str, NoneType])\u00b6\n```", "```py\nclass torch.export.graph_signature.ExportGraphSignature(input_specs, output_specs)\u00b6\n```", "```py\nInputs = [*parameters_buffers_constant_tensors, *flattened_user_inputs]\nOutputs = [*mutated_inputs, *flattened_user_outputs] \n```", "```py\nclass CustomModule(nn.Module):\n    def __init__(self):\n        super(CustomModule, self).__init__()\n\n        # Define a parameter\n        self.my_parameter = nn.Parameter(torch.tensor(2.0))\n\n        # Define two buffers\n        self.register_buffer('my_buffer1', torch.tensor(3.0))\n        self.register_buffer('my_buffer2', torch.tensor(4.0))\n\n    def forward(self, x1, x2):\n        # Use the parameter, buffers, and both inputs in the forward method\n        output = (x1 + self.my_parameter) * self.my_buffer1 + x2 * self.my_buffer2\n\n        # Mutate one of the buffers (e.g., increment it by 1)\n        self.my_buffer2.add_(1.0) # In-place addition\n\n        return output \n```", "```py\ngraph():\n    %arg0_1 := placeholder[target=arg0_1]\n    %arg1_1 := placeholder[target=arg1_1]\n    %arg2_1 := placeholder[target=arg2_1]\n    %arg3_1 := placeholder[target=arg3_1]\n    %arg4_1 := placeholder[target=arg4_1]\n    %add_tensor := call_function[target=torch.ops.aten.add.Tensor](args = (%arg3_1, %arg0_1), kwargs = {})\n    %mul_tensor := call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor, %arg1_1), kwargs = {})\n    %mul_tensor_1 := call_function[target=torch.ops.aten.mul.Tensor](args = (%arg4_1, %arg2_1), kwargs = {})\n    %add_tensor_1 := call_function[target=torch.ops.aten.add.Tensor](args = (%mul_tensor, %mul_tensor_1), kwargs = {})\n    %add_tensor_2 := call_function[target=torch.ops.aten.add.Tensor](args = (%arg2_1, 1.0), kwargs = {})\n    return (add_tensor_2, add_tensor_1) \n```", "```py\nExportGraphSignature(\n    input_specs=[\n        InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='arg0_1'), target='my_parameter'),\n        InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='arg1_1'), target='my_buffer1'),\n        InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='arg2_1'), target='my_buffer2'),\n        InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='arg3_1'), target=None),\n        InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='arg4_1'), target=None)\n    ],\n    output_specs=[\n        OutputSpec(kind=<OutputKind.BUFFER_MUTATION: 3>, arg=TensorArgument(name='add_2'), target='my_buffer2'),\n        OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add_1'), target=None)\n    ]\n) \n```", "```py\nreplace_all_uses(old, new)\u00b6\n```"]