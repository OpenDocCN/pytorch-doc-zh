- en: torchtext.transforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/text/stable/transforms.html](https://pytorch.org/text/stable/transforms.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Transforms are common text transforms. They can be chained together using [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential
    "(in PyTorch v2.1)") or using [`torchtext.transforms.Sequential`](#torchtext.transforms.Sequential
    "torchtext.transforms.Sequential") to support torch-scriptability.
  prefs: []
  type: TYPE_NORMAL
- en: SentencePieceTokenizer[](#sentencepiecetokenizer "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Transform for Sentence Piece tokenizer from pre-trained sentencepiece model
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional details: [https://github.com/google/sentencepiece](https://github.com/google/sentencepiece)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**sp_model_path** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – Path to pre-trained sentencepiece model'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Tutorials using `SentencePieceTokenizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SST-2 Binary text classification with XLM-RoBERTa model](../Images/98241cb68ab73fa3d56bc87944e16fd8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[SST-2 Binary text classification with XLM-RoBERTa model](tutorials/sst2_classification_non_distributed.html#sphx-glr-tutorials-sst2-classification-non-distributed-py)'
  prefs: []
  type: TYPE_NORMAL
- en: SST-2 Binary text classification with XLM-RoBERTa model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Union**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]*) – Input sentence or list of sentences on which to
    apply tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: tokenized text
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Union[List[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)")], List[List[[str](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")]]]
  prefs: []
  type: TYPE_NORMAL
- en: GPT2BPETokenizer[](#gpt2bpetokenizer "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Transform for GPT-2 BPE Tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Reimplements openai GPT-2 BPE in TorchScript. Original openai implementation
    [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**encoder_json_path** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – Path to GPT-2 BPE encoder json file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vocab_bpe_path** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – Path to bpe vocab file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_tokens** – Indicate whether to return split tokens. If False, it will
    return encoded token IDs as strings (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Union**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]*) – Input sentence or list of sentences on which to
    apply tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: tokenized text
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Union[List[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)")], List[List([str](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"))]]
  prefs: []
  type: TYPE_NORMAL
- en: CLIPTokenizer[](#cliptokenizer "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Transform for CLIP Tokenizer. Based on Byte-Level BPE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reimplements CLIP Tokenizer in TorchScript. Original implementation: [https://github.com/mlfoundations/open_clip/blob/main/src/clip/tokenizer.py](https://github.com/mlfoundations/open_clip/blob/main/src/clip/tokenizer.py)'
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer has been trained to treat spaces like parts of the tokens (a
    bit like sentencepiece) so a word will be encoded differently whether it is at
    the beginning of the sentence (without space) or not.
  prefs: []
  type: TYPE_NORMAL
- en: The below code snippet shows how to use the CLIP tokenizer with encoder and
    merges file taken from the original paper implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**merges_path** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – Path to bpe merges file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_json_path** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – Optional, path to BPE encoder json file. When specified,
    this is used to infer num_merges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_merges** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – Optional, number of merges to read from the bpe merges
    file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_tokens** – Indicate whether to return split tokens. If False, it will
    return encoded token IDs as strings (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Union**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]*) – Input sentence or list of sentences on which to
    apply tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: tokenized text
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Union[List[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)")], List[List([str](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"))]]
  prefs: []
  type: TYPE_NORMAL
- en: RegexTokenizer[](#regextokenizer "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Regex tokenizer for a string sentence that applies all regex replacements defined
    in patterns_list. It is backed by the [C++ RE2 regular expression engine](https://github.com/google/re2)
    from Google.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**patterns_list** (*List**[**Tuple**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* [*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]*) – a list of tuples (ordered pairs) which contain the
    regex pattern string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**element.** (*as the first element and the replacement string as the second*)
    –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caveats
  prefs: []
  type: TYPE_NORMAL
- en: The RE2 library does not support arbitrary lookahead or lookbehind assertions,
    nor does it support backreferences. Look at the [docs](https://swtch.com/~rsc/regexp/regexp3.html#caveats)
    here for more info.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final tokenization step always uses spaces as separators. To split strings
    based on a specific regex pattern, similar to Python’s [re.split](https://docs.python.org/3/library/re.html#re.split),
    a tuple of `('<regex_pattern>', ' ')` can be provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: Regex tokenization based on `(patterns, replacements)` list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Regex tokenization based on `(single_pattern, ' ')` list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**lines** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in
    Python v3.12)")) – a text string to tokenize.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: a token list after regex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)")]
  prefs: []
  type: TYPE_NORMAL
- en: BERTTokenizer[](#berttokenizer "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Transform for BERT Tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on WordPiece algorithm introduced in paper: [https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The backend kernel implementation is taken and modified from [https://github.com/LieluoboAi/radish](https://github.com/LieluoboAi/radish).
  prefs: []
  type: TYPE_NORMAL
- en: See PR [https://github.com/pytorch/text/pull/1707](https://github.com/pytorch/text/pull/1707)
    summary for more details.
  prefs: []
  type: TYPE_NORMAL
- en: The below code snippet shows how to use the BERT tokenizer using the pre-trained
    vocab files.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_path** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – Path to pre-trained vocabulary file. The path can be either
    local or URL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_lower_case** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*]*) – Indicate whether to do lower case. (default: True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**strip_accents** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*]*) – Indicate whether to strip accents. (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_tokens** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Indicate whether to return tokens. If false, returns corresponding
    token IDs as strings (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**never_split** (*Optional**[**List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]*) – Collection of tokens which will not be split during
    tokenization. (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Union**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]*) – Input sentence or list of sentences on which to
    apply tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: tokenized text
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Union[List[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)")], List[List([str](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"))]]
  prefs: []
  type: TYPE_NORMAL
- en: VocabTransform[](#vocabtransform "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Vocab transform to convert input batch of tokens into corresponding token ids
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab** – an instance of [`torchtext.vocab.Vocab`](vocab.html#torchtext.vocab.Vocab
    "torchtext.vocab.Vocab") class.'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Tutorials using `VocabTransform`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SST-2 Binary text classification with XLM-RoBERTa model](../Images/98241cb68ab73fa3d56bc87944e16fd8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[SST-2 Binary text classification with XLM-RoBERTa model](tutorials/sst2_classification_non_distributed.html#sphx-glr-tutorials-sst2-classification-non-distributed-py)'
  prefs: []
  type: TYPE_NORMAL
- en: SST-2 Binary text classification with XLM-RoBERTa model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Union**[**List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**,* *List**[**List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]**]*) – Input batch of token to convert to correspnding
    token ids'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: Converted input into corresponding token ids
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Union[List[[int](https://docs.python.org/3/library/functions.html#int "(in Python
    v3.12)")], List[List[[int](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")]]]
  prefs: []
  type: TYPE_NORMAL
- en: ToTensor[](#totensor "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Convert input to torch tensor
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**padding_value** (*Optional**[*[*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*]*) – Pad value to make each input in the batch of length
    equal to the longest sequence in the batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dtype** ([`torch.dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype
    "(in PyTorch v2.1)")) – [`torch.dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype
    "(in PyTorch v2.1)") of output tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Union**[**List**[*[*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*]**,* *List**[**List**[*[*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*]**]**]*) – Sequence or batch of token ids'
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor
  prefs: []
  type: TYPE_NORMAL
- en: LabelToIndex[](#labeltoindex "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Transform labels from string names to ids.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**label_names** (*Optional**[**List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]*) – a list of unique label names'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**label_path** (*Optional**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]*) – a path to file containing unique label names containing
    1 label per line. Note that either label_names or label_path should be supplied
    but not both.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Union**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]*) – Input labels to convert to corresponding ids'
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Union[[int](https://docs.python.org/3/library/functions.html#int "(in Python
    v3.12)"), List[[int](https://docs.python.org/3/library/functions.html#int "(in
    Python v3.12)")]]
  prefs: []
  type: TYPE_NORMAL
- en: Truncate[](#truncate "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Truncate input sequence
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**max_seq_len** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – The maximum allowable length for input sequence'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tutorials using `Truncate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SST-2 Binary text classification with XLM-RoBERTa model](../Images/98241cb68ab73fa3d56bc87944e16fd8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[SST-2 Binary text classification with XLM-RoBERTa model](tutorials/sst2_classification_non_distributed.html#sphx-glr-tutorials-sst2-classification-non-distributed-py)'
  prefs: []
  type: TYPE_NORMAL
- en: SST-2 Binary text classification with XLM-RoBERTa model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Union**[**List**[**Union**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*]**]**,* *List**[**List**[**Union**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*]**]**]**]*) – Input sequence or batch of sequence to be
    truncated'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: Truncated sequence
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Union[List[Union[[str](https://docs.python.org/3/library/stdtypes.html#str "(in
    Python v3.12)"), [int](https://docs.python.org/3/library/functions.html#int "(in
    Python v3.12)")]], List[List[Union[[str](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [int](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")]]]]
  prefs: []
  type: TYPE_NORMAL
- en: AddToken[](#addtoken "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Add token to beginning or end of sequence
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**token** (*Union**[*[*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* [*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]*) – The token to be added'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**begin** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in
    Python v3.12)")*,* *optional*) – Whether to insert token at start or end or sequence,
    defaults to True'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tutorials using `AddToken`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SST-2 Binary text classification with XLM-RoBERTa model](../Images/98241cb68ab73fa3d56bc87944e16fd8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[SST-2 Binary text classification with XLM-RoBERTa model](tutorials/sst2_classification_non_distributed.html#sphx-glr-tutorials-sst2-classification-non-distributed-py)'
  prefs: []
  type: TYPE_NORMAL
- en: SST-2 Binary text classification with XLM-RoBERTa model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Union**[**List**[**Union**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*]**]**,* *List**[**List**[**Union**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*]**]**]**]*) – Input sequence or batch'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential[](#sequential "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: A container to host a sequence of text transforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tutorials using `Sequential`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SST-2 Binary text classification with XLM-RoBERTa model](../Images/98241cb68ab73fa3d56bc87944e16fd8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[SST-2 Binary text classification with XLM-RoBERTa model](tutorials/sst2_classification_non_distributed.html#sphx-glr-tutorials-sst2-classification-non-distributed-py)'
  prefs: []
  type: TYPE_NORMAL
- en: SST-2 Binary text classification with XLM-RoBERTa model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (Any) – Input sequence or batch. The input type must be supported
    by the first transform in the sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: PadTransform[](#padtransform "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Pad tensor to a fixed length with given padding value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**max_length** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – Maximum length to pad to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_value** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Value to pad the tensor with'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**x** (*Tensor*) – The tensor to pad'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor padded up to max_length with pad_value
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor
  prefs: []
  type: TYPE_NORMAL
- en: StrToIntTransform[](#strtointtransform "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Convert string tokens to integers (either single sequence or batch).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Union**[**List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**,* *List**[**List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]**]*) – sequence or batch of string tokens to convert'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: sequence or batch converted into corresponding token ids
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Union[List[[int](https://docs.python.org/3/library/functions.html#int "(in Python
    v3.12)")], List[List[[int](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")]]]
  prefs: []
  type: TYPE_NORMAL
- en: CharBPETokenizer[](#charbpetokenizer "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Transform for a Character Byte-Pair-Encoding Tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: ':param : param bpe_encoder_path: Path to the BPE encoder json file. :param
    : type bpe_encoder_path: str :param : param bpe_merges_path: Path to the BPE merges
    text file. :param : type bpe_merges_path: str :param : param return_tokens: Indicate
    whether to return split tokens. If False, it will return encoded token IDs (default:
    False). :param : type return_tokens: bool :param : param unk_token: The unknown
    token. If provided, it must exist in encoder. :param : type unk_token: Optional[str]
    :param : param suffix: The suffix to be used for every subword that is an end-of-word.
    :param : type suffix: Optional[str] :param : param special_tokens: Special tokens
    which should not be split into individual characters. If provided, these must
    exist in encoder. :param : type special_tokens: Optional[List[str]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Forward method of module encodes strings or list of strings into token ids
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** – Input sentence or list of sentences on which to apply tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: A list or list of lists of token IDs
  prefs: []
  type: TYPE_NORMAL
