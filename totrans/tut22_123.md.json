["```py\nimport threading\nimport torchvision\nimport torch\nimport torch.distributed.rpc as rpc\nfrom torch import optim\n\nnum_classes, batch_update_size = 30, 5\n\nclass BatchUpdateParameterServer(object):\n    def __init__(self, batch_update_size=batch_update_size):\n        self.model = torchvision.models.resnet50(num_classes=num_classes)\n        self.lock = threading.Lock()\n        self.future_model = torch.futures.Future()\n        self.batch_update_size = batch_update_size\n        self.curr_update_size = 0\n        self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n        for p in self.model.parameters():\n            p.grad = torch.zeros_like(p)\n\n    def get_model(self):\n        return self.model\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def update_and_fetch_model(ps_rref, grads):\n        # Using the RRef to retrieve the local PS instance\n        self = ps_rref.local_value()\n        with self.lock:\n            self.curr_update_size += 1\n            # accumulate gradients into .grad field\n            for p, g in zip(self.model.parameters(), grads):\n                p.grad += g\n\n            # Save the current future_model and return it to make sure the\n            # returned Future object holds the correct model even if another\n            # thread modifies future_model before this thread returns.\n            fut = self.future_model\n\n            if self.curr_update_size >= self.batch_update_size:\n                # update the model\n                for p in self.model.parameters():\n                    p.grad /= self.batch_update_size\n                self.curr_update_size = 0\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n                # by settiing the result on the Future object, all previous\n                # requests expecting this updated model will be notified and\n                # the their responses will be sent accordingly.\n                fut.set_result(self.model)\n                self.future_model = torch.futures.Future()\n\n        return fut \n```", "```py\nbatch_size, image_w, image_h  = 20, 64, 64\n\nclass Trainer(object):\n    def __init__(self, ps_rref):\n        self.ps_rref, self.loss_fn = ps_rref, torch.nn.MSELoss()\n        self.one_hot_indices = torch.LongTensor(batch_size) \\\n                                    .random_(0, num_classes) \\\n                                    .view(batch_size, 1)\n\n    def get_next_batch(self):\n        for _ in range(6):\n            inputs = torch.randn(batch_size, 3, image_w, image_h)\n            labels = torch.zeros(batch_size, num_classes) \\\n                        .scatter_(1, self.one_hot_indices, 1)\n            yield inputs.cuda(), labels.cuda()\n\n    def train(self):\n        name = rpc.get_worker_info().name\n        # get initial model parameters\n        m = self.ps_rref.rpc_sync().get_model().cuda()\n        # start training\n        for inputs, labels in self.get_next_batch():\n            self.loss_fn(m(inputs), labels).backward()\n            m = rpc.rpc_sync(\n                self.ps_rref.owner(),\n                BatchUpdateParameterServer.update_and_fetch_model,\n                args=(self.ps_rref, [p.grad for p in m.cpu().parameters()]),\n            ).cuda() \n```", "```py\nimport argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nparser = argparse.ArgumentParser(description='PyTorch RPC Batch RL example')\nparser.add_argument('--gamma', type=float, default=1.0, metavar='G',\n                    help='discount factor (default: 1.0)')\nparser.add_argument('--seed', type=int, default=543, metavar='N',\n                    help='random seed (default: 543)')\nparser.add_argument('--num-episode', type=int, default=10, metavar='E',\n                    help='number of episodes (default: 10)')\nargs = parser.parse_args()\n\ntorch.manual_seed(args.seed)\n\nclass Policy(nn.Module):\n    def __init__(self, batch=True):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n        self.dim = 2 if batch else 1\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=self.dim) \n```", "```py\nimport gym\nimport torch.distributed.rpc as rpc\n\nclass Observer:\n    def __init__(self, batch=True):\n        self.id = rpc.get_worker_info().id - 1\n        self.env = gym.make('CartPole-v1')\n        self.env.seed(args.seed)\n        self.select_action = Agent.select_action_batch if batch else Agent.select_action \n```", "```py\nimport torch\n\nclass Observer:\n    ...\n\n    def run_episode(self, agent_rref, n_steps):\n        state, ep_reward = self.env.reset(), NUM_STEPS\n        rewards = torch.zeros(n_steps)\n        start_step = 0\n        for step in range(n_steps):\n            state = torch.from_numpy(state).float().unsqueeze(0)\n            # send the state to the agent to get an action\n            action = rpc.rpc_sync(\n                agent_rref.owner(),\n                self.select_action,\n                args=(agent_rref, self.id, state)\n            )\n\n            # apply the action to the environment, and get the reward\n            state, reward, done, _ = self.env.step(action)\n            rewards[step] = reward\n\n            if done or step + 1 >= n_steps:\n                curr_rewards = rewards[start_step:(step + 1)]\n                R = 0\n                for i in range(curr_rewards.numel() -1, -1, -1):\n                    R = curr_rewards[i] + args.gamma * R\n                    curr_rewards[i] = R\n                state = self.env.reset()\n                if start_step == 0:\n                    ep_reward = min(ep_reward, step - start_step + 1)\n                start_step = step + 1\n\n        return [rewards, ep_reward] \n```", "```py\nimport threading\nfrom torch.distributed.rpc import RRef\n\nclass Agent:\n    def __init__(self, world_size, batch=True):\n        self.ob_rrefs = []\n        self.agent_rref = RRef(self)\n        self.rewards = {}\n        self.policy = Policy(batch).cuda()\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n        self.running_reward = 0\n\n        for ob_rank in range(1, world_size):\n            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n            self.ob_rrefs.append(rpc.remote(ob_info, Observer, args=(batch,)))\n            self.rewards[ob_info.id] = []\n\n        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n        self.batch = batch\n        self.saved_log_probs = [] if batch else {k:[] for k in range(len(self.ob_rrefs))}\n        self.future_actions = torch.futures.Future()\n        self.lock = threading.Lock()\n        self.pending_states = len(self.ob_rrefs) \n```", "```py\nfrom torch.distributions import Categorical\n\nclass Agent:\n    ...\n\n    @staticmethod\n    def select_action(agent_rref, ob_id, state):\n        self = agent_rref.local_value()\n        probs = self.policy(state.cuda())\n        m = Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs[ob_id].append(m.log_prob(action))\n        return action.item() \n```", "```py\nclass Agent:\n    ...\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def select_action_batch(agent_rref, ob_id, state):\n        self = agent_rref.local_value()\n        self.states[ob_id].copy_(state)\n        future_action = self.future_actions.then(\n            lambda future_actions: future_actions.wait()[ob_id].item()\n        )\n\n        with self.lock:\n            self.pending_states -= 1\n            if self.pending_states == 0:\n                self.pending_states = len(self.ob_rrefs)\n                probs = self.policy(self.states.cuda())\n                m = Categorical(probs)\n                actions = m.sample()\n                self.saved_log_probs.append(m.log_prob(actions).t()[0])\n                future_actions = self.future_actions\n                self.future_actions = torch.futures.Future()\n                future_actions.set_result(actions.cpu())\n        return future_action \n```", "```py\nclass Agent:\n    ...\n\n    def run_episode(self, n_steps=0):\n        futs = []\n        for ob_rref in self.ob_rrefs:\n            # make async RPC to kick off an episode on all observers\n            futs.append(ob_rref.rpc_async().run_episode(self.agent_rref, n_steps))\n\n        # wait until all obervers have finished this episode\n        rets = torch.futures.wait_all(futs)\n        rewards = torch.stack([ret[0] for ret in rets]).cuda().t()\n        ep_rewards = sum([ret[1] for ret in rets]) / len(rets)\n\n        # stack saved probs into one tensor\n        if self.batch:\n            probs = torch.stack(self.saved_log_probs)\n        else:\n            probs = [torch.stack(self.saved_log_probs[i]) for i in range(len(rets))]\n            probs = torch.stack(probs)\n\n        policy_loss = -probs * rewards / len(rets)\n        policy_loss.sum().backward()\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n\n        # reset variables\n        self.saved_log_probs = [] if self.batch else {k:[] for k in range(len(self.ob_rrefs))}\n        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n\n        # calculate running rewards\n        self.running_reward = 0.5 * ep_rewards + 0.5 * self.running_reward\n        return ep_rewards, self.running_reward \n```", "```py\ndef run_worker(rank, world_size, n_episode, batch, print_log=True):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 0:\n        # rank0 is the agent\n        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n\n        agent = Agent(world_size, batch)\n        for i_episode in range(n_episode):\n            last_reward, running_reward = agent.run_episode(n_steps=NUM_STEPS)\n\n            if print_log:\n                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n                    i_episode, last_reward, running_reward))\n    else:\n        # other ranks are the observer\n        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n        # observers passively waiting for instructions from agents\n    rpc.shutdown()\n\ndef main():\n    for world_size in range(2, 12):\n        delays = []\n        for batch in [True, False]:\n            tik = time.time()\n            mp.spawn(\n                run_worker,\n                args=(world_size, args.num_episode, batch),\n                nprocs=world_size,\n                join=True\n            )\n            tok = time.time()\n            delays.append(tok - tik)\n\n        print(f\"{world_size}, {delays[0]}, {delays[1]}\")\n\nif __name__ == '__main__':\n    main() \n```"]