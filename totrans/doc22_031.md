# torch

> 原文：[https://pytorch.org/docs/stable/torch.html](https://pytorch.org/docs/stable/torch.html)

torch包含用于多维张量的数据结构，并定义了这些张量上的数学操作。此外，它还提供了许多用于高效序列化张量和任意类型的工具，以及其他有用的实用程序。

它有一个CUDA对应项，可以让您在具有计算能力>= 3.0的NVIDIA GPU上运行张量计算。

## 张量

| [`is_tensor`](generated/torch.is_tensor.html#torch.is_tensor "torch.is_tensor") | 如果obj是PyTorch张量，则返回True。 |
| --- | --- |
| [`is_storage`](generated/torch.is_storage.html#torch.is_storage "torch.is_storage") | 如果obj是PyTorch存储对象，则返回True。 |
| [`is_complex`](generated/torch.is_complex.html#torch.is_complex "torch.is_complex") | 如果`input`的数据类型是复数数据类型，即`torch.complex64`和`torch.complex128`之一，则返回True。 |
| [`is_conj`](generated/torch.is_conj.html#torch.is_conj "torch.is_conj") | 如果`input`是一个共轭张量，即其共轭位设置为True，则返回True。 |
| [`is_floating_point`](generated/torch.is_floating_point.html#torch.is_floating_point "torch.is_floating_point") | 如果`input`的数据类型是浮点数据类型，即`torch.float64`、`torch.float32`、`torch.float16`和`torch.bfloat16`之一，则返回True。 |
| [`is_nonzero`](generated/torch.is_nonzero.html#torch.is_nonzero "torch.is_nonzero") | 如果`input`是一个经过类型转换后不等于零的单个元素张量，则返回True。 |
| [`set_default_dtype`](generated/torch.set_default_dtype.html#torch.set_default_dtype "torch.set_default_dtype") | 将默认的浮点dtype设置为`d`。 |
| [`get_default_dtype`](generated/torch.get_default_dtype.html#torch.get_default_dtype "torch.get_default_dtype") | 获取当前默认的浮点[`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype")。 |
| [`set_default_device`](generated/torch.set_default_device.html#torch.set_default_device "torch.set_default_device") | 将默认的`torch.Tensor`分配到`device`上。 |
| [`set_default_tensor_type`](generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type "torch.set_default_tensor_type") | 将默认的`torch.Tensor`类型设置为浮点张量类型`t`。 |
| [`numel`](generated/torch.numel.html#torch.numel "torch.numel") | 返回`input`张量中的总元素数。 |
| [`set_printoptions`](generated/torch.set_printoptions.html#torch.set_printoptions "torch.set_printoptions") | 设置打印选项。 |
| [`set_flush_denormal`](generated/torch.set_flush_denormal.html#torch.set_flush_denormal "torch.set_flush_denormal") | 禁用CPU上的非规格化浮点数。 |

### 创建操作

注意

随机抽样创建操作列在[随机抽样](#random-sampling)下，包括：[`torch.rand()`](generated/torch.rand.html#torch.rand "torch.rand") [`torch.rand_like()`](generated/torch.rand_like.html#torch.rand_like "torch.rand_like") [`torch.randn()`](generated/torch.randn.html#torch.randn "torch.randn") [`torch.randn_like()`](generated/torch.randn_like.html#torch.randn_like "torch.randn_like") [`torch.randint()`](generated/torch.randint.html#torch.randint "torch.randint") [`torch.randint_like()`](generated/torch.randint_like.html#torch.randint_like "torch.randint_like") [`torch.randperm()`](generated/torch.randperm.html#torch.randperm "torch.randperm") 您还可以使用[`torch.empty()`](generated/torch.empty.html#torch.empty "torch.empty")与[原地随机抽样](#inplace-random-sampling)方法一起创建从更广泛的分布中抽样值的[`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor")。

| [`tensor`](generated/torch.tensor.html#torch.tensor "torch.tensor") | 通过复制`data`构建一个没有自动求导历史的张量（也称为“叶子张量”，参见[自动求导机制](notes/autograd.html)）。 |
| --- | --- |
| [`sparse_coo_tensor`](generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor "torch.sparse_coo_tensor") | 使用给定的`indices`构建一个COO（坐标）格式的稀疏张量。 |
| [`sparse_csr_tensor`](generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor "torch.sparse_csr_tensor") | 使用给定的`crow_indices`和`col_indices`构建一个CSR（压缩稀疏行）格式的稀疏张量。 |
| [`sparse_csc_tensor`](generated/torch.sparse_csc_tensor.html#torch.sparse_csc_tensor "torch.sparse_csc_tensor") | 使用给定的`ccol_indices`和`row_indices`构建一个CSC（压缩稀疏列）格式的稀疏张量。 |
| [`sparse_bsr_tensor`](generated/torch.sparse_bsr_tensor.html#torch.sparse_bsr_tensor "torch.sparse_bsr_tensor") | 使用给定的`crow_indices`和`col_indices`构建一个BSR（块压缩稀疏行）格式的稀疏张量。 |
| [`sparse_bsc_tensor`](generated/torch.sparse_bsc_tensor.html#torch.sparse_bsc_tensor "torch.sparse_bsc_tensor") | 使用给定的`ccol_indices`和`row_indices`构建一个BSC（块压缩稀疏列）格式的稀疏张量。 |
| [`asarray`](generated/torch.asarray.html#torch.asarray "torch.asarray") | 将`obj`转换为张量。 |
| [`as_tensor`](generated/torch.as_tensor.html#torch.as_tensor "torch.as_tensor") | 将`data`转换为张量，如果可能的话共享数据并保留自动求导历史。 |
| [`as_strided`](generated/torch.as_strided.html#torch.as_strided "torch.as_strided") | 使用指定的`size`、`stride`和`storage_offset`创建一个现有torch.Tensor `input`的视图。 |
| [`from_file`](generated/torch.from_file.html#torch.from_file "torch.from_file") | 创建一个由内存映射文件支持的CPU张量。 |
| [`from_numpy`](generated/torch.from_numpy.html#torch.from_numpy "torch.from_numpy") | 从一个[`numpy.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray)创建一个[`Tensor`](tensors.html#torch.Tensor)。 |
| [`from_dlpack`](generated/torch.from_dlpack.html#torch.from_dlpack "torch.from_dlpack") | 将来自外部库的张量转换为`torch.Tensor`。 |
| [`frombuffer`](generated/torch.frombuffer.html#torch.frombuffer "torch.frombuffer") | 从实现Python缓冲区协议的对象创建一个1维张量。 |
| [`zeros`](generated/torch.zeros.html#torch.zeros "torch.zeros") | 返回一个填充了标量值0且形状由可变参数`size`定义的张量。 |
| [`zeros_like`](generated/torch.zeros_like.html#torch.zeros_like "torch.zeros_like") | 返回一个与`input`相同大小且填充了标量值0的张量。 |
| [`ones`](generated/torch.ones.html#torch.ones "torch.ones") | 返回一个填充了标量值1且形状由可变参数`size`定义的张量。 |
| [`ones_like`](generated/torch.ones_like.html#torch.ones_like "torch.ones_like") | 返回一个与`input`相同大小且填充了标量值1的张量。 |
| [`arange`](generated/torch.arange.html#torch.arange "torch.arange") | 返回一个大小为$\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil$的一维张量，其值来自区间`[start, end)`，以`step`为公差从`start`开始。 |
| [`range`](生成/torch.range.html#torch.range "torch.range") | 返回一个大小为$\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1$的一维张量，其值从`start`到`end`，步长为`step`。 |
| [`linspace`](生成/torch.linspace.html#torch.linspace "torch.linspace") | 创建一个大小为`steps`的一维张量，其值从`start`到`end`均匀间隔。 |
| [`logspace`](生成/torch.logspace.html#torch.logspace "torch.logspace") | 创建一个大小为`steps`的一维张量，其值在对数刻度上从${{\text{{base}}}}^{{\text{{start}}}}$basestart到${{\text{{base}}}}^{{\text{{end}}}}$baseend均匀间隔。 |
| [`eye`](生成/torch.eye.html#torch.eye "torch.eye") | 返回一个对角线为1，其他位置为0的二维张量。 |
| [`empty`](生成/torch.empty.html#torch.empty "torch.empty") | 返回一个填充未初始化数据的张量。 |
| [`empty_like`](生成/torch.empty_like.html#torch.empty_like "torch.empty_like") | 返回一个与`input`大小相同的未初始化张量。 |
| [`empty_strided`](生成/torch.empty_strided.html#torch.empty_strided "torch.empty_strided") | 创建一个指定`size`和`stride`的张量，并填充未定义数据。 |
| [`full`](生成/torch.full.html#torch.full "torch.full") | 创建一个大小为`size`且填充为`fill_value`的张量。 |
| [`full_like`](生成/torch.full_like.html#torch.full_like "torch.full_like") | 返回一个与`input`大小相同且填充为`fill_value`的张量。 |
| [`quantize_per_tensor`](生成/torch.quantize_per_tensor.html#torch.quantize_per_tensor "torch.quantize_per_tensor") | 将浮点张量转换为具有给定比例和零点的量化张量。 |
| [`quantize_per_channel`](生成/torch.quantize_per_channel.html#torch.quantize_per_channel "torch.quantize_per_channel") | 将浮点张量转换为具有给定比例和零点的按通道量化张量。 |
| [`dequantize`](生成/torch.dequantize.html#torch.dequantize "torch.dequantize") | 通过去量化一个量化张量返回一个fp32张量。 |
| [`complex`](生成/torch.complex.html#torch.complex "torch.complex") | 构造一个复数张量，其实部等于[`real`](生成/torch.real.html#torch.real "torch.real")，虚部等于[`imag`](生成/torch.imag.html#torch.imag "torch.imag")。 |
| [`polar`](生成/torch.polar.html#torch.polar "torch.polar") | 构造一个复数张量，其元素是对应于绝对值[`abs`](生成/torch.abs.html#torch.abs "torch.abs")和角度[`angle`](生成/torch.angle.html#torch.angle "torch.angle")的极坐标。 |

| [`heaviside`](生成/torch.heaviside.html#torch.heaviside "torch.heaviside") | 计算`input`中每个元素的Heaviside阶跃函数。 |  ### 索引、切片、连接、变异操作[](#indexing-slicing-joining-mutating-ops "跳转到此标题的永久链接")

| [`adjoint`](生成/torch.adjoint.html#torch.adjoint "torch.adjoint") | 返回一个共轭并且最后两个维度转置的张量视图。 |
| --- | --- |
| [`argwhere`](生成/torch.argwhere.html#torch.argwhere "torch.argwhere") | 返回一个包含`input`所有非零元素索引的张量。 |
| [`cat`](生成/torch.cat.html#torch.cat "torch.cat") | 在给定维度上连接给定序列`seq`的张量。 |
| [`concat`](生成/torch.concat.html#torch.concat "torch.concat") | [`torch.cat()`](生成/torch.cat.html#torch.cat "torch.cat")的别名。 |
| [`concatenate`](生成/torch.concatenate.html#torch.concatenate "torch.concatenate") | [`torch.cat()`](生成/torch.cat.html#torch.cat "torch.cat")的别名。 |
| [`conj`](生成/torch.conj.html#torch.conj "torch.conj") | 返回一个具有翻转共轭位的`input`视图。 |
| [`chunk`](generated/torch.chunk.html#torch.chunk "torch.chunk") | 尝试将张量分割成指定数量的块。 |
| [`dsplit`](generated/torch.dsplit.html#torch.dsplit "torch.dsplit") | 根据`indices_or_sections`，将具有三个或更多维度的`input`张量沿深度方向分割成多个张量。 |
| [`column_stack`](generated/torch.column_stack.html#torch.column_stack "torch.column_stack") | 通过水平堆叠`tensors`在`tensors`中创建一个新的张量。 |
| [`dstack`](generated/torch.dstack.html#torch.dstack "torch.dstack") | 深度顺序堆叠张量（沿第三轴）。 |
| [`gather`](generated/torch.gather.html#torch.gather "torch.gather") | 沿着由dim指定的轴收集值。 |
| [`hsplit`](generated/torch.hsplit.html#torch.hsplit "torch.hsplit") | 根据`indices_or_sections`，将具有一个或多个维度的`input`张量水平分割成多个张量。 |
| [`hstack`](generated/torch.hstack.html#torch.hstack "torch.hstack") | 水平顺序堆叠张量（按列）。 |
| [`index_add`](generated/torch.index_add.html#torch.index_add "torch.index_add") | 有关函数描述，请参阅[`index_add_()`](generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_ "torch.Tensor.index_add_")。 |
| [`index_copy`](generated/torch.index_copy.html#torch.index_copy "torch.index_copy") | 有关函数描述，请参阅[`index_add_()`](generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_ "torch.Tensor.index_add_")。 |
| [`index_reduce`](generated/torch.index_reduce.html#torch.index_reduce "torch.index_reduce") | 有关函数描述，请参阅[`index_reduce_()`](generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_ "torch.Tensor.index_reduce_")。 |
| [`index_select`](generated/torch.index_select.html#torch.index_select "torch.index_select") | 返回一个新的张量，沿着维度`dim`使用`index`中的条目对`input`张量进行索引，其中`index`是一个LongTensor。 |
| [`masked_select`](generated/torch.masked_select.html#torch.masked_select "torch.masked_select") | 返回一个新的1-D张量，根据布尔掩码`mask`（BoolTensor）对`input`张量进行索引。 |
| [`movedim`](generated/torch.movedim.html#torch.movedim "torch.movedim") | 将`input`张量的维度移动到`source`中的位置到`destination`中的位置。 |
| [`moveaxis`](generated/torch.moveaxis.html#torch.moveaxis "torch.moveaxis") | [`torch.movedim()`](generated/torch.movedim.html#torch.movedim "torch.movedim")的别名。 |
| [`narrow`](generated/torch.narrow.html#torch.narrow "torch.narrow") | 返回一个缩小版本的`input`张量的新张量。 |
| [`narrow_copy`](generated/torch.narrow_copy.html#torch.narrow_copy "torch.narrow_copy") | 与[`Tensor.narrow()`](generated/torch.Tensor.narrow.html#torch.Tensor.narrow "torch.Tensor.narrow")相同，除了这里返回的是副本而不是共享存储。 |
| [`nonzero`](generated/torch.nonzero.html#torch.nonzero "torch.nonzero") |  |
| [`permute`](generated/torch.permute.html#torch.permute "torch.permute") | 返回原始张量`input`的维度重新排列视图。 |
| [`reshape`](generated/torch.reshape.html#torch.reshape "torch.reshape") | 返回一个与`input`具有相同数据和元素数量的张量，但具有指定的形状。 |
| [`row_stack`](generated/torch.row_stack.html#torch.row_stack "torch.row_stack") | [`torch.vstack()`](generated/torch.vstack.html#torch.vstack "torch.vstack")的别名。 |
| [`select`](generated/torch.select.html#torch.select "torch.select") | 在给定索引处沿所选维度切片`input`张量。 |
| [`scatter`](generated/torch.scatter.html#torch.scatter "torch.scatter") | [`torch.Tensor.scatter_()`](generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_ "torch.Tensor.scatter_")的就地版本 |
| [`diagonal_scatter`](generated/torch.diagonal_scatter.html#torch.diagonal_scatter "torch.diagonal_scatter") | 将`src`张量的值嵌入到`input`的对角线元素中，相对于`dim1`和`dim2`。 |
| [`select_scatter`](generated/torch.select_scatter.html#torch.select_scatter "torch.select_scatter") | 将`src`张量的值嵌入到给定索引的`input`中。 |
| [`slice_scatter`](generated/torch.slice_scatter.html#torch.slice_scatter "torch.slice_scatter") | 将`src`张量的值嵌入到给定维度的`input`中。 |
| [`scatter_add`](generated/torch.scatter_add.html#torch.scatter_add "torch.scatter_add") | [`torch.Tensor.scatter_add_()`](generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_ "torch.Tensor.scatter_add_")的就地版本。 |
| [`scatter_reduce`](generated/torch.scatter_reduce.html#torch.scatter_reduce "torch.scatter_reduce") | [`torch.Tensor.scatter_reduce_()`](generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_ "torch.Tensor.scatter_reduce_")的就地版本。 |
| [`split`](generated/torch.split.html#torch.split "torch.split") | 将张量分割成多个块。 |
| [`squeeze`](generated/torch.squeeze.html#torch.squeeze "torch.squeeze") | 返回一个去除`input`中所有指定大小为1的维度的张量。 |
| [`stack`](generated/torch.stack.html#torch.stack "torch.stack") | 沿着新维度连接一系列张量。 |
| [`swapaxes`](generated/torch.swapaxes.html#torch.swapaxes "torch.swapaxes") | [`torch.transpose()`](generated/torch.transpose.html#torch.transpose "torch.transpose")的别名。 |
| [`swapdims`](generated/torch.swapdims.html#torch.swapdims "torch.swapdims") | [`torch.transpose()`](generated/torch.transpose.html#torch.transpose "torch.transpose")的别名。 |
| [`t`](generated/torch.t.html#torch.t "torch.t") | 期望`input`是 <= 2-D 张量，并转置维度0和1。 |
| [`take`](generated/torch.take.html#torch.take "torch.take") | 返回一个包含`input`在给定索引处的元素的新张量。 |
| [`take_along_dim`](generated/torch.take_along_dim.html#torch.take_along_dim "torch.take_along_dim") | 在给定`dim`上沿着`indices`的一维索引从`input`中选择值。 |
| [`tensor_split`](generated/torch.tensor_split.html#torch.tensor_split "torch.tensor_split") | 将张量沿着维度`dim`根据`indices_or_sections`指定的索引或分段数拆分为多个子张量，所有这些子张量都是`input`的视图。 |
| [`tile`](generated/torch.tile.html#torch.tile "torch.tile") | 通过重复`input`的元素构造张量。 |
| [`transpose`](generated/torch.transpose.html#torch.transpose "torch.transpose") | 返回`input`的转置版本的张量。 |
| [`unbind`](generated/torch.unbind.html#torch.unbind "torch.unbind") | 移除张量的一个维度。 |
| [`unravel_index`](generated/torch.unravel_index.html#torch.unravel_index "torch.unravel_index") | 将扁平索引的张量转换为索引到指定形状的任意张量的坐标张量元组。 |
| [`unsqueeze`](generated/torch.unsqueeze.html#torch.unsqueeze "torch.unsqueeze") | 在指定位置插入一个大小为一的维度，返回一个新的张量。 |
| [`vsplit`](generated/torch.vsplit.html#torch.vsplit "torch.vsplit") | 根据`indices_or_sections`将具有两个或更多维度的`input`张量垂直拆分为多个张量。 |
| [`vstack`](generated/torch.vstack.html#torch.vstack "torch.vstack") | 按行将张量依次堆叠起来。 |

| [`where`](generated/torch.where.html#torch.where "torch.where") | 根据`condition`从`input`或`other`中选择元素并返回张量。 |

| [`Generator`](generated/torch.Generator.html#torch.Generator "torch.Generator") | 创建并返回一个生成器对象，该对象管理产生伪随机数的算法的状态。 |  ## 随机抽样[](#random-sampling "Permalink to this heading")

| [`seed`](generated/torch.seed.html#torch.seed "torch.seed") | 将生成随机数的种子设置为非确定性随机数。 |
| --- | --- |
| [`manual_seed`](generated/torch.manual_seed.html#torch.manual_seed "torch.manual_seed") | 设置生成随机数的种子。 |
| [`initial_seed`](generated/torch.initial_seed.html#torch.initial_seed "torch.initial_seed") | 将生成随机数的初始种子作为Python长整型返回。 |
| [`get_rng_state`](generated/torch.get_rng_state.html#torch.get_rng_state "torch.get_rng_state") | 将随机数生成器状态作为torch.ByteTensor返回。 |
| [`set_rng_state`](generated/torch.set_rng_state.html#torch.set_rng_state "torch.set_rng_state") | 设置随机数生成器状态。 |

```py
torch.default_generator Returns the default CPU torch.Generator¶
```

| [`bernoulli`](generated/torch.bernoulli.html#torch.bernoulli "torch.bernoulli") | 从伯努利分布中抽取二进制随机数（0或1）。 |
| --- | --- |
| [`multinomial`](generated/torch.multinomial.html#torch.multinomial "torch.multinomial") | 返回一个张量，其中每行包含从多项式（更严格的定义是多变量，有关更多细节，请参考torch.distributions.multinomial.Multinomial）概率分布中抽样的`num_samples`个索引，这些概率分布位于张量`input`相应行中。 |
| [`normal`](generated/torch.normal.html#torch.normal "torch.normal") | 返回从具有给定均值和标准差的单独正态分布中抽取的随机数的张量。 |
| [`poisson`](generated/torch.poisson.html#torch.poisson "torch.poisson") | 返回与`input`相同大小的张量，其中每个元素从泊松分布中抽样，速率参数由`input`中相应元素给出。 |
| [`rand`](generated/torch.rand.html#torch.rand "torch.rand") | 返回一个填充有来自区间$[0, 1)$的均匀分布的随机数的张量。 |
| [`rand_like`](generated/torch.rand_like.html#torch.rand_like "torch.rand_like") | 返回与`input`相同大小的张量，其中填充有来自区间$[0, 1)$的均匀分布的随机数。 |
| [`randint`](generated/torch.randint.html#torch.randint "torch.randint") | 返回一个填充有在`low`（包含）和`high`（不包含）之间均匀生成的随机整数的张量。 |
| [`randint_like`](generated/torch.randint_like.html#torch.randint_like "torch.randint_like") | 返回与张量`input`形状相同的张量，其中填充有在`low`（包含）和`high`（不包含）之间均匀生成的随机整数。 |
| [`randn`](generated/torch.randn.html#torch.randn "torch.randn") | 返回一个填充有来自均值为0，方差为1（也称为标准正态分布）的随机数的张量。 |
| [`randn_like`](generated/torch.randn_like.html#torch.randn_like "torch.randn_like") | 返回与`input`相同大小的张量，其中填充有来自均值为0，方差为1的正态分布的随机数。 |
| [`randperm`](generated/torch.randperm.html#torch.randperm "torch.randperm") | 返回从`0`到`n - 1`的整数的随机排列。 |

### 就地随机抽样[](#in-place-random-sampling "Permalink to this heading")

还有一些在张量上定义的就地随机抽样函数。点击查看它们的文档：

+   [`torch.Tensor.bernoulli_()`](generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_ "torch.Tensor.bernoulli_") - [`torch.bernoulli()`](generated/torch.bernoulli.html#torch.bernoulli "torch.bernoulli")的就地版本

+   [`torch.Tensor.cauchy_()`](generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_ "torch.Tensor.cauchy_") - 从柯西分布中抽取的数字

+   [`torch.Tensor.exponential_()`](generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_ "torch.Tensor.exponential_") - 从指数分布中抽取的数字

+   [`torch.Tensor.geometric_()`](generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_ "torch.Tensor.geometric_") - 从几何分布中抽取的元素

+   [`torch.Tensor.log_normal_()`](generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_ "torch.Tensor.log_normal_") - 从对数正态分布中抽样

+   [`torch.Tensor.normal_()`](generated/torch.Tensor.normal_.html#torch.Tensor.normal_ "torch.Tensor.normal_") - [`torch.normal()`](generated/torch.normal.html#torch.normal "torch.normal") 的原地版本

+   [`torch.Tensor.random_()`](generated/torch.Tensor.random_.html#torch.Tensor.random_ "torch.Tensor.random_") - 从离散均匀分布中抽取的数字

+   [`torch.Tensor.uniform_()`](generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_ "torch.Tensor.uniform_") - 从连续均匀分布中抽取的数字

### 准随机抽样

| [`quasirandom.SobolEngine`](generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine "torch.quasirandom.SobolEngine") | [`torch.quasirandom.SobolEngine`](generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine "torch.quasirandom.SobolEngine") 是用于生成（混淆）Sobol 序列的引擎。 |
| --- | --- |

## 序列化

| [`save`](generated/torch.save.html#torch.save "torch.save") | 将对象保存到磁盘文件中。 |
| --- | --- |
| [`load`](generated/torch.load.html#torch.load "torch.load") | 从文件中加载使用 [`torch.save()`](generated/torch.save.html#torch.save "torch.save") 保存的对象。 |

## 并行性

| [`get_num_threads`](generated/torch.get_num_threads.html#torch.get_num_threads "torch.get_num_threads") | 返回用于并行化 CPU 操作的线程数 |
| --- | --- |
| [`set_num_threads`](generated/torch.set_num_threads.html#torch.set_num_threads "torch.set_num_threads") | 设置在 CPU 上用于内部并行性的线程数。 |
| [`get_num_interop_threads`](generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads "torch.get_num_interop_threads") | 返回在 CPU 上用于互操作并行性的线程数（例如 |
| [`set_num_interop_threads`](generated/torch.set_num_interop_threads.html#torch.set_num_interop_threads "torch.set_num_interop_threads") | 设置用于互操作并行性的线程数（例如 |

## 本地禁用梯度计算[](#locally-disabling-gradient-computation "Permalink to this heading")

上下文管理器 [`torch.no_grad()`](generated/torch.no_grad.html#torch.no_grad "torch.no_grad")、[`torch.enable_grad()`](generated/torch.enable_grad.html#torch.enable_grad "torch.enable_grad") 和 [`torch.set_grad_enabled()`](generated/torch.set_grad_enabled.html#torch.set_grad_enabled "torch.set_grad_enabled") 对于本地禁用和启用梯度计算非常有用。有关它们的使用详情，请参阅[本地禁用梯度计算](autograd.html#locally-disable-grad)。这些上下文管理器是线程局部的，因此如果使用 `threading` 模块等将工作发送到另一个线程，则它们将无法工作。

示例：

```py
>>> x = torch.zeros(1, requires_grad=True)
>>> with torch.no_grad():
...     y = x * 2
>>> y.requires_grad
False

>>> is_train = False
>>> with torch.set_grad_enabled(is_train):
...     y = x * 2
>>> y.requires_grad
False

>>> torch.set_grad_enabled(True)  # this can also be used as a function
>>> y = x * 2
>>> y.requires_grad
True

>>> torch.set_grad_enabled(False)
>>> y = x * 2
>>> y.requires_grad
False 
```

| [`no_grad`](generated/torch.no_grad.html#torch.no_grad "torch.no_grad") | 禁用梯度计算的上下文管理器。 |
| --- | --- |
| [`enable_grad`](generated/torch.enable_grad.html#torch.enable_grad "torch.enable_grad") | 启用梯度计算的上下文管理器。 |
| [`set_grad_enabled`](generated/torch.set_grad_enabled.html#torch.set_grad_enabled "torch.set_grad_enabled") | 上下文管理器，用于打开或关闭梯度计算。 |
| [`is_grad_enabled`](generated/torch.is_grad_enabled.html#torch.is_grad_enabled "torch.is_grad_enabled") | 如果当前启用梯度模式，则返回True。 |
| [`inference_mode`](generated/torch.inference_mode.html#torch.inference_mode "torch.inference_mode") | 启用或禁用推理模式的上下文管理器。 |
| [`is_inference_mode_enabled`](generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled "torch.is_inference_mode_enabled") | 如果当前启用推理模式，则返回True。 |

## Math operations

### Pointwise Ops

| [`abs`](generated/torch.abs.html#torch.abs "torch.abs") | 计算`input`中每个元素的绝对值。 |
| --- | --- |
| [`absolute`](generated/torch.absolute.html#torch.absolute "torch.absolute") | [`torch.abs()`](generated/torch.abs.html#torch.abs "torch.abs")的别名。 |
| [`acos`](generated/torch.acos.html#torch.acos "torch.acos") | 计算`input`中每个元素的反余弦值。 |
| [`arccos`](generated/torch.arccos.html#torch.arccos "torch.arccos") | [`torch.acos()`](generated/torch.acos.html#torch.acos "torch.acos")的别名。 |
| [`acosh`](generated/torch.acosh.html#torch.acosh "torch.acosh") | 返回一个新的张量，其中包含`input`元素的反双曲余弦值。 |
| [`arccosh`](generated/torch.arccosh.html#torch.arccosh "torch.arccosh") | [`torch.acosh()`](generated/torch.acosh.html#torch.acosh "torch.acosh")的别名。 |
| [`add`](generated/torch.add.html#torch.add "torch.add") | 将`other`按`alpha`缩放后加到`input`中。 |
| [`addcdiv`](generated/torch.addcdiv.html#torch.addcdiv "torch.addcdiv") | 对`tensor1`和`tensor2`进行逐元素除法，将结果乘以标量`value`并加到`input`中。 |
| [`addcmul`](generated/torch.addcmul.html#torch.addcmul "torch.addcmul") | 对`tensor1`和`tensor2`进行逐元素相乘，将结果乘以标量`value`并加到`input`中。 |
| [`angle`](generated/torch.angle.html#torch.angle "torch.angle") | 计算给定`input`张量的逐元素角度（弧度）。 |
| [`asin`](generated/torch.asin.html#torch.asin "torch.asin") | 返回一个新的张量，其中包含`input`元素的反正弦值。 |
| [`arcsin`](generated/torch.arcsin.html#torch.arcsin "torch.arcsin") | [`torch.asin()`](generated/torch.asin.html#torch.asin "torch.asin")的别名。 |
| [`asinh`](generated/torch.asinh.html#torch.asinh "torch.asinh") | 返回一个新的张量，其中包含`input`元素的反双曲正弦值。 |
| [`arcsinh`](generated/torch.arcsinh.html#torch.arcsinh "torch.arcsinh") | [`torch.asinh()`](generated/torch.asinh.html#torch.asinh "torch.asinh")的别名。 |
| [`atan`](generated/torch.atan.html#torch.atan "torch.atan") | 返回一个新的张量，其中包含`input`元素的反正切值。 |
| [`arctan`](generated/torch.arctan.html#torch.arctan "torch.arctan") | [`torch.atan()`](generated/torch.atan.html#torch.atan "torch.atan")的别名。 |
| [`atanh`](generated/torch.atanh.html#torch.atanh "torch.atanh") | 返回一个新的张量，其中包含`input`元素的反双曲正切值。 |
| [`arctanh`](generated/torch.arctanh.html#torch.arctanh "torch.arctanh") | [`torch.atanh()`](generated/torch.atanh.html#torch.atanh "torch.atanh")的别名。 |
| [`atan2`](generated/torch.atan2.html#torch.atan2 "torch.atan2") | 考虑象限的`inputi​/otheri​`的逐元素反正切。 |
| [`arctan2`](generated/torch.arctan2.html#torch.arctan2 "torch.arctan2") | [`torch.atan2()`](generated/torch.atan2.html#torch.atan2 "torch.atan2")的别名。 |
| [`bitwise_not`](generated/torch.bitwise_not.html#torch.bitwise_not "torch.bitwise_not") | 计算给定输入张量的按位非。 |
| [`bitwise_and`](generated/torch.bitwise_and.html#torch.bitwise_and "torch.bitwise_and") | 计算`input`和`other`的按位与。 |
| [`bitwise_or`](generated/torch.bitwise_or.html#torch.bitwise_or "torch.bitwise_or") | 计算`input`和`other`的按位或。 |
| [`bitwise_xor`](generated/torch.bitwise_xor.html#torch.bitwise_xor "torch.bitwise_xor") | 计算`input`和`other`的按位异或。 |
| [`bitwise_left_shift`](generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift "torch.bitwise_left_shift") | 计算`input`按`other`位的左算术移位。 |
| [`bitwise_right_shift`](generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift "torch.bitwise_right_shift") | 计算`input`按`other`位的右算术移位。 |
| [`ceil`](generated/torch.ceil.html#torch.ceil "torch.ceil") | 返回一个新的张量，其中包含`input`元素的上限，即大于或等于每个元素的最小整数。 |
| [`clamp`](generated/torch.clamp.html#torch.clamp "torch.clamp") | 将`input`中的所有元素夹紧到范围[`min`](generated/torch.min.html#torch.min "torch.min"), [`max`](generated/torch.max.html#torch.max "torch.max")内。 |
| [`clip`](generated/torch.clip.html#torch.clip "torch.clip") | [`torch.clamp()`](generated/torch.clamp.html#torch.clamp "torch.clamp")的别名。 |
| [`conj_physical`](generated/torch.conj_physical.html#torch.conj_physical "torch.conj_physical") | 计算给定`input`张量的逐元素共轭。 |
| [`copysign`](generated/torch.copysign.html#torch.copysign "torch.copysign") | 创建一个新的浮点张量，其大小为`input`，符号为`other`，逐元素。 |
| [`cos`](generated/torch.cos.html#torch.cos "torch.cos") | 返回一个新的张量，其中包含`input`元素的余弦值。 |
| [`cosh`](generated/torch.cosh.html#torch.cosh "torch.cosh") | 返回一个新的张量，其中包含`input`元素的双曲余弦值。 |
| [`deg2rad`](generated/torch.deg2rad.html#torch.deg2rad "torch.deg2rad") | 返回一个新的张量，其中包含`input`中每个元素从角度转换为弧度。 |
| [`div`](generated/torch.div.html#torch.div "torch.div") | 将输入`input`的每个元素除以相应的`other`元素。 |
| [`divide`](generated/torch.divide.html#torch.divide "torch.divide") | [`torch.div()`](generated/torch.div.html#torch.div "torch.div")的别名。 |
| [`digamma`](generated/torch.digamma.html#torch.digamma "torch.digamma") | [`torch.special.digamma()`](special.html#torch.special.digamma "torch.special.digamma")的别名。 |
| [`erf`](generated/torch.erf.html#torch.erf "torch.erf") | [`torch.special.erf()`](special.html#torch.special.erf "torch.special.erf")的别名。 |
| [`erfc`](generated/torch.erfc.html#torch.erfc "torch.erfc") | [`torch.special.erfc()`](special.html#torch.special.erfc "torch.special.erfc")的别名。 |
| [`erfinv`](generated/torch.erfinv.html#torch.erfinv "torch.erfinv") | [`torch.special.erfinv()`](special.html#torch.special.erfinv "torch.special.erfinv")的别名。 |
| [`exp`](generated/torch.exp.html#torch.exp "torch.exp") | 返回一个新的张量，其中包含输入张量`input`元素的指数。 |
| [`exp2`](generated/torch.exp2.html#torch.exp2 "torch.exp2") | [`torch.special.exp2()`](special.html#torch.special.exp2 "torch.special.exp2")的别名。 |
| [`expm1`](generated/torch.expm1.html#torch.expm1 "torch.expm1") | [`torch.special.expm1()`](special.html#torch.special.expm1 "torch.special.expm1")的别名。 |
| [`fake_quantize_per_channel_affine`](generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine "torch.fake_quantize_per_channel_affine") | 返回一个新的张量，其中包含使用`scale`、`zero_point`、`quant_min`和`quant_max`对`input`进行每通道伪量化的数据，跨通道由`axis`指定。 |
| [`fake_quantize_per_tensor_affine`](generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine "torch.fake_quantize_per_tensor_affine") | 使用 `scale`、`zero_point`、`quant_min` 和 `quant_max` 对 `input` 中的数据进行伪量化，并返回一个新的张量。 |
| [`fix`](generated/torch.fix.html#torch.fix "torch.fix") | [`torch.trunc()`](generated/torch.trunc.html#torch.trunc "torch.trunc") 的别名 |
| [`float_power`](generated/torch.float_power.html#torch.float_power "torch.float_power") | 以双精度计算，对 `input` 的每个元素进行 `exponent` 次幂运算。 |
| [`floor`](generated/torch.floor.html#torch.floor "torch.floor") | 返回一个新的张量，其元素为 `input` 的下取整，即小于或等于每个元素的最大整数。 |
| [`floor_divide`](generated/torch.floor_divide.html#torch.floor_divide "torch.floor_divide") |  |
| [`fmod`](generated/torch.fmod.html#torch.fmod "torch.fmod") | 对每个元素应用 C++ 的 [std::fmod](https://en.cppreference.com/w/cpp/numeric/math/fmod)。 |
| [`frac`](generated/torch.frac.html#torch.frac "torch.frac") | 计算 `input` 中每个元素的小数部分。 |
| [`frexp`](generated/torch.frexp.html#torch.frexp "torch.frexp") | 将 `input` 分解为尾数和指数张量，使得 $\text{input} = \text{mantissa} \times 2^{\text{exponent}}$。 |
| [`gradient`](generated/torch.gradient.html#torch.gradient "torch.gradient") | 使用 [二阶中心差分方法](https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf) 在一个或多个维度上估计函数 $g : \mathbb{R}^n \rightarrow \mathbb{R}$g:Rn→R 的梯度，并在边界处使用一阶或二阶估计。 |
| [`imag`](generated/torch.imag.html#torch.imag "torch.imag") | 返回一个包含 `self` 张量的虚部的新张量。 |
| [`ldexp`](generated/torch.ldexp.html#torch.ldexp "torch.ldexp") | 将 `input` 乘以 2 ** `other`。 |
| [`lerp`](generated/torch.lerp.html#torch.lerp "torch.lerp") | 根据标量或张量 `weight` 对两个张量 `start`（由 `input` 给出）和 `end` 进行线性插值，并返回结果张量 `out`。 |
| [`lgamma`](generated/torch.lgamma.html#torch.lgamma "torch.lgamma") | 计算 `input` 上伽玛函数绝对值的自然对数。 |
| [`log`](generated/torch.log.html#torch.log "torch.log") | 返回一个新的张量，其元素为 `input` 的自然对数。 |
| [`log10`](generated/torch.log10.html#torch.log10 "torch.log10") | 返回一个新的张量，其元素为 `input` 的以 10 为底的对数。 |
| [`log1p`](generated/torch.log1p.html#torch.log1p "torch.log1p") | 返回一个新的张量，其元素为 (1 + `input`) 的自然对数。 |
| [`log2`](generated/torch.log2.html#torch.log2 "torch.log2") | 返回一个新的张量，其元素为 `input` 的以 2 为底的对数。 |
| [`logaddexp`](generated/torch.logaddexp.html#torch.logaddexp "torch.logaddexp") | 对输入的指数求和的对数。 |
| [`logaddexp2`](generated/torch.logaddexp2.html#torch.logaddexp2 "torch.logaddexp2") | 以 2 为底对输入的指数求和的对数。 |
| [`logical_and`](generated/torch.logical_and.html#torch.logical_and "torch.logical_and") | 计算给定输入张量的逐元素逻辑与。 |
| [`logical_not`](generated/torch.logical_not.html#torch.logical_not "torch.logical_not") | 计算给定输入张量的逐元素逻辑非。 |
| [`logical_or`](generated/torch.logical_or.html#torch.logical_or "torch.logical_or") | 计算给定输入张量的逐元素逻辑或。 |
| [`logical_xor`](generated/torch.logical_xor.html#torch.logical_xor "torch.logical_xor") | 计算给定输入张量的逐元素逻辑异或。 |
| [`logit`](生成/torch.logit.html#torch.logit "torch.logit") | [`torch.special.logit()`](special.html#torch.special.logit "torch.special.logit") 的别名。 |
| [`hypot`](生成/torch.hypot.html#torch.hypot "torch.hypot") | 给定直角三角形的两条直角边，返回其斜边。 |
| [`i0`](生成/torch.i0.html#torch.i0 "torch.i0") | [`torch.special.i0()`](special.html#torch.special.i0 "torch.special.i0") 的别名。 |
| [`igamma`](生成/torch.igamma.html#torch.igamma "torch.igamma") | [`torch.special.gammainc()`](special.html#torch.special.gammainc "torch.special.gammainc") 的别名。 |
| [`igammac`](生成/torch.igammac.html#torch.igammac "torch.igammac") | [`torch.special.gammaincc()`](special.html#torch.special.gammaincc "torch.special.gammaincc") 的别名。 |
| [`mul`](生成/torch.mul.html#torch.mul "torch.mul") | 将 `input` 乘以 `other`。 |
| [`multiply`](生成/torch.multiply.html#torch.multiply "torch.multiply") | [`torch.mul()`](生成/torch.mul.html#torch.mul "torch.mul") 的别名。 |
| [`mvlgamma`](生成/torch.mvlgamma.html#torch.mvlgamma "torch.mvlgamma") | [`torch.special.multigammaln()`](special.html#torch.special.multigammaln "torch.special.multigammaln") 的别名。 |
| [`nan_to_num`](生成/torch.nan_to_num.html#torch.nan_to_num "torch.nan_to_num") | 用 `nan`、`posinf` 和 `neginf` 指定的值替换 `input` 中的 `NaN`、正无穷大和负无穷大值。 |
| [`neg`](生成/torch.neg.html#torch.neg "torch.neg") | 返回一个新的张量，其元素为 `input` 的负数。 |
| [`negative`](生成/torch.negative.html#torch.negative "torch.negative") | [`torch.neg()`](生成/torch.neg.html#torch.neg "torch.neg") 的别名。 |
| [`nextafter`](生成/torch.nextafter.html#torch.nextafter "torch.nextafter") | 返回 `input` 向 `other` 方向的下一个浮点值，逐元素进行。 |
| [`polygamma`](生成/torch.polygamma.html#torch.polygamma "torch.polygamma") | [`torch.special.polygamma()`](special.html#torch.special.polygamma "torch.special.polygamma") 的别名。 |
| [`positive`](生成/torch.positive.html#torch.positive "torch.positive") | 返回 `input`。 |
| [`pow`](生成/torch.pow.html#torch.pow "torch.pow") | 对 `input` 中的每个元素进行 `exponent` 次幂运算，并返回结果张量。 |
| [`quantized_batch_norm`](生成/torch.quantized_batch_norm.html#torch.quantized_batch_norm "torch.quantized_batch_norm") | 对4D（NCHW）量化张量应用批量归一化。 |
| [`quantized_max_pool1d`](生成/torch.quantized_max_pool1d.html#torch.quantized_max_pool1d "torch.quantized_max_pool1d") | 对由多个输入平面组成的输入量化张量应用1D最大池化。 |
| [`quantized_max_pool2d`](生成/torch.quantized_max_pool2d.html#torch.quantized_max_pool2d "torch.quantized_max_pool2d") | 对由多个输入平面组成的输入量化张量应用2D最大池化。 |
| [`rad2deg`](生成/torch.rad2deg.html#torch.rad2deg "torch.rad2deg") | 返回一个新的张量，其中 `input` 的每个元素从弧度转换为度。 |
| [`real`](生成/torch.real.html#torch.real "torch.real") | 返回一个包含 `self` 张量的实数值的新张量。 |
| [`reciprocal`](生成/torch.reciprocal.html#torch.reciprocal "torch.reciprocal") | 返回一个新的张量，其元素为 `input` 的倒数。 |
| [`remainder`](生成/torch.remainder.html#torch.remainder "torch.remainder") | 计算逐元素的[Python取模运算](https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations)。 |
| [`round`](生成/torch.round.html#torch.round "torch.round") | 将 `input` 的元素四舍五入到最接近的整数。 |
| [`rsqrt`](生成/torch.rsqrt.html#torch.rsqrt "torch.rsqrt") | 返回一个新的张量，其元素为 `input` 的平方根的倒数。 |
| [`sigmoid`](generated/torch.sigmoid.html#torch.sigmoid "torch.sigmoid") | [`torch.special.expit()`](special.html#torch.special.expit "torch.special.expit")的别名。 |
| [`sign`](generated/torch.sign.html#torch.sign "torch.sign") | 返回具有`input`元素的符号的新张量。 |
| [`sgn`](generated/torch.sgn.html#torch.sgn "torch.sgn") | 这个函数是对复数张量的torch.sign()的扩展。 |
| [`signbit`](generated/torch.signbit.html#torch.signbit "torch.signbit") | 检查`input`的每个元素是否设置了符号位。 |
| [`sin`](generated/torch.sin.html#torch.sin "torch.sin") | 返回具有`input`元素的正弦的新张量。 |
| [`sinc`](generated/torch.sinc.html#torch.sinc "torch.sinc") | [`torch.special.sinc()`](special.html#torch.special.sinc "torch.special.sinc")的别名。 |
| [`sinh`](generated/torch.sinh.html#torch.sinh "torch.sinh") | 返回具有`input`元素的双曲正弦的新张量。 |
| [`softmax`](generated/torch.softmax.html#torch.softmax "torch.softmax") | [`torch.nn.functional.softmax()`](generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax "torch.nn.functional.softmax")的别名。 |
| [`sqrt`](generated/torch.sqrt.html#torch.sqrt "torch.sqrt") | 返回具有`input`元素的平方根的新张量。 |
| [`square`](generated/torch.square.html#torch.square "torch.square") | 返回具有`input`元素的平方的新张量。 |
| [`sub`](generated/torch.sub.html#torch.sub "torch.sub") | 从`input`中减去经过`alpha`缩放的`other`。 |
| [`subtract`](generated/torch.subtract.html#torch.subtract "torch.subtract") | [`torch.sub()`](generated/torch.sub.html#torch.sub "torch.sub")的别名。 |
| [`tan`](generated/torch.tan.html#torch.tan "torch.tan") | 返回具有`input`元素的正切的新张量。 |
| [`tanh`](generated/torch.tanh.html#torch.tanh "torch.tanh") | 返回具有`input`元素的双曲正切的新张量。 |
| [`true_divide`](generated/torch.true_divide.html#torch.true_divide "torch.true_divide") | 使用`rounding_mode=None`的[`torch.div()`](generated/torch.div.html#torch.div "torch.div")的别名。 |
| [`trunc`](generated/torch.trunc.html#torch.trunc "torch.trunc") | 返回具有`input`元素的截断整数值的新张量。 |
| [`xlogy`](generated/torch.xlogy.html#torch.xlogy "torch.xlogy") | [`torch.special.xlogy()`](special.html#torch.special.xlogy "torch.special.xlogy")的别名。 |

### Reduction Ops

| [`argmax`](generated/torch.argmax.html#torch.argmax "torch.argmax") | 返回`input`张量中所有元素的最大值的索引。 |
| --- | --- |
| [`argmin`](generated/torch.argmin.html#torch.argmin "torch.argmin") | 返回扁平张量或沿着维度的最小值的索引。 |
| [`amax`](generated/torch.amax.html#torch.amax "torch.amax") | 返回给定维度`dim`中`input`张量每个切片的最大值。 |
| [`amin`](generated/torch.amin.html#torch.amin "torch.amin") | 返回给定维度`dim`中`input`张量每个切片的最小值。 |
| [`aminmax`](generated/torch.aminmax.html#torch.aminmax "torch.aminmax") | 计算`input`张量的最小值和最大值。 |
| [`all`](generated/torch.all.html#torch.all "torch.all") | 检查`input`中是否所有元素评估为True。 |
| [`any`](generated/torch.any.html#torch.any "torch.any") | 检查`input`中是否有任何元素评估为True。 |
| [`max`](generated/torch.max.html#torch.max "torch.max") | 返回`input`张量中所有元素的最大值。 |
| [`min`](generated/torch.min.html#torch.min "torch.min") | 返回`input`张量中所有元素的最小值。 |
| [`dist`](generated/torch.dist.html#torch.dist "torch.dist") | 返回(`input` - `other`)的p-范数 |
| [`logsumexp`](生成/torch.logsumexp.html#torch.logsumexp "torch.logsumexp") | 返回`input`张量每行在给定维度`dim`上的对数求和指数。 |
| [`mean`](生成/torch.mean.html#torch.mean "torch.mean") | 返回`input`张量中所有元素的均值。 |
| [`nanmean`](生成/torch.nanmean.html#torch.nanmean "torch.nanmean") | 计算指定维度上所有非NaN元素的均值。 |
| [`median`](生成/torch.median.html#torch.median "torch.median") | 返回`input`中值的中位数。 |
| [`nanmedian`](生成/torch.nanmedian.html#torch.nanmedian "torch.nanmedian") | 返回`input`中值的中位数，忽略`NaN`值。 |
| [`mode`](生成/torch.mode.html#torch.mode "torch.mode") | 返回一个命名元组`(values, indices)`，其中`values`是`input`张量每行在给定维度`dim`上的众数值，即在该行中出现最频繁的值，`indices`是找到的每个众数值的索引位置。 |
| [`norm`](生成/torch.norm.html#torch.norm "torch.norm") | 返回给定张量的矩阵范数或向量范数。 |
| [`nansum`](生成/torch.nansum.html#torch.nansum "torch.nansum") | 返回所有元素的和，将非数值（NaN）视为零。 |
| [`prod`](生成/torch.prod.html#torch.prod "torch.prod") | 返回`input`张量中所有元素的乘积。 |
| [`quantile`](生成/torch.quantile.html#torch.quantile "torch.quantile") | 计算`input`张量每行沿维度`dim`的q分位数。 |
| [`nanquantile`](生成/torch.nanquantile.html#torch.nanquantile "torch.nanquantile") | 这是[`torch.quantile()`](生成/torch.quantile.html#torch.quantile "torch.quantile")的一个变体，"忽略" `NaN` 值，计算`input`中的分位数`q`，就好像`input`中不存在`NaN`值一样。 |
| [`std`](生成/torch.std.html#torch.std "torch.std") | 计算由`dim`指定的维度上的标准差。 |
| [`std_mean`](生成/torch.std_mean.html#torch.std_mean "torch.std_mean") | 计算由`dim`指定的维度上的标准差和均值。 |
| [`sum`](生成/torch.sum.html#torch.sum "torch.sum") | 返回`input`张量中所有元素的和。 |
| [`unique`](生成/torch.unique.html#torch.unique "torch.unique") | 返回输入张量的唯一元素。 |
| [`unique_consecutive`](生成/torch.unique_consecutive.html#torch.unique_consecutive "torch.unique_consecutive") | 消除每个连续等价元素组中除第一个元素之外的所有元素。 |
| [`var`](生成/torch.var.html#torch.var "torch.var") | 计算由`dim`指定的维度上的方差。 |
| [`var_mean`](生成/torch.var_mean.html#torch.var_mean "torch.var_mean") | 计算由`dim`指定的维度上的方差和均值。 |
| [`count_nonzero`](生成/torch.count_nonzero.html#torch.count_nonzero "torch.count_nonzero") | 计算张量`input`沿给定`dim`中的非零值的数量。 |

### Comparison Ops

| [`allclose`](生成/torch.allclose.html#torch.allclose "torch.allclose") | 此函数检查`input`和`other`是否满足条件： |
| --- | --- |
| [`argsort`](生成/torch.argsort.html#torch.argsort "torch.argsort") | 返回按值升序沿给定维度对张量进行排序的索引。 |
| [`eq`](生成/torch.eq.html#torch.eq "torch.eq") | 计算逐元素相等 |
| [`equal`](生成/torch.equal.html#torch.equal "torch.equal") | 如果两个张量具有相同的大小和元素，则为`True`，否则为`False`。 |
| [`ge`](生成/torch.ge.html#torch.ge "torch.ge") | 计算$\text{input} \geq \text{other}$逐元素。 |
| [`greater_equal`](generated/torch.greater_equal.html#torch.greater_equal "torch.greater_equal") | [`torch.ge()`](generated/torch.ge.html#torch.ge "torch.ge") 的别名。 |
| [`gt`](generated/torch.gt.html#torch.gt "torch.gt") | 计算 $\text{input} > \text{other}$input>other 逐元素。 |
| [`greater`](generated/torch.greater.html#torch.greater "torch.greater") | [`torch.gt()`](generated/torch.gt.html#torch.gt "torch.gt") 的别名。 |
| [`isclose`](generated/torch.isclose.html#torch.isclose "torch.isclose") | 返回一个新张量，其中的布尔元素表示 `input` 的每个元素是否与 `other` 的对应元素“接近”。 |
| [`isfinite`](generated/torch.isfinite.html#torch.isfinite "torch.isfinite") | 返回一个新张量，其中的布尔元素表示每个元素是否为有限数。 |
| [`isin`](generated/torch.isin.html#torch.isin "torch.isin") | 检查 `elements` 的每个元素是否在 `test_elements` 中。 |
| [`isinf`](generated/torch.isinf.html#torch.isinf "torch.isinf") | 检查 `input` 的每个元素是否为无穷大（正无穷大或负无穷大）。 |
| [`isposinf`](generated/torch.isposinf.html#torch.isposinf "torch.isposinf") | 检查 `input` 的每个元素是否为正无穷大。 |
| [`isneginf`](generated/torch.isneginf.html#torch.isneginf "torch.isneginf") | 检查 `input` 的每个元素是否为负无穷大。 |
| [`isnan`](generated/torch.isnan.html#torch.isnan "torch.isnan") | 返回一个新张量，其中的布尔元素表示 `input` 的每个元素是否为 NaN。 |
| [`isreal`](generated/torch.isreal.html#torch.isreal "torch.isreal") | 返回一个新张量，其中的布尔元素表示 `input` 的每个元素是否为实数或非实数。 |
| [`kthvalue`](generated/torch.kthvalue.html#torch.kthvalue "torch.kthvalue") | 返回一个命名元组 `(values, indices)`，其中 `values` 是 `input` 张量在给定维度 `dim` 中每行的第 `k` 小元素。 |
| [`le`](generated/torch.le.html#torch.le "torch.le") | 计算 $\text{input} \leq \text{other}$input≤other 逐元素。 |
| [`less_equal`](generated/torch.less_equal.html#torch.less_equal "torch.less_equal") | [`torch.le()`](generated/torch.le.html#torch.le "torch.le") 的别名。 |
| [`lt`](generated/torch.lt.html#torch.lt "torch.lt") | 计算 $\text{input} < \text{other}$input<other 逐元素。 |
| [`less`](generated/torch.less.html#torch.less "torch.less") | [`torch.lt()`](generated/torch.lt.html#torch.lt "torch.lt") 的别名。 |
| [`maximum`](generated/torch.maximum.html#torch.maximum "torch.maximum") | 计算 `input` 和 `other` 的逐元素最大值。 |
| [`minimum`](generated/torch.minimum.html#torch.minimum "torch.minimum") | 计算 `input` 和 `other` 的逐元素最小值。 |
| [`fmax`](generated/torch.fmax.html#torch.fmax "torch.fmax") | 计算 `input` 和 `other` 的逐元素最大值。 |
| [`fmin`](generated/torch.fmin.html#torch.fmin "torch.fmin") | 计算 `input` 和 `other` 的逐元素最小值。 |
| [`ne`](generated/torch.ne.html#torch.ne "torch.ne") | 计算 $\text{input} \neq \text{other}$input=other 逐元素。 |
| [`not_equal`](generated/torch.not_equal.html#torch.not_equal "torch.not_equal") | [`torch.ne()`](generated/torch.ne.html#torch.ne "torch.ne") 的别名。 |
| [`sort`](generated/torch.sort.html#torch.sort "torch.sort") | 按值升序对 `input` 张量沿指定维度排序。 |
| [`topk`](generated/torch.topk.html#torch.topk "torch.topk") | 返回给定 `input` 张量沿指定维度的前 `k` 个最大元素。 |
| [`msort`](generated/torch.msort.html#torch.msort "torch.msort") | 按值升序对 `input` 张量沿其第一维排序。 |

### Spectral Ops

| [`stft`](generated/torch.stft.html#torch.stft "torch.stft") | 短时傅里叶变换（STFT）。 |
| --- | --- |
| [`istft`](generated/torch.istft.html#torch.istft "torch.istft") | 短时傅里叶逆变换。 |
| [`bartlett_window`](generated/torch.bartlett_window.html#torch.bartlett_window "torch.bartlett_window") | Bartlett窗口函数。 |
| [`blackman_window`](generated/torch.blackman_window.html#torch.blackman_window "torch.blackman_window") | Blackman窗口函数。 |
| [`hamming_window`](generated/torch.hamming_window.html#torch.hamming_window "torch.hamming_window") | Hamming窗口函数。 |
| [`hann_window`](generated/torch.hann_window.html#torch.hann_window "torch.hann_window") | Hann窗口函数。 |
| [`kaiser_window`](generated/torch.kaiser_window.html#torch.kaiser_window "torch.kaiser_window") | 计算具有窗口长度`window_length`和形状参数`beta`的Kaiser窗口。 |

### 其他操作

| [`atleast_1d`](generated/torch.atleast_1d.html#torch.atleast_1d "torch.atleast_1d") | 返回每个输入张量的零维视图的一维视图。 |
| --- | --- |
| [`atleast_2d`](generated/torch.atleast_2d.html#torch.atleast_2d "torch.atleast_2d") | 返回每个输入张量的零维视图的二维视图。 |
| [`atleast_3d`](generated/torch.atleast_3d.html#torch.atleast_3d "torch.atleast_3d") | 返回每个输入张量的零维视图的三维视图。 |
| [`bincount`](generated/torch.bincount.html#torch.bincount "torch.bincount") | 计算非负整数数组中每个值的频率。 |
| [`block_diag`](generated/torch.block_diag.html#torch.block_diag "torch.block_diag") | 从提供的张量创建一个分块对角矩阵。 |
| [`broadcast_tensors`](generated/torch.broadcast_tensors.html#torch.broadcast_tensors "torch.broadcast_tensors") | 根据[广播语义](notes/broadcasting.html#broadcasting-semantics)广播给定的张量。 |
| [`broadcast_to`](generated/torch.broadcast_to.html#torch.broadcast_to "torch.broadcast_to") | 将`input`广播到形状`shape`。 |
| [`broadcast_shapes`](generated/torch.broadcast_shapes.html#torch.broadcast_shapes "torch.broadcast_shapes") | 类似于[`broadcast_tensors()`](generated/torch.broadcast_tensors.html#torch.broadcast_tensors "torch.broadcast_tensors")，但用于形状。 |
| [`bucketize`](generated/torch.bucketize.html#torch.bucketize "torch.bucketize") | 返回`input`中每个值所属的桶的索引，其中桶的边界由`boundaries`设置。 |
| [`cartesian_prod`](generated/torch.cartesian_prod.html#torch.cartesian_prod "torch.cartesian_prod") | 对给定的张量序列进行笛卡尔积。 |
| [`cdist`](generated/torch.cdist.html#torch.cdist "torch.cdist") | 计算两个行向量集合中每对之间的批次p-范数距离。 |
| [`clone`](generated/torch.clone.html#torch.clone "torch.clone") | 返回`input`的副本。 |
| [`combinations`](generated/torch.combinations.html#torch.combinations "torch.combinations") | 计算给定张量的长度为$r$的组合。 |
| [`corrcoef`](generated/torch.corrcoef.html#torch.corrcoef "torch.corrcoef") | 估计由`input`矩阵给出的变量的Pearson积矩相关系数矩阵，其中行是变量，列是观测。 |
| [`cov`](generated/torch.cov.html#torch.cov "torch.cov") | 估计由`input`矩阵给出的变量的协方差矩阵，其中行是变量，列是观测。 |
| [`cross`](generated/torch.cross.html#torch.cross "torch.cross") | 返回`input`和`other`在维度`dim`中向量的叉积。 |
| [`cummax`](generated/torch.cummax.html#torch.cummax "torch.cummax") | 返回一个命名元组`(values, indices)`，其中`values`是维度`dim`中`input`元素的累积最大值。 |
| [`cummin`](generated/torch.cummin.html#torch.cummin "torch.cummin") | 返回一个命名元组 `(values, indices)`，其中 `values` 是 `input` 在维度 `dim` 中元素的累积最小值。 |
| [`cumprod`](generated/torch.cumprod.html#torch.cumprod "torch.cumprod") | 返回 `input` 在维度 `dim` 中元素的累积积。 |
| [`cumsum`](generated/torch.cumsum.html#torch.cumsum "torch.cumsum") | 返回 `input` 在维度 `dim` 中元素的累积和。 |
| [`diag`](generated/torch.diag.html#torch.diag "torch.diag") |

+   如果`input`是一个向量（1-D张量），则返回一个2-D方形张量

|

| [`diag_embed`](generated/torch.diag_embed.html#torch.diag_embed "torch.diag_embed") | 创建一个张量，其中某些二维平面（由 `dim1` 和 `dim2` 指定）的对角线由 `input` 填充。 |
| --- | --- |
| [`diagflat`](generated/torch.diagflat.html#torch.diagflat "torch.diagflat") |

+   如果`input`是一个向量（1-D张量），则返回一个2-D方形张量

|

| [`diagonal`](generated/torch.diagonal.html#torch.diagonal "torch.diagonal") | 返回 `input` 的部分视图，其对角线元素相对于 `dim1` 和 `dim2` 附加为形状末尾的维度。 |
| --- | --- |
| [`diff`](generated/torch.diff.html#torch.diff "torch.diff") | 计算沿给定维度的第n个前向差分。 |
| [`einsum`](generated/torch.einsum.html#torch.einsum "torch.einsum") | 按照基于Einstein求和约定的符号，沿着指定维度对输入`operands`的元素的乘积求和。 |
| [`flatten`](generated/torch.flatten.html#torch.flatten "torch.flatten") | 通过将其重新形状为一维张量来展平 `input`。 |
| [`flip`](generated/torch.flip.html#torch.flip "torch.flip") | 沿着给定轴在dims中反转n-D张量的顺序。 |
| [`fliplr`](generated/torch.fliplr.html#torch.fliplr "torch.fliplr") | 在左/右方向上翻转张量，返回一个新的张量。 |
| [`flipud`](generated/torch.flipud.html#torch.flipud "torch.flipud") | 在上/下方向上翻转张量，返回一个新的张量。 |
| [`kron`](generated/torch.kron.html#torch.kron "torch.kron") | 计算 `input` 和 `other` 的Kronecker积，表示为 $\otimes$⊗。 |
| [`rot90`](generated/torch.rot90.html#torch.rot90 "torch.rot90") | 将n-D张量按dims轴指定的平面旋转90度。 |
| [`gcd`](generated/torch.gcd.html#torch.gcd "torch.gcd") | 计算 `input` 和 `other` 的逐元素最大公约数（GCD）。 |
| [`histc`](generated/torch.histc.html#torch.histc "torch.histc") | 计算张量的直方图。 |
| [`histogram`](generated/torch.histogram.html#torch.histogram "torch.histogram") | 计算张量中值的直方图。 |
| [`histogramdd`](generated/torch.histogramdd.html#torch.histogramdd "torch.histogramdd") | 计算张量中值的多维直方图。 |
| [`meshgrid`](generated/torch.meshgrid.html#torch.meshgrid "torch.meshgrid") | 创建由属性`tensors`中的1D输入指定的坐标网格。 |
| [`lcm`](generated/torch.lcm.html#torch.lcm "torch.lcm") | 计算 `input` 和 `other` 的逐元素最小公倍数（LCM）。 |
| [`logcumsumexp`](generated/torch.logcumsumexp.html#torch.logcumsumexp "torch.logcumsumexp") | 返回 `input` 元素的指数的累积求和的对数，维度为 `dim`。 |
| [`ravel`](generated/torch.ravel.html#torch.ravel "torch.ravel") | 返回一个连续的展平张量。 |
| [`renorm`](generated/torch.renorm.html#torch.renorm "torch.renorm") | 返回一个张量，其中沿着维度`dim`的每个子张量被归一化，使得子张量的p-范数低于值`maxnorm` |
| [`repeat_interleave`](generated/torch.repeat_interleave.html#torch.repeat_interleave "torch.repeat_interleave") | 重复张量的元素。 |
| [`roll`](generated/torch.roll.html#torch.roll "torch.roll") | 沿给定维度滚动张量`input`。 |
| [`searchsorted`](generated/torch.searchsorted.html#torch.searchsorted "torch.searchsorted") | 找到`sorted_sequence`的*最内层*维度中的索引，使得如果将`values`中的相应值插入到这些索引之前，排序后，`sorted_sequence`中的*最内层*维度的顺序将被保留。 |
| [`tensordot`](generated/torch.tensordot.html#torch.tensordot "torch.tensordot") | 在多个维度上返回a和b的收缩。 |
| [`trace`](generated/torch.trace.html#torch.trace "torch.trace") | 返回输入2-D矩阵对角线元素的和。 |
| [`tril`](generated/torch.tril.html#torch.tril "torch.tril") | 返回矩阵（2-D张量）或批量矩阵`input`的下三角部分，结果张量`out`的其他元素设置为0。 |
| [`tril_indices`](generated/torch.tril_indices.html#torch.tril_indices "torch.tril_indices") | 返回`row`-by-`col`矩阵的下三角部分的索引，以2xN张量的形式返回，其中第一行包含所有索引的行坐标，第二行包含列坐标。 |
| [`triu`](generated/torch.triu.html#torch.triu "torch.triu") | 返回矩阵（2-D张量）或批量矩阵`input`的上三角部分，结果张量`out`的其他元素设置为0。 |
| [`triu_indices`](generated/torch.triu_indices.html#torch.triu_indices "torch.triu_indices") | 返回`row`乘以`col`矩阵的上三角部分的索引，以2xN张量的形式返回，其中第一行包含所有索引的行坐标，第二行包含列坐标。 |
| [`unflatten`](generated/torch.unflatten.html#torch.unflatten "torch.unflatten") | 将输入张量的一个维度扩展到多个维度。 |
| [`vander`](generated/torch.vander.html#torch.vander "torch.vander") | 生成Vandermonde矩阵。 |
| [`view_as_real`](generated/torch.view_as_real.html#torch.view_as_real "torch.view_as_real") | 将`input`作为实数张量返回视图。 |
| [`view_as_complex`](generated/torch.view_as_complex.html#torch.view_as_complex "torch.view_as_complex") | 将`input`作为复数张量返回视图。 |
| [`resolve_conj`](generated/torch.resolve_conj.html#torch.resolve_conj "torch.resolve_conj") | 如果`input`的共轭位设置为True，则返回具有实现共轭的新张量，否则返回`input`。 |
| [`resolve_neg`](generated/torch.resolve_neg.html#torch.resolve_neg "torch.resolve_neg") | 如果`input`的负位设置为True，则返回具有实现否定的新张量，否则返回`input`。 |

### BLAS和LAPACK操作

| [`addbmm`](generated/torch.addbmm.html#torch.addbmm "torch.addbmm") | 对存储在`batch1`和`batch2`中的矩阵执行批量矩阵乘法，具有减少的加法步骤（所有矩阵乘法沿第一维度累积）。 |
| --- | --- |
| [`addmm`](generated/torch.addmm.html#torch.addmm "torch.addmm") | 执行矩阵`mat1`和`mat2`的矩阵乘法。 |
| [`addmv`](generated/torch.addmv.html#torch.addmv "torch.addmv") | 执行矩阵`mat`和向量`vec`的矩阵-向量乘积。 |
| [`addr`](generated/torch.addr.html#torch.addr "torch.addr") | 执行向量`vec1`和`vec2`的外积，并将其添加到矩阵`input`中。 |
| [`baddbmm`](generated/torch.baddbmm.html#torch.baddbmm "torch.baddbmm") | 对`batch1`和`batch2`中的矩阵执行批量矩阵乘法。 |
| [`bmm`](generated/torch.bmm.html#torch.bmm "torch.bmm") | 对存储在`input`和`mat2`中的矩阵执行批量矩阵乘法。 |
| [`chain_matmul`](generated/torch.chain_matmul.html#torch.chain_matmul "torch.chain_matmul") | 返回$N$个2-D张量的矩阵乘积。 |
| [`cholesky`](generated/torch.cholesky.html#torch.cholesky "torch.cholesky") | 计算对称正定矩阵$A$A或对称正定矩阵批次的Cholesky分解。 |
| [`cholesky_inverse`](generated/torch.cholesky_inverse.html#torch.cholesky_inverse "torch.cholesky_inverse") | 计算具有Cholesky分解的复Hermite或实对称正定矩阵的逆矩阵。 |
| [`cholesky_solve`](generated/torch.cholesky_solve.html#torch.cholesky_solve "torch.cholesky_solve") | 计算具有Cholesky分解的复Hermite或实对称正定lhs的线性方程组的解。 |
| [`dot`](generated/torch.dot.html#torch.dot "torch.dot") | 计算两个1D张量的点积。 |
| [`geqrf`](generated/torch.geqrf.html#torch.geqrf "torch.geqrf") | 这是一个直接调用LAPACK的geqrf的低级函数。 |
| [`ger`](generated/torch.ger.html#torch.ger "torch.ger") | [`torch.outer()`](generated/torch.outer.html#torch.outer "torch.outer")的别名。 |
| [`inner`](generated/torch.inner.html#torch.inner "torch.inner") | 计算1D张量的点积。 |
| [`inverse`](generated/torch.inverse.html#torch.inverse "torch.inverse") | [`torch.linalg.inv()`](generated/torch.linalg.inv.html#torch.linalg.inv "torch.linalg.inv")的别名 |
| [`det`](generated/torch.det.html#torch.det "torch.det") | [`torch.linalg.det()`](generated/torch.linalg.det.html#torch.linalg.det "torch.linalg.det")的别名 |
| [`logdet`](generated/torch.logdet.html#torch.logdet "torch.logdet") | 计算方阵或方阵批次的对数行列式。 |
| [`slogdet`](generated/torch.slogdet.html#torch.slogdet "torch.slogdet") | [`torch.linalg.slogdet()`](generated/torch.linalg.slogdet.html#torch.linalg.slogdet "torch.linalg.slogdet")的别名 |
| [`lu`](generated/torch.lu.html#torch.lu "torch.lu") | 计算矩阵或矩阵批次`A`的LU分解。 |
| [`lu_solve`](generated/torch.lu_solve.html#torch.lu_solve "torch.lu_solve") | 使用从[`lu_factor()`](generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor "torch.linalg.lu_factor")得到的A的部分主元LU分解返回线性系统$Ax = b$Ax=b的LU解。 |
| [`lu_unpack`](generated/torch.lu_unpack.html#torch.lu_unpack "torch.lu_unpack") | 将[`lu_factor()`](generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor "torch.linalg.lu_factor")返回的LU分解解包成P、L、U矩阵。 |
| [`matmul`](generated/torch.matmul.html#torch.matmul "torch.matmul") | 两个张量的矩阵乘积。 |
| [`matrix_power`](generated/torch.matrix_power.html#torch.matrix_power "torch.matrix_power") | [`torch.linalg.matrix_power()`](generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power "torch.linalg.matrix_power")的别名 |
| [`matrix_exp`](generated/torch.matrix_exp.html#torch.matrix_exp "torch.matrix_exp") | [`torch.linalg.matrix_exp()`](generated/torch.linalg.matrix_exp.html#torch.linalg.matrix_exp "torch.linalg.matrix_exp")的别名。 |
| [`mm`](generated/torch.mm.html#torch.mm "torch.mm") | 计算矩阵`input`和`mat2`的矩阵乘法。 |
| [`mv`](generated/torch.mv.html#torch.mv "torch.mv") | 计算矩阵`input`和向量`vec`的矩阵-向量乘积。 |
| [`orgqr`](generated/torch.orgqr.html#torch.orgqr "torch.orgqr") | [`torch.linalg.householder_product()`](generated/torch.linalg.householder_product.html#torch.linalg.householder_product "torch.linalg.householder_product")的别名。 |
| [`ormqr`](generated/torch.ormqr.html#torch.ormqr "torch.ormqr") | 计算Householder矩阵乘积与一般矩阵的矩阵-矩阵乘积。 |
| [`outer`](generated/torch.outer.html#torch.outer "torch.outer") | `input` 和 `vec2` 的外积。 |
| [`pinverse`](generated/torch.pinverse.html#torch.pinverse "torch.pinverse") | [`torch.linalg.pinv()`](generated/torch.linalg.pinv.html#torch.linalg.pinv "torch.linalg.pinv") 的别名 |
| [`qr`](generated/torch.qr.html#torch.qr "torch.qr") | 计算矩阵或批量矩阵 `input` 的 QR 分解，并返回一个命名元组 (Q, R)，使得 $\text{input} = Q R$input=QR，其中 $Q$Q 是正交矩阵或批量正交矩阵，$R$R 是上三角矩阵或批量上三角矩阵。 |
| [`svd`](generated/torch.svd.html#torch.svd "torch.svd") | 计算矩阵或矩阵批次 `input` 的奇异值分解。 |
| [`svd_lowrank`](generated/torch.svd_lowrank.html#torch.svd_lowrank "torch.svd_lowrank") | 返回矩阵、矩阵批次或稀疏矩阵 $A$A 的奇异值分解 `(U, S, V)`，使得 $A \approx U diag(S) V^T$A≈Udiag(S)VT。 |
| [`pca_lowrank`](generated/torch.pca_lowrank.html#torch.pca_lowrank "torch.pca_lowrank") | 对低秩矩阵、这类矩阵批次或稀疏矩阵执行线性主成分分析（PCA）。 |
| [`lobpcg`](generated/torch.lobpcg.html#torch.lobpcg "torch.lobpcg") | 使用无矩阵 LOBPCG 方法找到对称正定广义特征值问题的 k 个最大（或最小）特征值及其对应的特征向量。 |
| [`trapz`](generated/torch.trapz.html#torch.trapz "torch.trapz") | [`torch.trapezoid()`](generated/torch.trapezoid.html#torch.trapezoid "torch.trapezoid") 的别名。 |
| [`trapezoid`](generated/torch.trapezoid.html#torch.trapezoid "torch.trapezoid") | 计算沿着 `dim` 的 [梯形法则](https://en.wikipedia.org/wiki/Trapezoidal_rule)。 |
| [`cumulative_trapezoid`](generated/torch.cumulative_trapezoid.html#torch.cumulative_trapezoid "torch.cumulative_trapezoid") | 累积计算沿着 `dim` 的 [梯形法则](https://en.wikipedia.org/wiki/Trapezoidal_rule)。 |
| [`triangular_solve`](generated/torch.triangular_solve.html#torch.triangular_solve "torch.triangular_solve") | 解一个具有方形上三角或下三角可逆矩阵 $A$A 和多个右侧的方程组 $b$b。 |
| [`vdot`](generated/torch.vdot.html#torch.vdot "torch.vdot") | 计算沿着一个维度的两个 1D 向量的点积。 |

### Foreach 操作

警告

此 API 处于 beta 阶段，可能会有未来更改。不支持正向模式自动微分。

| [`_foreach_abs`](generated/torch._foreach_abs.html#torch._foreach_abs "torch._foreach_abs") | 对输入列表中的每个张量应用 [`torch.abs()`](generated/torch.abs.html#torch.abs "torch.abs")。 |
| --- | --- |
| [`_foreach_abs_`](generated/torch._foreach_abs_.html#torch._foreach_abs_ "torch._foreach_abs_") | 对输入列表中的每个张量应用 [`torch.abs()`](generated/torch.abs.html#torch.abs "torch.abs")。 |
| [`_foreach_acos`](generated/torch._foreach_acos.html#torch._foreach_acos "torch._foreach_acos") | 对输入列表中的每个张量应用 [`torch.acos()`](generated/torch.acos.html#torch.acos "torch.acos")。 |
| [`_foreach_acos_`](generated/torch._foreach_acos_.html#torch._foreach_acos_ "torch._foreach_acos_") | 对输入列表中的每个张量应用 [`torch.acos()`](generated/torch.acos.html#torch.acos "torch.acos")。 |
| [`_foreach_asin`](generated/torch._foreach_asin.html#torch._foreach_asin "torch._foreach_asin") | 对输入列表中的每个张量应用 [`torch.asin()`](generated/torch.asin.html#torch.asin "torch.asin")。 |
| [`_foreach_asin_`](generated/torch._foreach_asin_.html#torch._foreach_asin_ "torch._foreach_asin_") | 对输入列表中的每个张量应用 [`torch.asin()`](generated/torch.asin.html#torch.asin "torch.asin")。 |
| [`_foreach_atan`](generated/torch._foreach_atan.html#torch._foreach_atan "torch._foreach_atan") | 对输入列表中的每个张量应用 [`torch.atan()`](generated/torch.atan.html#torch.atan "torch.atan")。 |
| [`_foreach_atan_`](generated/torch._foreach_atan_.html#torch._foreach_atan_ "torch._foreach_atan_") | 对输入列表中的每个张量应用 [`torch.atan()`](generated/torch.atan.html#torch.atan "torch.atan")。 |
| [`_foreach_ceil`](generated/torch._foreach_ceil.html#torch._foreach_ceil "torch._foreach_ceil") | 对输入列表中的每个张量应用 [`torch.ceil()`](generated/torch.ceil.html#torch.ceil "torch.ceil")。 |
| [`_foreach_ceil_`](generated/torch._foreach_ceil_.html#torch._foreach_ceil_ "torch._foreach_ceil_") | 对输入列表中的每个张量应用 [`torch.ceil()`](generated/torch.ceil.html#torch.ceil "torch.ceil")。 |
| [`_foreach_cos`](generated/torch._foreach_cos.html#torch._foreach_cos "torch._foreach_cos") | 对输入列表中的每个张量应用 [`torch.cos()`](generated/torch.cos.html#torch.cos "torch.cos")。 |
| [`_foreach_cos_`](generated/torch._foreach_cos_.html#torch._foreach_cos_ "torch._foreach_cos_") | 对输入列表中的每个张量应用 [`torch.cos()`](generated/torch.cos.html#torch.cos "torch.cos")。 |
| [`_foreach_cosh`](generated/torch._foreach_cosh.html#torch._foreach_cosh "torch._foreach_cosh") | 对输入列表中的每个张量应用 [`torch.cosh()`](generated/torch.cosh.html#torch.cosh "torch.cosh")。 |
| [`_foreach_cosh_`](generated/torch._foreach_cosh_.html#torch._foreach_cosh_ "torch._foreach_cosh_") | 对输入列表中的每个张量应用 [`torch.cosh()`](generated/torch.cosh.html#torch.cosh "torch.cosh")。 |
| [`_foreach_erf`](generated/torch._foreach_erf.html#torch._foreach_erf "torch._foreach_erf") | 对输入列表中的每个张量应用 [`torch.erf()`](generated/torch.erf.html#torch.erf "torch.erf")。 |
| [`_foreach_erf_`](generated/torch._foreach_erf_.html#torch._foreach_erf_ "torch._foreach_erf_") | 对输入列表中的每个张量应用 [`torch.erf()`](generated/torch.erf.html#torch.erf "torch.erf")。 |
| [`_foreach_erfc`](generated/torch._foreach_erfc.html#torch._foreach_erfc "torch._foreach_erfc") | 对输入列表中的每个张量应用 [`torch.erfc()`](generated/torch.erfc.html#torch.erfc "torch.erfc")。 |
| [`_foreach_erfc_`](generated/torch._foreach_erfc_.html#torch._foreach_erfc_ "torch._foreach_erfc_") | 对输入列表中的每个张量应用 [`torch.erfc()`](generated/torch.erfc.html#torch.erfc "torch.erfc")。 |
| [`_foreach_exp`](generated/torch._foreach_exp.html#torch._foreach_exp "torch._foreach_exp") | 对输入列表中的每个张量应用 [`torch.exp()`](generated/torch.exp.html#torch.exp "torch.exp")。 |
| [`_foreach_exp_`](generated/torch._foreach_exp_.html#torch._foreach_exp_ "torch._foreach_exp_") | 对输入列表中的每个张量应用 [`torch.exp()`](generated/torch.exp.html#torch.exp "torch.exp")。 |
| [`_foreach_expm1`](generated/torch._foreach_expm1.html#torch._foreach_expm1 "torch._foreach_expm1") | 对输入列表中的每个张量应用 [`torch.expm1()`](generated/torch.expm1.html#torch.expm1 "torch.expm1")。 |
| [`_foreach_expm1_`](generated/torch._foreach_expm1_.html#torch._foreach_expm1_ "torch._foreach_expm1_") | 对输入列表中的每个张量应用 [`torch.expm1()`](generated/torch.expm1.html#torch.expm1 "torch.expm1")。 |
| [`_foreach_floor`](generated/torch._foreach_floor.html#torch._foreach_floor "torch._foreach_floor") | 对输入列表中的每个张量应用 [`torch.floor()`](generated/torch.floor.html#torch.floor "torch.floor")。 |
| [`_foreach_floor_`](generated/torch._foreach_floor_.html#torch._foreach_floor_ "torch._foreach_floor_") | 对输入列表中的每个张量应用 [`torch.floor()`](generated/torch.floor.html#torch.floor "torch.floor")。 |
| [`_foreach_log`](generated/torch._foreach_log.html#torch._foreach_log "torch._foreach_log") | 对输入列表中的每个张量应用 [`torch.log()`](generated/torch.log.html#torch.log "torch.log")。 |
| [`_foreach_log_`](generated/torch._foreach_log_.html#torch._foreach_log_ "torch._foreach_log_") | 对输入列表中的每个张量应用 [`torch.log()`](generated/torch.log.html#torch.log "torch.log")。 |
| [`_foreach_log10`](generated/torch._foreach_log10.html#torch._foreach_log10 "torch._foreach_log10") | 对输入列表中的每个张量应用 [`torch.log10()`](generated/torch.log10.html#torch.log10 "torch.log10")。 |
| [`_foreach_log10_`](generated/torch._foreach_log10_.html#torch._foreach_log10_ "torch._foreach_log10_") | 对输入列表中的每个张量应用 [`torch.log10()`](generated/torch.log10.html#torch.log10 "torch.log10")。 |
| [`_foreach_log1p`](generated/torch._foreach_log1p.html#torch._foreach_log1p "torch._foreach_log1p") | 对输入列表中的每个张量应用 [`torch.log1p()`](generated/torch.log1p.html#torch.log1p "torch.log1p")。 |
| [`_foreach_log1p_`](generated/torch._foreach_log1p_.html#torch._foreach_log1p_ "torch._foreach_log1p_") | 对输入列表中的每个张量应用 [`torch.log1p()`](generated/torch.log1p.html#torch.log1p "torch.log1p")。 |
| [`_foreach_log2`](generated/torch._foreach_log2.html#torch._foreach_log2 "torch._foreach_log2") | 对输入列表中的每个张量应用 [`torch.log2()`](generated/torch.log2.html#torch.log2 "torch.log2")。 |
| [`_foreach_log2_`](generated/torch._foreach_log2_.html#torch._foreach_log2_ "torch._foreach_log2_") | 对输入列表中的每个张量应用 [`torch.log2()`](generated/torch.log2.html#torch.log2 "torch.log2")。 |
| [`_foreach_neg`](generated/torch._foreach_neg.html#torch._foreach_neg "torch._foreach_neg") | 对输入列表中的每个张量应用 [`torch.neg()`](generated/torch.neg.html#torch.neg "torch.neg")。 |
| [`_foreach_neg_`](generated/torch._foreach_neg_.html#torch._foreach_neg_ "torch._foreach_neg_") | 对输入列表中的每个张量应用 [`torch.neg()`](generated/torch.neg.html#torch.neg "torch.neg")。 |
| [`_foreach_tan`](generated/torch._foreach_tan.html#torch._foreach_tan "torch._foreach_tan") | 对输入列表中的每个张量应用 [`torch.tan()`](generated/torch.tan.html#torch.tan "torch.tan")。 |
| [`_foreach_tan_`](generated/torch._foreach_tan_.html#torch._foreach_tan_ "torch._foreach_tan_") | 对输入列表中的每个张量应用 [`torch.tan()`](generated/torch.tan.html#torch.tan "torch.tan")。 |
| [`_foreach_sin`](generated/torch._foreach_sin.html#torch._foreach_sin "torch._foreach_sin") | 对输入列表中的每个张量应用 [`torch.sin()`](generated/torch.sin.html#torch.sin "torch.sin")。 |
| [`_foreach_sin_`](generated/torch._foreach_sin_.html#torch._foreach_sin_ "torch._foreach_sin_") | 对输入列表中的每个张量应用 [`torch.sin()`](generated/torch.sin.html#torch.sin "torch.sin")。 |
| [`_foreach_sinh`](generated/torch._foreach_sinh.html#torch._foreach_sinh "torch._foreach_sinh") | 对输入列表中的每个张量应用 [`torch.sinh()`](generated/torch.sinh.html#torch.sinh "torch.sinh")。 |
| [`_foreach_sinh_`](generated/torch._foreach_sinh_.html#torch._foreach_sinh_ "torch._foreach_sinh_") | 对输入列表中的每个张量应用 [`torch.sinh()`](generated/torch.sinh.html#torch.sinh "torch.sinh")。 |
| [`_foreach_round`](generated/torch._foreach_round.html#torch._foreach_round "torch._foreach_round") | 对输入列表中的每个张量应用 [`torch.round()`](generated/torch.round.html#torch.round "torch.round")。 |
| [`_foreach_round_`](generated/torch._foreach_round_.html#torch._foreach_round_ "torch._foreach_round_") | 对输入列表中的每个张量应用 [`torch.round()`](generated/torch.round.html#torch.round "torch.round")。 |
| [`_foreach_sqrt`](generated/torch._foreach_sqrt.html#torch._foreach_sqrt "torch._foreach_sqrt") | 对输入列表中的每个张量应用 [`torch.sqrt()`](generated/torch.sqrt.html#torch.sqrt "torch.sqrt")。 |
| [`_foreach_sqrt_`](generated/torch._foreach_sqrt_.html#torch._foreach_sqrt_ "torch._foreach_sqrt_") | 对输入列表中的每个张量应用 [`torch.sqrt()`](generated/torch.sqrt.html#torch.sqrt "torch.sqrt")。 |
| [`_foreach_lgamma`](generated/torch._foreach_lgamma.html#torch._foreach_lgamma "torch._foreach_lgamma") | 对输入列表中的每个张量应用 [`torch.lgamma()`](generated/torch.lgamma.html#torch.lgamma "torch.lgamma")。 |
| [`_foreach_lgamma_`](generated/torch._foreach_lgamma_.html#torch._foreach_lgamma_ "torch._foreach_lgamma_") | 对输入列表中的每个张量应用 [`torch.lgamma()`](generated/torch.lgamma.html#torch.lgamma "torch.lgamma")。 |
| [`_foreach_frac`](generated/torch._foreach_frac.html#torch._foreach_frac "torch._foreach_frac") | 对输入列表中的每个张量应用 [`torch.frac()`](generated/torch.frac.html#torch.frac "torch.frac")。 |
| [`_foreach_frac_`](generated/torch._foreach_frac_.html#torch._foreach_frac_ "torch._foreach_frac_") | 对输入列表中的每个张量应用 [`torch.frac()`](generated/torch.frac.html#torch.frac "torch.frac")。 |
| [`_foreach_reciprocal`](generated/torch._foreach_reciprocal.html#torch._foreach_reciprocal "torch._foreach_reciprocal") | 对输入列表中的每个张量应用 [`torch.reciprocal()`](generated/torch.reciprocal.html#torch.reciprocal "torch.reciprocal")。 |
| [`_foreach_reciprocal_`](generated/torch._foreach_reciprocal_.html#torch._foreach_reciprocal_ "torch._foreach_reciprocal_") | 对输入列表中的每个张量应用 [`torch.reciprocal()`](generated/torch.reciprocal.html#torch.reciprocal "torch.reciprocal")。 |
| [`_foreach_sigmoid`](generated/torch._foreach_sigmoid.html#torch._foreach_sigmoid "torch._foreach_sigmoid") | 对输入列表中的每个张量应用 [`torch.sigmoid()`](generated/torch.sigmoid.html#torch.sigmoid "torch.sigmoid")。 |
| [`_foreach_sigmoid_`](generated/torch._foreach_sigmoid_.html#torch._foreach_sigmoid_ "torch._foreach_sigmoid_") | 对输入列表中的每个张量应用 [`torch.sigmoid()`](generated/torch.sigmoid.html#torch.sigmoid "torch.sigmoid")。 |
| [`_foreach_trunc`](generated/torch._foreach_trunc.html#torch._foreach_trunc "torch._foreach_trunc") | 对输入列表中的每个张量应用 [`torch.trunc()`](generated/torch.trunc.html#torch.trunc "torch.trunc")。 |
| [`_foreach_trunc_`](generated/torch._foreach_trunc_.html#torch._foreach_trunc_ "torch._foreach_trunc_") | 对输入列表中的每个张量应用 [`torch.trunc()`](generated/torch.trunc.html#torch.trunc "torch.trunc")。 |
| [`_foreach_zero_`](generated/torch._foreach_zero_.html#torch._foreach_zero_ "torch._foreach_zero_") | 对输入列表中的每个张量应用 `torch.zero()`。 |

## Utilities

| [`compiled_with_cxx11_abi`](generated/torch.compiled_with_cxx11_abi.html#torch.compiled_with_cxx11_abi "torch.compiled_with_cxx11_abi") | 返回 PyTorch 是否使用 _GLIBCXX_USE_CXX11_ABI=1 构建。 |
| --- | --- |
| [`result_type`](generated/torch.result_type.html#torch.result_type "torch.result_type") | 返回在提供的输入张量上执行算术运算后将产生的 [`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype")。 |
| [`can_cast`](generated/torch.can_cast.html#torch.can_cast "torch.can_cast") | 确定在 PyTorch 类型转换规则下是否允许类型转换，规则描述在类型提升 [文档](tensor_attributes.html#type-promotion-doc)中。 |
| [`promote_types`](generated/torch.promote_types.html#torch.promote_types "torch.promote_types") | 返回具有不小于 type1 或 type2 的大小和标量类型的 [`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype")。 |
| [`use_deterministic_algorithms`](generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms "torch.use_deterministic_algorithms") | 设置 PyTorch 操作是否必须使用“确定性”算法。 |
| [`are_deterministic_algorithms_enabled`](generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled "torch.are_deterministic_algorithms_enabled") | 如果全局确定性标志打开，则返回True。 |
| [`is_deterministic_algorithms_warn_only_enabled`](generated/torch.is_deterministic_algorithms_warn_only_enabled.html#torch.is_deterministic_algorithms_warn_only_enabled "torch.is_deterministic_algorithms_warn_only_enabled") | 如果全局确定性标志设置为仅警告，则返回True。 |
| [`set_deterministic_debug_mode`](generated/torch.set_deterministic_debug_mode.html#torch.set_deterministic_debug_mode "torch.set_deterministic_debug_mode") | 设置确定性操作的调试模式。 |
| [`get_deterministic_debug_mode`](generated/torch.get_deterministic_debug_mode.html#torch.get_deterministic_debug_mode "torch.get_deterministic_debug_mode") | 返回确定性操作的调试模式的当前值。 |
| [`set_float32_matmul_precision`](generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision "torch.set_float32_matmul_precision") | 设置float32矩阵乘法的内部精度。 |
| [`get_float32_matmul_precision`](generated/torch.get_float32_matmul_precision.html#torch.get_float32_matmul_precision "torch.get_float32_matmul_precision") | 返回float32矩阵乘法精度的当前值。 |
| [`set_warn_always`](generated/torch.set_warn_always.html#torch.set_warn_always "torch.set_warn_always") | 当此标志为False（默认）时，一些PyTorch警告可能只会在进程中出现一次。 |
| [`is_warn_always_enabled`](generated/torch.is_warn_always_enabled.html#torch.is_warn_always_enabled "torch.is_warn_always_enabled") | 如果全局的warn_always标志打开，则返回True。 |
| [`vmap`](generated/torch.vmap.html#torch.vmap "torch.vmap") | vmap是矢量化映射；`vmap(func)`返回一个新函数，该函数将`func`映射到输入的某个维度上。 |
| [`_assert`](generated/torch._assert.html#torch._assert "torch._assert") | Python的assert的包装器，可进行符号跟踪。 |

## 符号数字

```py
class torch.SymInt(node)¶
```

像一个整数（包括魔术方法），但重定向包装节点上的所有操作。这特别用于在符号形状工作流程中符号记录操作。

```py
class torch.SymFloat(node)¶
```

像一个浮点数（包括魔术方法），但重定向包装节点上的所有操作。这特别用于在符号形状工作流程中符号记录操作。

```py
class torch.SymBool(node)¶
```

像一个布尔值（包括魔术方法），但重定向包装节点上的所有操作。这特别用于在符号形状工作流程中符号记录操作。

与常规布尔值不同，常规布尔运算符会强制额外的保护而不是符号化评估。请改用位运算符来处理这个问题。

| [`sym_float`](generated/torch.sym_float.html#torch.sym_float "torch.sym_float") | 用于浮点数转换的SymInt感知实用程序。 |
| --- | --- |
| [`sym_int`](generated/torch.sym_int.html#torch.sym_int "torch.sym_int") | 用于整数转换的SymInt感知实用程序。 |
| [`sym_max`](generated/torch.sym_max.html#torch.sym_max "torch.sym_max") | 用于max()的SymInt感知实用程序。 |
| [`sym_min`](generated/torch.sym_min.html#torch.sym_min "torch.sym_min") | 用于max()的SymInt感知实用程序。 |
| [`sym_not`](generated/torch.sym_not.html#torch.sym_not "torch.sym_not") | 用于逻辑否定的SymInt感知实用程序。 |
| [`sym_ite`](generated/torch.sym_ite.html#torch.sym_ite "torch.sym_ite") |  |

## 导出路径

警告

此功能是一个原型，未来可能会有兼容性破坏性的更改。

导出生成的/exportdb/index

## 控制流

警告

此功能是一个原型，未来可能会有兼容性破坏性的更改。

| [`cond`](generated/torch.cond.html#torch.cond "torch.cond") | 有条件地应用true_fn或false_fn。 |
| --- | --- |

## 优化

| [`compile`](generated/torch.compile.html#torch.compile "torch.compile") | 使用TorchDynamo和指定的后端优化给定的模型/函数。 |
| --- | --- |

[torch.compile文档](https://pytorch.org/docs/main/compile/index.html)

## 操作符标签

```py
class torch.Tag¶
```

成员：

核心

数据相关输出

动态输出形状

生成的

原地视图

非确定性位运算

非确定性种子

逐点

pt2兼容标签

视图复制

```py
property name¶
```
