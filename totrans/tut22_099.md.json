["```py\npip  install  sklearn\npip  install  transformers==4.29.2 \n```", "```py\nyes  y  |  pip  uninstall  torch  tochvision\nyes  y  |  pip  install  --pre  torch  -f  https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html \n```", "```py\nimport logging\nimport numpy as np\nimport os\nimport random\nimport sys\nimport time\nimport torch\n\nfrom argparse import Namespace\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom tqdm import tqdm\nfrom transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\nfrom transformers import glue_compute_metrics as compute_metrics\nfrom transformers import glue_output_modes as output_modes\nfrom transformers import glue_processors as processors\nfrom transformers import glue_convert_examples_to_features as convert_examples_to_features\n\n# Setup logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.WARN)\n\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(\n   logging.WARN)  # Reduce logging\n\nprint(torch.__version__) \n```", "```py\ntorch.set_num_threads(1)\nprint(torch.__config__.parallel_info()) \n```", "```py\npython  download_glue_data.py  --data_dir='glue_data'  --tasks='MRPC' \n```", "```py\nexport GLUE_DIR=./glue_data\nexport TASK_NAME=MRPC\nexport OUT_DIR=./$TASK_NAME/\npython ./run_glue.py \\\n    --model_type bert \\\n    --model_name_or_path bert-base-uncased \\\n    --task_name $TASK_NAME \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --data_dir $GLUE_DIR/$TASK_NAME \\\n    --max_seq_length 128 \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 3.0 \\\n    --save_steps 100000 \\\n    --output_dir $OUT_DIR \n```", "```py\nconfigs = Namespace()\n\n# The output directory for the fine-tuned model, $OUT_DIR.\nconfigs.output_dir = \"./MRPC/\"\n\n# The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME.\nconfigs.data_dir = \"./glue_data/MRPC\"\n\n# The model name or path for the pre-trained model.\nconfigs.model_name_or_path = \"bert-base-uncased\"\n# The maximum length of an input sequence\nconfigs.max_seq_length = 128\n\n# Prepare GLUE task.\nconfigs.task_name = \"MRPC\".lower()\nconfigs.processor = processors[configs.task_name]()\nconfigs.output_mode = output_modes[configs.task_name]\nconfigs.label_list = configs.processor.get_labels()\nconfigs.model_type = \"bert\".lower()\nconfigs.do_lower_case = True\n\n# Set the device, batch size, topology, and caching flags.\nconfigs.device = \"cpu\"\nconfigs.per_gpu_eval_batch_size = 8\nconfigs.n_gpu = 0\nconfigs.local_rank = -1\nconfigs.overwrite_cache = False\n\n# Set random seed for reproducibility.\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nset_seed(42) \n```", "```py\ntokenizer = BertTokenizer.from_pretrained(\n    configs.output_dir, do_lower_case=configs.do_lower_case)\n\nmodel = BertForSequenceClassification.from_pretrained(configs.output_dir)\nmodel.to(configs.device) \n```", "```py\n# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndef evaluate(args, model, tokenizer, prefix=\"\"):\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n\n    results = {}\n    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        # Note that DistributedSampler samples randomly\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n        # multi-gpu eval\n        if args.n_gpu > 1:\n            model = torch.nn.DataParallel(model)\n\n        # Eval!\n        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n        logger.info(\"  Num examples = %d\", len(eval_dataset))\n        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            model.eval()\n            batch = tuple(t.to(args.device) for t in batch)\n\n            with torch.no_grad():\n                inputs = {'input_ids':      batch[0],\n                          'attention_mask': batch[1],\n                          'labels':         batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n                outputs = model(**inputs)\n                tmp_eval_loss, logits = outputs[:2]\n\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == \"classification\":\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == \"regression\":\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n\n        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n        with open(output_eval_file, \"w\") as writer:\n            logger.info(\"***** Eval results {} *****\".format(prefix))\n            for key in sorted(result.keys()):\n                logger.info(\" %s = %s\", key, str(result[key]))\n                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return results\n\ndef load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n        'dev' if evaluate else 'train',\n        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n        str(args.max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(\"Loading features from cached file %s\", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']:\n            # HACK(label indices are swapped in RoBERTa pretrained model)\n            label_list[1], label_list[2] = label_list[2], label_list[1]\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples,\n                                                tokenizer,\n                                                label_list=label_list,\n                                                max_length=args.max_seq_length,\n                                                output_mode=output_mode,\n                                                pad_on_left=bool(args.model_type in ['xlnet']),                 # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0,\n        )\n        if args.local_rank in [-1, 0]:\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == \"classification\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == \"regression\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset \n```", "```py\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\nprint(quantized_model) \n```", "```py\ndef print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model) \n```", "```py\ndef time_model_evaluation(model, configs, tokenizer):\n    eval_start_time = time.time()\n    result = evaluate(configs, model, tokenizer, prefix=\"\")\n    eval_end_time = time.time()\n    eval_duration_time = eval_end_time - eval_start_time\n    print(result)\n    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n\n# Evaluate the original FP32 BERT model\ntime_model_evaluation(model, configs, tokenizer)\n\n# Evaluate the INT8 BERT model after the dynamic quantization\ntime_model_evaluation(quantized_model, configs, tokenizer) \n```", "```py\n| Prec | F1 score | Model Size | 1 thread | 4 threads |\n| FP32 |  0.9019  |   438 MB   | 160 sec  | 85 sec    |\n| INT8 |  0.902   |   181 MB   |  90 sec  | 46 sec    | \n```", "```py\ndef ids_tensor(shape, vocab_size):\n    #  Creates a random int32 tensor of the shape within the vocab size\n    return torch.randint(0, vocab_size, shape=shape, dtype=torch.int, device='cpu')\n\ninput_ids = ids_tensor([8, 128], 2)\ntoken_type_ids = ids_tensor([8, 128], 2)\nattention_mask = ids_tensor([8, 128], vocab_size=2)\ndummy_input = (input_ids, attention_mask, token_type_ids)\ntraced_model = torch.jit.trace(quantized_model, dummy_input)\ntorch.jit.save(traced_model, \"bert_traced_eager_quant.pt\") \n```", "```py\nloaded_quantized_model = torch.jit.load(\"bert_traced_eager_quant.pt\") \n```"]