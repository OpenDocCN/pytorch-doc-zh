["```py\ntorch.distributed.tensor.parallel.parallelize_module(module, device_mesh, parallelize_plan, tp_mesh_dim=0)\u00b6\n```", "```py\n>>> from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>>\n>>> # Define the module.\n>>> m = Model(...)\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>> m = parallelize_module(m, tp_mesh, {\"w1\": ColwiseParallel(), \"w2\": RowwiseParallel()})\n>>> \n```", "```py\nclass torch.distributed.tensor.parallel.ColwiseParallel(*, input_layouts=None, output_layouts=None, use_local_output=True)\u00b6\n```", "```py\n>>> from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel\n>>> ...\n>>> # By default, the input of the \"w1\" Linear will be annotated to Replicated DTensor\n>>> # and the output of \"w1\" will return :class:`torch.Tensor` that shards on the last dim.\n>>>>\n>>> parallelize_module(\n>>>     module=block, # this can be a submodule or module\n>>>     ...,\n>>>     parallelize_plan={\"w1\": ColwiseParallel()},\n>>> )\n>>> ... \n```", "```py\nclass torch.distributed.tensor.parallel.RowwiseParallel(*, input_layouts=None, output_layouts=None, use_local_output=True)\u00b6\n```", "```py\n>>> from torch.distributed.tensor.parallel import parallelize_module, RowwiseParallel\n>>> ...\n>>> # By default, the input of the \"w2\" Linear will be annotated to DTensor that shards on the last dim\n>>> # and the output of \"w2\" will return a replicated :class:`torch.Tensor`.\n>>>\n>>> parallelize_module(\n>>>     module=block, # this can be a submodule or module\n>>>     ...,\n>>>     parallelize_plan={\"w2\": RowwiseParallel()},\n>>> )\n>>> ... \n```", "```py\nclass torch.distributed.tensor.parallel.PrepareModuleInput(*, input_layouts, desired_input_layouts, use_local_output=False)\u00b6\n```", "```py\n>>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput\n>>> ...\n>>> # According to the style specified below, the first input of attn will be annotated to Sharded DTensor\n>>> # and then redistributed to Replicated DTensor.\n>>> parallelize_module(\n>>>     module=block, # this can be a submodule or module\n>>>     ...,\n>>>     parallelize_plan={\n>>>         \"attn\": PrepareModuleInput(\n>>>             input_layouts=(Shard(0), None, None, ...),\n>>>             desired_input_layouts=(Replicate(), None, None, ...)\n>>>         ),\n>>>     }\n>>> ) \n```", "```py\nclass torch.distributed.tensor.parallel.PrepareModuleOutput(*, output_layouts, desired_output_layouts, use_local_output=True)\u00b6\n```", "```py\n>>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleOutput\n>>> ...\n>>> # According to the style specified below, the first input of attn will be annotated to Sharded DTensor\n>>> # and then redistributed to Replicated DTensor.\n>>> parallelize_module(\n>>>     module=block, # this can be a submodule or module\n>>>     ...,\n>>>     parallelize_plan={\n>>>         \"submodule\": PrepareModuleOutput(\n>>>             output_layouts=Replicate(),\n>>>             desired_output_layouts=Shard(0)\n>>>         ),\n>>>     }\n>>> ) \n```"]