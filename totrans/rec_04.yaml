- en: torchrec.distributed
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torchrec.distributed
- en: 原文：[https://pytorch.org/torchrec/torchrec.distributed.html](https://pytorch.org/torchrec/torchrec.distributed.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/torchrec/torchrec.distributed.html](https://pytorch.org/torchrec/torchrec.distributed.html)
- en: Torchrec Distributed
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Torchrec Distributed
- en: Torchrec distributed provides the necessary modules and operations to enable
    model parallelism.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Torchrec distributed提供了必要的模块和操作来实现模型并行处理。
- en: 'These include:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包括：
- en: model parallelism through DistributedModelParallel.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过DistributedModelParallel进行模型并行处理。
- en: collective operations for comms, including All-to-All and Reduce-Scatter.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于通信的集合操作，包括All-to-All和Reduce-Scatter。
- en: collective operations wrappers for sparse features, KJT, and various embedding
    types.
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于稀疏特征、KJT和各种嵌入类型的集合操作包装器。
- en: sharded implementations of various modules including ShardedEmbeddingBag for
    nn.EmbeddingBag, ShardedEmbeddingBagCollection for EmbeddingBagCollection
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括ShardedEmbeddingBag用于nn.EmbeddingBag的各种模块的分片实现，ShardedEmbeddingBagCollection用于EmbeddingBagCollection
- en: embedding sharders that define sharding for any sharded module implementation.
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义任何分片模块实现的分片器。
- en: support for various compute kernels, which are optimized for compute device
    (CPU/GPU) and may include batching together embedding tables and/or optimizer
    fusion.
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持各种计算内核，针对计算设备（CPU/GPU）进行优化，可能包括将嵌入表和/或优化器融合在一起进行批处理。
- en: pipelined training through TrainPipelineSparseDist that overlaps dataloading
    device transfer (copy to GPU), inter*device communications (input_dist), and computation
    (forward, backward) for increased performance.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过TrainPipelineSparseDist进行流水线训练，可以重叠数据加载设备传输（复制到GPU）、设备间通信（input_dist）和计算（前向、后向）以提高性能。
- en: quantization support for reduced precision training and inference.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持减少精度训练和推断的量化。
- en: '## torchrec.distributed.collective_utils[](#module-torchrec.distributed.collective_utils
    "Permalink to this heading")'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '## torchrec.distributed.collective_utils[](#module-torchrec.distributed.collective_utils
    "Permalink to this heading")'
- en: This file contains utilities for constructing collective based control flows.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件包含用于构建基于集合的控制流的实用程序。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Invokes a function on the designated rank and broadcasts the result to all members
    within the group.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定的rank上调用函数，并将结果广播给组内的所有成员。
- en: 'Example:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Checks if the current processs is the leader.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 检查当前进程是否为领导者。
- en: 'Parameters:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*Optional**[**dist.ProcessGroup**]*) – the process’s rank within the
    pg is used to determine if the process is the leader. pg being None implies that
    the process is the only member in the group (e.g. a single process program).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg** (*Optional**[**dist.ProcessGroup**]*) – pg内的进程排名用于确定进程是否为领导者。pg为None意味着进程是组中唯一的成员（例如，单个进程程序）。'
- en: '**leader_rank** (*int*) – the definition of leader (defaults to 0). The caller
    can override it with a context-specific definition.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**leader_rank** (*int*) – 领导者的定义（默认为0）。调用者可以使用特定于上下文的定义进行覆盖。'
- en: '[PRE3]  ## torchrec.distributed.comm[](#module-torchrec.distributed.comm "Permalink
    to this heading")'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]  ## torchrec.distributed.comm[](#module-torchrec.distributed.comm "Permalink
    to this heading")'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Gets the group rank of the worker group. Also available with GROUP_RANK environment
    varible A number between 0 and get_num_groups() (See [https://pytorch.org/docs/stable/elastic/run.html](https://pytorch.org/docs/stable/elastic/run.html))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 获取工作组的组排名。也可通过GROUP_RANK环境变量获得，介于0和get_num_groups()之间（参见[https://pytorch.org/docs/stable/elastic/run.html](https://pytorch.org/docs/stable/elastic/run.html)）
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Gets the local rank of the local processes (see [https://pytorch.org/docs/stable/elastic/run.html](https://pytorch.org/docs/stable/elastic/run.html))
    This is usually the rank of the worker on its node
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 获取本地进程的本地排名（参见[https://pytorch.org/docs/stable/elastic/run.html](https://pytorch.org/docs/stable/elastic/run.html)）通常是工作节点上的工作进程的排名
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Gets the number of worker groups. Usually equivalent to max_nnodes (See [https://pytorch.org/docs/stable/elastic/run.html](https://pytorch.org/docs/stable/elastic/run.html))
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 获取工作组的数量。通常等同于max_nnodes（参见[https://pytorch.org/docs/stable/elastic/run.html](https://pytorch.org/docs/stable/elastic/run.html)）
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Creates sub process groups (intra and cross node)  ## torchrec.distributed.comm_ops[](#module-torchrec.distributed.comm_ops
    "Permalink to this heading")'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '创建子进程组（节点内和跨节点）  ## torchrec.distributed.comm_ops[](#module-torchrec.distributed.comm_ops
    "Permalink to this heading")'
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Bases: `object`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The data class that collects the attributes when calling the alltoall_dense
    operation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用alltoall_dense操作时收集属性的数据类。
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Bases: `object`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The data class that collects the attributes when calling the alltoall_pooled
    operation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用alltoall_pooled操作时收集属性的数据类。
- en: '[PRE15]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: batch size in each rank
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个rank中的批处理大小
- en: 'Type:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: List[int]
- en: '[PRE16]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: number of features (sum of dimensions) of the embedding in each rank.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每个rank中嵌入的特征数量（维度之和）。
- en: 'Type:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: List[int]
- en: '[PRE17]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: the tensor version of dim_sum_per_rank, this is only used by the fast kernel
    of _recat_pooled_embedding_grad_out.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: dim_sum_per_rank的张量版本，仅由_recat_pooled_embedding_grad_out的快速内核使用。
- en: 'Type:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Optional[Tensor]
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Optional[Tensor]
- en: '[PRE18]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: cumulative sum of dim_sum_per_rank, this is only used by the fast kernel of
    _recat_pooled_embedding_grad_out.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: dim_sum_per_rank的累积和，仅由_recat_pooled_embedding_grad_out的快速内核使用。
- en: 'Type:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Optional[Tensor]
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Optional[Tensor]
- en: '[PRE19]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: quantized communication codecs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 量化通信编解码器。
- en: 'Type:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Optional[[QuantizedCommCodecs](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Optional[[QuantizedCommCodecs](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")]
- en: '[PRE20]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Bases: `object`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The data class that collects the attributes when calling the alltoall_sequence
    operation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用alltoall_sequence操作时收集属性的数据类。
- en: '[PRE26]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: embedding dimension.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入维度。
- en: 'Type:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: int
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: int
- en: '[PRE27]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: lengths of sparse features after AlltoAll.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: AlltoAll后稀疏特征的长度。
- en: 'Type:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Tensor
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 张量
- en: '[PRE28]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: recat tensor for forward.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传递的recat张量。
- en: 'Type:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Optional[Tensor]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Optional[Tensor]
- en: '[PRE29]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: recat tensor for backward.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为后向传递的recat张量。
- en: 'Type:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Tensor
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 张量
- en: '[PRE30]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: input splits.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输入分割。
- en: 'Type:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: List[int]
- en: '[PRE31]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: output splits.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出分割。
- en: 'Type:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: List[int]
- en: '[PRE32]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: whether variable batch size is enabled.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 是否启用可变批处理大小。
- en: 'Type:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: bool
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 布尔值
- en: '[PRE33]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: quantized communication codecs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 量化通信编解码器。
- en: 'Type:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Optional[[QuantizedCommCodecs](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")]
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Optional[[QuantizedCommCodecs](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")]
- en: '[PRE34]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: lengths of sparse features before AlltoAll.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: AlltoAll之前稀疏特征的长度。
- en: 'Type:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Optional[Tensor]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Optional[Tensor]
- en: '[PRE35]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Bases: `object`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The data class that collects the attributes when calling the alltoallv operation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 调用alltoallv操作时收集属性的数据类。
- en: '[PRE45]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: number of features (sum of dimensions) of the embedding in each rank.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 每个排名中嵌入的特征数量（维度之和）。
- en: 'Type:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: List[int]
- en: '[PRE46]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: global batch size for each rank.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每个排名的全局批量大小。
- en: 'Type:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: int
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: int
- en: '[PRE47]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: local batch size before scattering.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '分散之前的本地批量大小。 '
- en: 'Type:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: int
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: int
- en: '[PRE48]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '(List[int]): local batch sizes for each embedding table locally (in my current
    rank).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (List[int])：每个嵌入表在我的当前排名中的本地批量大小。
- en: 'Type:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: List[int]
- en: '[PRE49]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: embedding dimension of each embedding table locally (in my current rank).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 每个嵌入表的嵌入维度（在我的当前排名中）。
- en: 'Type:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: List[int]
- en: '[PRE50]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The input split sizes for each rank, this remembers how to split the input when
    doing the all_to_all_single operation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 每个排名的输入分割大小，这记住了在执行all_to_all_single操作时如何分割输入。
- en: 'Type:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: List[int]
- en: '[PRE51]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The output split sizes for each rank, this remembers how to fill the output
    when doing the all_to_all_single operation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 每个排名的输出分割大小，这记住了在执行all_to_all_single操作时如何填充输出。
- en: 'Type:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: List[int]
- en: '[PRE52]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Bases: `Function`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE61]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 定义使用反向模式自动微分区分操作的公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将被所有子类覆盖。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受上下文`ctx`作为第一个参数，后面是[`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.forward")返回的尽可能多的输出（对于前向函数的非张量输出将传入None），并且应该返回与[`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.forward")的输入一样多的张量。每个参数都是相对于给定输出的梯度，每个返回值都应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，可以为该输入传递None作为梯度。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.backward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传递期间保存的张量。它还有一个属性`ctx.needs_input_grad`，是一个布尔值元组，表示每个输入是否需要梯度。例如，[`backward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.backward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.backward")将在第一个输入需要计算相对于输出的梯度时具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE62]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Define the forward of the custom autograd Function.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义自动求导函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将被所有子类覆盖。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE63]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文ctx作为第一个参数，后面是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分开前向和ctx）：
- en: '[PRE64]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须重写`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向的输入元组。
- en: See extending-autograd for more details
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储在反向传递期间可以检索的任意数据。张量不应直接存储在ctx上（尽管出于向后兼容性目的目前未强制执行）。相反，如果打算在`backward`中使用它们（等效地，`vjp`），则应使用`ctx.save_for_backward()`保存张量，或者如果打算在`jvp`中使用它们，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE65]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Bases: `Function`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE66]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 定义使用反向模式自动微分区分操作的公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将被所有子类覆盖。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文 `ctx` 作为第一个参数，后面是与 [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward") 返回的输出一样多的输出（对于前向函数的非张量输出，将传入
    None），并且应该返回与 [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward") 的输入一样多的张量。每个参数是相对于给定输出的梯度，每个返回值应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将
    None 作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.backward
    "torchrec.distributed.comm_ops.All2All_Pooled_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可以用来检索在前向传递期间保存的张量。它还有一个属性 `ctx.needs_input_grad`，是一个布尔值元组，表示每个输入是否需要梯度。例如，如果
    [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward "torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward")
    的第一个输入需要计算相对于输出的梯度，则 `backward()` 将具有 `ctx.needs_input_grad[0] = True`。
- en: '[PRE67]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Define the forward of the custom autograd Function.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义 autograd 函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数应该被所有子类重写。有两种定义前向的方式：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 用法 1（合并前向和 ctx）：
- en: '[PRE68]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文 ctx 作为第一个参数，后面是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看 combining-forward-context 以获取更多详细信息
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 用法 2（分开前向和 ctx）：
- en: '[PRE69]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受 ctx 参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须重写 `torch.autograd.Function.setup_context()` 静态方法来处理设置 `ctx` 对象。`output`
    是前向的输出，`inputs` 是前向的输入的元组。
- en: See extending-autograd for more details
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看 extending-autograd 以获取更多详细信息
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可以用来存储任意数据，然后在反向传播期间检索。张量不应直接存储在 ctx 上（尽管为了向后兼容性，目前没有强制执行）。相反，如果打算在 `backward`
    中使用张量，则应使用 `ctx.save_for_backward()` 保存它们（等效地，`vjp`），或者如果打算在 `jvp` 中使用，则应使用 `ctx.save_for_forward()`
    保存它们。
- en: '[PRE70]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Bases: `Function`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE71]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个用于反向模式自动微分的操作的公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数应该被所有子类重写。（定义这个函数等同于定义 `vjp` 函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.forward "torchrec.distributed.comm_ops.All2All_Seq_Req.forward")
    returned (None will be passed in for non tensor outputs of the forward function),
    and it should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req.forward"). Each argument is the
    gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文 `ctx` 作为第一个参数，后面是与 [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req.forward") 返回的输出一样多的输出（对于前向函数的非张量输出，将传入
    None），并且应该返回与 [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req.forward") 的输入一样多的张量。每个参数是相对于给定输出的梯度，每个返回值应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将
    None 作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.backward
    "torchrec.distributed.comm_ops.All2All_Seq_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可以用来检索在前向传递期间保存的张量。它还有一个属性 `ctx.needs_input_grad`，是一个布尔值元组，表示每个输入是否需要梯度。例如，如果
    [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.forward "torchrec.distributed.comm_ops.All2All_Seq_Req.forward")
    的第一个输入需要计算相对于输出的梯度，则 `backward()` 将具有 `ctx.needs_input_grad[0] = True`。
- en: '[PRE72]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Define the forward of the custom autograd Function.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义自动微分函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将被所有子类覆盖。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE73]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文ctx作为第一个参数，后面是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分开前向和ctx）：
- en: '[PRE74]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须覆盖`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向的输入元组。
- en: See extending-autograd for more details
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储在反向传递期间可以检索的任意数据。张量不应直接存储在ctx上（尽管目前为了向后兼容性而没有强制执行）。相反，如果打算在`backward`（等效于`vjp`）中使用它们，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用它们，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE75]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Bases: `Function`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE76]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个公式，用于对反向模式自动微分的操作进行微分。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将被所有子类覆盖。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文`ctx`作为第一个参数，后面是[`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward")返回的输出数量（对于前向函数的非张量输出将传递None），并且应返回与[`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward")的输入数量相同的张量。每个参数是相对于给定输出的梯度，每个返回值应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.backward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传递期间保存的张量。它还具有属性`ctx.needs_input_grad`，作为一个布尔值元组，表示每个输入是否需要梯度。例如，如果[`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward")的第一个输入需要计算相对于输出的梯度，则[`backward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.backward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.backward")将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE77]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Define the forward of the custom autograd Function.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义自动微分函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将被所有子类覆盖。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE78]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文ctx作为第一个参数，后面是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分开前向和ctx）：
- en: '[PRE79]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须覆盖`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向的输入元组。
- en: See extending-autograd for more details
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储在反向传递期间可以检索的任意数据。张量不应直接存储在ctx上（尽管目前为了向后兼容性而没有强制执行）。相反，如果打算在`backward`（等效于`vjp`）中使用它们，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用它们，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE80]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Bases: `Function`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE81]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个公式，用于对反向模式自动微分的操作进行微分。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须重写此函数。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Req.forward "torchrec.distributed.comm_ops.All2Allv_Req.forward")
    returned (None will be passed in for non tensor outputs of the forward function),
    and it should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Req.forward
    "torchrec.distributed.comm_ops.All2Allv_Req.forward"). Each argument is the gradient
    w.r.t the given output, and each returned value should be the gradient w.r.t.
    the corresponding input. If an input is not a Tensor or is a Tensor not requiring
    grads, you can just pass None as a gradient for that input.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受上下文`ctx`作为第一个参数，然后是[`forward()`](#torchrec.distributed.comm_ops.All2Allv_Req.forward
    "torchrec.distributed.comm_ops.All2Allv_Req.forward")返回的输出数量（对于前向函数的非张量输出将传递None），并且应返回与[`forward()`](#torchrec.distributed.comm_ops.All2Allv_Req.forward
    "torchrec.distributed.comm_ops.All2Allv_Req.forward")的输入数量相同的张量。每个参数都是相对于给定输出的梯度，每个返回值都应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2Allv_Req.backward
    "torchrec.distributed.comm_ops.All2Allv_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Req.forward
    "torchrec.distributed.comm_ops.All2Allv_Req.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传递期间保存的张量。它还具有属性`ctx.needs_input_grad`，作为布尔值元组，表示每个输入是否需要梯度。例如，如果[`forward()`](#torchrec.distributed.comm_ops.All2Allv_Req.forward
    "torchrec.distributed.comm_ops.All2Allv_Req.forward")的第一个输入需要计算相对于输出的梯度，则[`backward()`](#torchrec.distributed.comm_ops.All2Allv_Req.backward
    "torchrec.distributed.comm_ops.All2Allv_Req.backward")将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE82]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Define the forward of the custom autograd Function.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义autograd函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须重写此函数。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE83]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文ctx作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分离前向和ctx）：
- en: '[PRE84]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须重写`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向的输入元组。
- en: See extending-autograd for more details
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储在反向传递期间可以检索的任意数据。张量不应直接存储在ctx上（尽管出于向后兼容性目的目前未强制执行）。相反，如果打算在`backward`（等效于`vjp`）中使用它们，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用它们，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE85]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Bases: `Function`'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE86]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 定义使用反向模式自动微分的操作的微分公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须重写此函数。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.forward "torchrec.distributed.comm_ops.All2Allv_Wait.forward")
    returned (None will be passed in for non tensor outputs of the forward function),
    and it should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.forward
    "torchrec.distributed.comm_ops.All2Allv_Wait.forward"). Each argument is the gradient
    w.r.t the given output, and each returned value should be the gradient w.r.t.
    the corresponding input. If an input is not a Tensor or is a Tensor not requiring
    grads, you can just pass None as a gradient for that input.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受上下文`ctx`作为第一个参数，然后是[`forward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.forward
    "torchrec.distributed.comm_ops.All2Allv_Wait.forward")返回的输出数量（对于前向函数的非张量输出将传递None），并且应返回与[`forward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.forward
    "torchrec.distributed.comm_ops.All2Allv_Wait.forward")的输入数量相同的张量。每个参数都是相对于给定输出的梯度，每个返回值都应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.backward
    "torchrec.distributed.comm_ops.All2Allv_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.forward
    "torchrec.distributed.comm_ops.All2Allv_Wait.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传递期间保存的张量。它还具有属性 `ctx.needs_input_grad`，作为表示每个输入是否需要梯度的布尔值元组。例如，如果
    [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.forward "torchrec.distributed.comm_ops.All2Allv_Wait.forward")
    的第一个输入需要计算相对于输出的梯度，则 `backward()` 将具有 `ctx.needs_input_grad[0] = True`。
- en: '[PRE87]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Define the forward of the custom autograd Function.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义 autograd 函数的前向传播。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将被所有子类覆盖。有两种定义前向的方式：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 用法 1（合并前向传播和 ctx）：
- en: '[PRE88]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文 ctx 作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见 combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 用法 2（分离前向传播和 ctx）：
- en: '[PRE89]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受 ctx 参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须覆盖 `torch.autograd.Function.setup_context()` 静态方法来处理设置 `ctx` 对象。`output`
    是前向传播的输出，`inputs` 是前向传播的输入的元组。
- en: See extending-autograd for more details
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见 extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储在反向传递期间可以检索的任意数据。张量不应直接存储在 ctx 上（尽管目前为了向后兼容性而未强制执行）。相反，如果打算在 `backward`
    中使用，则应使用 `ctx.save_for_backward()` 保存张量（等效地，`vjp`），或者如果打算在 `jvp` 中使用，则应使用 `ctx.save_for_forward()`
    保存张量。
- en: '[PRE90]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Bases: `object`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The data class that collects the attributes when calling the all_gatther_base_pooled
    operation.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 all_gatther_base_pooled 操作时收集属性的数据类。
- en: '[PRE91]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: the size of the input tensor.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 输入张量的大小。
- en: 'Type:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: int
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 整数
- en: '[PRE92]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Bases: `Function`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE95]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 定义使用反向模式自动微分区分操作的公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将被所有子类覆盖。（定义此函数等同于定义 `vjp` 函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Req.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Req.forward"). Each argument is the
    gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受上下文 `ctx` 作为第一个参数，然后是与 [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Req.forward") 返回的输出一样多（对于前向函数的非张量输出，将传递
    None），并且应返回与 [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Req.forward") 的输入一样多的张量。每个参数是相对于给定输出的梯度，每个返回值应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将
    None 作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.backward
    "torchrec.distributed.comm_ops.AllGatherBase_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Req.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传递期间保存的张量。它还具有属性 `ctx.needs_input_grad`，作为表示每个输入是否需要梯度的布尔值元组。例如，如果
    [`backward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.backward "torchrec.distributed.comm_ops.AllGatherBase_Req.backward")
    的第一个输入需要计算相对于输出的梯度，则 `forward()` 将具有 `ctx.needs_input_grad[0] = True`。
- en: '[PRE96]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Define the forward of the custom autograd Function.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义 autograd 函数的前向传播。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将被所有子类覆盖。有两种定义前向的方式：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 用法 1（合并前向传播和 ctx）：
- en: '[PRE97]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文 ctx 作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见 combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 用法 2（分离前向传播和 ctx）：
- en: '[PRE98]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受 ctx 参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须覆盖`torch.autograd.Function.setup_context()`静态方法以处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向输入的元组。
- en: See extending-autograd for more details
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储在反向传播期间可以检索的任意数据。不应直接在ctx上存储张量（尽管目前为了向后兼容性而未强制执行）。相反，如果打算在`backward`（等效于`vjp`）中使用张量，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用张量，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE99]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Bases: `Function`'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE100]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 为使用后向模式自动微分区分操作定义一个公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须覆盖此函数。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文`ctx`作为第一个参数，然后是与[`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.forward")返回的输出一样多（对于前向函数的非张量输出将传递为None），并且应返回与[`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.forward")的输入一样多的张量。每个参数都是相对于给定输出的梯度，每个返回值都应是相对应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.backward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传递期间保存的张量。它还具有一个属性`ctx.needs_input_grad`，作为布尔值元组，表示每个输入是否需要梯度。例如，如果[`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.forward")的第一个输入需要计算相对于输出的梯度，则`backward()`将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE101]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Define the forward of the custom autograd Function.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义autograd函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须覆盖此函数。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE102]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文ctx作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分离前向和ctx）：
- en: '[PRE103]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须覆盖`torch.autograd.Function.setup_context()`静态方法以处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向输入的元组。
- en: See extending-autograd for more details
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储在反向传播期间可以检索的任意数据。不应直接在ctx上存储张量（尽管目前为了向后兼容性而未强制执行）。相反，如果打算在`backward`（等效于`vjp`）中使用张量，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用张量，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE104]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Bases: `object`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The data class that collects the attributes when calling the reduce_scatter_base_pooled
    operation.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用reduce_scatter_base_pooled操作时收集属性的数据类。
- en: '[PRE105]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: the sizes of the input flatten tensor.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 输入展平张量的大小。
- en: 'Type:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: torch.Size
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: torch.Size
- en: '[PRE106]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Bases: `Function`'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE109]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为使用后向模式自动微分区分操作定义一个公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须覆盖此函数。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward") returned (None
    will be passed in for non tensor outputs of the forward function), and it should
    return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward"). Each argument
    is the gradient w.r.t the given output, and each returned value should be the
    gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor
    not requiring grads, you can just pass None as a gradient for that input.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文`ctx`作为第一个参数，后面是与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward")返回的输出一样多（对于前向函数的非张量输出将传入None），并且应该返回与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward")中的输入一样多的张量。每个参数都是相对于给定输出的梯度，每个返回值都应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，您可以为该输入传递None作为梯度。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.backward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward") needs gradient
    computed w.r.t. the output.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可以用来检索在前向传递期间保存的张量。它还有一个属性`ctx.needs_input_grad`，是一个布尔值元组，表示每个输入是否需要梯度。例如，如果[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward")的第一个输入需要计算相对于输出的梯度，则[`backward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.backward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.backward")将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE110]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Define the forward of the custom autograd Function.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义自动微分函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将被所有子类覆盖。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE111]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文ctx作为第一个参数，后面是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看 combining-forward-context 以获取更多详细信息
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分离前向和ctx）：
- en: '[PRE112]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须覆盖`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向的输入元组。
- en: See extending-autograd for more details
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看 extending-autograd 以获取更多详细信息
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可以用来存储在反向传递期间可以检索的任意数据。张量不应直接存储在ctx上（尽管为了向后兼容性，目前没有强制执行）。相反，如果打算在`backward`中使用它们，则应使用`ctx.save_for_backward()`保存张量（等效地，`vjp`），或者如果打算在`jvp`中使用它们，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE113]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Bases: `Function`'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE114]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 为使用反向模式自动微分区分操作定义一个公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将被所有子类覆盖。（定义这个函数等同于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward") returned (None
    will be passed in for non tensor outputs of the forward function), and it should
    return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward"). Each argument
    is the gradient w.r.t the given output, and each returned value should be the
    gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor
    not requiring grads, you can just pass None as a gradient for that input.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文`ctx`作为第一个参数，后面是与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward")返回的输出一样多（对于前向函数的非张量输出将传入None），并且应该返回与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward")中的输入一样多的张量。每个参数都是相对于给定输出的梯度，每个返回值都应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，您可以为该输入传递None作为梯度。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.backward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward") needs gradient
    computed w.r.t. the output.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传递期间保存的张量。它还具有属性`ctx.needs_input_grad`，作为表示每个输入是否需要梯度的布尔值元组。例如，如果[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward")的第一个输入需要计算相对于输出的梯度，则[`backward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.backward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.backward")将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE115]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: Define the forward of the custom autograd Function.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义autograd Function的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数应该被所有子类重写。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE116]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文ctx作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分开前向和ctx）：
- en: '[PRE117]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须重写`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向输入的元组。
- en: See extending-autograd for more details
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储在反向传播期间可以检索的任意数据。张量不应直接存储在ctx上（尽管出于向后兼容性目的目前尚未强制执行）。相反，如果打算在`backward`（等效地，`vjp`）中使用它们，则应使用`ctx.save_for_backward()`保存张量，或者如果打算在`jvp`中使用它们，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE118]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Bases: `object`'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The data class that collects the attributes when calling the reduce_scatter_pooled
    operation.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用reduce_scatter_pooled操作时收集属性的数据类。
- en: '[PRE119]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: the sizes of the input tensors. This remembers the sizes of the input tensors
    when running the backward pass and producing the gradient.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 输入张量的大小。这会在运行反向传播并产生梯度时记住输入张量的大小。
- en: 'Type:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[torch.Size]
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 列表[torch.Size]
- en: '[PRE120]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Bases: `object`'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The data class that collects the attributes when calling the reduce_scatter_v_pooled
    operation.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用reduce_scatter_v_pooled操作时收集属性的数据类。
- en: '[PRE123]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: the sizes of the input tensors. This saves the sizes of the input tensors when
    running the backward pass and producing the gradient.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 输入张量的大小。这会在运行反向传播并产生梯度时保存输入张量的大小。
- en: 'Type:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[torch.Size]
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 列表[torch.Size]
- en: '[PRE124]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: the splits of the input tensors along dim 0.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 输入张量沿dim 0的拆分。
- en: 'Type:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 列表[int]
- en: '[PRE125]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '(List[int]): total input size.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: （列表[int]）：总输入大小。
- en: 'Type:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 列表[int]
- en: '[PRE126]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'Bases: `Function`'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE132]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 定义使用反向模式自动微分区分操作的公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数应该被所有子类重写。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受上下文`ctx`作为第一个参数，然后是与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.forward")返回的输出数量相同（对于前向函数的非张量输出将传入None），并且应该返回与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.forward")的输入数量相同的张量。每个参数都是相对于给定输出的梯度，每个返回值都应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.backward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传递期间保存的张量。它还具有属性`ctx.needs_input_grad`，作为表示每个输入是否需要梯度的布尔值元组。例如，如果[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.forward")的第一个输入需要计算相对于输出的梯度，则[`backward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.backward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.backward")将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE133]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: Define the forward of the custom autograd Function.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义autograd Function的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数应被所有子类重写。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE134]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文ctx作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分离前向和ctx）：
- en: '[PRE135]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须重写`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向的输入元组。
- en: See extending-autograd for more details
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可以用来存储任意数据，然后在反向传播期间检索这些数据。张量不应直接存储在ctx上（尽管目前为了向后兼容性而没有强制执行）。相反，如果打算在`backward`（或等效的`vjp`）中使用张量，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用张量，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE136]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Bases: `Function`'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE137]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 为使用反向模式自动微分定义一个操作的求导公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数应被所有子类重写。（定义此函数等同于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受上下文`ctx`作为第一个参数，然后是与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward")返回的输出一样多（对于前向函数的非张量输出，将传入None），并且应返回与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward")的输入一样多的张量。每个参数都是相对于给定输出的梯度，每个返回值都应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.backward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传递期间保存的张量。它还有一个属性`ctx.needs_input_grad`，作为布尔值元组，表示每个输入是否需要梯度。例如，如果[`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward")的第一个输入需要计算相对于输出的梯度，则`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE138]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: Define the forward of the custom autograd Function.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义autograd函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数应被所有子类重写。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE139]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文ctx作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分离前向和ctx）：
- en: '[PRE140]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须重写`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向的输入元组。
- en: See extending-autograd for more details
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可以用来存储任意数据，然后在反向传播期间检索这些数据。张量不应直接存储在ctx上（尽管目前为了向后兼容性而没有强制执行）。相反，如果打算在`backward`（或等效的`vjp`）中使用张量，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用张量，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE141]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'Bases: `Function`'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE142]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 为使用反向模式自动微分定义一个操作的求导公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数应被所有子类重写。（定义此函数等同于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.forward"). Each argument is the
    gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受上下文`ctx`作为第一个参数，然后是与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.forward")返回的输出一样多（对于前向函数的非张量输出将传入None），并且应返回与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.forward")的输入一样多的张量。每个参数都是相对于给定输出的梯度，每个返回值都应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.backward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传播期间保存的张量。它还具有属性`ctx.needs_input_grad`，作为表示每个输入是否需要梯度的布尔值元组。例如，如果[`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.forward")的第一个输入需要计算相对于输出的梯度，则[`backward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.backward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.backward")将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE143]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: Define the forward of the custom autograd Function.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义自动微分函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须覆盖此函数。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE144]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文ctx作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分开前向和ctx）：
- en: '[PRE145]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须覆盖`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是前向传播的输出，`inputs`是前向传播的输入的元组。
- en: See extending-autograd for more details
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储在反向传播期间可以检索的任意数据。张量不应直接存储在ctx上（尽管目前为了向后兼容性而未强制执行）。相反，如果打算在`backward`中使用它们，则应使用`ctx.save_for_backward()`保存张量，或者如果打算在`jvp`中使用它们，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE146]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Bases: `Function`'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE147]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 定义用于使用反向模式自动微分区分操作的公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须覆盖此函数。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受上下文`ctx`作为第一个参数，然后是与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.forward")返回的输出一样多（对于前向函数的非张量输出将传入None），并且应返回与[`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.forward")的输入一样多的张量。每个参数都是相对于给定输出的梯度，每个返回值都应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.backward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.forward") needs gradient computed
    w.r.t. the output.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传播期间保存的张量。它还具有属性`ctx.needs_input_grad`，作为表示每个输入是否需要梯度的布尔值元组。例如，如果[`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.forward")的第一个输入需要计算相对于输出的梯度，则[`backward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.backward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.backward")将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE148]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Define the forward of the custom autograd Function.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义自动微分函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将被所有子类覆盖。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE149]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文ctx作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分开前向和ctx）：
- en: '[PRE150]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须覆盖`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是前向的输出，`inputs`是前向的输入元组。
- en: See extending-autograd for more details
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可以用来存储任意数据，然后在反向传播期间检索。张量不应直接存储在ctx上（尽管为了向后兼容性，目前没有强制执行）。相反，如果打算在`backward`（等效地，`vjp`）中使用它们，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用它们，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE151]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`W`]'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`W`]
- en: Defines a collective operation request for a process group on a tensor.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了一个张量在进程组上的集体操作请求。
- en: 'Parameters:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – The process group the request is for.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 请求所属的进程组。'
- en: '[PRE152]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'Bases: `object`'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The data class that collects the attributes when calling the variable_batch_alltoall_pooled
    operation.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 调用variable_batch_alltoall_pooled操作时收集属性的数据类。
- en: '[PRE153]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: batch size per rank per feature.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 每个秩每个特征的批量大小。
- en: 'Type:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[List[int]]
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: List[List[int]]
- en: '[PRE154]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: local batch size before scattering.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 散播之前的本地批量大小。
- en: 'Type:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[int]
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: List[int]
- en: '[PRE155]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: embedding dimension per rank per feature
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 每个秩每个特征的嵌入维度
- en: 'Type:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[List[int]]
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: List[List[int]]
- en: '[PRE156]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: quantized communication codecs.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 量化通信编解码器。
- en: 'Type:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Optional[[QuantizedCommCodecs](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")]
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 可选[[QuantizedCommCodecs](#torchrec.distributed.types.QuantizedCommCodecs "torchrec.distributed.types.QuantizedCommCodecs")]
- en: '[PRE157]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: input splits of tensor all to all.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的所有输入拆分到所有。
- en: 'Type:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Optional[List[int]]
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 可选[List[int]]
- en: '[PRE158]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: output splits of tensor all to all.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的所有输出拆分到所有。
- en: 'Type:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Optional[List[int]]
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 可选[List[int]]
- en: '[PRE159]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'Bases: `Function`'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE166]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 定义使用反向模式自动微分区分操作的公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将被所有子类覆盖。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward") returned
    (None will be passed in for non tensor outputs of the forward function), and it
    should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward"). Each
    argument is the gradient w.r.t the given output, and each returned value should
    be the gradient w.r.t. the corresponding input. If an input is not a Tensor or
    is a Tensor not requiring grads, you can just pass None as a gradient for that
    input.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受上下文`ctx`作为第一个参数，然后是与[`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward")返回的输出一样多（对于前向函数的非张量输出将传入None），并且应返回与[`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward")的输入一样多的张量。每个参数都是相对于给定输出的梯度，每个返回值都应是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.backward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.backward") will
    have `ctx.needs_input_grad[0] = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward") needs
    gradient computed w.r.t. the output.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可以用来检索在前向传递期间保存的张量。它还具有属性`ctx.needs_input_grad`，表示每个输入是否需要梯度的布尔值元组。例如，如果第一个输入需要计算相对于输出的梯度，则[`backward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.backward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.backward")将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE167]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: Define the forward of the custom autograd Function.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义自动微分函数的前向。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将被所有子类覆盖。有两种定义前向的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE168]'
  id: totrans-532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文ctx作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分开前向和ctx）：
- en: '[PRE169]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传递不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须重写`torch.autograd.Function.setup_context()`静态方法以处理设置`ctx`对象。`output`是前向传递的输出，`inputs`是前向传递的输入的元组。
- en: See extending-autograd for more details
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见扩展自动微分
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储任意数据，然后在反向传递期间检索。张量不应直接存储在ctx上（尽管目前为了向后兼容性而未强制执行）。相反，如果打算在`backward`（等效于`vjp`）中使用它们，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用它们，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE170]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: 'Bases: `Function`'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE171]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 定义使用反向模式自动微分区分操作的公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须重写此函数。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward") returned
    (None will be passed in for non tensor outputs of the forward function), and it
    should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward"). Each
    argument is the gradient w.r.t the given output, and each returned value should
    be the gradient w.r.t. the corresponding input. If an input is not a Tensor or
    is a Tensor not requiring grads, you can just pass None as a gradient for that
    input.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文`ctx`作为第一个参数，然后是与[`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward")返回的输出一样多（对于前向函数的非张量输出将传递None），并且应返回与[`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward")的输入一样多的张量。每个参数都是相对于给定输出的梯度，每个返回值都应是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.backward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.backward") will
    have `ctx.needs_input_grad[0] = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward") needs
    gradient computed w.r.t. the output.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在前向传递期间保存的张量。它还具有一个属性`ctx.needs_input_grad`，作为表示每个输入是否需要梯度的布尔值元组。例如，如果[`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward")的第一个输入需要计算相对于输出的梯度，则[`backward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.backward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.backward")将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE172]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: Define the forward of the custom autograd Function.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义自动微分函数的前向传递。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须重写此函数。定义前向有两种方式：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并前向和ctx）：
- en: '[PRE173]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受一个上下文ctx作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见组合前向上下文
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分离前向和ctx）：
- en: '[PRE174]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传递不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须重写`torch.autograd.Function.setup_context()`静态方法以处理设置`ctx`对象。`output`是前向传递的输出，`inputs`是前向传递的输入的元组。
- en: See extending-autograd for more details
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见扩展自动微分
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储任意数据，然后在反向传递期间检索。张量不应直接存储在ctx上（尽管目前为了向后兼容性而未强制执行）。相反，如果打算在`backward`（等效于`vjp`）中使用它们，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用它们，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE175]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: All-gathers tensors from all processes in a group to form a flattened pooled
    embeddings tensor. Input tensor is of size output_tensor_size / world_size.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 从组中的所有进程中聚集张量以形成扁平化的汇总嵌入张量。输入张量的大小为output_tensor_size / world_size。
- en: 'Parameters:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**input** (*Tensor*) – tensor to gather.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input**（*张量*）-要收集的张量。'
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**group**（*可选**[**dist.ProcessGroup**]）-要处理的进程组。如果为None，则将使用默认进程组。'
- en: 'Returns:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 异步工作句柄（可等待），稍后可以等待()以获取生成的张量。
- en: 'Return type:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
- en: Warning
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: all_gather_base_pooled is experimental and subject to change.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: all_gather_base_pooled是实验性的，可能会发生变化。
- en: '[PRE176]'
  id: totrans-572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: Performs AlltoAll operation for a single pooled embedding tensor. Each process
    splits the input pooled embeddings tensor based on the world size, and then scatters
    the split list to all processes in the group. Then concatenates the received tensors
    from all processes in the group and returns a single output tensor.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 对单个池化嵌入张量执行AlltoAll操作。每个进程根据世界大小拆分输入池化嵌入张量，然后将拆分列表分发给组中的所有进程。然后将来自组中所有进程的接收张量连接起来并返回单个输出张量。
- en: 'Parameters:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**a2a_pooled_embs_tensor** (*Tensor*) – input pooled embeddings. Must be pooled
    together before passing into this function. Its shape is B x D_local_sum, where
    D_local_sum is the dimension sum of all the local embedding tables.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**a2a_pooled_embs_tensor**（*Tensor*） - 输入池化嵌入。在传递到此函数之前必须将其汇总在一起。其形状为B x D_local_sum，其中D_local_sum是所有本地嵌入表的维度总和。'
- en: '**batch_size_per_rank** (*List**[**int**]*) – batch size in each rank.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_rank**（*List**[**int**]*） - 每个rank中的批量大小。'
- en: '**dim_sum_per_rank** (*List**[**int**]*) – number of features (sum of dimensions)
    of the embedding in each rank.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dim_sum_per_rank**（*List**[**int**]*） - 每个rank中嵌入的特征数（维度之和）。'
- en: '**dim_sum_per_rank_tensor** (*Optional**[**Tensor**]*) – the tensor version
    of dim_sum_per_rank, this is only used by the fast kernel of _recat_pooled_embedding_grad_out.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dim_sum_per_rank_tensor**（*Optional**[**Tensor**]*） - dim_sum_per_rank的张量版本，仅由_recat_pooled_embedding_grad_out的快速内核使用。'
- en: '**cumsum_dim_sum_per_rank_tensor** (*Optional**[**Tensor**]*) – cumulative
    sum of dim_sum_per_rank, this is only used by the fast kernel of _recat_pooled_embedding_grad_out.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cumsum_dim_sum_per_rank_tensor**（*Optional**[**Tensor**]*） - dim_sum_per_rank的累积和，仅由_recat_pooled_embedding_grad_out的快速内核使用。'
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**group**（*Optional**[**dist.ProcessGroup**]*） - 要处理的进程组。如果为None，则将使用默认进程组。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs**（*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*） - 量化通信编解码器。'
- en: 'Returns:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 异步工作句柄（Awaitable），稍后可以等待()以获取结果张量。
- en: 'Return type:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
- en: Warning
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: alltoall_pooled is experimental and subject to change.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: alltoall_pooled是实验性的，可能会发生变化。
- en: '[PRE177]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: Performs AlltoAll operation for sequence embeddings. Each process splits the
    input tensor based on the world size, and then scatters the split list to all
    processes in the group. Then concatenates the received tensors from all processes
    in the group and returns a single output tensor.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 对序列嵌入执行AlltoAll操作。每个进程根据世界大小拆分输入张量，然后将拆分列表分发给组中的所有进程。然后将来自组中所有进程的接收张量连接起来并返回单个输出张量。
- en: Note
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: AlltoAll operator for Sequence embedding tensors. Does not support mixed dimensions.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 用于序列嵌入张量的AlltoAll运算符。不支持混合维度。
- en: 'Parameters:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**a2a_sequence_embs_tensor** (*Tensor*) – input embeddings.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**a2a_sequence_embs_tensor**（*Tensor*） - 输入嵌入。'
- en: '**forward_recat_tensor** (*Tensor*) – recat tensor for forward.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**forward_recat_tensor**（*Tensor*） - 用于前向的recat张量。'
- en: '**backward_recat_tensor** (*Tensor*) – recat tensor for backward.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**backward_recat_tensor**（*Tensor*） - 用于反向的recat张量。'
- en: '**lengths_after_sparse_data_all2all** (*Tensor*) – lengths of sparse features
    after AlltoAll.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lengths_after_sparse_data_all2all**（*Tensor*） - AlltoAll后稀疏特征的长度。'
- en: '**input_splits** (*List**[**int**]*) – input splits.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_splits**（*List**[**int**]*） - 输入拆分。'
- en: '**output_splits** (*List**[**int**]*) – output splits.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_splits**（*List**[**int**]*） - 输出拆分。'
- en: '**variable_batch_size** (*bool*) – whether variable batch size is enabled.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**variable_batch_size**（*bool*） - 是否启用可变批量大小。'
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**group**（*Optional**[**dist.ProcessGroup**]*） - 要处理的进程组。如果为None，则将使用默认进程组。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs**（*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*） - 量化通信编解码器。'
- en: 'Returns:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 异步工作句柄（Awaitable），稍后可以等待()以获取结果张量。
- en: 'Return type:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[List[Tensor]]'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[List[Tensor]]'
- en: Warning
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: alltoall_sequence is experimental and subject to change.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: alltoall_sequence是实验性的，可能会发生变化。
- en: '[PRE178]'
  id: totrans-608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: Performs alltoallv operation for a list of input embeddings. Each process scatters
    the list to all processes in the group.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 对一组输入嵌入执行alltoallv操作。每个进程将列表分发给组中的所有进程。
- en: 'Parameters:'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**inputs** (*List**[**Tensor**]*) – list of tensors to scatter, one per rank.
    The tensors in the list usually have different lengths.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**inputs**（*List**[**Tensor**]*） - 要分发的张量列表，每个rank一个。列表中的张量通常具有不同的长度。'
- en: '**out_split** (*Optional**[**List**[**int**]**]*) – output split sizes (or
    dim_sum_per_rank), if not specified, we will use per_rank_split_lengths to construct
    a output split with the assumption that all the embs have the same dimension.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**out_split**（*Optional**[**List**[**int**]**]*） - 输出拆分大小（或dim_sum_per_rank），如果未指定，我们将使用per_rank_split_lengths来构建一个输出拆分，假设所有嵌入具有相同的维度。'
- en: '**per_rank_split_lengths** (*Optional**[**List**[**int**]**]*) – split lengths
    per rank. If not specified, the out_split must be specified.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**per_rank_split_lengths**（*Optional**[**List**[**int**]**]*） - 每个rank的拆分长度。如果未指定，则必须指定out_split。'
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组**（*可选**[**dist.ProcessGroup**]*）- 要操作的进程组。如果为None，则将使用默认进程组。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编解码器**（*可选**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*）- 量化通信编解码器。'
- en: 'Returns:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    list of tensors.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 异步工作句柄（Awaitable），稍后可以等待(wait())以获取结果张量列表。
- en: 'Return type:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[List[Tensor]]'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[List[Tensor]]'
- en: Warning
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: alltoallv is experimental and subject to change.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: alltoallv是实验性的，可能会发生变化。
- en: '[PRE179]'
  id: totrans-622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: Reduces then scatters a flattened pooled embeddings tensor to all processes
    in a group. Input tensor is of size output_tensor_size * world_size.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个扁平的池化嵌入张量减少然后分散到组中的所有进程。输入张量的大小为output_tensor_size * world_size。
- en: 'Parameters:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**input** (*Tensor*) – flattened tensor to scatter.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**（*张量*）- 要分散的扁平张量。'
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组**（*可选**[**dist.ProcessGroup**]*）- 要操作的进程组。如果为None，则将使用默认进程组。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编解码器**（*可选**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*）- 量化通信编解码器。'
- en: 'Returns:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 异步工作句柄（Awaitable），稍后可以等待(wait())以获取结果张量。
- en: 'Return type:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
- en: Warning
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: reduce_scatter_base_pooled is experimental and subject to change.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: reduce_scatter_base_pooled是实验性的，可能会发生变化。
- en: '[PRE181]'
  id: totrans-635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: Performs reduce-scatter operation for a pooled embeddings tensor split into
    world size number of chunks. The result of the reduce operation gets scattered
    to all processes in the group.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 对一个分成世界大小数量的块的池化嵌入张量执行reduce-scatter操作。减少操作的结果被分散到组中的所有进程。
- en: 'Parameters:'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**inputs** (*List**[**Tensor**]*) – list of tensors to scatter, one per rank.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**（*List**[**Tensor**]*）- 要分散的张量列表，每个排名一个。'
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组**（*可选**[**dist.ProcessGroup**]*）- 要操作的进程组。如果为None，则将使用默认进程组。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编解码器**（*可选**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*）- 量化通信编解码器。'
- en: 'Returns:'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 异步工作句柄（Awaitable），稍后可以等待(wait())以获取结果张量。
- en: 'Return type:'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
- en: Warning
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: reduce_scatter_pooled is experimental and subject to change.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: reduce_scatter_pooled是实验性的，可能会发生变化。
- en: '[PRE182]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: Performs reduce-scatter-v operation for a 1-d pooled embeddings tensor of variable
    batch size per feature split unevenly into world size number of chunks. The result
    of the reduce operation gets scattered to all processes in the group.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 对一个1维池化嵌入张量执行reduce-scatter-v操作，每个特征的批处理大小不同，分成世界大小数量的块。减少操作的结果根据输入拆分分散到组中的所有进程。
- en: 'Parameters:'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**input** (*Tensor*) – tensors to scatter, one per rank.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**（*张量*）- 要分散的张量，每个排名一个。'
- en: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – batch
    size per rank per feature used to determine input splits.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_rank_per_feature**（*List**[**List**[**int**]**]*）- 用于确定输入拆分的每个特征的每个排名的批处理大小。'
- en: '**embedding_dims** (*List**[**int**]*) – embedding dimensions per feature used
    to determine input splits.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入维度**（*List**[**int**]*）- 用于确定输入拆分的每个特征的嵌入维度。'
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – The process group to work
    on. If None, the default process group will be used.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组**（*可选**[**dist.ProcessGroup**]*）- 要操作的进程组。如果为None，则将使用默认进程组。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编解码器**（*可选**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*）- 量化通信编解码器。'
- en: 'Returns:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 异步工作句柄（Awaitable），稍后可以等待(wait())以获取结果张量。
- en: 'Return type:'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
- en: Warning
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: reduce_scatter_v_per_feature_pooled is experimental and subject to change.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: reduce_scatter_v_per_feature_pooled是实验性的，可能会发生变化。
- en: '[PRE183]'
  id: totrans-661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: Performs reduce-scatter-v operation for a pooled embeddings tensor split unevenly
    into world size number of chunks. The result of the reduce operation gets scattered
    to all processes in the group according to input_splits.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 对一个分成世界大小数量的块的池化嵌入张量执行reduce-scatter-v操作。减少操作的结果根据输入拆分分散到组中的所有进程。
- en: 'Parameters:'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**input** (*Tensor*) – tensor to scatter.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**（*张量*）- 要分散的张量。'
- en: '**input_splits** (*List**[**int**]*) – input splits.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_splits**（*List**[**int**]*）- 输入拆分。'
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组**（*可选**[**dist.ProcessGroup**]*）- 要操作的进程组。如果为None，则将使用默认进程组。'
- en: 'Returns:'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 异步工作句柄（Awaitable），稍后可以等待(wait())以获取结果张量。
- en: 'Return type:'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
- en: Warning
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: reduce_scatter_v_pooled is experimental and subject to change.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: reduce_scatter_v_pooled是实验性的，可能会发生变化。
- en: '[PRE184]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]  ## torchrec.distributed.dist_data[](#module-torchrec.distributed.dist_data
    "Permalink to this heading")'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE185]  ## torchrec.distributed.dist_data[](#module-torchrec.distributed.dist_data
    "Permalink to this heading")'
- en: '[PRE186]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: 'Bases: `Module`'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Merges the pooled/sequence embedding tensor on each device into single tensor.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个设备上的汇总/序列嵌入张量合并为单个张量。
- en: 'Parameters:'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**device** (*torch.device*) – device on which buffer will be allocated.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*）- 将分配缓冲区的设备。'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size**（*int*）- 拓扑中的设备数量。'
- en: '**cat_dim** (*int*) – which dimension you would like to concatenate on. For
    pooled embedding it is 1; for sequence embedding it is 0.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cat_dim**（*int*）- 您希望在哪个维度上进行连接。对于汇总嵌入，它是1；对于序列嵌入，它是0。'
- en: '[PRE187]'
  id: totrans-682
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: Performs AlltoOne operation on pooled/sequence embeddings tensors.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 对汇总/序列嵌入张量执行AlltoOne操作。
- en: 'Parameters:'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of embedding tensors.'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '**tensors**（*List**[**torch.Tensor**]*）- 嵌入张量列表。'
- en: 'Returns:'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of the merged embeddings.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 合并嵌入的等待。
- en: 'Return type:'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE188]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-692
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: 'Bases: `Module`'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Merges the pooled embedding tensor on each device into single tensor.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个设备上的汇总嵌入张量合并为单个张量。
- en: 'Parameters:'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**device** (*torch.device*) – device on which buffer will be allocated.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*）- 将分配缓冲区的设备。'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size**（*int*）- 拓扑中的设备数量。'
- en: '[PRE191]'
  id: totrans-698
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: Performs AlltoOne operation with Reduce on pooled embeddings tensors.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 对汇总嵌入张量执行Reduce的AlltoOne操作。
- en: 'Parameters:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of embedding tensors.'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '**tensors**（*List**[**torch.Tensor**]*）- 嵌入张量列表。'
- en: 'Returns:'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of the reduced embeddings.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 减少嵌入的等待。
- en: 'Return type:'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE192]'
  id: totrans-706
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-707
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: 'Bases: `Module`'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Redistributes KeyedJaggedTensor to a ProcessGroup according to splits.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 将KeyedJaggedTensor根据拆分重新分配到ProcessGroup。
- en: Implementation utilizes AlltoAll collective as part of torch.distributed.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 实现利用torch.distributed的AlltoAll集合。
- en: The input provides the necessary tensors and input splits to distribute. The
    first collective call in KJTAllToAllSplitsAwaitable will transmit output splits
    (to allocate correct space for tensors) and batch size per rank. The following
    collective calls in KJTAllToAllTensorsAwaitable will transmit the actual tensors
    asynchronously.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 输入提供了分发所需的张量和输入拆分。 KJTAllToAllSplitsAwaitable中的第一个集合调用将传输输出拆分（以为张量分配正确的空间）和每个等级的批量大小。
    KJTAllToAllTensorsAwaitable中的后续集合调用将异步传输实际张量。
- en: 'Parameters:'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于AlltoAll通信的ProcessGroup。'
- en: '**splits** (*List**[**int**]*) – List of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**splits**（*List**[**int**]*）- 长度为pg.size()的列表，指示要发送到每个pg.rank()的特征数量。假定KeyedJaggedTensor按目标等级排序。对所有等级都是相同的。'
- en: '**stagger** (*int*) – stagger value to apply to recat tensor, see _get_recat
    function for more detail.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**stagger**（*int*）- 用于应用于recat张量的间隔值，请参见_get_recat函数以获取更多详细信息。'
- en: 'Example:'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE195]'
  id: totrans-718
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-719
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: Sends input to relevant ProcessGroup ranks.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入发送到相关的ProcessGroup等级。
- en: The first wait will get the output splits for the provided tensors and issue
    tensors AlltoAll. The second wait will get the tensors.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个等待将获取所提供张量的输出拆分并发出张量AlltoAll。第二个等待将获取张量。
- en: 'Parameters:'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – KeyedJaggedTensor of values
    to distribute.'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '**input**（[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")）- 要分发的值的KeyedJaggedTensor。'
- en: 'Returns:'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of a KJTAllToAllTensorsAwaitable.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: KJTAllToAllTensorsAwaitable的等待。
- en: 'Return type:'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[KJTAllToAllTensorsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[KJTAllToAllTensorsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]'
- en: '[PRE197]'
  id: totrans-728
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-729
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`KJTAllToAllTensorsAwaitable`](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`KJTAllToAllTensorsAwaitable`](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]
- en: Awaitable for KJT tensors splits AlltoAll.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: KJT张量拆分AlltoAll的等待。
- en: 'Parameters:'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于AlltoAll通信的ProcessGroup。'
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – input KJT.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input**（[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")）- 输入KJT。'
- en: '**splits** (*List**[**int**]*) – list of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**splits**（*List**[**int**]*）- 长度为pg.size()的列表，指示要发送到每个pg.rank()的特征数量。假定KeyedJaggedTensor按目标等级排序。对所有等级都是相同的。'
- en: '**tensor_splits** (*Dict**[**str**,* *List**[**int**]**]*) – tensor splits
    provided by input KJT.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tensor_splits** (*Dict**[**str**,* *List**[**int**]**]*) – 输入KJT提供的张量拆分。'
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – provided KJT tensors (ie.
    lengths, values) to redistribute according to splits.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_tensors** (*List**[**torch.Tensor**]*) – 根据splits重新分配的提供的KJT张量（即长度，值）。'
- en: '**keys** (*List**[**str**]*) – KJT keys after AlltoAll.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keys** (*List**[**str**]*) – AlltoAll后的KJT键。'
- en: '**device** (*torch.device*) – device on which buffers will be allocated.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 将分配缓冲区的设备。'
- en: '**stagger** (*int*) – stagger value to apply to recat tensor.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**stagger** (*int*) – 应用于recat张量的stagger值。'
- en: '[PRE199]'
  id: totrans-741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE199]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]
- en: Awaitable for KJT tensors AlltoAll.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: KJT张量AlltoAll的Awaitable。
- en: 'Parameters:'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg** (*dist.ProcessGroup*) – 用于AlltoAll通信的ProcessGroup。'
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – input KJT.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – 输入KJT。'
- en: '**splits** (*List**[**int**]*) – list of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**splits** (*List**[**int**]*) – 长度为pg.size()的列表，指示要发送到每个pg.rank()的特征数量。假定KeyedJaggedTensor按目标排名排序。对所有排名都是相同的。'
- en: '**input_splits** (*List**[**List**[**int**]**]*) – input splits (number of
    values each rank will get) for each tensor in AlltoAll.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_splits** (*List**[**List**[**int**]**]*) – 每个张量在AlltoAll中将获得的值的数量）。'
- en: '**output_splits** (*List**[**List**[**int**]**]*) – output splits (number of
    values per rank in output) for each tensor in AlltoAll.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_splits** (*List**[**List**[**int**]**]*) – 每个张量在AlltoAll中输出的每个排名的值的数量。'
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – provided KJT tensors (ie.
    lengths, values) to redistribute according to splits.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_tensors** (*List**[**torch.Tensor**]*) – 根据splits重新分配的提供的KJT张量（即长度，值）。'
- en: '**labels** (*List**[**str**]*) – labels for each provided tensor.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**labels** (*List**[**str**]*) – 每个提供的张量的标签。'
- en: '**keys** (*List**[**str**]*) – KJT keys after AlltoAll.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keys** (*List**[**str**]*) – AlltoAll后的KJT键。'
- en: '**device** (*torch.device*) – device on which buffers will be allocated.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 将分配缓冲区的设备。'
- en: '**stagger** (*int*) – stagger value to apply to recat tensor.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**stagger** (*int*) – 应用于recat张量的stagger值。'
- en: '**stride_per_rank** (*Optional**[**List**[**int**]**]*) – stride per rank in
    the non variable batch per feature case.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**stride_per_rank** (*Optional**[**List**[**int**]**]*) – 在非可变批次每个特征情况下的每个排名的步幅。'
- en: '[PRE200]'
  id: totrans-756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: 'Bases: `Module`'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Redistributes KeyedJaggedTensor to all devices.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 将KeyedJaggedTensor重新分配到所有设备。
- en: Implementation utilizes OnetoAll function, which essentially P2P copies the
    feature to the devices.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 实现利用OnetoAll函数，基本上是将特征P2P复制到设备上。
- en: 'Parameters:'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**splits** (*List**[**int**]*) – lengths of features to split the KeyJaggedTensor
    features into before copying them.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**splits** (*List**[**int**]*) – 将KeyJaggedTensor特征拆分为复制之前的长度。'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size** (*int*) – 拓扑中的设备数量。'
- en: '**device** (*torch.device*) – the device on which the KJTs will be allocated.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 将分配KJT的设备。'
- en: '[PRE201]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: Splits features first and then sends the slices to the corresponding devices.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 首先拆分特征，然后将切片发送到相应的设备。
- en: 'Parameters:'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**kjt** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – the input features.'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: '**kjt** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – 输入特征。'
- en: 'Returns:'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of KeyedJaggedTensor splits.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: KeyedJaggedTensor拆分的Awaitable。
- en: 'Return type:'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[List[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[List[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
- en: '[PRE202]'
  id: totrans-772
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-773
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: 'Bases: `Module`'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: The module class that wraps the all-gather communication primitive for pooled
    embedding communication.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 包装用于汇总嵌入通信的全聚合通信原语的模块类。
- en: Provided a local input tensor with a layout of [batch_size, dimension], we want
    to gather input tensors from all ranks into a flattened output tensor.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 提供一个具有布局[batch_size，dimension]的本地输入张量，我们希望从所有排名中收集输入张量到一个扁平化的输出张量中。
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    all-gather is only available for NCCL backend.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 该类返回汇总嵌入张量的异步Awaitable句柄。全聚合仅适用于NCCL后端。
- en: 'Parameters:'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – the process group that the all-gather communication
    happens within.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg** (*dist.ProcessGroup*) – 发生全聚合通信的进程组。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – 量化通信编解码器。'
- en: 'Example:'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE204]'
  id: totrans-782
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-783
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: Performs reduce scatter operation on pooled embeddings tensor.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 对汇总嵌入张量执行reduce scatter操作。
- en: 'Parameters:'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_emb** (*torch.Tensor*) – tensor of shape [num_buckets x batch_size,
    dimension].'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: '**local_emb** (*torch.Tensor*) – 形状为[num_buckets x batch_size, dimension]的张量。'
- en: 'Returns:'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总张量的Awaitable，形状为[batch_size, dimension]。
- en: 'Return type:'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
- en: '[PRE206]'
  id: totrans-791
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: 'Bases: `Module`'
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Shards batches and collects keys of tensor with a ProcessGroup according to
    dim_sum_per_rank.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 根据dim_sum_per_rank将批次分片并收集张量的键与ProcessGroup。
- en: Implementation utilizes alltoall_pooled operation.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 实现利用alltoall_pooled操作。
- en: 'Parameters:'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg** (*dist.ProcessGroup*) – 用于AlltoAll通信的ProcessGroup。'
- en: '**dim_sum_per_rank** (*List**[**int**]*) – number of features (sum of dimensions)
    of the embedding in each rank.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dim_sum_per_rank** (*List**[**int**]*) – 每个秩中嵌入的特征数量（维度之和）。'
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*Optional**[**torch.device**]*) – 将分配缓冲区的设备。'
- en: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    – callback functions.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    – 回调函数。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – 量化通信编解码器。'
- en: 'Example:'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE208]'
  id: totrans-803
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-804
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-805
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: Performs AlltoAll pooled operation on pooled embeddings tensor.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 对池化嵌入张量执行AlltoAll池化操作。
- en: 'Parameters:'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs** (*torch.Tensor*) – 要分发的值的张量。'
- en: '**batch_size_per_rank** (*Optional**[**List**[**int**]**]*) – batch size per
    rank, to support variable batch size.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_rank** (*Optional**[**List**[**int**]**]*) – 每个秩的批次大小，以支持可变批次大小。'
- en: 'Returns:'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 池化嵌入的awaitable。
- en: 'Return type:'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
- en: '[PRE211]'
  id: totrans-814
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`Tensor`]'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`Tensor`]
- en: Awaitable for pooled embeddings after collective operation.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 在集体操作后的池化嵌入的awaitable。
- en: 'Parameters:'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tensor_awaitable** ([*Awaitable*](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*)
    – awaitable of concatenated tensors from all the processes in the group after
    collective.'
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: '**tensor_awaitable** ([*Awaitable*](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*)
    – 集体后来自组内所有进程的张量的连接张量的awaitable。'
- en: '[PRE213]'
  id: totrans-820
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: 'Bases: `Module`'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: The module class that wraps reduce-scatter communication primitives for pooled
    embedding communication in row-wise and twrw sharding.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 包装池化嵌入通信的reduce-scatter通信原语的模块类，以行和twrw分片的方式。
- en: For pooled embeddings, we have a local model-parallel output tensor with a layout
    of [num_buckets x batch_size, dimension]. We need to sum over num_buckets dimension
    across batches. We split the tensor along the first dimension into unequal chunks
    (tensor slices of different buckets) according to input_splits and reduce them
    into the output tensor and scatter the results for corresponding ranks.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 对于池化嵌入，我们有一个本地模型并行输出张量，布局为[num_buckets x batch_size, dimension]。我们需要在批次之间的num_buckets维度上求和。我们根据input_splits沿第一维度将张量分成不均匀的块（不同桶的张量切片），将它们减少到输出张量并将结果分散到相应的秩。
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    reduce-scatter-v operation is only available for NCCL backend.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 该类返回池化嵌入张量的异步Awaitable句柄。reduce-scatter-v操作仅适用于NCCL后端。
- en: 'Parameters:'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – the process group that the reduce-scatter communication
    happens within.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg** (*dist.ProcessGroup*) – reduce-scatter通信发生的进程组。'
- en: '**codecs** – quantized communication codecs.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs** – 量化通信编解码器。'
- en: '[PRE215]'
  id: totrans-829
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: Performs reduce scatter operation on pooled embeddings tensor.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 对池化嵌入张量执行reduce scatter操作。
- en: 'Parameters:'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – tensor of shape [num_buckets * batch_size,
    dimension].'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs** (*torch.Tensor*) – 形状为[num_buckets * batch_size, dimension]的张量。'
- en: '**input_splits** (*Optional**[**List**[**int**]**]*) – list of splits for local_embs
    dim 0.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_splits** (*Optional**[**List**[**int**]**]*) – 本地嵌入维度0的拆分列表。'
- en: 'Returns:'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: tensor的池化嵌入的awaitable，形状为[batch_size, dimension]。
- en: 'Return type:'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
- en: '[PRE216]'
  id: totrans-838
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-839
  prefs: []
  type: TYPE_PRE
  zh: '[PRE217]'
- en: 'Bases: `Module`'
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Merges the pooled/sequence embedding tensor on each device into single tensor.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个设备上的池化/序列嵌入张量合并为单个张量。
- en: 'Parameters:'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**device** (*torch.device*) – device on which buffer will be allocated'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 将分配缓冲区的设备'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size** (*int*) – 拓扑中的设备数量。'
- en: '**cat_dim** (*int*) – which dimension you like to concate on. For pooled embedding
    it is 1; for sequence embedding it is 0.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cat_dim** (*int*) – 您希望在其上连接的维度。对于池化嵌入，它是1；对于序列嵌入，它是0。'
- en: '[PRE218]'
  id: totrans-846
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: Performs AlltoOne operation on pooled embeddings tensors.
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 对池化嵌入张量执行AlltoOne操作。
- en: 'Parameters:'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of pooled embedding tensors.'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: '**tensors** (*List**[**torch.Tensor**]*) – 池化嵌入张量的列表。'
- en: 'Returns:'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of the merged pooled embeddings.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 合并池化嵌入的awaitable。
- en: 'Return type:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE219]'
  id: totrans-854
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-855
  prefs: []
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-856
  prefs: []
  type: TYPE_PRE
  zh: '[PRE221]'
- en: 'Bases: `Module`'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Redistributes sequence embedding to a ProcessGroup according to splits.
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 根据拆分将序列嵌入重新分配到ProcessGroup。
- en: 'Parameters:'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – the process group that the AlltoAll communication
    happens within.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- AlltoAll 通信发生在其中的进程组。'
- en: '**features_per_rank** (*List**[**int**]*) – list of number of features per
    rank.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**features_per_rank**（*List**[**int**]*）- 每个 rank 的特征数量列表。'
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*可选**[**torch.device**]*）- 在其中分配缓冲区的设备。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs**（*可选**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*）- 量化通信编解码器。'
- en: 'Example:'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE222]'
  id: totrans-865
  prefs: []
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE223]'
- en: Performs AlltoAll operation on sequence embeddings tensor.
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列嵌入张量上执行 AlltoAll 操作。
- en: 'Parameters:'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – input embeddings tensor.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs**（*torch.Tensor*）- 输入嵌入张量。'
- en: '**lengths** (*torch.Tensor*) – lengths of sparse features after AlltoAll.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lengths**（*torch.Tensor*）- AlltoAll 后稀疏特征的长度。'
- en: '**input_splits** (*List**[**int**]*) – input splits of AlltoAll.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_splits**（*List**[**int**]*）- AlltoAll 的输入拆分。'
- en: '**output_splits** (*List**[**int**]*) – output splits of AlltoAll.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_splits**（*List**[**int**]*）- AlltoAll 的输出拆分。'
- en: '**unbucketize_permute_tensor** (*Optional**[**torch.Tensor**]*) – stores the
    permute order of the KJT bucketize (for row-wise sharding only).'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unbucketize_permute_tensor**（*可选**[**torch.Tensor**]*）- 存储 KJT 桶化的排列顺序（仅适用于逐行分片）。'
- en: '**batch_size_per_rank** – (Optional[List[int]]): batch size per rank.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_rank** -（可选[List[int]]）：每个 rank 的批次大小。'
- en: '**sparse_features_recat** (*Optional**[**torch.Tensor**]*) – recat tensor used
    for sparse feature input dist. Must be provided if using variable batch size.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sparse_features_recat**（*可选**[**torch.Tensor**]*）- 用于稀疏特征输入分布的 recat 张量。如果使用可变批次大小，则必须提供。'
- en: 'Returns:'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of sequence embeddings.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 序列嵌入的可等待对象。
- en: 'Return type:'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[SequenceEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable
    "torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable")'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '[SequenceEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable
    "torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable")'
- en: '[PRE224]'
  id: totrans-880
  prefs: []
  type: TYPE_PRE
  zh: '[PRE224]'
- en: '[PRE225]'
  id: totrans-881
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`Tensor`]'
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`Tensor`]
- en: Awaitable for sequence embeddings after collective operation.
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 集体操作后的序列嵌入的可等待对象。
- en: 'Parameters:'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tensor_awaitable** ([*Awaitable*](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*)
    – awaitable of concatenated tensors from all the processes in the group after
    collective.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tensor_awaitable**（[*Awaitable*](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*）-
    集体操作后来自组内所有进程的连接张量的可等待对象。'
- en: '**unbucketize_permute_tensor** (*Optional**[**torch.Tensor**]*) – stores the
    permute order of KJT bucketize (for row-wise sharding only).'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unbucketize_permute_tensor**（*可选**[**torch.Tensor**]*）- 存储 KJT 桶化的排列顺序（仅适用于逐行分片）。'
- en: '**embedding_dim** (*int*) – embedding dimension.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**embedding_dim**（*int*）- 嵌入维度。'
- en: '[PRE226]'
  id: totrans-888
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[`List`[`int`]]]'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[`List`[`int`]]]
- en: Awaitable for splits AlltoAll.
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: 拆分 AlltoAll 的可等待对象。
- en: 'Parameters:'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – tensor of splits to redistribute.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_tensors**（*List**[**torch.Tensor**]*）- 用于重新分配的拆分张量。'
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于 AlltoAll 通信的 ProcessGroup。'
- en: '[PRE227]'
  id: totrans-894
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: 'Bases: `Module`'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Shards batches and collects keys of tensor with a ProcessGroup according to
    dim_sum_per_rank.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 dim_sum_per_rank 对张量进行分片并收集 ProcessGroup 的键。
- en: Implementation utilizes variable_batch_alltoall_pooled operation.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 实现利用 variable_batch_alltoall_pooled 操作。
- en: 'Parameters:'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于 AlltoAll 通信的 ProcessGroup。'
- en: '**emb_dim_per_rank_per_feature** (*List**[**List**[**int**]**]*) – embedding
    dimensions per rank per feature.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**emb_dim_per_rank_per_feature**（*List**[**List**[**int**]**]*）- 每个 rank 每个特征的嵌入维度。'
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备**（*可选**[**torch.device**]*）- 在其中分配缓冲区的设备。'
- en: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    – callback functions.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**callbacks**（*可选**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*）-
    回调函数。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs**（*可选**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*）- 量化通信编解码器。'
- en: 'Example:'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE228]'
  id: totrans-905
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-906
  prefs: []
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-907
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: Performs AlltoAll pooled operation with variable batch size per feature on a
    pooled embeddings tensor.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 在池化嵌入张量上执行具有可变特征批次大小的 AlltoAll 池化操作。
- en: 'Parameters:'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs**（*torch.Tensor*）- 要分发的值张量。'
- en: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – batch
    size per rank per feature, post a2a. Used to get the input splits.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_rank_per_feature**（*List**[**List**[**int**]**]*）- 每个 rank
    每个特征的批次大小，a2a 后。用于获取输入拆分。'
- en: '**batch_size_per_feature_pre_a2a** (*List**[**int**]*) – local batch size before
    scattering, used to get the output splits. Ordered by rank_0 feature, rank_1 feature,
    …'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_feature_pre_a2a**（*List**[**int**]*）- 分散之前的本地批次大小，用于获取输出拆分。按
    rank_0 特征，rank_1 特征排序，...'
- en: 'Returns:'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 池化嵌入的可等待对象。
- en: 'Return type:'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
- en: '[PRE231]'
  id: totrans-917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: '[PRE232]'
  id: totrans-918
  prefs: []
  type: TYPE_PRE
  zh: '[PRE232]'
- en: 'Bases: `Module`'
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: The module class that wraps reduce-scatter communication primitives for pooled
    embedding communication of variable batch in rw and twrw sharding.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 包装了用于 rw 和 twrw 分片中可变批次池化嵌入通信的 reduce-scatter 通信原语的模块类。
- en: For variable batch per feature pooled embeddings, we have a local model-parallel
    output tensor with a 1d layout of the total sum of batch sizes per rank per feature
    multiplied by corresponding embedding dim [batch_size_r0_f0 * emb_dim_f0 + …)].
    We split the tensor into unequal chunks by rank according to batch_size_per_rank_per_feature
    and corresponding embedding_dims and reduce them into the output tensor and scatter
    the results for corresponding ranks.
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个特征池化嵌入的可变批处理，我们有一个局部模型并行输出张量，其 1d 布局为每个特征每个排名的批处理大小总和乘以相应的嵌入维度 [batch_size_r0_f0
    * emb_dim_f0 + …)]. 我们根据 batch_size_per_rank_per_feature 和相应的 embedding_dims 将张量分割成不均匀的块，并将它们减少到输出张量中，然后将结果分散到相应的排名。
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    reduce-scatter-v operation is only available for NCCL backend.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 该类返回用于池化嵌入张量的异步可等待句柄。reduce-scatter-v 操作仅适用于 NCCL 后端。
- en: 'Parameters:'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – the process group that the reduce-scatter communication
    happens within.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg** (*dist.ProcessGroup*) – reduce-scatter 通信发生的进程组。'
- en: '**codecs** – quantized communication codecs.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs** – 量化通信编解码器。'
- en: '[PRE233]'
  id: totrans-926
  prefs: []
  type: TYPE_PRE
  zh: '[PRE233]'
- en: Performs reduce scatter operation on pooled embeddings tensor.
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: 在池化嵌入张量上执行 reduce scatter 操作。
- en: 'Parameters:'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – tensor of shape [num_buckets * batch_size,
    dimension].'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs** (*torch.Tensor*) – 形状为 [num_buckets * batch_size, dimension]
    的张量。'
- en: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – batch
    size per rank per feature used to determine input splits.'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – 用于确定输入拆分的每个特征每个排名的批处理大小。'
- en: '**embedding_dims** (*List**[**int**]*) – embedding dimensions per feature used
    to determine input splits.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**embedding_dims** (*List**[**int**]*) – 用于确定输入拆分的每个特征的嵌入维度。'
- en: 'Returns:'
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为 [batch_size, dimension] 的张量的池化嵌入的可等待。
- en: 'Return type:'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
- en: '[PRE234]  ## torchrec.distributed.embedding[](#module-torchrec.distributed.embedding
    "Permalink to this heading")'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE234]  ## torchrec.distributed.embedding[](#module-torchrec.distributed.embedding
    "Permalink to this heading")'
- en: '[PRE235]'
  id: totrans-937
  prefs: []
  type: TYPE_PRE
  zh: '[PRE235]'
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[`Dict`[`str`,
    [`JaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor
    "torchrec.sparse.jagged_tensor.JaggedTensor")]]'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[`Dict`[`str`,
    [`JaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor
    "torchrec.sparse.jagged_tensor.JaggedTensor")]]
- en: '[PRE236]'
  id: totrans-939
  prefs: []
  type: TYPE_PRE
  zh: '[PRE236]'
- en: 'Bases: `Multistreamable`'
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Multistreamable`
- en: '[PRE237]'
  id: totrans-941
  prefs: []
  type: TYPE_PRE
  zh: '[PRE237]'
- en: '[PRE238]'
  id: totrans-942
  prefs: []
  type: TYPE_PRE
  zh: '[PRE238]'
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
- en: '[PRE239]'
  id: totrans-944
  prefs: []
  type: TYPE_PRE
  zh: '[PRE239]'
- en: '[PRE240]'
  id: totrans-945
  prefs: []
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '[PRE241]'
  id: totrans-946
  prefs: []
  type: TYPE_PRE
  zh: '[PRE241]'
- en: 'Bases: [`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[[`EmbeddingCollection`](torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection
    "torchrec.modules.embedding_modules.EmbeddingCollection")]'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[[`EmbeddingCollection`](torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection
    "torchrec.modules.embedding_modules.EmbeddingCollection")]
- en: '[PRE242]'
  id: totrans-948
  prefs: []
  type: TYPE_PRE
  zh: '[PRE242]'
- en: '[PRE243]'
  id: totrans-949
  prefs: []
  type: TYPE_PRE
  zh: '[PRE243]'
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 执行实际的分片。它将根据相应的 ParameterSharding 指定的位置在请求的位置上分配参数。
- en: Default implementation is data-parallel replication.
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 默认实现是数据并行复制。
- en: 'Parameters:'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*M*) – module to shard.'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module** (*M*) – 要分片的模块。'
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – 完全限定的参数名称字典（模块路径
    + 参数名称，用‘.’分隔）到其分片规范。'
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – 具有进程组的分片环境。'
- en: '**device** (*torch.device*) – compute device.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 计算设备。'
- en: 'Returns:'
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: sharded module implementation.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 分片模块实现。
- en: 'Return type:'
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
- en: '[PRE244]'
  id: totrans-961
  prefs: []
  type: TYPE_PRE
  zh: '[PRE244]'
- en: List of parameters that can be sharded.
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 可以分片的参数列表。
- en: '[PRE245]'
  id: totrans-963
  prefs: []
  type: TYPE_PRE
  zh: '[PRE245]'
- en: List of supported sharding types. See ShardingType for well-known examples.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的分片类型列表。请参阅 ShardingType 以获取众所周知的示例。
- en: '[PRE246]'
  id: totrans-965
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: 'Bases: [`ShardedEmbeddingModule`](#torchrec.distributed.embedding_types.ShardedEmbeddingModule
    "torchrec.distributed.embedding_types.ShardedEmbeddingModule")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Dict`[`str`,
    [`JaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor
    "torchrec.sparse.jagged_tensor.JaggedTensor")], [`EmbeddingCollectionContext`](#torchrec.distributed.embedding.EmbeddingCollectionContext
    "torchrec.distributed.embedding.EmbeddingCollectionContext")], [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")'
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ShardedEmbeddingModule`](#torchrec.distributed.embedding_types.ShardedEmbeddingModule
    "torchrec.distributed.embedding_types.ShardedEmbeddingModule")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Dict`[`str`,
    [`JaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor
    "torchrec.sparse.jagged_tensor.JaggedTensor")], [`EmbeddingCollectionContext`](#torchrec.distributed.embedding.EmbeddingCollectionContext
    "torchrec.distributed.embedding.EmbeddingCollectionContext")], [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")
- en: Sharded implementation of EmbeddingCollection. This is part of the public API
    to allow for manual data dist pipelining.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: ShardedEmbeddingCollection的实现。这是公共API的一部分，允许手动数据分发流水线。
- en: '[PRE247]'
  id: totrans-968
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: '[PRE248]'
  id: totrans-969
  prefs: []
  type: TYPE_PRE
  zh: '[PRE248]'
- en: In case of multiple output distributions it makes sense to override this method
    and initiate the output distibution as soon as the corresponding compute completes.
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在多个输出分布的情况下，重写此方法并在相应的计算完成后立即启动输出分布是有意义的。
- en: '[PRE249]'
  id: totrans-971
  prefs: []
  type: TYPE_PRE
  zh: '[PRE249]'
- en: '[PRE250]'
  id: totrans-972
  prefs: []
  type: TYPE_PRE
  zh: '[PRE250]'
- en: '[PRE251]'
  id: totrans-973
  prefs: []
  type: TYPE_PRE
  zh: '[PRE251]'
- en: '[PRE252]'
  id: totrans-974
  prefs: []
  type: TYPE_PRE
  zh: '[PRE252]'
- en: '[PRE253]'
  id: totrans-975
  prefs: []
  type: TYPE_PRE
  zh: '[PRE253]'
- en: '[PRE254]'
  id: totrans-976
  prefs: []
  type: TYPE_PRE
  zh: '[PRE254]'
- en: '[PRE255]'
  id: totrans-977
  prefs: []
  type: TYPE_PRE
  zh: '[PRE255]'
- en: '[PRE256]'
  id: totrans-978
  prefs: []
  type: TYPE_PRE
  zh: '[PRE256]'
- en: '[PRE257]'
  id: totrans-979
  prefs: []
  type: TYPE_PRE
  zh: '[PRE257]'
- en: '[PRE258]  ## torchrec.distributed.embedding_lookup[](#module-torchrec.distributed.embedding_lookup
    "Permalink to this heading")'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE258]  ## torchrec.distributed.embedding_lookup[](#module-torchrec.distributed.embedding_lookup
    "Permalink to this heading")'
- en: '[PRE259]'
  id: totrans-981
  prefs: []
  type: TYPE_PRE
  zh: '[PRE259]'
- en: 'Bases: `Function`'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Function`
- en: '[PRE260]'
  id: totrans-983
  prefs: []
  type: TYPE_PRE
  zh: '[PRE260]'
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 为不同iating操作定义一个用于反向模式自动微分的公式。
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须重写此函数。（定义此函数等效于定义`vjp`函数。）
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward") returned
    (None will be passed in for non tensor outputs of the forward function), and it
    should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward"). Each argument
    is the gradient w.r.t the given output, and each returned value should be the
    gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor
    not requiring grads, you can just pass None as a gradient for that input.
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须接受上下文`ctx`作为第一个参数，然后是与[`forward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward")返回的输出一样多（对于正向函数的非张量输出将传递None），并且应返回与[`forward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward")的输入一样多的张量。每个参数都是相对于给定输出的梯度，每个返回值都应该是相对于相应输入的梯度。如果输入不是张量或是不需要梯度的张量，则可以将None作为该输入的梯度传递。
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.backward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.backward") will have
    `ctx.needs_input_grad[0] = True` if the first input to [`forward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward") needs gradient
    computed w.r.t. the output.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于检索在正向传递期间保存的张量。它还具有属性`ctx.needs_input_grad`，表示每个输入是否需要梯度的布尔值元组。例如，如果[`forward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward")的第一个输入需要计算相对于输出的梯度，则`backward()`将具有`ctx.needs_input_grad[0]
    = True`。
- en: '[PRE261]'
  id: totrans-988
  prefs: []
  type: TYPE_PRE
  zh: '[PRE261]'
- en: Define the forward of the custom autograd Function.
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义autograd函数的正向传递。
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 所有子类都必须重写此函数。有两种定义正向传递的方法：
- en: 'Usage 1 (Combined forward and ctx):'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 用法1（合并正向传递和ctx）：
- en: '[PRE262]'
  id: totrans-992
  prefs: []
  type: TYPE_PRE
  zh: '[PRE262]'
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须接受上下文ctx作为第一个参数，然后是任意数量的参数（张量或其他类型）。
- en: See combining-forward-context for more details
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见combining-forward-context
- en: 'Usage 2 (Separate forward and ctx):'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: 用法2（分开正向传递和ctx）：
- en: '[PRE263]'
  id: totrans-996
  prefs: []
  type: TYPE_PRE
  zh: '[PRE263]'
- en: The forward no longer accepts a ctx argument.
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正向不再接受ctx参数。
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，您还必须重写`torch.autograd.Function.setup_context()`静态方法来处理设置`ctx`对象。`output`是正向传递的输出，`inputs`是正向传递的输入的元组。
- en: See extending-autograd for more details
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参见extending-autograd
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文可用于存储在反向传递期间可以检索的任意数据。不应直接在ctx上存储张量（尽管出于向后兼容性目的目前未强制执行）。相反，如果打算在`backward`（等效于`vjp`）中使用张量，则应使用`ctx.save_for_backward()`保存张量，如果打算在`jvp`中使用张量，则应使用`ctx.save_for_forward()`保存张量。
- en: '[PRE264]'
  id: totrans-1001
  prefs: []
  type: TYPE_PRE
  zh: '[PRE264]'
- en: 'Bases: [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`]'
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`]
- en: Lookup modules for Sequence embeddings (i.e Embeddings)
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 查找序列嵌入的模块（即嵌入）
- en: '[PRE265]'
  id: totrans-1004
  prefs: []
  type: TYPE_PRE
  zh: '[PRE265]'
- en: '[PRE266]'
  id: totrans-1005
  prefs: []
  type: TYPE_PRE
  zh: '[PRE266]'
- en: Define the computation performed at every call.
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类覆盖。
- en: Note
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传播的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行注册的钩子，而后者则会默默地忽略它们。
- en: '[PRE267]'
  id: totrans-1010
  prefs: []
  type: TYPE_PRE
  zh: '[PRE267]'
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict") into
    this module and its descendants.
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: 从[`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict")中复制参数和缓冲区到此模块及其后代。
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict") must
    exactly match the keys returned by this module’s `state_dict()` function.
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`strict`为`True`，则[`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict")的键必须与此模块的`state_dict()`函数返回的键完全匹配。
- en: Warning
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict").
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`assign`为`True`，则必须在调用[`load_state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict")之后创建优化器。
- en: 'Parameters:'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict** (*dict*) – 包含参数和持久缓冲区的字典。'
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict") match
    the keys returned by this module’s `state_dict()` function. Default: `True`'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**strict** (*bool**,* *optional*) – 是否严格执行[`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict")中的键与此模块的`state_dict()`函数返回的键完全匹配。默认值：`True`'
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**assign** (*bool**,* *optional*) – 是否将状态字典中的项目分配给模块中对应的键，而不是将它们原地复制到模块的当前参数和缓冲区中。当为`False`时，保留当前模块中张量的属性，而为`True`时，保留状态字典中张量的属性。默认值：`False`'
- en: 'Returns:'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: '**missing_keys** is a list of str containing the missing keys'
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**missing_keys** 是一个包含缺失键的字符串列表'
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unexpected_keys** 是一个包含意外键的字符串列表'
- en: 'Return type:'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 带有`missing_keys`和`unexpected_keys`字段的`NamedTuple`
- en: Note
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict"), [`load_state_dict()`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict")
    will raise a `RuntimeError`.
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数或缓冲区注册为`None`，并且其对应的键存在于[`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict")中，[`load_state_dict()`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict")将引发`RuntimeError`。
- en: '[PRE268]'
  id: totrans-1026
  prefs: []
  type: TYPE_PRE
  zh: '[PRE268]'
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个模块缓冲区的迭代器，产生缓冲区的名称以及缓冲区本身。
- en: 'Parameters:'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str*) – 要添加到所有缓冲区名称前面的前缀。'
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  id: totrans-1030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse** (*bool**,* *optional*) – 如果为True，则产生此模块和所有子模块的缓冲区。否则，只产生直接属于此模块的缓冲区。默认为True。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate** (*bool**,* *optional*) – 是否删除结果中的重复缓冲区。默认为True。'
- en: 'Yields:'
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: 产生：
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, torch.Tensor)* – 包含名称和缓冲区的元组'
- en: 'Example:'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE269]'
  id: totrans-1035
  prefs: []
  type: TYPE_PRE
  zh: '[PRE269]'
- en: '[PRE270]'
  id: totrans-1036
  prefs: []
  type: TYPE_PRE
  zh: '[PRE270]'
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个模块参数的迭代器，产生参数的名称以及参数本身。
- en: 'Parameters:'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str*) – 要添加到所有参数名称前面的前缀。'
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse** (*bool*) – 如果为True，则产生此模块和所有子模块的参数。否则，只产生直接属于此模块的参数。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate**（*bool**,* *可选*）- 是否删除结果中的重复参数。默认为True。'
- en: 'Yields:'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 产出：
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, Parameter)*- 包含名称和参数的元组'
- en: 'Example:'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE271]'
  id: totrans-1045
  prefs: []
  type: TYPE_PRE
  zh: '[PRE271]'
- en: '[PRE272]'
  id: totrans-1046
  prefs: []
  type: TYPE_PRE
  zh: '[PRE272]'
- en: Like named_parameters(), but yields table_name and embedding_weights which are
    wrapped in TableBatchedEmbeddingSlice. For a single table with multiple shards
    (i.e CW) these are combined into one table/weight. Used in composability.
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于named_parameters()，但会产出包含在TableBatchedEmbeddingSlice中的table_name和embedding_weights。对于具有多个分片的单个表（即CW），这些会合并成一个表/权重。用于可组合性。
- en: '[PRE273]'
  id: totrans-1048
  prefs: []
  type: TYPE_PRE
  zh: '[PRE273]'
- en: '[PRE274]'
  id: totrans-1049
  prefs: []
  type: TYPE_PRE
  zh: '[PRE274]'
- en: '[PRE275]'
  id: totrans-1050
  prefs: []
  type: TYPE_PRE
  zh: '[PRE275]'
- en: Return a dictionary containing references to the whole state of the module.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个包含模块整体状态引用的字典。
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 包括参数和持久缓冲区（例如运行平均值）。键对应参数和缓冲区名称。设置为`None`的参数和缓冲区不包括在内。
- en: Note
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的对象是一个浅拷贝。它包含对模块参数和缓冲区的引用。
- en: Warning
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: 目前`state_dict()`还接受`destination`、`prefix`和`keep_vars`的位置参数。但是，这将被弃用，并且将在未来版本中强制使用关键字参数。
- en: Warning
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免使用参数`destination`，因为它不是为最终用户设计的。
- en: 'Parameters:'
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  id: totrans-1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**destination**（*dict**,* *可选*）- 如果提供，模块的状态将更新到字典中，并返回相同的对象。否则，将创建并返回一个`OrderedDict`。默认值：`None`。'
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix**（*str**,* *可选*）- 添加到参数和缓冲区名称以组成state_dict中键的前缀。默认值：`''''`。'
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keep_vars**（*bool**,* *可选*）- 默认情况下，状态字典中返回的`Tensor`会从自动求导中分离。如果设置为`True`，则不会执行分离。默认值：`False`。'
- en: 'Returns:'
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: a dictionary containing a whole state of the module
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 包含模块整体状态的字典
- en: 'Return type:'
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: dict
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 字典
- en: 'Example:'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE276]'
  id: totrans-1068
  prefs: []
  type: TYPE_PRE
  zh: '[PRE276]'
- en: '[PRE277]'
  id: totrans-1069
  prefs: []
  type: TYPE_PRE
  zh: '[PRE277]'
- en: '[PRE278]'
  id: totrans-1070
  prefs: []
  type: TYPE_PRE
  zh: '[PRE278]'
- en: 'Bases: [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`]'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`]
- en: Lookup modules for Pooled embeddings (i.e EmbeddingBags)
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: Pooled embeddings的查找模块（即EmbeddingBags）
- en: '[PRE279]'
  id: totrans-1073
  prefs: []
  type: TYPE_PRE
  zh: '[PRE279]'
- en: '[PRE280]'
  id: totrans-1074
  prefs: []
  type: TYPE_PRE
  zh: '[PRE280]'
- en: Define the computation performed at every call.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类重写。
- en: Note
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行注册的钩子，而后者会默默地忽略它们。
- en: '[PRE281]'
  id: totrans-1079
  prefs: []
  type: TYPE_PRE
  zh: '[PRE281]'
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict")
    into this module and its descendants.
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: 从[`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict")中复制参数和缓冲区到此模块及其后代。
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict")
    must exactly match the keys returned by this module’s `state_dict()` function.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`strict`为`True`，那么[`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict")的键必须与此模块的`state_dict()`函数返回的键完全匹配。
- en: Warning
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict").
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`assign`为`True`，则必须在调用[`load_state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict")之后创建优化器。
- en: 'Parameters:'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict**（*dict*）- 包含参数和持久缓冲区的字典。'
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict")
    match the keys returned by this module’s `state_dict()` function. Default: `True`'
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**strict**（*bool**,* *可选*）- 是否严格执行[`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict")中的键与此模块的`state_dict()`函数返回的键匹配。默认值：`True`'
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  id: totrans-1087
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**assign**（*布尔值**，*可选*）-是否将状态字典中的项目分配给模块中对应的键，而不是将它们原地复制到模块的当前参数和缓冲区中。当为`False`时，保留当前模块中张量的属性，而为`True`时，保留状态字典中张量的属性。默认：`False`'
- en: 'Returns:'
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: '**missing_keys** is a list of str containing the missing keys'
  id: totrans-1089
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**missing_keys**是一个包含缺失键的字符串列表'
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unexpected_keys**是一个包含意外键的字符串列表'
- en: 'Return type:'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: '返回类型:'
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: 带有`missing_keys`和`unexpected_keys`字段的`NamedTuple`
- en: Note
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict"),
    [`load_state_dict()`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict")
    will raise a `RuntimeError`.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数或缓冲区注册为`None`，并且其对应的键存在于[`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict")中，[`load_state_dict()`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict")将引发`RuntimeError`。
- en: '[PRE282]'
  id: totrans-1095
  prefs: []
  type: TYPE_PRE
  zh: '[PRE282]'
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个迭代器，遍历模块缓冲区，同时产生缓冲区的名称和缓冲区本身。
- en: 'Parameters:'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前缀**（*字符串*）-要添加到所有缓冲区名称前面的前缀。'
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归**（*布尔值**，*可选*）-如果为True，则产生此模块及所有子模块的缓冲区。否则，仅产生此模块的直接成员缓冲区。默认为True。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate**（*布尔值**，*可选*）-是否在结果中删除重复的缓冲区。默认为True。'
- en: 'Yields:'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 产量：
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: '*（字符串，torch.Tensor）* - 包含名称和缓冲区的元组'
- en: 'Example:'
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE283]'
  id: totrans-1104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE283]'
- en: '[PRE284]'
  id: totrans-1105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE284]'
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个迭代器，遍历模块参数，同时产生参数的名称和参数本身。
- en: 'Parameters:'
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  id: totrans-1108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前缀**（*字符串*）-要添加到所有参数名称前面的前缀。'
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归**（*布尔值*）-如果为True，则产生此模块及所有子模块的参数。否则，仅产生此模块的直接成员参数。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  id: totrans-1110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate**（*布尔值**，*可选*）-是否在结果中删除重复的参数。默认为True。'
- en: 'Yields:'
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 产量：
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: '*（字符串，参数）* - 包含名称和参数的元组'
- en: 'Example:'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE285]'
  id: totrans-1114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE285]'
- en: '[PRE286]'
  id: totrans-1115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE286]'
- en: Like named_parameters(), but yields table_name and embedding_weights which are
    wrapped in TableBatchedEmbeddingSlice. For a single table with multiple shards
    (i.e CW) these are combined into one table/weight. Used in composability.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于named_parameters()，但产生包含在TableBatchedEmbeddingSlice中的table_name和embedding_weights。对于具有多个分片的单个表（即CW），这些被合并为一个表/权重。用于可组合性。
- en: '[PRE287]'
  id: totrans-1117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE287]'
- en: '[PRE288]'
  id: totrans-1118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE288]'
- en: '[PRE289]'
  id: totrans-1119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE289]'
- en: Return a dictionary containing references to the whole state of the module.
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个包含对模块整体状态的引用的字典。
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: 包括参数和持久缓冲区（例如运行平均值）。键是相应的参数和缓冲区名称。设置为`None`的参数和缓冲区不包括在内。
- en: Note
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的对象是一个浅拷贝。它包含对模块参数和缓冲区的引用。
- en: Warning
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: 目前`state_dict()`还接受`destination`、`prefix`和`keep_vars`的位置参数。但是，这将被弃用，并且将在未来版本中强制使用关键字参数。
- en: Warning
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免使用参数`destination`，因为它不是为最终用户设计的。
- en: 'Parameters:'
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**destination**（*字典**，*可选*）-如果提供了，则模块的状态将更新到字典中，并返回相同的对象。否则，将创建并返回一个`OrderedDict`。默认：`None`。'
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  id: totrans-1130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前缀**（*字符串**，*可选*）-添加到参数和缓冲区名称以组成state_dict中键的前缀。默认：`''''`。'
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keep_vars**（*布尔值**，*可选*）-默认情况下，状态字典中返回的`Tensor`会从自动求导中分离。如果设置为`True`，则不会执行分离。默认：`False`。'
- en: 'Returns:'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: a dictionary containing a whole state of the module
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: 包含模块整体状态的字典
- en: 'Return type:'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: dict
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: 字典
- en: 'Example:'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE290]'
  id: totrans-1137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE290]'
- en: '[PRE291]'
  id: totrans-1138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE291]'
- en: '[PRE292]'
  id: totrans-1139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE292]'
- en: 'Bases: [`InferGroupedLookupMixin`](#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin
    "torchrec.distributed.embedding_lookup.InferGroupedLookupMixin"), [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`]], `TBEToRegisterMixIn`'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`InferGroupedLookupMixin`](#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin
    "torchrec.distributed.embedding_lookup.InferGroupedLookupMixin"), [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`]], `TBEToRegisterMixIn`
- en: '[PRE293]'
  id: totrans-1141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE293]'
- en: '[PRE294]'
  id: totrans-1142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE294]'
- en: '[PRE295]'
  id: totrans-1143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE295]'
- en: 'Bases: `ABC`'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`
- en: '[PRE296]'
  id: totrans-1145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE296]'
- en: '[PRE297]'
  id: totrans-1146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE297]'
- en: '[PRE298]'
  id: totrans-1147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE298]'
- en: '[PRE299]'
  id: totrans-1148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE299]'
- en: '[PRE300]'
  id: totrans-1149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE300]'
- en: '[PRE301]'
  id: totrans-1150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE301]'
- en: 'Bases: [`InferGroupedLookupMixin`](#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin
    "torchrec.distributed.embedding_lookup.InferGroupedLookupMixin"), [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`]], `TBEToRegisterMixIn`'
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`InferGroupedLookupMixin`](#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin
    "torchrec.distributed.embedding_lookup.InferGroupedLookupMixin"), [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`]], `TBEToRegisterMixIn`
- en: '[PRE302]'
  id: totrans-1152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE302]'
- en: '[PRE303]'
  id: totrans-1153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE303]'
- en: '[PRE304]'
  id: totrans-1154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE304]'
- en: 'Bases: [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`], `TBEToRegisterMixIn`'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`], `TBEToRegisterMixIn`
- en: meta embedding lookup module for inference since inference lookup has references
    for multiple TBE ops over all gpu workers. inference grouped embedding lookup
    module contains meta modules allocated over gpu workers.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 元嵌入查找模块用于推断，因为推断查找引用了所有GPU工作器上的多个TBE操作。推断分组嵌入查找模块包含在GPU工作器上分配的元模块。
- en: '[PRE305]'
  id: totrans-1157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE305]'
- en: '[PRE306]'
  id: totrans-1158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE306]'
- en: Define the computation performed at every call.
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类覆盖。
- en: Note
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行注册的钩子，而后者会默默地忽略它们。
- en: '[PRE307]'
  id: totrans-1163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE307]'
- en: '[PRE308]'
  id: totrans-1164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE308]'
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict")
    into this module and its descendants.
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 将参数和缓冲区从[`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict")复制到此模块及其后代中。
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict")
    must exactly match the keys returned by this module’s `state_dict()` function.
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`strict`为`True`，则[`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict")的键必须与此模块的`state_dict()`函数返回的键完全匹配。
- en: Warning
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict").
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`assign`为`True`，则必须在调用[`load_state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict")之后创建优化器。
- en: 'Parameters:'
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  id: totrans-1170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict** (*dict*) – 包含参数和持久缓冲区的字典。'
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict")
    match the keys returned by this module’s `state_dict()` function. Default: `True`'
  id: totrans-1171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**strict** (*bool**,* *optional*) – 是否严格执行[`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict")中的键与此模块的`state_dict()`函数返回的键匹配。默认值：`True`'
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  id: totrans-1172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**assign** (*bool**,* *optional*) – 是否将状态字典中的项目分配给模块中对应的键，而不是将它们原地复制到模块的当前参数和缓冲区中。当`False`时，保留当前模块中张量的属性，而当`True`时，保留状态字典中张量的属性。默认值：`False`'
- en: 'Returns:'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: '**missing_keys** is a list of str containing the missing keys'
  id: totrans-1174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**missing_keys**是一个包含缺少键的str列表'
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  id: totrans-1175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unexpected_keys**是一个包含意外键的str列表'
- en: 'Return type:'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 带有`missing_keys`和`unexpected_keys`字段的`NamedTuple`
- en: Note
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict"),
    [`load_state_dict()`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict")
    will raise a `RuntimeError`.
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数或缓冲区注册为“None”，并且其对应的键存在于[`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict")中，[`load_state_dict()`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict")将引发`RuntimeError`。
- en: '[PRE309]'
  id: totrans-1180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE309]'
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个迭代器，遍历模块缓冲区，同时返回缓冲区的名称和缓冲区本身。
- en: 'Parameters:'
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  id: totrans-1183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str*) – 添加到所有缓冲区名称前面的前缀。'
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  id: totrans-1184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse** (*bool**,* *optional*) – 如果为True，则产生此模块和所有子模块的缓冲区。否则，仅产生此模块的直接成员。默认为True。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  id: totrans-1185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate** (*bool**,* *optional*) – 是否删除结果中的重复缓冲区。默认为True。'
- en: 'Yields:'
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 产出：
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, torch.Tensor)* – 包含名称和缓冲区的元组'
- en: 'Example:'
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE310]'
  id: totrans-1189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE310]'
- en: '[PRE311]'
  id: totrans-1190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE311]'
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个迭代器，遍历模块参数，同时返回参数的名称和参数本身。
- en: 'Parameters:'
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  id: totrans-1193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str*) – 添加到所有参数名称前面的前缀。'
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  id: totrans-1194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse** (*bool*) – 如果为True，则产生此模块和所有子模块的参数。否则，仅产生此模块的直接成员的参数。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  id: totrans-1195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate** (*bool**,* *optional*) – 是否删除结果中的重复参数。默认为True。'
- en: 'Yields:'
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 产出：
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, Parameter)* – 包含名称和参数的元组'
- en: 'Example:'
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE312]'
  id: totrans-1199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE312]'
- en: '[PRE313]'
  id: totrans-1200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE313]'
- en: '[PRE314]'
  id: totrans-1201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE314]'
- en: Return a dictionary containing references to the whole state of the module.
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个包含模块整体状态引用的字典。
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: 包括参数和持久缓冲区（例如运行平均值）。键是相应的参数和缓冲区名称。设置为`None`的参数和缓冲区不包括在内。
- en: Note
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的对象是一个浅拷贝。它包含对模块参数和缓冲区的引用。
- en: Warning
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`state_dict()`还接受`destination`、`prefix`和`keep_vars`的位置参数。但是，这将被弃用，并且将在未来版本中强制使用关键字参数。
- en: Warning
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免使用参数“destination”，因为它不是为最终用户设计的。
- en: 'Parameters:'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  id: totrans-1211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**destination** (*dict**,* *optional*) – 如果提供，则模块的状态将更新到字典中，并返回相同的对象。否则，将创建并返回一个`OrderedDict`。默认值：`None`。'
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  id: totrans-1212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str**,* *optional*) – 添加到state_dict中键的参数和缓冲区名称的前缀。默认值：`''''`。'
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  id: totrans-1213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keep_vars** (*bool**,* *optional*) – 默认情况下，状态字典中返回的`Tensor`是从自动求导中分离的。如果设置为`True`，则不会执行分离。默认值：`False`。'
- en: 'Returns:'
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: a dictionary containing a whole state of the module
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: 包含模块整体状态的字典
- en: 'Return type:'
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: dict
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 字典
- en: 'Example:'
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE315]'
  id: totrans-1219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE315]'
- en: '[PRE316]'
  id: totrans-1220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE316]'
- en: '[PRE317]'
  id: totrans-1221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE317]'
- en: 'Bases: [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`], `TBEToRegisterMixIn`'
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`], `TBEToRegisterMixIn`
- en: meta embedding bag lookup module for inference since inference lookup has references
    for multiple TBE ops over all gpu workers. inference grouped embedding bag lookup
    module contains meta modules allocated over gpu workers.
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: 元嵌入袋查找模块用于推理，因为推理查找引用了所有GPU工作器上的多个TBE操作。推理分组嵌入袋查找模块包含在GPU工作器上分配的元模块。
- en: '[PRE318]'
  id: totrans-1224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE318]'
- en: '[PRE319]'
  id: totrans-1225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE319]'
- en: Define the computation performed at every call.
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类覆盖。
- en: Note
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数中定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行注册的钩子，而后者会默默地忽略它们。
- en: '[PRE320]'
  id: totrans-1230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE320]'
- en: '[PRE321]'
  id: totrans-1231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE321]'
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict")
    into this module and its descendants.
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: 将参数和缓冲区从[`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict")复制到此模块及其后代。
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict")
    must exactly match the keys returned by this module’s `state_dict()` function.
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`strict`为`True`，则[`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict")的键必须与此模块的`state_dict()`函数返回的键完全匹配。
- en: Warning
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict").
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`assign`为`True`，则必须在调用[`load_state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict")之后创建优化器。
- en: 'Parameters:'
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  id: totrans-1237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict** (*dict*) – 包含参数和持久缓冲区的字典。'
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict")
    match the keys returned by this module’s `state_dict()` function. Default: `True`'
  id: totrans-1238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**strict** (*布尔值**,* *可选*) – 是否严格执行[`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict")中的键必须与此模块的`state_dict()`函数返回的键完全匹配。默认值：`True`'
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  id: totrans-1239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**assign** (*布尔值**,* *可选*) – 是否将状态字典中的项目分配给模块中对应的键，而不是将它们原地复制到模块的当前参数和缓冲区中。当为`False`时，保留当前模块张量的属性，而为`True`时，保留状态字典中张量的属性。默认值：`False`'
- en: 'Returns:'
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: '**missing_keys** is a list of str containing the missing keys'
  id: totrans-1241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**missing_keys**是一个包含缺失键的str列表'
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  id: totrans-1242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unexpected_keys**是一个包含意外键的str列表'
- en: 'Return type:'
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: 带有`missing_keys`和`unexpected_keys`字段的`NamedTuple`
- en: Note
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict"),
    [`load_state_dict()`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict")
    will raise a `RuntimeError`.
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数或缓冲区注册为`None`，并且其对应的键存在于[`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict")中，[`load_state_dict()`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict")将引发`RuntimeError`。
- en: '[PRE322]'
  id: totrans-1247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE322]'
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个模块缓冲区的迭代器，产出缓冲区的名称以及缓冲区本身。
- en: 'Parameters:'
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str*) – 要添加到所有缓冲区名称前面的前缀。'
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse** (*布尔值**,* *可选*) – 如果为True，则产出此模块及所有子模块的缓冲区。否则，仅产出此模块的直接成员。默认为True。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  id: totrans-1252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate** (*布尔值**,* *可选*) – 是否在结果中删除重复的缓冲区。默认为True。'
- en: 'Yields:'
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: 产出：
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, torch.Tensor)* – 包含名称和缓冲区的元组'
- en: 'Example:'
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE323]'
  id: totrans-1256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE323]'
- en: '[PRE324]'
  id: totrans-1257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE324]'
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个模块参数的迭代器，产出参数的名称以及参数本身。
- en: 'Parameters:'
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  id: totrans-1260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str*) – 要添加到所有参数名称前面的前缀。'
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  id: totrans-1261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse** (*布尔值*) – 如果为True，则产出此模块及所有子模块的参数。否则，仅产出此模块的直接成员。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  id: totrans-1262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate** (*布尔值**,* *可选*) – 是否在结果中删除重复的参数。默认为True。'
- en: 'Yields:'
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: 产出：
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, Parameter)* – 包含名称和参数的元组'
- en: 'Example:'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE325]'
  id: totrans-1266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE325]'
- en: '[PRE326]'
  id: totrans-1267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE326]'
- en: '[PRE327]'
  id: totrans-1268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE327]'
- en: Return a dictionary containing references to the whole state of the module.
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个包含模块整体状态引用的字典。
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: 包括参数和持久缓冲区（例如运行平均值）。键是对应的参数和缓冲区名称。设置为`None`的参数和缓冲区不包括在内。
- en: Note
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的对象是一个浅拷贝。它包含对模块参数和缓冲区的引用。
- en: Warning
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`state_dict()`还接受`destination`、`prefix`和`keep_vars`的位置参数，但这将被弃用，并且将在未来版本中强制使用关键字参数。
- en: Warning
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免使用参数`destination`，因为它不是为最终用户设计的。
- en: 'Parameters:'
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  id: totrans-1278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**destination**（*dict**，*可选*）- 如果提供，模块的状态将更新到字典中，并返回相同的对象。否则，将创建并返回一个`OrderedDict`。默认值：`None`。'
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  id: totrans-1279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix**（*str**，*可选*）- 添加到参数和缓冲区名称以组成state_dict中键的前缀。默认值：`''''`。'
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  id: totrans-1280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keep_vars**（*bool**，*可选*）- 默认情况下，state dict中返回的`Tensor`会从自动求导中分离。如果设置为`True`，则不会执行分离。默认值：`False`。'
- en: 'Returns:'
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: a dictionary containing a whole state of the module
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: 包含模块整体状态的字典
- en: 'Return type:'
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: dict
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 字典
- en: 'Example:'
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE328]'
  id: totrans-1286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE328]'
- en: '[PRE329]'
  id: totrans-1287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE329]'
- en: '[PRE330]'
  id: totrans-1288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE330]'
- en: '[PRE331]  ## torchrec.distributed.embedding_sharding[](#module-torchrec.distributed.embedding_sharding
    "Permalink to this heading")'
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE331]  ## torchrec.distributed.embedding_sharding[](#module-torchrec.distributed.embedding_sharding
    "Permalink to this heading")'
- en: '[PRE332]'
  id: totrans-1290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE332]'
- en: 'Bases: `ABC`, `Module`, `Generic`[`C`, `T`, `W`]'
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`，`Module`，`Generic`[`C`，`T`，`W`]
- en: Converts output of EmbeddingLookup from model-parallel to data-parallel.
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: 将EmbeddingLookup的输出从模型并行转换为数据并行。
- en: '[PRE333]'
  id: totrans-1293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE333]'
- en: Define the computation performed at every call.
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类重写。
- en: Note
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传播的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行注册的钩子，而后者则默默地忽略它们。
- en: '[PRE334]'
  id: totrans-1298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE334]'
- en: '[PRE335]'
  id: totrans-1299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE335]'
- en: 'Bases: `ABC`, `Module`, `Generic`[`F`]'
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`，`Module`，`Generic`[`F`]
- en: Converts input from data-parallel to model-parallel.
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入从数据并行转换为模型并行。
- en: '[PRE336]'
  id: totrans-1302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE336]'
- en: Define the computation performed at every call.
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类重写。
- en: Note
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传播的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行注册的钩子，而后者则默默地忽略它们。
- en: '[PRE337]'
  id: totrans-1307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE337]'
- en: '[PRE338]'
  id: totrans-1308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE338]'
- en: 'Bases: `ABC`, `Generic`[`C`, `F`, `T`, `W`], [`FeatureShardingMixIn`](#torchrec.distributed.embedding_types.FeatureShardingMixIn
    "torchrec.distributed.embedding_types.FeatureShardingMixIn")'
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`，`Generic`[`C`，`F`，`T`，`W`]，[`FeatureShardingMixIn`](#torchrec.distributed.embedding_types.FeatureShardingMixIn
    "torchrec.distributed.embedding_types.FeatureShardingMixIn")
- en: Used to implement different sharding types for EmbeddingBagCollection, e.g.
    table_wise.
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: 用于为EmbeddingBagCollection实现不同的分片类型，例如table_wise。
- en: '[PRE339]'
  id: totrans-1311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE339]'
- en: '[PRE340]'
  id: totrans-1312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE340]'
- en: '[PRE341]'
  id: totrans-1313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE341]'
- en: '[PRE342]'
  id: totrans-1314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE342]'
- en: '[PRE343]'
  id: totrans-1315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE343]'
- en: '[PRE344]'
  id: totrans-1316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE344]'
- en: '[PRE345]'
  id: totrans-1317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE345]'
- en: '[PRE346]'
  id: totrans-1318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE346]'
- en: '[PRE347]'
  id: totrans-1319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE347]'
- en: '[PRE348]'
  id: totrans-1320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE348]'
- en: '[PRE349]'
  id: totrans-1321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE349]'
- en: '[PRE350]'
  id: totrans-1322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE350]'
- en: 'Bases: `Multistreamable`'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Multistreamable`
- en: '[PRE351]'
  id: totrans-1324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE351]'
- en: '[PRE352]'
  id: totrans-1325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE352]'
- en: '[PRE353]'
  id: totrans-1326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE353]'
- en: '[PRE354]'
  id: totrans-1327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE354]'
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
- en: '[PRE355]'
  id: totrans-1329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE355]'
- en: '[PRE356]'
  id: totrans-1330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE356]'
- en: 'Bases: `object`'
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE357]'
  id: totrans-1332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE357]'
- en: '[PRE358]'
  id: totrans-1333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE358]'
- en: '[PRE359]'
  id: totrans-1334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE359]'
- en: '[PRE360]'
  id: totrans-1335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE360]'
- en: '[PRE361]'
  id: totrans-1336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE361]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[[`KJTListAwaitable`](#torchrec.distributed.embedding_sharding.KJTListAwaitable
    "torchrec.distributed.embedding_sharding.KJTListAwaitable")]]'
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[[`KJTListAwaitable`](#torchrec.distributed.embedding_sharding.KJTListAwaitable
    "torchrec.distributed.embedding_sharding.KJTListAwaitable")]]
- en: '[PRE362]'
  id: totrans-1338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE362]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]'
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]
- en: Awaitable of KJTList.
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: 可等待的KJTList。
- en: 'Parameters:'
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")*]**]*) – list of Awaitable
    of sparse features.'
  id: totrans-1342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**awaitables**（*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")*]**]*)- 稀疏特征的可等待列表。'
- en: '**ctx** (*C*) – sharding context to save the batch size info from the KJT for
    the embedding AlltoAll.'
  id: totrans-1343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ctx**（*C*）- 用于保存从KJT到嵌入AlltoAll的批量大小信息的分片上下文。'
- en: '[PRE363]'
  id: totrans-1344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE363]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`Awaitable`](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]], `Generic`[`C`]'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`Awaitable`](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]]，`Generic`[`C`]
- en: Awaitable of Awaitable of KJTList.
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
  zh: 可等待的可等待的KJTList。
- en: 'Parameters:'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")*]**]**]*) – result from calling
    forward on KJTAllToAll with sparse features to redistribute.'
  id: totrans-1348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")*]**]**]*) – 调用具有稀疏特征的KJTAllToAll的前向结果以重新分配。'
- en: '**ctx** (*C*) – sharding context to save the metadata from the input dist to
    for the embedding AlltoAll.'
  id: totrans-1349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ctx** (*C*) – 保存从输入分布到嵌入AlltoAll的元数据的分片上下文。'
- en: '[PRE364]'
  id: totrans-1350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE364]'
- en: 'Bases: `object`'
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE365]'
  id: totrans-1352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE365]'
- en: '[PRE366]'
  id: totrans-1353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE366]'
- en: '[PRE367]'
  id: totrans-1354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE367]'
- en: '[PRE368]'
  id: totrans-1355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE368]'
- en: '[PRE369]'
  id: totrans-1356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE369]'
- en: '[PRE370]'
  id: totrans-1357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE370]'
- en: '[PRE371]'
  id: totrans-1358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE371]'
- en: '[PRE372]'
  id: totrans-1359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE372]'
- en: '[PRE373]'
  id: totrans-1360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE373]'
- en: '[PRE374]'
  id: totrans-1361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE374]'
- en: '[PRE375]'
  id: totrans-1362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE375]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`ListOfKJTList`](#torchrec.distributed.embedding_types.ListOfKJTList
    "torchrec.distributed.embedding_types.ListOfKJTList")]'
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`ListOfKJTList`](#torchrec.distributed.embedding_types.ListOfKJTList
    "torchrec.distributed.embedding_types.ListOfKJTList")]
- en: This module handles the tables-wise sharding input features distribution for
    inference.
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
  zh: 此模块处理推断的表格级分片输入特征分布。
- en: 'Parameters:'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KJTList*](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")*]**]*) – list of Awaitable of
    KJTList.'
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
  zh: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KJTList*](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")*]**]*) – KJTList的Awaitable列表。'
- en: '[PRE376]'
  id: totrans-1367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE376]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`Awaitable`](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`ListOfKJTList`](#torchrec.distributed.embedding_types.ListOfKJTList
    "torchrec.distributed.embedding_types.ListOfKJTList")]]'
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`Awaitable`](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`ListOfKJTList`](#torchrec.distributed.embedding_types.ListOfKJTList
    "torchrec.distributed.embedding_types.ListOfKJTList")]]
- en: Awaitable of Awaitable of ListOfKJTList.
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: Awaitable的Awaitable的ListOfKJTList。
- en: 'Parameters:'
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KJTList*](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")*]**]**]*) – list of Awaitable
    of Awaitable of sparse features list.'
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KJTList*](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")*]**]**]*) – 稀疏特征列表的Awaitable的Awaitable列表。'
- en: '[PRE377]'
  id: totrans-1372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE377]'
- en: Bucketizes the values in KeyedJaggedTensor into num_buckets buckets, lengths
    are readjusted based on the bucketization results.
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
  zh: 将KeyedJaggedTensor中的值分桶为num_buckets个桶，长度根据桶化结果重新调整。
- en: 'Note: This function should be used only for row-wise sharding before calling
    KJTAllToAll.'
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此函数应仅用于在调用KJTAllToAll之前进行逐行分片。
- en: 'Parameters:'
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**num_buckets** (*int*) – number of buckets to bucketize the values into.'
  id: totrans-1376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_buckets** (*int*) – 将值分桶的桶数。'
- en: '**block_sizes** – (torch.Tensor): bucket sizes for the keyed dimension.'
  id: totrans-1377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**block_sizes** – (torch.Tensor): 键控维度的桶大小。'
- en: '**output_permute** (*bool*) – output the memory location mapping from the unbucketized
    values to bucketized values or not.'
  id: totrans-1378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_permute** (*bool*) – 输出未分桶值到分桶值的内存位置映射或不输出。'
- en: '**bucketize_pos** (*bool*) – output the changed position of the bucketized
    values or not.'
  id: totrans-1379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bucketize_pos** (*bool*) – 输出桶化值的更改位置或不输出。'
- en: '**block_bucketize_row_pos** (*Optional**[**List**[**torch.Tensor**]**]*) –
    The offsets of shard size for each feature.'
  id: totrans-1380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**block_bucketize_row_pos** (*Optional**[**List**[**torch.Tensor**]**]*) –
    每个特征的分片大小的偏移量。'
- en: 'Returns:'
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: the bucketized KeyedJaggedTensor and the optional permute mapping from the unbucketized
    values to bucketized value.
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
  zh: 桶化的KeyedJaggedTensor和未桶化值到桶化值的可选置换映射。
- en: 'Return type:'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: Tuple[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), Optional[torch.Tensor]]
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
  zh: Tuple[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), Optional[torch.Tensor]]
- en: '[PRE378]'
  id: totrans-1385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE378]'
- en: Groups tables by DataType, PoolingType, and EmbeddingComputeKernel.
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: 按照DataType、PoolingType和EmbeddingComputeKernel对表进行分组。
- en: 'Parameters:'
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tables_per_rank** (*List**[**List**[*[*ShardedEmbeddingTable*](#torchrec.distributed.embedding_types.ShardedEmbeddingTable
    "torchrec.distributed.embedding_types.ShardedEmbeddingTable")*]**]*) – list of
    sharded embedding tables per rank with consistent weightedness.'
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: '**tables_per_rank** (*List**[**List**[*[*ShardedEmbeddingTable*](#torchrec.distributed.embedding_types.ShardedEmbeddingTable
    "torchrec.distributed.embedding_types.ShardedEmbeddingTable")*]**]*) – 每个秩的一致加权的分片嵌入表列表。'
- en: 'Returns:'
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: per rank list of GroupedEmbeddingConfig for features.
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
  zh: 每个秩的特征的GroupedEmbeddingConfig列表。
- en: 'Return type:'
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: 'List[List[[GroupedEmbeddingConfig](#torchrec.distributed.embedding_types.GroupedEmbeddingConfig
    "torchrec.distributed.embedding_types.GroupedEmbeddingConfig")]]  ## torchrec.distributed.embedding_types[](#module-torchrec.distributed.embedding_types
    "Permalink to this heading")'
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: 'List[List[[GroupedEmbeddingConfig](#torchrec.distributed.embedding_types.GroupedEmbeddingConfig
    "torchrec.distributed.embedding_types.GroupedEmbeddingConfig")]]  ## torchrec.distributed.embedding_types[](#module-torchrec.distributed.embedding_types
    "Permalink to this heading")'
- en: '[PRE379]'
  id: totrans-1393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE379]'
- en: 'Bases: `ABC`, `Module`, `Generic`[`F`, `T`]'
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`，`Module`，`Generic`[`F`，`T`]
- en: 'Interface implemented by different embedding implementations: e.g. one, which
    relies on nn.EmbeddingBag or table-batched one, etc.'
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 由不同的嵌入实现实现的接口：例如，依赖于nn.EmbeddingBag或表批处理的接口等。
- en: '[PRE380]'
  id: totrans-1396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE380]'
- en: Define the computation performed at every call.
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类覆盖。
- en: Note
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行注册的钩子，而后者会默默地忽略它们。
- en: '[PRE381]'
  id: totrans-1401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE381]'
- en: '[PRE382]'
  id: totrans-1402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE382]'
- en: 'Bases: [`ModuleSharder`](#torchrec.distributed.types.ModuleSharder "torchrec.distributed.types.ModuleSharder")[`M`]'
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ModuleSharder`](#torchrec.distributed.types.ModuleSharder "torchrec.distributed.types.ModuleSharder")[`M`]
- en: '[PRE383]'
  id: totrans-1404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE383]'
- en: List of supported compute kernels for a given sharding type and compute device.
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: 给定分片类型和计算设备的支持计算内核列表。
- en: '[PRE384]'
  id: totrans-1406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE384]'
- en: '[PRE385]'
  id: totrans-1407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE385]'
- en: List of supported sharding types. See ShardingType for well-known examples.
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的分片类型列表。查看ShardingType以获取常见示例。
- en: '[PRE386]'
  id: totrans-1409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE386]'
- en: List of system resources and corresponding usage given a compute device and
    compute kernel
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: 给定计算设备和计算内核，列出系统资源及相应的使用情况
- en: '[PRE387]'
  id: totrans-1411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE387]'
- en: 'Bases: `Module`'
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Abstract base class for grouped feature processor
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
  zh: 分组特征处理器的抽象基类
- en: '[PRE388]'
  id: totrans-1414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE388]'
- en: Define the computation performed at every call.
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类重写。
- en: Note
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行注册的钩子，而后者会默默地忽略它们。
- en: '[PRE389]'
  id: totrans-1419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE389]'
- en: '[PRE390]'
  id: totrans-1420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE390]'
- en: 'Bases: [`ModuleSharder`](#torchrec.distributed.types.ModuleSharder "torchrec.distributed.types.ModuleSharder")[`M`]'
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ModuleSharder`](#torchrec.distributed.types.ModuleSharder "torchrec.distributed.types.ModuleSharder")[`M`]
- en: '[PRE391]'
  id: totrans-1422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE391]'
- en: List of supported compute kernels for a given sharding type and compute device.
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
  zh: 给定分片类型和计算设备的支持计算内核列表。
- en: '[PRE392]'
  id: totrans-1424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE392]'
- en: '[PRE393]'
  id: totrans-1425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE393]'
- en: List of parameters that can be sharded.
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
  zh: 可以进行分片的参数列表。
- en: '[PRE394]'
  id: totrans-1427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE394]'
- en: List of supported sharding types. See ShardingType for well-known examples.
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的分片类型列表。查看ShardingType以获取常见示例。
- en: '[PRE395]'
  id: totrans-1429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE395]'
- en: List of system resources and corresponding usage given a compute device and
    compute kernel
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
  zh: 给定计算设备和计算内核，列出系统资源及相应的使用情况
- en: '[PRE396]'
  id: totrans-1431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE396]'
- en: 'Bases: `object`'
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE397]'
  id: totrans-1433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE397]'
- en: '[PRE398]'
  id: totrans-1434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE398]'
- en: 'Bases: `Enum`'
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Enum`
- en: An enumeration.
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: 一个枚举。
- en: '[PRE399]'
  id: totrans-1437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE399]'
- en: '[PRE400]'
  id: totrans-1438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE400]'
- en: '[PRE401]'
  id: totrans-1439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE401]'
- en: '[PRE402]'
  id: totrans-1440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE402]'
- en: '[PRE403]'
  id: totrans-1441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE403]'
- en: '[PRE404]'
  id: totrans-1442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE404]'
- en: '[PRE405]'
  id: totrans-1443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE405]'
- en: '[PRE406]'
  id: totrans-1444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE406]'
- en: 'Bases: `object`'
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: Feature Sharding Interface to provide sharding-aware feature metadata.
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
  zh: 特征分片接口，提供分片感知特征元数据。
- en: '[PRE407]'
  id: totrans-1447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE407]'
- en: '[PRE408]'
  id: totrans-1448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE408]'
- en: '[PRE409]'
  id: totrans-1449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE409]'
- en: '[PRE410]'
  id: totrans-1450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE410]'
- en: 'Bases: `object`'
  id: totrans-1451
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE411]'
  id: totrans-1452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE411]'
- en: '[PRE412]'
  id: totrans-1453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE412]'
- en: '[PRE413]'
  id: totrans-1454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE413]'
- en: '[PRE414]'
  id: totrans-1455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE414]'
- en: '[PRE415]'
  id: totrans-1456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE415]'
- en: '[PRE416]'
  id: totrans-1457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE416]'
- en: '[PRE417]'
  id: totrans-1458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE417]'
- en: '[PRE418]'
  id: totrans-1459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE418]'
- en: '[PRE419]'
  id: totrans-1460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE419]'
- en: '[PRE420]'
  id: totrans-1461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE420]'
- en: '[PRE421]'
  id: totrans-1462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE421]'
- en: '[PRE422]'
  id: totrans-1463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE422]'
- en: '[PRE423]'
  id: totrans-1464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE423]'
- en: '[PRE424]'
  id: totrans-1465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE424]'
- en: '[PRE425]'
  id: totrans-1466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE425]'
- en: '[PRE426]'
  id: totrans-1467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE426]'
- en: 'Bases: `Multistreamable`'
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Multistreamable`
- en: '[PRE427]'
  id: totrans-1469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE427]'
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
- en: '[PRE428]'
  id: totrans-1471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE428]'
- en: 'Bases: `Multistreamable`'
  id: totrans-1472
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Multistreamable`
- en: '[PRE429]'
  id: totrans-1473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE429]'
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
- en: '[PRE430]'
  id: totrans-1475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE430]'
- en: 'Bases: `object`'
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The interface to access a sharded module’s sharding scheme.
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: 访问分片模块的分片方案的接口。
- en: '[PRE431]'
  id: totrans-1478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE431]'
- en: '[PRE432]'
  id: totrans-1479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE432]'
- en: 'Bases: `Enum`'
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Enum`
- en: An enumeration.
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
  zh: 一个枚举。
- en: '[PRE433]'
  id: totrans-1482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE433]'
- en: '[PRE434]'
  id: totrans-1483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE434]'
- en: '[PRE435]'
  id: totrans-1484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE435]'
- en: '[PRE436]'
  id: totrans-1485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE436]'
- en: '[PRE437]'
  id: totrans-1486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE437]'
- en: '[PRE438]'
  id: totrans-1487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE438]'
- en: '[PRE439]'
  id: totrans-1488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE439]'
- en: '[PRE440]'
  id: totrans-1489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE440]'
- en: '[PRE441]'
  id: totrans-1490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE441]'
- en: '[PRE442]'
  id: totrans-1491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE442]'
- en: '[PRE443]'
  id: totrans-1492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE443]'
- en: '[PRE444]'
  id: totrans-1493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE444]'
- en: '[PRE445]'
  id: totrans-1494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE445]'
- en: 'Bases: `object`'
  id: totrans-1495
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE446]'
  id: totrans-1496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE446]'
- en: '[PRE447]'
  id: totrans-1497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE447]'
- en: '[PRE448]'
  id: totrans-1498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE448]'
- en: 'Bases: [`ShardedModule`](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[`CompIn`,
    `DistOut`, `Out`, `ShrdCtx`], [`ModuleShardingMixIn`](#torchrec.distributed.embedding_types.ModuleShardingMixIn
    "torchrec.distributed.embedding_types.ModuleShardingMixIn")'
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ShardedModule`](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[`CompIn`,
    `DistOut`, `Out`, `ShrdCtx`], [`ModuleShardingMixIn`](#torchrec.distributed.embedding_types.ModuleShardingMixIn
    "torchrec.distributed.embedding_types.ModuleShardingMixIn")
- en: All model-parallel embedding modules implement this interface. Inputs and outputs
    are data-parallel.
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型并行嵌入模块都实现了这个接口。输入和输出是数据并行的。
- en: 'Args::'
  id: totrans-1501
  prefs: []
  type: TYPE_NORMAL
  zh: '参数::'
- en: 'qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]) : Mapping
    of CommOp name to QuantizedCommCodecs'
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
  zh: 'qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]) : CommOp名称到QuantizedCommCodecs的映射'
- en: '[PRE449]'
  id: totrans-1503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE449]'
- en: Pretty prints representation of the module’s lookup modules, input_dists and
    output_dists
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
  zh: 漂亮地打印模块的查找模块、输入分布和输出分布的表示
- en: '[PRE450]'
  id: totrans-1505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE450]'
- en: Prefetch input features for each lookup module.
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个查找模块预取输入特征。
- en: '[PRE451]'
  id: totrans-1507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE451]'
- en: '[PRE452]'
  id: totrans-1508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE452]'
- en: 'Bases: [`ShardedMetaConfig`](#torchrec.distributed.embedding_types.ShardedMetaConfig
    "torchrec.distributed.embedding_types.ShardedMetaConfig"), [`EmbeddingAttributes`](#torchrec.distributed.embedding_types.EmbeddingAttributes
    "torchrec.distributed.embedding_types.EmbeddingAttributes"), [`EmbeddingTableConfig`](torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingTableConfig
    "torchrec.modules.embedding_configs.EmbeddingTableConfig")'
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ShardedMetaConfig`](#torchrec.distributed.embedding_types.ShardedMetaConfig
    "torchrec.distributed.embedding_types.ShardedMetaConfig"), [`EmbeddingAttributes`](#torchrec.distributed.embedding_types.EmbeddingAttributes
    "torchrec.distributed.embedding_types.EmbeddingAttributes"), [`EmbeddingTableConfig`](torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingTableConfig
    "torchrec.modules.embedding_configs.EmbeddingTableConfig")
- en: '[PRE453]'
  id: totrans-1510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE453]'
- en: '[PRE454]'
  id: totrans-1511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE454]'
- en: 'Bases: [`ShardedConfig`](#torchrec.distributed.embedding_types.ShardedConfig
    "torchrec.distributed.embedding_types.ShardedConfig")'
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ShardedConfig`](#torchrec.distributed.embedding_types.ShardedConfig "torchrec.distributed.embedding_types.ShardedConfig")
- en: '[PRE455]'
  id: totrans-1513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE455]'
- en: '[PRE456]'
  id: totrans-1514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE456]'
- en: '[PRE457]  ## torchrec.distributed.embeddingbag[](#module-torchrec.distributed.embeddingbag
    "Permalink to this heading")'
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE457]  ## torchrec.distributed.embeddingbag[](#module-torchrec.distributed.embeddingbag
    "Permalink to this heading")'
- en: '[PRE458]'
  id: totrans-1516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE458]'
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[`Tensor`]'
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[`Tensor`]
- en: '[PRE459]'
  id: totrans-1518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE459]'
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[[`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor")]'
  id: totrans-1519
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[[`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor")]
- en: '[PRE460]'
  id: totrans-1520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE460]'
- en: 'Bases: `Multistreamable`'
  id: totrans-1521
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Multistreamable`
- en: '[PRE461]'
  id: totrans-1522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE461]'
- en: '[PRE462]'
  id: totrans-1523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE462]'
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
- en: '[PRE463]'
  id: totrans-1525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE463]'
- en: '[PRE464]'
  id: totrans-1526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE464]'
- en: '[PRE465]'
  id: totrans-1527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE465]'
- en: 'Bases: [`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[[`EmbeddingBagCollection`](torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection
    "torchrec.modules.embedding_modules.EmbeddingBagCollection")]'
  id: totrans-1528
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[[`EmbeddingBagCollection`](torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection
    "torchrec.modules.embedding_modules.EmbeddingBagCollection")]
- en: This implementation uses non-fused EmbeddingBagCollection
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现使用非融合的EmbeddingBagCollection
- en: '[PRE466]'
  id: totrans-1530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE466]'
- en: '[PRE467]'
  id: totrans-1531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE467]'
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  id: totrans-1532
  prefs: []
  type: TYPE_NORMAL
  zh: 执行实际的分片。它将根据相应的ParameterSharding在请求的位置上分配参数。
- en: Default implementation is data-parallel replication.
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
  zh: 默认实现是数据并行复制。
- en: 'Parameters:'
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*M*) – module to shard.'
  id: totrans-1535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module**（*M*） - 要分片的模块。'
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  id: totrans-1536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params**（[*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")） - 完全限定的参数名称（模块路径+参数名称，用''.''分隔）到其分片规范的字典。'
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  id: totrans-1537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env**（[*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv")）
    - 具有进程组的分片环境。'
- en: '**device** (*torch.device*) – compute device.'
  id: totrans-1538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*） - 计算设备。'
- en: 'Returns:'
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: sharded module implementation.
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
  zh: 分片模块实现。
- en: 'Return type:'
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
- en: '[PRE468]'
  id: totrans-1543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE468]'
- en: List of parameters that can be sharded.
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
  zh: 可以分片的参数列表。
- en: '[PRE469]'
  id: totrans-1545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE469]'
- en: 'Bases: [`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[`EmbeddingBag`]'
  id: totrans-1546
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[`EmbeddingBag`]
- en: This implementation uses non-fused nn.EmbeddingBag
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现使用非融合的nn.EmbeddingBag
- en: '[PRE470]'
  id: totrans-1548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE470]'
- en: '[PRE471]'
  id: totrans-1549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE471]'
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
  zh: 执行实际的分片。它将根据相应的ParameterSharding在请求的位置上分配参数。
- en: Default implementation is data-parallel replication.
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
  zh: 默认实现是数据并行复制。
- en: 'Parameters:'
  id: totrans-1552
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*M*) – module to shard.'
  id: totrans-1553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module**（*M*） - 要分片的模块。'
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  id: totrans-1554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params**（[*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")） - 完全限定的参数名称（模块路径+参数名称，用''.''分隔）到其分片规范的字典。'
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  id: totrans-1555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env**（[*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv")）
    - 具有进程组的分片环境。'
- en: '**device** (*torch.device*) – compute device.'
  id: totrans-1556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*） - 计算设备。'
- en: 'Returns:'
  id: totrans-1557
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: sharded module implementation.
  id: totrans-1558
  prefs: []
  type: TYPE_NORMAL
  zh: 分片模块实现。
- en: 'Return type:'
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  id: totrans-1560
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
- en: '[PRE472]'
  id: totrans-1561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE472]'
- en: List of parameters that can be sharded.
  id: totrans-1562
  prefs: []
  type: TYPE_NORMAL
  zh: 可以分片的参数列表。
- en: '[PRE473]'
  id: totrans-1563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE473]'
- en: 'Bases: [`ShardedEmbeddingModule`](#torchrec.distributed.embedding_types.ShardedEmbeddingModule
    "torchrec.distributed.embedding_types.ShardedEmbeddingModule")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`, [`NullShardedModuleContext`](#torchrec.distributed.types.NullShardedModuleContext
    "torchrec.distributed.types.NullShardedModuleContext")], [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")'
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ShardedEmbeddingModule`](#torchrec.distributed.embedding_types.ShardedEmbeddingModule
    "torchrec.distributed.embedding_types.ShardedEmbeddingModule")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`, [`NullShardedModuleContext`](#torchrec.distributed.types.NullShardedModuleContext
    "torchrec.distributed.types.NullShardedModuleContext")], [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")
- en: Sharded implementation of nn.EmbeddingBag. This is part of the public API to
    allow for manual data dist pipelining.
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
  zh: nn.EmbeddingBag的分片实现。这是公共API的一部分，允许手动数据分布流水线。
- en: '[PRE474]'
  id: totrans-1566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE474]'
- en: '[PRE475]'
  id: totrans-1567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE475]'
- en: '[PRE476]'
  id: totrans-1568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE476]'
- en: '[PRE477]'
  id: totrans-1569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE477]'
- en: '[PRE478]'
  id: totrans-1570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE478]'
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict") into this
    module and its descendants.
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
  zh: 将参数和缓冲区从[`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict")复制到此模块及其后代。
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict") must exactly
    match the keys returned by this module’s `state_dict()` function.
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`strict`为`True`，则[`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict")的键必须与此模块的`state_dict()`函数返回的键完全匹配。
- en: Warning
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict").
  id: totrans-1574
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`assign`为`True`，则必须在调用[`load_state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict")之后创建优化器。
- en: 'Parameters:'
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  id: totrans-1576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict**（*dict*） - 包含参数和持久缓冲区的字典。'
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict") match the
    keys returned by this module’s `state_dict()` function. Default: `True`'
  id: totrans-1577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**strict**（*bool**，*可选*）- 是否严格执行[`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict")中的键与此模块的`state_dict()`函数返回的键匹配。默认值：`True`'
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  id: totrans-1578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**assign**（*bool**，*可选*）- 是否将状态字典中的项目分配给模块中对应的键，而不是将它们原地复制到模块的当前参数和缓冲区中。当为`False`时，保留当前模块中张量的属性，而为`True`时，保留状态字典中张量的属性。默认值：`False`'
- en: 'Returns:'
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: '**missing_keys** is a list of str containing the missing keys'
  id: totrans-1580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**missing_keys**是包含缺少键的str列表'
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  id: totrans-1581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unexpected_keys**是包含意外键的str列表'
- en: 'Return type:'
  id: totrans-1582
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
  zh: 带有`missing_keys`和`unexpected_keys`字段的`NamedTuple`
- en: Note
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict"), [`load_state_dict()`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict") will
    raise a `RuntimeError`.
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将参数或缓冲区注册为“None”，并且其相应的键存在于[`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict")中，[`load_state_dict()`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict")将引发`RuntimeError`。
- en: '[PRE479]'
  id: totrans-1586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE479]'
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  id: totrans-1587
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个迭代器，遍历模块缓冲区，产生缓冲区的名称以及缓冲区本身。
- en: 'Parameters:'
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  id: totrans-1589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix**（*str*）- 要添加到所有缓冲区名称前面的前缀。'
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  id: totrans-1590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse**（*bool**，*可选*）- 如果为True，则会产生此模块及所有子模块的缓冲区。否则，只会产生此模块的直接成员缓冲区。默认为True。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  id: totrans-1591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate**（*bool**，*可选*）- 是否删除结果中的重复缓冲区。默认为True。'
- en: 'Yields:'
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
  zh: 产生：
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  id: totrans-1593
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, torch.Tensor)* - 包含名称和缓冲区的元组'
- en: 'Example:'
  id: totrans-1594
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE480]'
  id: totrans-1595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE480]'
- en: '[PRE481]'
  id: totrans-1596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE481]'
- en: Return an iterator over all modules in the network, yielding both the name of
    the module as well as the module itself.
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个迭代器，遍历网络中的所有模块，产生模块的名称以及模块本身。
- en: 'Parameters:'
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**memo** – a memo to store the set of modules already added to the result'
  id: totrans-1599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**memo** - 一个备忘录，用于存储已添加到结果中的模块集合'
- en: '**prefix** – a prefix that will be added to the name of the module'
  id: totrans-1600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** - 将添加到模块名称前面的前缀'
- en: '**remove_duplicate** – whether to remove the duplicated module instances in
    the result or not'
  id: totrans-1601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate** - 是否删除结果中的重复模块实例'
- en: 'Yields:'
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
  zh: 产生：
- en: '*(str, Module)* – Tuple of name and module'
  id: totrans-1603
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, Module)* - 名称和模块的元组'
- en: Note
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Duplicate modules are returned only once. In the following example, `l` will
    be returned only once.
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
  zh: 重复模块只返回一次。在以下示例中，`l`只会返回一次。
- en: 'Example:'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE482]'
  id: totrans-1607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE482]'
- en: '[PRE483]'
  id: totrans-1608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE483]'
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个迭代器，遍历模块参数，产生参数的名称以及参数本身。
- en: 'Parameters:'
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  id: totrans-1611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix**（*str*）- 要添加到所有参数名称前面的前缀。'
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  id: totrans-1612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse**（*bool*）- 如果为True，则会产生此模块及所有子模块的参数。否则，只会产生此模块的直接成员参数。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  id: totrans-1613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate**（*bool**，*可选*）- 是否删除结果中的重复参数。默认为True。'
- en: 'Yields:'
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: 产生：
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, Parameter)* - 包含名称和参数的元组'
- en: 'Example:'
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE484]'
  id: totrans-1617
  prefs: []
  type: TYPE_PRE
  zh: '[PRE484]'
- en: '[PRE485]'
  id: totrans-1618
  prefs: []
  type: TYPE_PRE
  zh: '[PRE485]'
- en: '[PRE486]'
  id: totrans-1619
  prefs: []
  type: TYPE_PRE
  zh: '[PRE486]'
- en: '[PRE487]'
  id: totrans-1620
  prefs: []
  type: TYPE_PRE
  zh: '[PRE487]'
- en: Return a dictionary containing references to the whole state of the module.
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个包含模块整体状态引用的字典。
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  id: totrans-1622
  prefs: []
  type: TYPE_NORMAL
  zh: 包括参数和持久缓冲区（例如运行平均值）。键是相应的参数和缓冲区名称。设置为“None”的参数和缓冲区不包括在内。
- en: Note
  id: totrans-1623
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的对象是一个浅拷贝。它包含对模块参数和缓冲区的引用。
- en: Warning
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  id: totrans-1626
  prefs: []
  type: TYPE_NORMAL
  zh: 目前`state_dict()`还接受`destination`、`prefix`和`keep_vars`的位置参数。但是，这将被弃用，并且将在未来版本中强制使用关键字参数。
- en: Warning
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免使用参数`destination`，因为它不是为最终用户设计的。
- en: 'Parameters:'
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  id: totrans-1630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**destination**（*dict**，*可选*）- 如果提供，则模块的状态将更新到字典中，并返回相同的对象。否则，将创建并返回一个`OrderedDict`。默认值：`None`。'
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  id: totrans-1631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str**,* *optional*) – 用于组成状态字典中键的参数和缓冲区名称的前缀。默认值：`''''`。'
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  id: totrans-1632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keep_vars** (*bool**,* *optional*) – 默认情况下，状态字典中返回的`Tensor`会从自动求导中分离出来。如果设置为`True`，则不会执行分离操作。默认值：`False`。'
- en: 'Returns:'
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: a dictionary containing a whole state of the module
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
  zh: 包含模块整体状态的字典
- en: 'Return type:'
  id: totrans-1635
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: dict
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
  zh: 字典
- en: 'Example:'
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE488]'
  id: totrans-1638
  prefs: []
  type: TYPE_PRE
  zh: '[PRE488]'
- en: '[PRE489]'
  id: totrans-1639
  prefs: []
  type: TYPE_PRE
  zh: '[PRE489]'
- en: '[PRE490]'
  id: totrans-1640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE490]'
- en: 'Bases: [`ShardedEmbeddingModule`](#torchrec.distributed.embedding_types.ShardedEmbeddingModule
    "torchrec.distributed.embedding_types.ShardedEmbeddingModule")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], [`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor"), [`EmbeddingBagCollectionContext`](#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext
    "torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext")], [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")'
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ShardedEmbeddingModule`](#torchrec.distributed.embedding_types.ShardedEmbeddingModule
    "torchrec.distributed.embedding_types.ShardedEmbeddingModule")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], [`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor"), [`EmbeddingBagCollectionContext`](#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext
    "torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext")], [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")
- en: Sharded implementation of EmbeddingBagCollection. This is part of the public
    API to allow for manual data dist pipelining.
  id: totrans-1642
  prefs: []
  type: TYPE_NORMAL
  zh: EmbeddingBagCollection的分片实现。这是公共API的一部分，允许手动数据分布流水线化。
- en: '[PRE491]'
  id: totrans-1643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE491]'
- en: '[PRE492]'
  id: totrans-1644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE492]'
- en: In case of multiple output distributions it makes sense to override this method
    and initiate the output distibution as soon as the corresponding compute completes.
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在多个输出分布的情况下，重写此方法并在相应的计算完成后立即初始化输出分布是有意义的。
- en: '[PRE493]'
  id: totrans-1646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE493]'
- en: '[PRE494]'
  id: totrans-1647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE494]'
- en: '[PRE495]'
  id: totrans-1648
  prefs: []
  type: TYPE_PRE
  zh: '[PRE495]'
- en: '[PRE496]'
  id: totrans-1649
  prefs: []
  type: TYPE_PRE
  zh: '[PRE496]'
- en: '[PRE497]'
  id: totrans-1650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE497]'
- en: '[PRE498]'
  id: totrans-1651
  prefs: []
  type: TYPE_PRE
  zh: '[PRE498]'
- en: '[PRE499]'
  id: totrans-1652
  prefs: []
  type: TYPE_PRE
  zh: '[PRE499]'
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[[`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor")]'
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[[`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor")]
- en: '[PRE500]'
  id: totrans-1654
  prefs: []
  type: TYPE_PRE
  zh: '[PRE500]'
- en: '[PRE501]'
  id: totrans-1655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE501]'
- en: '[PRE502]'
  id: totrans-1656
  prefs: []
  type: TYPE_PRE
  zh: '[PRE502]'
- en: '[PRE503]'
  id: totrans-1657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE503]'
- en: 'Placement device and tensor device could be unmatched in some scenarios, e.g.
    passing meta device to DMP and passing cuda to EmbeddingShardingPlanner. We need
    to make device consistent after getting sharding planner.  ## torchrec.distributed.grouped_position_weighted[](#module-torchrec.distributed.grouped_position_weighted
    "Permalink to this heading")'
  id: totrans-1658
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，放置设备和张量设备可能不匹配，例如将元设备传递给DMP并将cuda传递给EmbeddingShardingPlanner。在获取分片规划器后，我们需要使设备保持一致。##
    torchrec.distributed.grouped_position_weighted[](#module-torchrec.distributed.grouped_position_weighted
    "Permalink to this heading")
- en: '[PRE504]'
  id: totrans-1659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE504]'
- en: 'Bases: [`BaseGroupedFeatureProcessor`](#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor
    "torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor")'
  id: totrans-1660
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseGroupedFeatureProcessor`](#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor
    "torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor")
- en: '[PRE505]'
  id: totrans-1661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE505]'
- en: Define the computation performed at every call.
  id: totrans-1662
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-1663
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类重写。
- en: Note
  id: totrans-1664
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-1665
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行注册的钩子，而后者会默默地忽略它们。
- en: '[PRE506]'
  id: totrans-1666
  prefs: []
  type: TYPE_PRE
  zh: '[PRE506]'
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  id: totrans-1667
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个模块缓冲区的迭代器，同时返回缓冲区的名称和缓冲区本身。
- en: 'Parameters:'
  id: totrans-1668
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  id: totrans-1669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str*) – 添加到所有缓冲区名称前面的前缀。'
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  id: totrans-1670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse** (*bool**,* *optional*) – 如果为True，则返回此模块及所有子模块的缓冲区。否则，只返回此模块的直接成员缓冲区。默认为True。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  id: totrans-1671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate** (*bool**,* *optional*) – 是否在结果中删除重复的缓冲区。默认为True。'
- en: 'Yields:'
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
  zh: 产出：
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  id: totrans-1673
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, torch.Tensor)* – 包含名称和缓冲区的元组'
- en: 'Example:'
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE507]'
  id: totrans-1675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE507]'
- en: '[PRE508]'
  id: totrans-1676
  prefs: []
  type: TYPE_PRE
  zh: '[PRE508]'
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个模块参数的迭代器，同时返回参数的名称和参数本身。
- en: 'Parameters:'
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  id: totrans-1679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str*) – 添加到所有参数名称前面的前缀。'
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  id: totrans-1680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse** (*bool*) – 如果为True，则返回此模块及所有子模块的参数。否则，只返回此模块的直接成员参数。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  id: totrans-1681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate** (*bool**,* *optional*) – 是否在结果中删除重复的参数。默认为True。'
- en: 'Yields:'
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
  zh: 产出：
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  id: totrans-1683
  prefs: []
  type: TYPE_NORMAL
  zh: '*(str, Parameter)* – 包含名称和参数的元组'
- en: 'Example:'
  id: totrans-1684
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE509]'
  id: totrans-1685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE509]'
- en: '[PRE510]'
  id: totrans-1686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE510]'
- en: Return a dictionary containing references to the whole state of the module.
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个包含对模块整体状态的引用的字典。
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  id: totrans-1688
  prefs: []
  type: TYPE_NORMAL
  zh: 包括参数和持久缓冲区（例如运行平均值）。键是对应的参数和缓冲区名称。设置为`None`的参数和缓冲区不包括在内。
- en: Note
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的对象是一个浅拷贝。它包含对模块参数和缓冲区的引用。
- en: Warning
  id: totrans-1691
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
  zh: 当前`state_dict()`还接受位置参数`destination`、`prefix`和`keep_vars`。但是，这将被弃用，并且将在未来版本中强制使用关键字参数。
- en: Warning
  id: totrans-1693
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  id: totrans-1694
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免使用参数`destination`，因为它不是为最终用户设计的。
- en: 'Parameters:'
  id: totrans-1695
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  id: totrans-1696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**destination** (*dict**,* *可选*) – 如果提供，模块的状态将更新到字典中，并返回相同的对象。否则，将创建并返回一个`OrderedDict`。默认值：`None`。'
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  id: totrans-1697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str**,* *可选*) – 添加到参数和缓冲区名称以组成state_dict中键的前缀。默认值：`''''`。'
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  id: totrans-1698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keep_vars** (*bool**,* *可选*) – 默认情况下，在状态字典中返回的`Tensor`会与自动求导分离。如果设置为`True`，则不会执行分离。默认值：`False`。'
- en: 'Returns:'
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: a dictionary containing a whole state of the module
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
  zh: 包含模块整体状态的字典
- en: 'Return type:'
  id: totrans-1701
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: dict
  id: totrans-1702
  prefs: []
  type: TYPE_NORMAL
  zh: 字典
- en: 'Example:'
  id: totrans-1703
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE511]'
  id: totrans-1704
  prefs: []
  type: TYPE_PRE
  zh: '[PRE511]'
- en: '[PRE512]  ## torchrec.distributed.model_parallel[](#module-torchrec.distributed.model_parallel
    "Permalink to this heading")'
  id: totrans-1705
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE512]  ## torchrec.distributed.model_parallel[](#module-torchrec.distributed.model_parallel
    "Permalink to this heading")'
- en: '[PRE513]'
  id: totrans-1706
  prefs: []
  type: TYPE_PRE
  zh: '[PRE513]'
- en: 'Bases: `ABC`'
  id: totrans-1707
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`
- en: Interface implemented by custom data parallel wrappers.
  id: totrans-1708
  prefs: []
  type: TYPE_NORMAL
  zh: 由自定义数据并行包装器实现的接口。
- en: '[PRE514]'
  id: totrans-1709
  prefs: []
  type: TYPE_PRE
  zh: '[PRE514]'
- en: '[PRE515]'
  id: totrans-1710
  prefs: []
  type: TYPE_PRE
  zh: '[PRE515]'
- en: 'Bases: [`DataParallelWrapper`](#torchrec.distributed.model_parallel.DataParallelWrapper
    "torchrec.distributed.model_parallel.DataParallelWrapper")'
  id: totrans-1711
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`DataParallelWrapper`](#torchrec.distributed.model_parallel.DataParallelWrapper
    "torchrec.distributed.model_parallel.DataParallelWrapper")
- en: Default data parallel wrapper, which applies data parallel to all unsharded
    modules.
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
  zh: 默认数据并行包装器，将数据并行应用于所有未分片的模块。
- en: '[PRE516]'
  id: totrans-1713
  prefs: []
  type: TYPE_PRE
  zh: '[PRE516]'
- en: '[PRE517]'
  id: totrans-1714
  prefs: []
  type: TYPE_PRE
  zh: '[PRE517]'
- en: 'Bases: `Module`, [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")'
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`，[`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")
- en: Entry point to model parallelism.
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行的入口点。
- en: 'Parameters:'
  id: totrans-1717
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*nn.Module*) – module to wrap.'
  id: totrans-1718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module** (*nn.Module*) – 要包装的模块。'
- en: '**env** (*Optional**[*[*ShardingEnv*](#torchrec.distributed.types.ShardingEnv
    "torchrec.distributed.types.ShardingEnv")*]*) – sharding environment that has
    the process group.'
  id: totrans-1719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env** (*可选**[*[*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv")*]*)
    – 具有进程组的分片环境。'
- en: '**device** (*Optional**[**torch.device**]*) – compute device, defaults to cpu.'
  id: totrans-1720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*可选**[**torch.device**]*) – 计算设备，默认为cpu。'
- en: '**plan** (*Optional**[*[*ShardingPlan*](#torchrec.distributed.types.ShardingPlan
    "torchrec.distributed.types.ShardingPlan")*]*) – plan to use when sharding, defaults
    to EmbeddingShardingPlanner.collective_plan().'
  id: totrans-1721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**plan** (*可选**[*[*ShardingPlan*](#torchrec.distributed.types.ShardingPlan
    "torchrec.distributed.types.ShardingPlan")*]*) – 在分片时使用的计划，默认为EmbeddingShardingPlanner.collective_plan()。'
- en: '**sharders** (*Optional**[**List**[*[*ModuleSharder*](#torchrec.distributed.types.ModuleSharder
    "torchrec.distributed.types.ModuleSharder")*[**nn.Module**]**]**]*) – ModuleSharders
    available to shard with, defaults to EmbeddingBagCollectionSharder().'
  id: totrans-1722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sharders** (*可选**[**List**[*[*ModuleSharder*](#torchrec.distributed.types.ModuleSharder
    "torchrec.distributed.types.ModuleSharder")*[**nn.Module**]**]**]*) – 可用于分片的ModuleSharders，默认为EmbeddingBagCollectionSharder()。'
- en: '**init_data_parallel** (*bool*) – data-parallel modules can be lazy, i.e. they
    delay parameter initialization until the first forward pass. Pass True to delay
    initialization of data parallel modules. Do first forward pass and then call DistributedModelParallel.init_data_parallel().'
  id: totrans-1723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**init_data_parallel** (*bool*) – 数据并行模块可以是懒惰的，即它们延迟参数初始化直到第一次前向传递。传递True以延迟数据并行模块的初始化。进行第一次前向传递，然后调用DistributedModelParallel.init_data_parallel()。'
- en: '**init_parameters** (*bool*) – initialize parameters for modules still on meta
    device.'
  id: totrans-1724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**init_parameters** (*bool*) – 为仍在元设备上的模块初始化参数。'
- en: '**data_parallel_wrapper** (*Optional**[*[*DataParallelWrapper*](#torchrec.distributed.model_parallel.DataParallelWrapper
    "torchrec.distributed.model_parallel.DataParallelWrapper")*]*) – custom wrapper
    for data parallel modules.'
  id: totrans-1725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**data_parallel_wrapper** (*可选**[*[*DataParallelWrapper*](#torchrec.distributed.model_parallel.DataParallelWrapper
    "torchrec.distributed.model_parallel.DataParallelWrapper")*]*) – 数据并行模块的自定义包装器。'
- en: 'Example:'
  id: totrans-1726
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE518]'
  id: totrans-1727
  prefs: []
  type: TYPE_PRE
  zh: '[PRE518]'
- en: '[PRE519]'
  id: totrans-1728
  prefs: []
  type: TYPE_PRE
  zh: '[PRE519]'
- en: '[PRE520]'
  id: totrans-1729
  prefs: []
  type: TYPE_PRE
  zh: '[PRE520]'
- en: Recursively copy submodules to new device by calling per-module customized copy
    process, since some modules needs to use the original references (like ShardedModule
    for inference).
  id: totrans-1730
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用每个模块的自定义复制过程递归地将子模块复制到新设备，因为一些模块需要使用原始引用（例如用于推理的ShardedModule）。
- en: '[PRE521]'
  id: totrans-1731
  prefs: []
  type: TYPE_PRE
  zh: '[PRE521]'
- en: Define the computation performed at every call.
  id: totrans-1732
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-1733
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类重写。
- en: Note
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是此方法，因为前者会负责运行注册的钩子，而后者会默默地忽略它们。
- en: '[PRE522]'
  id: totrans-1736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE522]'
- en: '[PRE523]'
  id: totrans-1737
  prefs: []
  type: TYPE_PRE
  zh: '[PRE523]'
- en: See init_data_parallel c-tor argument for usage. It’s safe to call this method
    multiple times.
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
  zh: 查看init_data_parallel c-tor参数的用法。可以多次调用此方法。
- en: '[PRE524]'
  id: totrans-1739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE524]'
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict") into
    this module and its descendants.
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
  zh: 将参数和缓冲区从[`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict")复制到此模块及其后代中。
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict") must
    exactly match the keys returned by this module’s `state_dict()` function.
  id: totrans-1741
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`strict`为`True`，则[`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict")的键必须与此模块的`state_dict()`函数返回的键完全匹配。
- en: Warning
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict").
  id: totrans-1743
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`assign`为`True`，则必须在调用[`load_state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict")之后创建优化器。
- en: 'Parameters:'
  id: totrans-1744
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  id: totrans-1745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict**（*dict*）- 包含参数和持久缓冲区的字典。'
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict") match
    the keys returned by this module’s `state_dict()` function. Default: `True`'
  id: totrans-1746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**strict**（*bool**，*可选*）- 是否严格执行[`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict")中的键与此模块的`state_dict()`函数返回的键匹配。默认：`True`'
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  id: totrans-1747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**assign**（*bool**，*可选*）- 是否将状态字典中的项目分配给模块中的相应键，而不是将它们原地复制到模块的当前参数和缓冲区中。当`False`时，保留当前模块张量的属性，而当`True`时，保留状态字典中张量的属性。默认：`False`'
- en: 'Returns:'
  id: totrans-1748
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: '**missing_keys** is a list of str containing the missing keys'
  id: totrans-1749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**missing_keys**是一个包含缺失键的字符串列表'
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  id: totrans-1750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unexpected_keys**是一个包含意外键的字符串列表'
- en: 'Return type:'
  id: totrans-1751
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  id: totrans-1752
  prefs: []
  type: TYPE_NORMAL
  zh: 带有`missing_keys`和`unexpected_keys`字段的`NamedTuple`
- en: Note
  id: totrans-1753
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict"), [`load_state_dict()`](#torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict")
    will raise a `RuntimeError`.
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数或缓冲区注册为`None`，并且其对应的键存在于[`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict")中，[`load_state_dict()`](#torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict")将引发`RuntimeError`。
- en: '[PRE525]'
  id: totrans-1755
  prefs: []
  type: TYPE_PRE
  zh: '[PRE525]'
- en: Property to directly access sharded module, which will not be wrapped in DDP,
    FSDP, DMP, or any other parallelism wrappers.
  id: totrans-1756
  prefs: []
  type: TYPE_NORMAL
  zh: 直接访问分片模块的属性，该模块不会包含在DDP、FSDP、DMP或任何其他并行包装器中。
- en: '[PRE526]'
  id: totrans-1757
  prefs: []
  type: TYPE_PRE
  zh: '[PRE526]'
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个迭代器，遍历模块缓冲区，产出缓冲区的名称以及缓冲区本身。
- en: 'Parameters:'
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  id: totrans-1760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix**（*str*）- 要添加到所有缓冲区名称前面的前缀。'
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  id: totrans-1761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse**（*bool**，*可选*）- 如果为True，则产出此模块及所有子模块的缓冲区。否则，仅产出此模块的直接成员缓冲区。默认为True。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  id: totrans-1762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate**（*bool**，*可选*）- 是否在结果中删除重复的缓冲区。默认为True。'
- en: 'Yields:'
  id: totrans-1763
  prefs: []
  type: TYPE_NORMAL
  zh: 产量：
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
  zh: （str，torch.Tensor）- 包含名称和缓冲区的元组
- en: 'Example:'
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE527]'
  id: totrans-1766
  prefs: []
  type: TYPE_PRE
  zh: '[PRE527]'
- en: '[PRE528]'
  id: totrans-1767
  prefs: []
  type: TYPE_PRE
  zh: '[PRE528]'
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个迭代器，遍历模块参数，产出参数的名称以及参数本身。
- en: 'Parameters:'
  id: totrans-1769
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  id: totrans-1770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix**（*str*）- 要添加到所有参数名称前面的前缀。'
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  id: totrans-1771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse**（*bool*）- 如果为True，则产出此模块及所有子模块的参数。否则，仅产出此模块的直接成员参数。'
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  id: totrans-1772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**remove_duplicate**（*bool**，*可选*）- 是否在结果中删除重复的参数。默认为True。'
- en: 'Yields:'
  id: totrans-1773
  prefs: []
  type: TYPE_NORMAL
  zh: 产量：
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  id: totrans-1774
  prefs: []
  type: TYPE_NORMAL
  zh: （str，Parameter）- 包含名称和参数的元组
- en: 'Example:'
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE529]'
  id: totrans-1776
  prefs: []
  type: TYPE_PRE
  zh: '[PRE529]'
- en: '[PRE530]'
  id: totrans-1777
  prefs: []
  type: TYPE_PRE
  zh: '[PRE530]'
- en: '[PRE531]'
  id: totrans-1778
  prefs: []
  type: TYPE_PRE
  zh: '[PRE531]'
- en: '[PRE532]'
  id: totrans-1779
  prefs: []
  type: TYPE_PRE
  zh: '[PRE532]'
- en: Return a dictionary containing references to the whole state of the module.
  id: totrans-1780
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个包含模块整体状态引用的字典。
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
  zh: 包括参数和持久缓冲区（例如运行平均值）。键是相应的参数和缓冲区名称。设置为`None`的参数和缓冲区不包括在内。
- en: Note
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  id: totrans-1783
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的对象是一个浅拷贝。它包含对模块参数和缓冲区的引用。
- en: Warning
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  id: totrans-1785
  prefs: []
  type: TYPE_NORMAL
  zh: 目前`state_dict()`还接受`destination`、`prefix`和`keep_vars`的位置参数，但这将被弃用，并且将在未来的版本中强制使用关键字参数。
- en: Warning
  id: totrans-1786
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  id: totrans-1787
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免使用参数`destination`，因为它不是为最终用户设计的。
- en: 'Parameters:'
  id: totrans-1788
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  id: totrans-1789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**destination**（*dict**，*可选*）- 如果提供，则模块的状态将更新到字典中，并返回相同的对象。否则，将创建并返回一个`OrderedDict`。默认值：`None`。'
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  id: totrans-1790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix**（*str**，*可选*）- 添加到参数和缓冲区名称以组成state_dict中键的前缀。默认值：`''''`。'
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  id: totrans-1791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keep_vars**（*bool**，*可选*）- 默认情况下，在状态字典中返回的`Tensor`是从自动求导中分离的。如果设置为`True`，则不会执行分离。默认值：`False`。'
- en: 'Returns:'
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: a dictionary containing a whole state of the module
  id: totrans-1793
  prefs: []
  type: TYPE_NORMAL
  zh: 包含模块整体状态的字典
- en: 'Return type:'
  id: totrans-1794
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: dict
  id: totrans-1795
  prefs: []
  type: TYPE_NORMAL
  zh: 字典
- en: 'Example:'
  id: totrans-1796
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE533]'
  id: totrans-1797
  prefs: []
  type: TYPE_PRE
  zh: '[PRE533]'
- en: '[PRE534]'
  id: totrans-1798
  prefs: []
  type: TYPE_PRE
  zh: '[PRE534]'
- en: '[PRE535]'
  id: totrans-1799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE535]'
- en: Unwraps DMP module.
  id: totrans-1800
  prefs: []
  type: TYPE_NORMAL
  zh: 解开DMP模块。
- en: Does not unwrap data parallel wrappers (i.e. DDP/FSDP), so overriding implementations
    by the wrappers can be used.
  id: totrans-1801
  prefs: []
  type: TYPE_NORMAL
  zh: 不解开数据并行包装器（即DDP/FSDP），因此可以使用包装器的覆盖实现。
- en: '[PRE536]'
  id: totrans-1802
  prefs: []
  type: TYPE_PRE
  zh: '[PRE536]'
- en: 'Unwraps module wrapped by DMP, DDP, or FSDP.  ## torchrec.distributed.quant_embeddingbag[](#module-torchrec.distributed.quant_embeddingbag
    "Permalink to this heading")'
  id: totrans-1803
  prefs: []
  type: TYPE_NORMAL
  zh: 解开由DMP、DDP或FSDP包装的模块。## torchrec.distributed.quant_embeddingbag[](#module-torchrec.distributed.quant_embeddingbag
    "Permalink to this heading")
- en: '[PRE537]'
  id: totrans-1804
  prefs: []
  type: TYPE_PRE
  zh: '[PRE537]'
- en: 'Bases: [`BaseQuantEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder")[[`EmbeddingBagCollection`](torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection
    "torchrec.quant.embedding_modules.EmbeddingBagCollection")]'
  id: totrans-1805
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseQuantEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder")[[`EmbeddingBagCollection`](torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection
    "torchrec.quant.embedding_modules.EmbeddingBagCollection")]
- en: '[PRE538]'
  id: totrans-1806
  prefs: []
  type: TYPE_PRE
  zh: '[PRE538]'
- en: '[PRE539]'
  id: totrans-1807
  prefs: []
  type: TYPE_PRE
  zh: '[PRE539]'
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  id: totrans-1808
  prefs: []
  type: TYPE_NORMAL
  zh: 执行实际分片。它将根据相应的ParameterSharding在请求的位置上分配参数。
- en: Default implementation is data-parallel replication.
  id: totrans-1809
  prefs: []
  type: TYPE_NORMAL
  zh: 默认实现是数据并行复制。
- en: 'Parameters:'
  id: totrans-1810
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*M*) – module to shard.'
  id: totrans-1811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module**（*M*）- 要分片的模块。'
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  id: totrans-1812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params**（[*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan"））- 完全限定的参数名称字典（模块路径+参数名称，用‘.’分隔）到其分片规范。'
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  id: totrans-1813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env**（[*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"））-
    具有进程组的分片环境。'
- en: '**device** (*torch.device*) – compute device.'
  id: totrans-1814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*）- 计算设备。'
- en: 'Returns:'
  id: totrans-1815
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: sharded module implementation.
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
  zh: 分片模块实现。
- en: 'Return type:'
  id: totrans-1817
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
- en: '[PRE540]'
  id: totrans-1819
  prefs: []
  type: TYPE_PRE
  zh: '[PRE540]'
- en: 'Bases: [`BaseQuantEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder")[[`FeatureProcessedEmbeddingBagCollection`](torchrec.quant.html#torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection
    "torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection")]'
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseQuantEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder")[[`FeatureProcessedEmbeddingBagCollection`](torchrec.quant.html#torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection
    "torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection")]
- en: '[PRE541]'
  id: totrans-1821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE541]'
- en: List of supported compute kernels for a given sharding type and compute device.
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
  zh: 给定分片类型和计算设备的支持计算内核列表。
- en: '[PRE542]'
  id: totrans-1823
  prefs: []
  type: TYPE_PRE
  zh: '[PRE542]'
- en: '[PRE543]'
  id: totrans-1824
  prefs: []
  type: TYPE_PRE
  zh: '[PRE543]'
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  id: totrans-1825
  prefs: []
  type: TYPE_NORMAL
  zh: 执行实际分片。它将根据相应的ParameterSharding在请求的位置上分配参数。
- en: Default implementation is data-parallel replication.
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
  zh: 默认实现是数据并行复制。
- en: 'Parameters:'
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*M*) – module to shard.'
  id: totrans-1828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module**（*M*）- 要分片的模块。'
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  id: totrans-1829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params**（[*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan"））- 完全限定的参数名称字典（模块路径+参数名称，用‘.’分隔）到其分片规范。'
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  id: totrans-1830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env**（[*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"））-
    具有进程组的分片环境。'
- en: '**device** (*torch.device*) – compute device.'
  id: totrans-1831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*）- 计算设备。'
- en: 'Returns:'
  id: totrans-1832
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: sharded module implementation.
  id: totrans-1833
  prefs: []
  type: TYPE_NORMAL
  zh: 分片模块实现。
- en: 'Return type:'
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  id: totrans-1835
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
- en: '[PRE544]'
  id: totrans-1836
  prefs: []
  type: TYPE_PRE
  zh: '[PRE544]'
- en: List of supported sharding types. See ShardingType for well-known examples.
  id: totrans-1837
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的分片类型列表。请参见ShardingType以获取知名示例。
- en: '[PRE545]'
  id: totrans-1838
  prefs: []
  type: TYPE_PRE
  zh: '[PRE545]'
- en: 'Bases: `ShardedQuantEmbeddingModuleState`[[`ListOfKJTList`](#torchrec.distributed.embedding_types.ListOfKJTList
    "torchrec.distributed.embedding_types.ListOfKJTList"), `List`[`List`[`Tensor`]],
    [`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor"), [`NullShardedModuleContext`](#torchrec.distributed.types.NullShardedModuleContext
    "torchrec.distributed.types.NullShardedModuleContext")]'
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ShardedQuantEmbeddingModuleState`[[`ListOfKJTList`](#torchrec.distributed.embedding_types.ListOfKJTList
    "torchrec.distributed.embedding_types.ListOfKJTList"), `List`[`List`[`Tensor`]],
    [`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor"), [`NullShardedModuleContext`](#torchrec.distributed.types.NullShardedModuleContext
    "torchrec.distributed.types.NullShardedModuleContext")]
- en: Sharded implementation of EmbeddingBagCollection. This is part of the public
    API to allow for manual data dist pipelining.
  id: totrans-1840
  prefs: []
  type: TYPE_NORMAL
  zh: EmbeddingBagCollection 的 Sharded 实现。这是公共 API 的一部分，允许手动数据分布流水线化。
- en: '[PRE546]'
  id: totrans-1841
  prefs: []
  type: TYPE_PRE
  zh: '[PRE546]'
- en: '[PRE547]'
  id: totrans-1842
  prefs: []
  type: TYPE_PRE
  zh: '[PRE547]'
- en: In case of multiple output distributions it makes sense to override this method
    and initiate the output distibution as soon as the corresponding compute completes.
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在多个输出分布的情况下，重写此方法并在相应的计算完成后立即启动输出分布是有意义的。
- en: '[PRE548]'
  id: totrans-1844
  prefs: []
  type: TYPE_PRE
  zh: '[PRE548]'
- en: '[PRE549]'
  id: totrans-1845
  prefs: []
  type: TYPE_PRE
  zh: '[PRE549]'
- en: '[PRE550]'
  id: totrans-1846
  prefs: []
  type: TYPE_PRE
  zh: '[PRE550]'
- en: '[PRE551]'
  id: totrans-1847
  prefs: []
  type: TYPE_PRE
  zh: '[PRE551]'
- en: Executes the input dist, compute, and output dist steps.
  id: totrans-1848
  prefs: []
  type: TYPE_NORMAL
  zh: 执行输入 dist、compute 和输出 dist 步骤。
- en: 'Parameters:'
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '***input** – input.'
  id: totrans-1850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***input** – 输入。'
- en: '****kwargs** – keyword arguments.'
  id: totrans-1851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '****kwargs** – 关键字参数。'
- en: 'Returns:'
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of output from output dist.
  id: totrans-1853
  prefs: []
  type: TYPE_NORMAL
  zh: 来自输出 dist 的可等待对象。
- en: 'Return type:'
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[LazyAwaitable](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[Out]'
  id: totrans-1855
  prefs: []
  type: TYPE_NORMAL
  zh: '[LazyAwaitable](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[Out]'
- en: '[PRE552]'
  id: totrans-1856
  prefs: []
  type: TYPE_PRE
  zh: '[PRE552]'
- en: '[PRE553]'
  id: totrans-1857
  prefs: []
  type: TYPE_PRE
  zh: '[PRE553]'
- en: '[PRE554]'
  id: totrans-1858
  prefs: []
  type: TYPE_PRE
  zh: '[PRE554]'
- en: '[PRE555]'
  id: totrans-1859
  prefs: []
  type: TYPE_PRE
  zh: '[PRE555]'
- en: '[PRE556]'
  id: totrans-1860
  prefs: []
  type: TYPE_PRE
  zh: '[PRE556]'
- en: '[PRE557]'
  id: totrans-1861
  prefs: []
  type: TYPE_PRE
  zh: '[PRE557]'
- en: '[PRE558]'
  id: totrans-1862
  prefs: []
  type: TYPE_PRE
  zh: '[PRE558]'
- en: 'Bases: [`ShardedQuantEmbeddingBagCollection`](#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection
    "torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection")'
  id: totrans-1863
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ShardedQuantEmbeddingBagCollection`](#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection
    "torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection")
- en: '[PRE559]'
  id: totrans-1864
  prefs: []
  type: TYPE_PRE
  zh: '[PRE559]'
- en: '[PRE560]'
  id: totrans-1865
  prefs: []
  type: TYPE_PRE
  zh: '[PRE560]'
- en: '[PRE561]'
  id: totrans-1866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE561]'
- en: '[PRE562]'
  id: totrans-1867
  prefs: []
  type: TYPE_PRE
  zh: '[PRE562]'
- en: '[PRE563]'
  id: totrans-1868
  prefs: []
  type: TYPE_PRE
  zh: '[PRE563]'
- en: '[PRE564]'
  id: totrans-1869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE564]'
- en: '[PRE565]  ## torchrec.distributed.train_pipeline[](#module-torchrec.distributed.train_pipeline
    "Permalink to this heading")'
  id: totrans-1870
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE565]  ## torchrec.distributed.train_pipeline[](#module-torchrec.distributed.train_pipeline
    "Permalink to this heading")'
- en: 'NOTE: Due to an internal packaging issue, train_pipeline.py must be compatible
    with older versions of TorchRec. Importing new modules from other files may break
    model publishing flows.'
  id: totrans-1871
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于内部打包问题，train_pipeline.py 必须与较旧版本的 TorchRec 兼容。从其他文件导入新模块可能会破坏模型发布流程。
- en: '[PRE566]'
  id: totrans-1872
  prefs: []
  type: TYPE_PRE
  zh: '[PRE566]'
- en: 'Bases: `object`'
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: Representation of args from a node.
  id: totrans-1874
  prefs: []
  type: TYPE_NORMAL
  zh: 来自节点的参数表示。
- en: '[PRE567]'
  id: totrans-1875
  prefs: []
  type: TYPE_PRE
  zh: '[PRE567]'
- en: attributes of input batch, e.g. batch.attr1.attr2 will produce [“attr1”, “attr2”].
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
  zh: 输入批次的属性，例如 batch.attr1.attr2 将产生 [“attr1”, “attr2”]。
- en: 'Type:'
  id: totrans-1877
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[str]
  id: totrans-1878
  prefs: []
  type: TYPE_NORMAL
  zh: List[str]
- en: '[PRE568]'
  id: totrans-1879
  prefs: []
  type: TYPE_PRE
  zh: '[PRE568]'
- en: batch[attr1].attr2 will produce [True, False].
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
  zh: batch[attr1].attr2 将产生 [True, False]。
- en: 'Type:'
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[bool]
  id: totrans-1882
  prefs: []
  type: TYPE_NORMAL
  zh: List[bool]
- en: '[PRE569]'
  id: totrans-1883
  prefs: []
  type: TYPE_PRE
  zh: '[PRE569]'
- en: name for kwarg of pipelined forward() call or None for a positional arg.
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
  zh: 用于流水线化 forward() 调用的关键字参数的名称，或者对于位置参数为 None。
- en: 'Type:'
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Optional[str]
  id: totrans-1886
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的[str]
- en: '[PRE570]'
  id: totrans-1887
  prefs: []
  type: TYPE_PRE
  zh: '[PRE570]'
- en: '[PRE571]'
  id: totrans-1888
  prefs: []
  type: TYPE_PRE
  zh: '[PRE571]'
- en: '[PRE572]'
  id: totrans-1889
  prefs: []
  type: TYPE_PRE
  zh: '[PRE572]'
- en: '[PRE573]'
  id: totrans-1890
  prefs: []
  type: TYPE_PRE
  zh: '[PRE573]'
- en: 'Bases: `object`'
  id: totrans-1891
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE574]'
  id: totrans-1892
  prefs: []
  type: TYPE_PRE
  zh: '[PRE574]'
- en: '[PRE575]'
  id: totrans-1893
  prefs: []
  type: TYPE_PRE
  zh: '[PRE575]'
- en: '[PRE576]'
  id: totrans-1894
  prefs: []
  type: TYPE_PRE
  zh: '[PRE576]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[[`KJTListAwaitable`](#torchrec.distributed.embedding_sharding.KJTListAwaitable
    "torchrec.distributed.embedding_sharding.KJTListAwaitable")]]'
  id: totrans-1895
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[[`KJTListAwaitable`](#torchrec.distributed.embedding_sharding.KJTListAwaitable
    "torchrec.distributed.embedding_sharding.KJTListAwaitable")]]
- en: '[PRE577]'
  id: totrans-1896
  prefs: []
  type: TYPE_PRE
  zh: '[PRE577]'
- en: 'Bases: `object`'
  id: totrans-1897
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE578]'
  id: totrans-1898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE578]'
- en: 'Bases: `object`'
  id: totrans-1899
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE579]'
  id: totrans-1900
  prefs: []
  type: TYPE_PRE
  zh: '[PRE579]'
- en: '[PRE580]'
  id: totrans-1901
  prefs: []
  type: TYPE_PRE
  zh: '[PRE580]'
- en: '[PRE581]'
  id: totrans-1902
  prefs: []
  type: TYPE_PRE
  zh: '[PRE581]'
- en: '[PRE582]'
  id: totrans-1903
  prefs: []
  type: TYPE_PRE
  zh: '[PRE582]'
- en: '[PRE583]'
  id: totrans-1904
  prefs: []
  type: TYPE_PRE
  zh: '[PRE583]'
- en: '[PRE584]'
  id: totrans-1905
  prefs: []
  type: TYPE_PRE
  zh: '[PRE584]'
- en: '[PRE585]'
  id: totrans-1906
  prefs: []
  type: TYPE_PRE
  zh: '[PRE585]'
- en: '[PRE586]'
  id: totrans-1907
  prefs: []
  type: TYPE_PRE
  zh: '[PRE586]'
- en: '[PRE587]'
  id: totrans-1908
  prefs: []
  type: TYPE_PRE
  zh: '[PRE587]'
- en: '[PRE588]'
  id: totrans-1909
  prefs: []
  type: TYPE_PRE
  zh: '[PRE588]'
- en: 'Bases: [`BaseForward`](#torchrec.distributed.train_pipeline.BaseForward "torchrec.distributed.train_pipeline.BaseForward")'
  id: totrans-1910
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseForward`](#torchrec.distributed.train_pipeline.BaseForward "torchrec.distributed.train_pipeline.BaseForward")
- en: '[PRE589]'
  id: totrans-1911
  prefs: []
  type: TYPE_PRE
  zh: '[PRE589]'
- en: 'Bases: [`BaseForward`](#torchrec.distributed.train_pipeline.BaseForward "torchrec.distributed.train_pipeline.BaseForward")'
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseForward`](#torchrec.distributed.train_pipeline.BaseForward "torchrec.distributed.train_pipeline.BaseForward")
- en: '[PRE590]'
  id: totrans-1913
  prefs: []
  type: TYPE_PRE
  zh: '[PRE590]'
- en: 'Bases: [`TrainPipelineContext`](#torchrec.distributed.train_pipeline.TrainPipelineContext
    "torchrec.distributed.train_pipeline.TrainPipelineContext")'
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`TrainPipelineContext`](#torchrec.distributed.train_pipeline.TrainPipelineContext
    "torchrec.distributed.train_pipeline.TrainPipelineContext")
- en: '[PRE591]'
  id: totrans-1915
  prefs: []
  type: TYPE_PRE
  zh: '[PRE591]'
- en: '[PRE592]'
  id: totrans-1916
  prefs: []
  type: TYPE_PRE
  zh: '[PRE592]'
- en: '[PRE593]'
  id: totrans-1917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE593]'
- en: 'Bases: [`TrainPipelineSparseDist`](#torchrec.distributed.train_pipeline.TrainPipelineSparseDist
    "torchrec.distributed.train_pipeline.TrainPipelineSparseDist")[`In`, `Out`]'
  id: totrans-1918
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`TrainPipelineSparseDist`](#torchrec.distributed.train_pipeline.TrainPipelineSparseDist
    "torchrec.distributed.train_pipeline.TrainPipelineSparseDist")[`In`, `Out`]
- en: This pipeline overlaps device transfer, ShardedModule.input_dist(), and cache
    prefetching with forward and backward. This helps hide the all2all latency while
    preserving the training forward / backward ordering.
  id: totrans-1919
  prefs: []
  type: TYPE_NORMAL
  zh: 该流水线将设备传输、ShardedModule.input_dist() 和缓存预取与前向和后向操作重叠。这有助于隐藏 all2all 延迟，同时保留训练前向/后向顺序。
- en: 'stage 4: forward, backward - uses default CUDA stream stage 3: prefetch - uses
    prefetch CUDA stream stage 2: ShardedModule.input_dist() - uses data_dist CUDA
    stream stage 1: device transfer - uses memcpy CUDA stream'
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段 4：前向、后向 - 使用默认的 CUDA 流 阶段 3：预取 - 使用预取 CUDA 流 阶段 2：ShardedModule.input_dist()
    - 使用数据分布 CUDA 流 阶段 1：数据传输 - 使用 memcpy CUDA 流
- en: ShardedModule.input_dist() is only done for top-level modules in the call graph.
    To be considered a top-level module, a module can only depend on ‘getattr’ calls
    on input.
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
  zh: ShardedModule.input_dist() 仅针对调用图中的顶层模块执行。要被视为顶层模块，一个模块只能依赖于输入上的 'getattr' 调用。
- en: Input model must be symbolically traceable with the exception of ShardedModule
    and DistributedDataParallel modules.
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: 输入模型必须是符号化可跟踪的，除了 ShardedModule 和 DistributedDataParallel 模块。
- en: 'Parameters:'
  id: totrans-1923
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**model** (*torch.nn.Module*) – model to pipeline.'
  id: totrans-1924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model** (*torch.nn.Module*) – 要进行流水线处理的模型。'
- en: '**optimizer** (*torch.optim.Optimizer*) – optimizer to use.'
  id: totrans-1925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optimizer** (*torch.optim.Optimizer*) – 要使用的优化器。'
- en: '**device** (*torch.device*) – device where device transfer, sparse data dist,
    prefetch, and forward/backward pass will happen.'
  id: totrans-1926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 设备，数据传输、稀疏数据分布、预取和前向/后向传递将在该设备上进行。'
- en: '**execute_all_batches** (*bool*) – executes remaining batches in pipeline after
    exhausting dataloader iterator.'
  id: totrans-1927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**execute_all_batches** (*bool*) – 在耗尽数据加载器迭代器后执行流水线中剩余的批次。'
- en: '**apply_jit** (*bool*) – apply torch.jit.script to non-pipelined (unsharded)
    modules.'
  id: totrans-1928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**apply_jit** (*bool*) – 对非流水线化（未分片）模块应用 torch.jit.script。'
- en: '[PRE594]'
  id: totrans-1929
  prefs: []
  type: TYPE_PRE
  zh: '[PRE594]'
- en: '[PRE595]'
  id: totrans-1930
  prefs: []
  type: TYPE_PRE
  zh: '[PRE595]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[`List`[`int`]]]'
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[`List`[`int`]]]
- en: '[PRE596]'
  id: totrans-1932
  prefs: []
  type: TYPE_PRE
  zh: '[PRE596]'
- en: 'Bases: `Tracer`'
  id: totrans-1933
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Tracer`
- en: Disables proxying buffers during tracing. Ideally, proxying buffers would be
    disabled, but some models are currently mutating buffer values, which causes errors
    during tracing. If those models can be rewritten to not do that, we can likely
    remove this line.
  id: totrans-1934
  prefs: []
  type: TYPE_NORMAL
  zh: 在跟踪期间禁用代理缓冲区。理想情况下，代理缓冲区应该被禁用，但是一些模型目前正在改变缓冲区的值，这会在跟踪期间导致错误。如果这些模型可以重写以避免这种情况，我们很可能可以删除这行。
- en: '[PRE597]'
  id: totrans-1935
  prefs: []
  type: TYPE_PRE
  zh: '[PRE597]'
- en: '[PRE598]'
  id: totrans-1936
  prefs: []
  type: TYPE_PRE
  zh: '[PRE598]'
- en: A method to specify whether a given `nn.Module` is a “leaf” module.
  id: totrans-1937
  prefs: []
  type: TYPE_NORMAL
  zh: 指定给定的 `nn.Module` 是否是“叶”模块的方法。
- en: Leaf modules are the atomic units that appear in the IR, referenced by `call_module`
    calls. By default, Modules in the PyTorch standard library namespace (torch.nn)
    are leaf modules. All other modules are traced through and their constituent ops
    are recorded, unless specified otherwise via this parameter.
  id: totrans-1938
  prefs: []
  type: TYPE_NORMAL
  zh: 叶子模块是出现在IR中的原子单元，由`call_module`调用引用。默认情况下，PyTorch标准库命名空间（torch.nn）中的模块是叶子模块。除非通过此参数另有规定，否则将跟踪所有其他模块，并记录其组成操作。
- en: 'Parameters:'
  id: totrans-1939
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**m** (*Module*) – The module being queried about'
  id: totrans-1940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**m** (*Module*) – 正在查询的模块'
- en: '**module_qualified_name** (*str*) – The path to root of this module. For example,
    if you have a module hierarchy where submodule `foo` contains submodule `bar`,
    which contains submodule `baz`, that module will appear with the qualified name
    `foo.bar.baz` here.'
  id: totrans-1941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module_qualified_name** (*str*) – 此模块根目录的路径。例如，如果您有一个模块层次结构，其中子模块`foo`包含子模块`bar`，子模块`bar`包含子模块`baz`，那么该模块将在此处显示为限定名称`foo.bar.baz`。'
- en: Note
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Backwards-compatibility for this API is guaranteed.
  id: totrans-1943
  prefs: []
  type: TYPE_NORMAL
  zh: 此API的向后兼容性已得到保证。
- en: '[PRE599]'
  id: totrans-1944
  prefs: []
  type: TYPE_PRE
  zh: '[PRE599]'
- en: '[PRE600]'
  id: totrans-1945
  prefs: []
  type: TYPE_PRE
  zh: '[PRE600]'
- en: '[PRE601]'
  id: totrans-1946
  prefs: []
  type: TYPE_PRE
  zh: '[PRE601]'
- en: '[PRE602]'
  id: totrans-1947
  prefs: []
  type: TYPE_PRE
  zh: '[PRE602]'
- en: '[PRE603]'
  id: totrans-1948
  prefs: []
  type: TYPE_PRE
  zh: '[PRE603]'
- en: 'Bases: `ABC`, `Generic`[`In`, `Out`]'
  id: totrans-1949
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`，`Generic`[`In`, `Out`]
- en: '[PRE604]'
  id: totrans-1950
  prefs: []
  type: TYPE_PRE
  zh: '[PRE604]'
- en: '[PRE605]'
  id: totrans-1951
  prefs: []
  type: TYPE_PRE
  zh: '[PRE605]'
- en: 'Bases: [`TrainPipeline`](#torchrec.distributed.train_pipeline.TrainPipeline
    "torchrec.distributed.train_pipeline.TrainPipeline")[`In`, `Out`]'
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`TrainPipeline`](#torchrec.distributed.train_pipeline.TrainPipeline "torchrec.distributed.train_pipeline.TrainPipeline")[`In`,
    `Out`]
- en: This class runs training iterations using a pipeline of two stages, each as
    a CUDA stream, namely, the current (default) stream and self._memcpy_stream. For
    each iteration, self._memcpy_stream moves the input from host (CPU) memory to
    GPU memory, and the default stream runs forward, backward, and optimization.
  id: totrans-1953
  prefs: []
  type: TYPE_NORMAL
  zh: 此类使用两个阶段的管道运行训练迭代，每个阶段作为一个CUDA流，即当前（默认）流和self._memcpy_stream。对于每次迭代，self._memcpy_stream将输入从主机（CPU）内存移动到GPU内存，而默认流则运行前向、后向和优化。
- en: '[PRE606]'
  id: totrans-1954
  prefs: []
  type: TYPE_PRE
  zh: '[PRE606]'
- en: '[PRE607]'
  id: totrans-1955
  prefs: []
  type: TYPE_PRE
  zh: '[PRE607]'
- en: 'Bases: `object`'
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: Context information for a TrainPipelineSparseDist instance.
  id: totrans-1957
  prefs: []
  type: TYPE_NORMAL
  zh: TrainPipelineSparseDist实例的上下文信息。
- en: '[PRE608]'
  id: totrans-1958
  prefs: []
  type: TYPE_PRE
  zh: '[PRE608]'
- en: Stores input dist requests in the splits awaitable stage, which occurs after
    starting the input dist.
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入分布请求存储在分片等待阶段，在启动输入分布之后发生。
- en: 'Type:'
  id: totrans-1960
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Dict[str, [Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Any]]
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
  zh: Dict[str, [Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Any]]
- en: '[PRE609]'
  id: totrans-1962
  prefs: []
  type: TYPE_PRE
  zh: '[PRE609]'
- en: Stores input dist requests in the tensors awaitable stage, which occurs after
    calling wait() on the splits awaitable.
  id: totrans-1963
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在张量等待阶段的输入分布请求，发生在对分片等待进行等待()之后。
- en: 'Type:'
  id: totrans-1964
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Dict[str, [Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Any]]
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
  zh: Dict[str, [Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Any]]
- en: '[PRE610]'
  id: totrans-1966
  prefs: []
  type: TYPE_PRE
  zh: '[PRE610]'
- en: Stores module contexts from the input dist for the current batch.
  id: totrans-1967
  prefs: []
  type: TYPE_NORMAL
  zh: 存储来自输入分布的模块上下文，用于当前批次。
- en: 'Type:'
  id: totrans-1968
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Dict[str, Multistreamable]
  id: totrans-1969
  prefs: []
  type: TYPE_NORMAL
  zh: Dict[str, Multistreamable]
- en: '[PRE611]'
  id: totrans-1970
  prefs: []
  type: TYPE_PRE
  zh: '[PRE611]'
- en: Stores module contexts from the input dist for the next batch.
  id: totrans-1971
  prefs: []
  type: TYPE_NORMAL
  zh: 存储来自输入分布的模块上下文，用于下一批次。
- en: 'Type:'
  id: totrans-1972
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Dict[str, Multistreamable]
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
  zh: Dict[str, Multistreamable]
- en: '[PRE612]'
  id: totrans-1974
  prefs: []
  type: TYPE_PRE
  zh: '[PRE612]'
- en: List of fused splits input dist awaitable and the corresponding module names
    of each awaitable.
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
  zh: 融合分片输入分布等待和每个等待的相应模块名称的列表。
- en: 'Type:'
  id: totrans-1976
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: List[Tuple[List[str], [FusedKJTListSplitsAwaitable](#torchrec.distributed.train_pipeline.FusedKJTListSplitsAwaitable
    "torchrec.distributed.train_pipeline.FusedKJTListSplitsAwaitable")]]
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
  zh: List[Tuple[List[str], [FusedKJTListSplitsAwaitable](#torchrec.distributed.train_pipeline.FusedKJTListSplitsAwaitable
    "torchrec.distributed.train_pipeline.FusedKJTListSplitsAwaitable")]]
- en: '[PRE613]'
  id: totrans-1978
  prefs: []
  type: TYPE_PRE
  zh: '[PRE613]'
- en: '[PRE614]'
  id: totrans-1979
  prefs: []
  type: TYPE_PRE
  zh: '[PRE614]'
- en: '[PRE615]'
  id: totrans-1980
  prefs: []
  type: TYPE_PRE
  zh: '[PRE615]'
- en: '[PRE616]'
  id: totrans-1981
  prefs: []
  type: TYPE_PRE
  zh: '[PRE616]'
- en: '[PRE617]'
  id: totrans-1982
  prefs: []
  type: TYPE_PRE
  zh: '[PRE617]'
- en: '[PRE618]'
  id: totrans-1983
  prefs: []
  type: TYPE_PRE
  zh: '[PRE618]'
- en: 'Bases: [`TrainPipeline`](#torchrec.distributed.train_pipeline.TrainPipeline
    "torchrec.distributed.train_pipeline.TrainPipeline")[`In`, `Out`]'
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`TrainPipeline`](#torchrec.distributed.train_pipeline.TrainPipeline "torchrec.distributed.train_pipeline.TrainPipeline")[`In`,
    `Out`]
- en: This pipeline overlaps device transfer, and ShardedModule.input_dist() with
    forward and backward. This helps hide the all2all latency while preserving the
    training forward / backward ordering.
  id: totrans-1985
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道重叠设备传输和ShardedModule.input_dist()与前向和后向。这有助于隐藏all2all延迟，同时保留训练前向/后向顺序。
- en: 'stage 3: forward, backward - uses default CUDA stream stage 2: ShardedModule.input_dist()
    - uses data_dist CUDA stream stage 1: device transfer - uses memcpy CUDA stream'
  id: totrans-1986
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段3：前向、后向 - 使用默认的CUDA流阶段2：ShardedModule.input_dist() - 使用data_dist CUDA流阶段1：设备传输
    - 使用memcpy CUDA流
- en: ShardedModule.input_dist() is only done for top-level modules in the call graph.
    To be considered a top-level module, a module can only depend on ‘getattr’ calls
    on input.
  id: totrans-1987
  prefs: []
  type: TYPE_NORMAL
  zh: ShardedModule.input_dist()仅针对调用图中的顶级模块执行。要被视为顶级模块，模块只能依赖于对输入的‘getattr’调用。
- en: Input model must be symbolically traceable with the exception of ShardedModule
    and DistributedDataParallel modules.
  id: totrans-1988
  prefs: []
  type: TYPE_NORMAL
  zh: 输入模型必须是符号跟踪的，除了ShardedModule和DistributedDataParallel模块。
- en: 'Parameters:'
  id: totrans-1989
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**model** (*torch.nn.Module*) – model to pipeline.'
  id: totrans-1990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model** (*torch.nn.Module*) – 要进行流水线处理的模型。'
- en: '**optimizer** (*torch.optim.Optimizer*) – optimizer to use.'
  id: totrans-1991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optimizer** (*torch.optim.Optimizer*) – 要使用的优化器。'
- en: '**device** (*torch.device*) – device where device transfer, sparse data dist,
    and forward/backward pass will happen.'
  id: totrans-1992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 设备，其中将发生设备传输、稀疏数据分布和前向/后向传递。'
- en: '**execute_all_batches** (*bool*) – executes remaining batches in pipeline after
    exhausting dataloader iterator.'
  id: totrans-1993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**execute_all_batches** (*bool*) – 在耗尽数据加载器迭代器后执行管道中剩余的批次。'
- en: '**apply_jit** (*bool*) – apply torch.jit.script to non-pipelined (unsharded)
    modules.'
  id: totrans-1994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**apply_jit** (*bool*) – 对非流水线（未分片）模块应用torch.jit.script。'
- en: '[PRE619]  ## torchrec.distributed.types[](#module-torchrec.distributed.types
    "Permalink to this heading")'
  id: totrans-1995
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE619]  ## torchrec.distributed.types[](#module-torchrec.distributed.types
    "Permalink to this heading")'
- en: '[PRE620]'
  id: totrans-1996
  prefs: []
  type: TYPE_PRE
  zh: '[PRE620]'
- en: 'Bases: `ABC`, `Generic`[`W`]'
  id: totrans-1997
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`，`Generic`[`W`]
- en: '[PRE621]'
  id: totrans-1998
  prefs: []
  type: TYPE_PRE
  zh: '[PRE621]'
- en: '[PRE622]'
  id: totrans-1999
  prefs: []
  type: TYPE_PRE
  zh: '[PRE622]'
- en: '[PRE623]'
  id: totrans-2000
  prefs: []
  type: TYPE_PRE
  zh: '[PRE623]'
- en: 'Bases: `object`'
  id: totrans-2001
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE624]'
  id: totrans-2002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE624]'
- en: '[PRE625]'
  id: totrans-2003
  prefs: []
  type: TYPE_PRE
  zh: '[PRE625]'
- en: '[PRE626]'
  id: totrans-2004
  prefs: []
  type: TYPE_PRE
  zh: '[PRE626]'
- en: '[PRE627]'
  id: totrans-2005
  prefs: []
  type: TYPE_PRE
  zh: '[PRE627]'
- en: '[PRE628]'
  id: totrans-2006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE628]'
- en: '[PRE629]'
  id: totrans-2007
  prefs: []
  type: TYPE_PRE
  zh: '[PRE629]'
- en: '[PRE630]'
  id: totrans-2008
  prefs: []
  type: TYPE_PRE
  zh: '[PRE630]'
- en: 'Bases: `ABC`'
  id: totrans-2009
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`
- en: '[PRE631]'
  id: totrans-2010
  prefs: []
  type: TYPE_PRE
  zh: '[PRE631]'
- en: Summarized measure of the difficulty to cache a dataset that is independent
    of cache size. A score of 0 means the dataset is very cacheable (e.g. high locality
    between accesses), a score of 1 is very difficult to cache.
  id: totrans-2011
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存数据集的难度的总结性度量，独立于缓存大小。得分为0表示数据集非常适合缓存（例如，访问之间的局部性很高），得分为1表示非常难以缓存。
- en: '[PRE632]'
  id: totrans-2012
  prefs: []
  type: TYPE_PRE
  zh: '[PRE632]'
- en: Number of expected cache lookups per training step.
  id: totrans-2013
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练步骤中预期的缓存查找次数。
- en: This is the expected number of distinct values in a global training batch.
  id: totrans-2014
  prefs: []
  type: TYPE_NORMAL
  zh: 这是全局训练批次中预期的不同值的数量。
- en: '[PRE633]'
  id: totrans-2015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE633]'
- en: Expected cache lookup miss rate for a given cache size.
  id: totrans-2016
  prefs: []
  type: TYPE_NORMAL
  zh: 给定缓存大小的预期缓存查找未命中率。
- en: When clf (cache load factor) is 0, returns 1.0 (100% miss). When clf is 1.0,
    returns 0 (100% hit). For values of clf between these extremes, returns the estimated
    miss rate of the cache, e.g. based on knowledge of the statistical properties
    of the training data set.
  id: totrans-2017
  prefs: []
  type: TYPE_NORMAL
  zh: 当 clf（缓存加载因子）为 0 时，返回 1.0（100% 未命中）。当 clf 为 1.0 时，返回 0（100% 命中）。对于介于这些极端值之间的
    clf 值，返回缓存的估计未命中率，例如基于训练数据集的统计属性的知识。
- en: '[PRE634]'
  id: totrans-2018
  prefs: []
  type: TYPE_PRE
  zh: '[PRE634]'
- en: 'Bases: `Enum`'
  id: totrans-2019
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Enum`
- en: An enumeration.
  id: totrans-2020
  prefs: []
  type: TYPE_NORMAL
  zh: 一个枚举。
- en: '[PRE635]'
  id: totrans-2021
  prefs: []
  type: TYPE_PRE
  zh: '[PRE635]'
- en: '[PRE636]'
  id: totrans-2022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE636]'
- en: '[PRE637]'
  id: totrans-2023
  prefs: []
  type: TYPE_PRE
  zh: '[PRE637]'
- en: '[PRE638]'
  id: totrans-2024
  prefs: []
  type: TYPE_PRE
  zh: '[PRE638]'
- en: 'Bases: `Enum`'
  id: totrans-2025
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Enum`
- en: An enumeration.
  id: totrans-2026
  prefs: []
  type: TYPE_NORMAL
  zh: 一个枚举。
- en: '[PRE639]'
  id: totrans-2027
  prefs: []
  type: TYPE_PRE
  zh: '[PRE639]'
- en: '[PRE640]'
  id: totrans-2028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE640]'
- en: 'Bases: [`ModuleShardingPlan`](#torchrec.distributed.types.ModuleShardingPlan
    "torchrec.distributed.types.ModuleShardingPlan"), `Dict`[`str`, [`ParameterSharding`](#torchrec.distributed.types.ParameterSharding
    "torchrec.distributed.types.ParameterSharding")]'
  id: totrans-2029
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ModuleShardingPlan`](#torchrec.distributed.types.ModuleShardingPlan "torchrec.distributed.types.ModuleShardingPlan")，`Dict`[`str`,
    [`ParameterSharding`](#torchrec.distributed.types.ParameterSharding "torchrec.distributed.types.ParameterSharding")]
- en: Map of ParameterSharding per parameter (usually a table). This describes the
    sharding plan for a torchrec module (e.g. EmbeddingBagCollection)
  id: totrans-2030
  prefs: []
  type: TYPE_NORMAL
  zh: 每个参数（通常是一个表）的 ParameterSharding 映射。这描述了 torchrec 模块的分片计划（例如 EmbeddingBagCollection）
- en: '[PRE641]'
  id: totrans-2031
  prefs: []
  type: TYPE_PRE
  zh: '[PRE641]'
- en: 'Bases: `type`'
  id: totrans-2032
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`type`
- en: '[PRE642]'
  id: totrans-2033
  prefs: []
  type: TYPE_PRE
  zh: '[PRE642]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`W`]'
  id: totrans-2034
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`W`]
- en: The LazyAwaitable type which exposes a wait() API, concrete types can control
    how to initialize and how the wait() behavior should be in order to achieve specific
    async operation.
  id: totrans-2035
  prefs: []
  type: TYPE_NORMAL
  zh: LazyAwaitable 类型公开了一个 wait() API，具体类型可以控制如何初始化以及等待() 行为应该如何以实现特定的异步操作。
- en: 'This base LazyAwaitable type is a “lazy” async type, which means it will delay
    wait() as late as possible, see details in __torch_function__ below. This could
    help the model automatically enable computation and communication overlap, model
    author doesn’t need to manually call wait() if the results is used by a pytorch
    function, or by other python operations (NOTE: need to implement corresponding
    magic methods like __getattr__ below)'
  id: totrans-2036
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本的 LazyAwaitable 类型是一个“懒惰”的异步类型，这意味着它会尽可能延迟 wait()，请参见下面的__torch_function__中的详细信息。这可以帮助模型自动启用计算和通信重叠，模型作者不需要手动调用
    wait()，如果结果被一个 pytorch 函数使用，或者被其他 python 操作使用（注意：需要实现类似__getattr__的相应魔术方法）
- en: 'Some caveats:'
  id: totrans-2037
  prefs: []
  type: TYPE_NORMAL
  zh: 一些注意事项：
- en: This works with Pytorch functions, but not any generic method, if you would
    like to do arbitary python operations, you need to implement the corresponding
    magic methods
  id: totrans-2038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这适用于 Pytorch 函数，但不适用于任何通用方法，如果您想执行任意的 python 操作，您需要实现相应的魔术方法
- en: In the case that one function have two or more arguments are LazyAwaitable,
    the lazy wait mechanism can’t ensure perfect computation/communication overlap
    (i.e. quickly waited the first one but long wait on the second)
  id: totrans-2039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个函数有两个或更多个 LazyAwaitable 参数的情况下，懒惰等待机制无法确保完美的计算/通信重叠（即快速等待第一个，但在第二个上等待时间较长）
- en: '[PRE643]'
  id: totrans-2040
  prefs: []
  type: TYPE_PRE
  zh: '[PRE643]'
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[`W`]'
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[`W`]
- en: '[PRE644]'
  id: totrans-2042
  prefs: []
  type: TYPE_PRE
  zh: '[PRE644]'
- en: 'Bases: `ABC`, `Generic`[`M`]'
  id: totrans-2043
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`，`Generic`[`M`]
- en: ModuleSharder is per each module, which supports sharding, e.g. EmbeddingBagCollection.
  id: totrans-2044
  prefs: []
  type: TYPE_NORMAL
  zh: ModuleSharder 是每个支持分片的模块，例如 EmbeddingBagCollection。
- en: 'Args::'
  id: totrans-2045
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: 'qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]) : Mapping
    of CommOp name to QuantizedCommCodecs'
  id: totrans-2046
  prefs: []
  type: TYPE_NORMAL
  zh: qcomm_codecs_registry（可选[Dict[str, QuantizedCommCodecs]]）：CommOp 名称到 QuantizedCommCodecs
    的映射
- en: '[PRE645]'
  id: totrans-2047
  prefs: []
  type: TYPE_PRE
  zh: '[PRE645]'
- en: List of supported compute kernels for a given sharding type and compute device.
  id: totrans-2048
  prefs: []
  type: TYPE_NORMAL
  zh: 给定分片类型和计算设备的支持的计算内核列表。
- en: '[PRE646]'
  id: totrans-2049
  prefs: []
  type: TYPE_PRE
  zh: '[PRE646]'
- en: '[PRE647]'
  id: totrans-2050
  prefs: []
  type: TYPE_PRE
  zh: '[PRE647]'
- en: '[PRE648]'
  id: totrans-2051
  prefs: []
  type: TYPE_PRE
  zh: '[PRE648]'
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  id: totrans-2052
  prefs: []
  type: TYPE_NORMAL
  zh: 执行实际的分片。它将根据相应的 ParameterSharding 在请求的位置上分配参数。
- en: Default implementation is data-parallel replication.
  id: totrans-2053
  prefs: []
  type: TYPE_NORMAL
  zh: 默认实现是数据并行复制。
- en: 'Parameters:'
  id: totrans-2054
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*M*) – module to shard.'
  id: totrans-2055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module**（*M*）- 要分片的模块。'
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  id: totrans-2056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params**（[*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")）- 完全限定的参数名称（模块路径 + 参数名称，用‘.’分隔）到其分片规范的字典。'
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  id: totrans-2057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env**（[*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv")）-
    具有进程组的分片环境。'
- en: '**device** (*torch.device*) – compute device.'
  id: totrans-2058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*）- 计算设备。'
- en: 'Returns:'
  id: totrans-2059
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: sharded module implementation.
  id: totrans-2060
  prefs: []
  type: TYPE_NORMAL
  zh: 分片模块实现。
- en: 'Return type:'
  id: totrans-2061
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  id: totrans-2062
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
- en: '[PRE649]'
  id: totrans-2063
  prefs: []
  type: TYPE_PRE
  zh: '[PRE649]'
- en: List of parameters that can be sharded.
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
  zh: 可以分片的参数列表。
- en: '[PRE650]'
  id: totrans-2065
  prefs: []
  type: TYPE_PRE
  zh: '[PRE650]'
- en: List of supported sharding types. See ShardingType for well-known examples.
  id: totrans-2066
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的分片类型列表。查看 ShardingType 以获取知名示例。
- en: '[PRE651]'
  id: totrans-2067
  prefs: []
  type: TYPE_PRE
  zh: '[PRE651]'
- en: List of system resources and corresponding usage given a compute device and
    compute kernel.
  id: totrans-2068
  prefs: []
  type: TYPE_NORMAL
  zh: 给定计算设备和计算内核的系统资源列表及相应的使用情况。
- en: '[PRE652]'
  id: totrans-2069
  prefs: []
  type: TYPE_PRE
  zh: '[PRE652]'
- en: 'Bases: `object`'
  id: totrans-2070
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE653]'
  id: totrans-2071
  prefs: []
  type: TYPE_PRE
  zh: '[PRE653]'
- en: 'Bases: `Generic`[`QuantizationContext`]'
  id: totrans-2072
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Generic`[`QuantizationContext`]
- en: Default No-Op implementation of QuantizedCommCodec
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
  zh: QuantizedCommCodec 的默认无操作实现
- en: '[PRE654]'
  id: totrans-2074
  prefs: []
  type: TYPE_PRE
  zh: '[PRE654]'
- en: '[PRE655]'
  id: totrans-2075
  prefs: []
  type: TYPE_PRE
  zh: '[PRE655]'
- en: '[PRE656]'
  id: totrans-2076
  prefs: []
  type: TYPE_PRE
  zh: '[PRE656]'
- en: '[PRE657]'
  id: totrans-2077
  prefs: []
  type: TYPE_PRE
  zh: '[PRE657]'
- en: '[PRE658]'
  id: totrans-2078
  prefs: []
  type: TYPE_PRE
  zh: '[PRE658]'
- en: '[PRE659]'
  id: totrans-2079
  prefs: []
  type: TYPE_PRE
  zh: '[PRE659]'
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`W`]'
  id: totrans-2080
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`W`]
- en: '[PRE660]'
  id: totrans-2081
  prefs: []
  type: TYPE_PRE
  zh: '[PRE660]'
- en: 'Bases: `Multistreamable`'
  id: totrans-2082
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Multistreamable`
- en: '[PRE661]'
  id: totrans-2083
  prefs: []
  type: TYPE_PRE
  zh: '[PRE661]'
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  id: totrans-2084
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
- en: '[PRE662]'
  id: totrans-2085
  prefs: []
  type: TYPE_PRE
  zh: '[PRE662]'
- en: 'Bases: `Multistreamable`'
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Multistreamable`
- en: '[PRE663]'
  id: totrans-2087
  prefs: []
  type: TYPE_PRE
  zh: '[PRE663]'
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
- en: '[PRE664]'
  id: totrans-2089
  prefs: []
  type: TYPE_PRE
  zh: '[PRE664]'
- en: 'Bases: `object`'
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: Describes the sharding of the parameter.
  id: totrans-2091
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 描述参数的分片。
- en: ''
  id: totrans-2092
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'sharding_type (str): how this parameter is sharded. See ShardingType for well-known'
  id: totrans-2093
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: sharding_type（str）：此参数的分片方式。有关众所周知的ShardingType
- en: ''
  id: totrans-2094
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: types.
  id: totrans-2095
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 类型。
- en: ''
  id: totrans-2096
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'compute_kernel (str): compute kernel to be used by this parameter. ranks (Optional[List[int]]):
    rank of each shard. sharding_spec (Optional[ShardingSpec]): list of ShardMetadata
    for each shard. cache_params (Optional[CacheParams]): cache params for embedding
    lookup. enforce_hbm (Optional[bool]): whether to use HBM. stochastic_rounding
    (Optional[bool]): whether to use stochastic rounding. bounds_check_mode (Optional[BoundsCheckMode]):
    bounds check mode.'
  id: totrans-2097
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: compute_kernel（str）：此参数要使用的计算内核。ranks（Optional[List[int]]）：每个分片的等级。sharding_spec（Optional[ShardingSpec]）：每个分片的ShardMetadata列表。cache_params（Optional[CacheParams]）：嵌入查找的缓存参数。enforce_hbm（Optional[bool]）：是否使用HBM。stochastic_rounding（Optional[bool]）：是否使用随机舍入。bounds_check_mode（Optional[BoundsCheckMode]）：边界检查模式。
- en: Note
  id: totrans-2098
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: ShardingType.TABLE_WISE - rank where this embedding is placed ShardingType.COLUMN_WISE
    - rank where the embedding shards are placed, seen as individual tables ShardingType.TABLE_ROW_WISE
    - first rank when this embedding is placed ShardingType.ROW_WISE, ShardingType.DATA_PARALLEL
    - unused
  id: totrans-2099
  prefs: []
  type: TYPE_NORMAL
  zh: ShardingType.TABLE_WISE - 放置此嵌入的等级 ShardingType.COLUMN_WISE - 放置嵌入分片的等级，视为单独的表
    ShardingType.TABLE_ROW_WISE - 放置此嵌入时的第一个等级 ShardingType.ROW_WISE，ShardingType.DATA_PARALLEL
    - 未使用
- en: '[PRE665]'
  id: totrans-2100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE665]'
- en: '[PRE666]'
  id: totrans-2101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE666]'
- en: '[PRE667]'
  id: totrans-2102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE667]'
- en: '[PRE668]'
  id: totrans-2103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE668]'
- en: '[PRE669]'
  id: totrans-2104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE669]'
- en: '[PRE670]'
  id: totrans-2105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE670]'
- en: '[PRE671]'
  id: totrans-2106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE671]'
- en: '[PRE672]'
  id: totrans-2107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE672]'
- en: '[PRE673]'
  id: totrans-2108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE673]'
- en: 'Bases: `Enum`'
  id: totrans-2109
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Enum`
- en: Well-known physical resources, which can be used as constraints by ShardingPlanner.
  id: totrans-2110
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知的物理资源，可用作ShardingPlanner的约束。
- en: '[PRE674]'
  id: totrans-2111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE674]'
- en: '[PRE675]'
  id: totrans-2112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE675]'
- en: '[PRE676]'
  id: totrans-2113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE676]'
- en: 'Bases: `Generic`[`QuantizationContext`]'
  id: totrans-2114
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Generic`[`QuantizationContext`]
- en: Provide an implementation to quantized, or apply mixed precision, to the tensors
    used in collective calls (pooled_all_to_all, reduce_scatter, etc). The dtype is
    the dtype of the tensor called from encode.
  id: totrans-2115
  prefs: []
  type: TYPE_NORMAL
  zh: 为在集体调用（pooled_all_to_all、reduce_scatter等）中使用的张量提供量化或应用混合精度的实现。dtype是从encode调用的张量的dtype。
- en: This makes the assumption that the input tensor has type torch.float32
  id: totrans-2116
  prefs: []
  type: TYPE_NORMAL
  zh: 这假设输入张量的类型为torch.float32
- en: '[PRE677]'
  id: totrans-2117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE677]'
- en: torch.assert_close(input_tensors, output_tensor)
  id: totrans-2118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: torch.assert_close(input_tensors, output_tensor)
- en: '[PRE678]'
  id: totrans-2119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE678]'
- en: Given the length of input tensor, returns the length of tensor after quantization.
    Used by INT8 codecs where the quantized tensor have some additional parameters.
    For other cases, the quantized tensor should have the same length with input.
  id: totrans-2120
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输入张量的长度，返回量化后的张量长度。由INT8编解码器使用，其中量化张量具有一些额外参数。对于其他情况，量化张量应与输入具有相同的长度。
- en: '[PRE679]'
  id: totrans-2121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE679]'
- en: Create a context object that can be used to carry session-based parameters between
    encoder and decoder.
  id: totrans-2122
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个可用于在编码器和解码器之间传递基于会话的参数的上下文对象。
- en: '[PRE680]'
  id: totrans-2123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE680]'
- en: '[PRE681]'
  id: totrans-2124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE681]'
- en: '[PRE682]'
  id: totrans-2125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE682]'
- en: tensor.dtype of the resultant encode(input_tensor)
  id: totrans-2126
  prefs: []
  type: TYPE_NORMAL
  zh: 结果编码（input_tensor）的张量数据类型
- en: '[PRE683]'
  id: totrans-2127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE683]'
- en: 'Bases: `object`'
  id: totrans-2128
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: The quantization codecs to use for the forward and backward pass respectively
    of a comm op (e.g. pooled_all_to_all, reduce_scatter, sequence_all_to_all).
  id: totrans-2129
  prefs: []
  type: TYPE_NORMAL
  zh: 用于comm op（例如pooled_all_to_all、reduce_scatter、sequence_all_to_all）的前向和后向传递的量化编解码器。
- en: '[PRE684]'
  id: totrans-2130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE684]'
- en: '[PRE685]'
  id: totrans-2131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE685]'
- en: '[PRE686]'
  id: totrans-2132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE686]'
- en: 'Bases: `ABC`, `Module`, `Generic`[`CompIn`, `DistOut`, `Out`, `ShrdCtx`], `ModuleNoCopyMixin`'
  id: totrans-2133
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`，`Module`，`Generic`[`CompIn`，`DistOut`，`Out`，`ShrdCtx`]，`ModuleNoCopyMixin`
- en: All model-parallel modules implement this interface. Inputs and outputs are
    data-parallel.
  id: totrans-2134
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型并行模块都实现了此接口。输入和输出是数据并行的。
- en: 'Args::'
  id: totrans-2135
  prefs: []
  type: TYPE_NORMAL
  zh: 'Args::'
- en: 'qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]) : Mapping
    of CommOp name to QuantizedCommCodecs'
  id: totrans-2136
  prefs: []
  type: TYPE_NORMAL
  zh: qcomm_codecs_registry（Optional[Dict[str, QuantizedCommCodecs]]）：CommOp名称到QuantizedCommCodecs的映射
- en: Note
  id: totrans-2137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: ‘input_dist’ / ‘output_dist’ are responsible of transforming inputs / outputs
    from data-parallel to model parallel and vise-versa.
  id: totrans-2138
  prefs: []
  type: TYPE_NORMAL
  zh: ‘input_dist’ / ‘output_dist’负责将输入/输出从数据并行转换为模型并行，反之亦然。
- en: '[PRE687]'
  id: totrans-2139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE687]'
- en: '[PRE688]'
  id: totrans-2140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE688]'
- en: In case of multiple output distributions it makes sense to override this method
    and initiate the output distibution as soon as the corresponding compute completes.
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在多个输出分布的情况下，重写此方法并在相应的计算完成后立即启动输出分布是有意义的。
- en: '[PRE689]'
  id: totrans-2142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE689]'
- en: '[PRE690]'
  id: totrans-2143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE690]'
- en: Executes the input dist, compute, and output dist steps.
  id: totrans-2144
  prefs: []
  type: TYPE_NORMAL
  zh: 执行输入分布、计算和输出分布步骤。
- en: 'Parameters:'
  id: totrans-2145
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '***input** – input.'
  id: totrans-2146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***input** - 输入。'
- en: '****kwargs** – keyword arguments.'
  id: totrans-2147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '****kwargs** - 关键字参数。'
- en: 'Returns:'
  id: totrans-2148
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of output from output dist.
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出分布中获取输出的可等待对象。
- en: 'Return type:'
  id: totrans-2150
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[LazyAwaitable](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[Out]'
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
  zh: '[LazyAwaitable](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[Out]'
- en: '[PRE691]'
  id: totrans-2152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE691]'
- en: '[PRE692]'
  id: totrans-2153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE692]'
- en: '[PRE693]'
  id: totrans-2154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE693]'
- en: '[PRE694]'
  id: totrans-2155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE694]'
- en: '[PRE695]'
  id: totrans-2156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE695]'
- en: '[PRE696]'
  id: totrans-2157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE696]'
- en: 'Bases: `object`'
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: Provides an abstraction over torch.distributed.ProcessGroup, which practically
    enables DistributedModelParallel to be used during inference.
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了一个对torch.distributed.ProcessGroup的抽象，实际上使得DistributedModelParallel在推断期间可用。
- en: '[PRE697]'
  id: totrans-2160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE697]'
- en: Creates a local host-based sharding environment.
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个基于本地主机的分片环境。
- en: Note
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Typically used during single host inference.
  id: totrans-2163
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在单个主机推断期间使用。
- en: '[PRE698]'
  id: totrans-2164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE698]'
- en: Creates ProcessGroup-based sharding environment.
  id: totrans-2165
  prefs: []
  type: TYPE_NORMAL
  zh: 创建基于ProcessGroup的分片环境。
- en: Note
  id: totrans-2166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Typically used during training.
  id: totrans-2167
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在训练期间使用。
- en: '[PRE699]'
  id: totrans-2168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE699]'
- en: 'Bases: `object`'
  id: totrans-2169
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: Representation of sharding plan. This uses the FQN of the larger wrapped model
    (i.e the model that is wrapped using DistributedModelParallel) EmbeddingModuleShardingPlan
    should be used when TorchRec composability is desired.
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
  zh: 分片计划的表示。这使用了较大包装模型的FQN（即使用DistributedModelParallel包装的模型）。当需要TorchRec的可组合性时，应使用EmbeddingModuleShardingPlan。
- en: '[PRE700]'
  id: totrans-2171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE700]'
- en: dict keyed by module path of dict of parameter sharding specs keyed by parameter
    name.
  id: totrans-2172
  prefs: []
  type: TYPE_NORMAL
  zh: 按模块路径为键的字典，按参数名称为键的参数分片规范字典。
- en: 'Type:'
  id: totrans-2173
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Dict[str, [EmbeddingModuleShardingPlan](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")]
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
  zh: Dict[str，[EmbeddingModuleShardingPlan](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")]
- en: '[PRE701]'
  id: totrans-2175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE701]'
- en: 'Parameters:'
  id: totrans-2176
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module_path** (*str*) –'
  id: totrans-2177
  prefs: []
  type: TYPE_NORMAL
  zh: '**module_path**（*str*）- '
- en: 'Returns:'
  id: totrans-2178
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: dict of parameter sharding specs keyed by parameter name. None if sharding specs
    do not exist for given module_path.
  id: totrans-2179
  prefs: []
  type: TYPE_NORMAL
  zh: 按参数名称为键的参数分片规范字典。如果给定的module_path不存在分片规范，则为None。
- en: 'Return type:'
  id: totrans-2180
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: Optional[[ModuleShardingPlan](#torchrec.distributed.types.ModuleShardingPlan
    "torchrec.distributed.types.ModuleShardingPlan")]
  id: totrans-2181
  prefs: []
  type: TYPE_NORMAL
  zh: Optional[[ModuleShardingPlan](#torchrec.distributed.types.ModuleShardingPlan
    "torchrec.distributed.types.ModuleShardingPlan")]
- en: '[PRE702]'
  id: totrans-2182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE702]'
- en: '[PRE703]'
  id: totrans-2183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE703]'
- en: 'Bases: `ABC`'
  id: totrans-2184
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`
- en: Plans sharding. This plan can be saved and re-used to ensure sharding stability.
  id: totrans-2185
  prefs: []
  type: TYPE_NORMAL
  zh: 计划分片。此计划可以保存并重复使用以确保分片稳定。
- en: '[PRE704]'
  id: totrans-2186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE704]'
- en: Calls self.plan(…) on rank 0 and broadcasts.
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
  zh: 在rank 0上调用self.plan(…)并广播。
- en: 'Parameters:'
  id: totrans-2188
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*nn.Module*) – module that sharding is planned for.'
  id: totrans-2189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module** (*nn.Module*) – 计划进行分片的模块。'
- en: '**sharders** (*List**[*[*ModuleSharder*](#torchrec.distributed.types.ModuleSharder
    "torchrec.distributed.types.ModuleSharder")*[**nn.Module**]**]*) – provided sharders
    for module.'
  id: totrans-2190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sharders** (*List**[*[*ModuleSharder*](#torchrec.distributed.types.ModuleSharder
    "torchrec.distributed.types.ModuleSharder")*[**nn.Module**]**]*) – 提供的模块分片器。'
- en: 'Returns:'
  id: totrans-2191
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: the computed sharding plan.
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
  zh: 计算的分片计划。
- en: 'Return type:'
  id: totrans-2193
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardingPlan](#torchrec.distributed.types.ShardingPlan "torchrec.distributed.types.ShardingPlan")'
  id: totrans-2194
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardingPlan](#torchrec.distributed.types.ShardingPlan "torchrec.distributed.types.ShardingPlan")'
- en: '[PRE705]'
  id: totrans-2195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE705]'
- en: Plans sharding for provided module and given sharders.
  id: totrans-2196
  prefs: []
  type: TYPE_NORMAL
  zh: 为提供的模块和给定的分片器制定分片计划。
- en: 'Parameters:'
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*nn.Module*) – module that sharding is planned for.'
  id: totrans-2198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module** (*nn.Module*) – 计划进行分片的模块。'
- en: '**sharders** (*List**[*[*ModuleSharder*](#torchrec.distributed.types.ModuleSharder
    "torchrec.distributed.types.ModuleSharder")*[**nn.Module**]**]*) – provided sharders
    for module.'
  id: totrans-2199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sharders** (*List**[*[*ModuleSharder*](#torchrec.distributed.types.ModuleSharder
    "torchrec.distributed.types.ModuleSharder")*[**nn.Module**]**]*) – 提供的模块分片器。'
- en: 'Returns:'
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: the computed sharding plan.
  id: totrans-2201
  prefs: []
  type: TYPE_NORMAL
  zh: 计算的分片计划。
- en: 'Return type:'
  id: totrans-2202
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardingPlan](#torchrec.distributed.types.ShardingPlan "torchrec.distributed.types.ShardingPlan")'
  id: totrans-2203
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardingPlan](#torchrec.distributed.types.ShardingPlan "torchrec.distributed.types.ShardingPlan")'
- en: '[PRE706]'
  id: totrans-2204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE706]'
- en: 'Bases: `Enum`'
  id: totrans-2205
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Enum`
- en: Well-known sharding types, used by inter-module optimizations.
  id: totrans-2206
  prefs: []
  type: TYPE_NORMAL
  zh: 已知的分片类型，用于模块间优化。
- en: '[PRE707]'
  id: totrans-2207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE707]'
- en: '[PRE708]'
  id: totrans-2208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE708]'
- en: '[PRE709]'
  id: totrans-2209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE709]'
- en: '[PRE710]'
  id: totrans-2210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE710]'
- en: '[PRE711]'
  id: totrans-2211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE711]'
- en: '[PRE712]'
  id: totrans-2212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE712]'
- en: '[PRE713]'
  id: totrans-2213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE713]'
- en: '[PRE714]  ## torchrec.distributed.utils[](#module-torchrec.distributed.utils
    "Permalink to this heading")'
  id: totrans-2214
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE714]  ## torchrec.distributed.utils[](#module-torchrec.distributed.utils
    "Permalink to this heading")'
- en: '[PRE715]'
  id: totrans-2215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE715]'
- en: 'Bases: `Module`'
  id: totrans-2216
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Allows copying of module to a target device.
  id: totrans-2217
  prefs: []
  type: TYPE_NORMAL
  zh: 允许将模块复制到目标设备。
- en: 'Example:'
  id: totrans-2218
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE716]'
  id: totrans-2219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE716]'
- en: 'Parameters:'
  id: totrans-2220
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**device** – torch.device to copy to'
  id: totrans-2221
  prefs: []
  type: TYPE_NORMAL
  zh: '**device** – 要复制到的torch.device'
- en: Returns
  id: totrans-2222
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: nn.Module on new device
  id: totrans-2223
  prefs: []
  type: TYPE_NORMAL
  zh: 在新设备上的nn.Module
- en: '[PRE717]'
  id: totrans-2224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE717]'
- en: '[PRE718]'
  id: totrans-2225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE718]'
- en: '[PRE719]'
  id: totrans-2226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE719]'
- en: 'Bases: `object`'
  id: totrans-2227
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE720]'
  id: totrans-2228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE720]'
- en: Extract params from parameter sharding and then add them to fused_params.
  id: totrans-2229
  prefs: []
  type: TYPE_NORMAL
  zh: 从参数分片中提取参数，然后将它们添加到融合参数中。
- en: Params from parameter sharding will override the ones in fused_params if they
    exist already.
  id: totrans-2230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数分片中存在参数，则将覆盖融合参数中的参数。
- en: 'Parameters:'
  id: totrans-2231
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**fused_params** (*Optional**[**Dict**[**str**,* *Any**]**]*) – the existing
    fused_params'
  id: totrans-2232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fused_params** (*Optional**[**Dict**[**str**,* *Any**]**]*) – 现有的融合参数'
- en: '**parameter_sharding** ([*ParameterSharding*](#torchrec.distributed.types.ParameterSharding
    "torchrec.distributed.types.ParameterSharding")) – the parameter sharding to use'
  id: totrans-2233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**parameter_sharding** ([*ParameterSharding*](#torchrec.distributed.types.ParameterSharding
    "torchrec.distributed.types.ParameterSharding")) – 要使用的参数分片'
- en: 'Returns:'
  id: totrans-2234
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: the fused_params dictionary with params from parameter sharding added.
  id: totrans-2235
  prefs: []
  type: TYPE_NORMAL
  zh: 包含从参数分片中添加的参数的融合参数字典。
- en: 'Return type:'
  id: totrans-2236
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Dict[str, Any]]'
  id: totrans-2237
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dict[str, Any]]'
- en: '[PRE721]'
  id: totrans-2238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE721]'
- en: Adds prefix to all keys in state dict, in place.
  id: totrans-2239
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态字典中所有键添加前缀，原地操作。
- en: 'Parameters:'
  id: totrans-2240
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**state_dict** (*Dict**[**str**,* *Any**]*) – input state dict to update.'
  id: totrans-2241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict** (*Dict**[**str**,* *Any**]*) – 要更新的输入状态字典。'
- en: '**prefix** (*str*) – name to filter from state dict keys.'
  id: totrans-2242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prefix** (*str*) – 要从状态字典键中过滤的名称。'
- en: 'Returns:'
  id: totrans-2243
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: None.
  id: totrans-2244
  prefs: []
  type: TYPE_NORMAL
  zh: 无。
- en: '[PRE722]'
  id: totrans-2245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE722]'
- en: Appends provided prefix to provided name.
  id: totrans-2246
  prefs: []
  type: TYPE_NORMAL
  zh: 将提供的前缀附加到提供的名称。
- en: '[PRE723]'
  id: totrans-2247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE723]'
- en: '[PRE724]'
  id: totrans-2248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE724]'
- en: '[PRE725]'
  id: totrans-2249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE725]'
- en: Filters state dict for keys that start with provided name. Strips provided name
    from beginning of key in the resulting state dict.
  id: totrans-2250
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤以提供的名称开头的状态字典键。从结果状态字典的键的开头剥离提供的名称。
- en: 'Parameters:'
  id: totrans-2251
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**state_dict** (*OrderedDict**[**str**,* *torch.Tensor**]*) – input state dict
    to filter.'
  id: totrans-2252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict** (*OrderedDict**[**str**,* *torch.Tensor**]*) – 要过滤的输入状态字典。'
- en: '**name** (*str*) – name to filter from state dict keys.'
  id: totrans-2253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**name** (*str*) – 要从状态字典键中过滤的名称。'
- en: 'Returns:'
  id: totrans-2254
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: filtered state dict.
  id: totrans-2255
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤后的状态字典。
- en: 'Return type:'
  id: totrans-2256
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: OrderedDict[str, torch.Tensor]
  id: totrans-2257
  prefs: []
  type: TYPE_NORMAL
  zh: OrderedDict[str, torch.Tensor]
- en: '[PRE726]'
  id: totrans-2258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE726]'
- en: Retrieves names of top level modules that do not contain any sharded sub-modules.
  id: totrans-2259
  prefs: []
  type: TYPE_NORMAL
  zh: 检索不包含任何分片子模块的顶层模块的名称。
- en: 'Parameters:'
  id: totrans-2260
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**model** (*torch.nn.Module*) – model to retrieve unsharded module names from.'
  id: totrans-2261
  prefs: []
  type: TYPE_NORMAL
  zh: '**model** (*torch.nn.Module*) – 从中检索未分片模块名称的模型。'
- en: 'Returns:'
  id: totrans-2262
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: list of names of modules that don’t have sharded sub-modules.
  id: totrans-2263
  prefs: []
  type: TYPE_NORMAL
  zh: 不包含分片子模块的模块名称列表。
- en: 'Return type:'
  id: totrans-2264
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: List[str]
  id: totrans-2265
  prefs: []
  type: TYPE_NORMAL
  zh: List[str]
- en: '[PRE727]'
  id: totrans-2266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE727]'
- en: '[PRE728]'
  id: totrans-2267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE728]'
- en: Configure the fused_params including cache_precision if the value is not preset.
  id: totrans-2268
  prefs: []
  type: TYPE_NORMAL
  zh: 配置融合参数，包括cache_precision，如果值未设置。
- en: Values set in table_level_fused_params take precidence over the global fused_params
  id: totrans-2269
  prefs: []
  type: TYPE_NORMAL
  zh: 在table_level_fused_params中设置的值优先于全局融合参数
- en: 'Parameters:'
  id: totrans-2270
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**fused_params** (*Optional**[**Dict**[**str**,* *Any**]**]*) – the original
    fused_params'
  id: totrans-2271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fused_params** (*Optional**[**Dict**[**str**,* *Any**]**]*) – 原始融合参数'
- en: '**grouped_fused_params** –'
  id: totrans-2272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**grouped_fused_params** –'
- en: 'Returns:'
  id: totrans-2273
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: a non-null configured fused_params dictionary to be used to configure the embedding
    lookup kernel
  id: totrans-2274
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非空配置的融合参数字典，用于配置嵌入查找内核
- en: 'Return type:'
  id: totrans-2275
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Dict[str, Any]]'
  id: totrans-2276
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dict[str, Any]]'
- en: '[PRE729]'
  id: totrans-2277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE729]'
- en: Convert an optional to its value. Raises an AssertionError if the value is None
  id: totrans-2278
  prefs: []
  type: TYPE_NORMAL
  zh: 将可选项转换为其值。如果值为None，则引发AssertionError
- en: '[PRE730]'
  id: totrans-2279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE730]'
- en: '[PRE731]'
  id: totrans-2280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE731]'
- en: 'Bases: `object`'
  id: totrans-2281
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: Allows copying of DistributedModelParallel module to a target device.
  id: totrans-2282
  prefs: []
  type: TYPE_NORMAL
  zh: 允许将DistributedModelParallel模块复制到目标设备。
- en: 'Example:'
  id: totrans-2283
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE732]'
  id: totrans-2284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE732]'
- en: torchrec.distributed.mc_modules[](#torchrec-distributed-mc-modules "Permalink
    to this heading")
  id: totrans-2285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: torchrec.distributed.mc_modules[](#torchrec-distributed-mc-modules "Permalink
    to this heading")
- en: '[PRE733]'
  id: totrans-2286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE733]'
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  id: totrans-2287
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]
- en: '[PRE734]'
  id: totrans-2288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE734]'
- en: 'Bases: [`EmbeddingCollectionContext`](#torchrec.distributed.embedding.EmbeddingCollectionContext
    "torchrec.distributed.embedding.EmbeddingCollectionContext")'
  id: totrans-2289
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`EmbeddingCollectionContext`](#torchrec.distributed.embedding.EmbeddingCollectionContext
    "torchrec.distributed.embedding.EmbeddingCollectionContext")
- en: '[PRE735]'
  id: totrans-2290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE735]'
- en: '[PRE736]'
  id: totrans-2291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE736]'
- en: '[PRE737]'
  id: totrans-2292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE737]'
- en: '[PRE738]'
  id: totrans-2293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE738]'
- en: 'Bases: [`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[[`ManagedCollisionCollection`](torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionCollection
    "torchrec.modules.mc_modules.ManagedCollisionCollection")]'
  id: totrans-2294
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[[`ManagedCollisionCollection`](torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionCollection
    "torchrec.modules.mc_modules.ManagedCollisionCollection")]
- en: '[PRE739]'
  id: totrans-2295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE739]'
- en: '[PRE740]'
  id: totrans-2296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE740]'
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  id: totrans-2297
  prefs: []
  type: TYPE_NORMAL
  zh: 执行实际的分片。它将根据相应的ParameterSharding在请求的位置上分配参数。
- en: Default implementation is data-parallel replication.
  id: totrans-2298
  prefs: []
  type: TYPE_NORMAL
  zh: 默认实现是数据并行复制。
- en: 'Parameters:'
  id: totrans-2299
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*M*) – module to shard.'
  id: totrans-2300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module** (*M*) – 要分片的模块。'
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  id: totrans-2301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – 完全限定的参数名称字典（模块路径
    + 参数名称，用‘.’分隔）及其分片规范。'
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  id: totrans-2302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – 具有进程组的分片环境。'
- en: '**device** (*torch.device*) – compute device.'
  id: totrans-2303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 计算设备。'
- en: 'Returns:'
  id: totrans-2304
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: sharded module implementation.
  id: totrans-2305
  prefs: []
  type: TYPE_NORMAL
  zh: 分片模块实现。
- en: 'Return type:'
  id: totrans-2306
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  id: totrans-2307
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
- en: '[PRE741]'
  id: totrans-2308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE741]'
- en: List of parameters that can be sharded.
  id: totrans-2309
  prefs: []
  type: TYPE_NORMAL
  zh: 可分片的参数列表。
- en: '[PRE742]'
  id: totrans-2310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE742]'
- en: List of supported sharding types. See ShardingType for well-known examples.
  id: totrans-2311
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的分片类型列表。查看ShardingType以获取常见示例。
- en: '[PRE743]'
  id: totrans-2312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE743]'
- en: 'Bases: [`ShardedModule`](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), [`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), [`ManagedCollisionCollectionContext`](#torchrec.distributed.mc_modules.ManagedCollisionCollectionContext
    "torchrec.distributed.mc_modules.ManagedCollisionCollectionContext")]'
  id: totrans-2313
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`ShardedModule`](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), [`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), [`ManagedCollisionCollectionContext`](#torchrec.distributed.mc_modules.ManagedCollisionCollectionContext
    "torchrec.distributed.mc_modules.ManagedCollisionCollectionContext")]
- en: '[PRE744]'
  id: totrans-2314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE744]'
- en: '[PRE745]'
  id: totrans-2315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE745]'
- en: '[PRE746]'
  id: totrans-2316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE746]'
- en: '[PRE747]'
  id: totrans-2317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE747]'
- en: '[PRE748]'
  id: totrans-2318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE748]'
- en: '[PRE749]'
  id: totrans-2319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE749]'
- en: '[PRE750]'
  id: totrans-2320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE750]'
- en: '[PRE751]'
  id: totrans-2321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE751]'
- en: torchrec.distributed.mc_embeddingbag[](#torchrec-distributed-mc-embeddingbag
    "Permalink to this heading")
  id: totrans-2322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: torchrec.distributed.mc_embeddingbag[](#torchrec-distributed-mc-embeddingbag
    "Permalink to this heading")
- en: '[PRE752]'
  id: totrans-2323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE752]'
- en: 'Bases: [`EmbeddingBagCollectionContext`](#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext
    "torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext")'
  id: totrans-2324
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`EmbeddingBagCollectionContext`](#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext
    "torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext")
- en: '[PRE753]'
  id: totrans-2325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE753]'
- en: '[PRE754]'
  id: totrans-2326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE754]'
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  id: totrans-2327
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
- en: '[PRE755]'
  id: totrans-2328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE755]'
- en: '[PRE756]'
  id: totrans-2329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE756]'
- en: 'Bases: `BaseManagedCollisionEmbeddingCollectionSharder`[[`ManagedCollisionEmbeddingBagCollection`](torchrec.modules.html#torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingBagCollection
    "torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingBagCollection")]'
  id: totrans-2330
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`BaseManagedCollisionEmbeddingCollectionSharder`[[`ManagedCollisionEmbeddingBagCollection`](torchrec.modules.html#torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingBagCollection
    "torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingBagCollection")]
- en: '[PRE757]'
  id: totrans-2331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE757]'
- en: '[PRE758]'
  id: totrans-2332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE758]'
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  id: totrans-2333
  prefs: []
  type: TYPE_NORMAL
  zh: 执行实际的分片。它将根据相应的ParameterSharding在请求的位置上分配参数。
- en: Default implementation is data-parallel replication.
  id: totrans-2334
  prefs: []
  type: TYPE_NORMAL
  zh: 默认实现是数据并行复制。
- en: 'Parameters:'
  id: totrans-2335
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*M*) – module to shard.'
  id: totrans-2336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module** (*M*) – 要分片的模块。'
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  id: totrans-2337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – 完全限定的参数名称字典（模块路径
    + 参数名称，用‘.’分隔）及其分片规范。'
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  id: totrans-2338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – 具有进程组的分片环境。'
- en: '**device** (*torch.device*) – compute device.'
  id: totrans-2339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 计算设备。'
- en: 'Returns:'
  id: totrans-2340
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: sharded module implementation.
  id: totrans-2341
  prefs: []
  type: TYPE_NORMAL
  zh: 分片模块实现。
- en: 'Return type:'
  id: totrans-2342
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  id: totrans-2343
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
- en: '[PRE759]'
  id: totrans-2344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE759]'
- en: 'Bases: `BaseShardedManagedCollisionEmbeddingCollection`[[`ManagedCollisionEmbeddingBagCollectionContext`](#torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext
    "torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext")]'
  id: totrans-2345
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`BaseShardedManagedCollisionEmbeddingCollection`[[`ManagedCollisionEmbeddingBagCollectionContext`](#torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext
    "torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext")]
- en: '[PRE760]'
  id: totrans-2346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE760]'
- en: '[PRE761]'
  id: totrans-2347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE761]'
- en: torchrec.distributed.mc_embedding[](#torchrec-distributed-mc-embedding "Permalink
    to this heading")
  id: totrans-2348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: torchrec.distributed.mc_embedding[](#torchrec-distributed-mc-embedding "Permalink
    to this heading")
- en: '[PRE762]'
  id: totrans-2349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE762]'
- en: 'Bases: [`EmbeddingCollectionContext`](#torchrec.distributed.embedding.EmbeddingCollectionContext
    "torchrec.distributed.embedding.EmbeddingCollectionContext")'
  id: totrans-2350
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`EmbeddingCollectionContext`](#torchrec.distributed.embedding.EmbeddingCollectionContext
    "torchrec.distributed.embedding.EmbeddingCollectionContext")
- en: '[PRE763]'
  id: totrans-2351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE763]'
- en: '[PRE764]'
  id: totrans-2352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE764]'
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  id: totrans-2353
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
- en: '[PRE765]'
  id: totrans-2354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE765]'
- en: '[PRE766]'
  id: totrans-2355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE766]'
- en: 'Bases: `BaseManagedCollisionEmbeddingCollectionSharder`[[`ManagedCollisionEmbeddingCollection`](torchrec.modules.html#torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingCollection
    "torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingCollection")]'
  id: totrans-2356
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`BaseManagedCollisionEmbeddingCollectionSharder`[[`ManagedCollisionEmbeddingCollection`](torchrec.modules.html#torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingCollection
    "torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingCollection")]
- en: '[PRE767]'
  id: totrans-2357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE767]'
- en: '[PRE768]'
  id: totrans-2358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE768]'
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  id: totrans-2359
  prefs: []
  type: TYPE_NORMAL
  zh: 执行实际的分片。它将根据相应的ParameterSharding在请求的位置上分配参数。
- en: Default implementation is data-parallel replication.
  id: totrans-2360
  prefs: []
  type: TYPE_NORMAL
  zh: 默认实现是数据并行复制。
- en: 'Parameters:'
  id: totrans-2361
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**module** (*M*) – module to shard.'
  id: totrans-2362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module** (*M*) – 要分片的模块。'
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  id: totrans-2363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – 完全限定的参数名称字典（模块路径+参数名称，用‘.’分隔）及其分片规范。'
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  id: totrans-2364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – 具有进程组的分片环境。'
- en: '**device** (*torch.device*) – compute device.'
  id: totrans-2365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 计算设备。'
- en: 'Returns:'
  id: totrans-2366
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: sharded module implementation.
  id: totrans-2367
  prefs: []
  type: TYPE_NORMAL
  zh: 分片模块实现。
- en: 'Return type:'
  id: totrans-2368
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  id: totrans-2369
  prefs: []
  type: TYPE_NORMAL
  zh: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
- en: '[PRE769]'
  id: totrans-2370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE769]'
- en: 'Bases: `BaseShardedManagedCollisionEmbeddingCollection`[[`ManagedCollisionEmbeddingCollectionContext`](#torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionContext
    "torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionContext")]'
  id: totrans-2371
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`BaseShardedManagedCollisionEmbeddingCollection`[[`ManagedCollisionEmbeddingCollectionContext`](#torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionContext
    "torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionContext")]
- en: '[PRE770]'
  id: totrans-2372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE770]'
- en: '[PRE771]'
  id: totrans-2373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE771]'
