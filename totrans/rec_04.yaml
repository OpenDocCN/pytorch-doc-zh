- en: torchrec.distributed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/torchrec/torchrec.distributed.html](https://pytorch.org/torchrec/torchrec.distributed.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Torchrec Distributed
  prefs: []
  type: TYPE_NORMAL
- en: Torchrec distributed provides the necessary modules and operations to enable
    model parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'These include:'
  prefs: []
  type: TYPE_NORMAL
- en: model parallelism through DistributedModelParallel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: collective operations for comms, including All-to-All and Reduce-Scatter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: collective operations wrappers for sparse features, KJT, and various embedding
    types.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: sharded implementations of various modules including ShardedEmbeddingBag for
    nn.EmbeddingBag, ShardedEmbeddingBagCollection for EmbeddingBagCollection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: embedding sharders that define sharding for any sharded module implementation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: support for various compute kernels, which are optimized for compute device
    (CPU/GPU) and may include batching together embedding tables and/or optimizer
    fusion.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: pipelined training through TrainPipelineSparseDist that overlaps dataloading
    device transfer (copy to GPU), inter*device communications (input_dist), and computation
    (forward, backward) for increased performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: quantization support for reduced precision training and inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '## torchrec.distributed.collective_utils[](#module-torchrec.distributed.collective_utils
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: This file contains utilities for constructing collective based control flows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Invokes a function on the designated rank and broadcasts the result to all members
    within the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Checks if the current processs is the leader.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*Optional**[**dist.ProcessGroup**]*) – the process’s rank within the
    pg is used to determine if the process is the leader. pg being None implies that
    the process is the only member in the group (e.g. a single process program).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**leader_rank** (*int*) – the definition of leader (defaults to 0). The caller
    can override it with a context-specific definition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]  ## torchrec.distributed.comm[](#module-torchrec.distributed.comm "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Gets the group rank of the worker group. Also available with GROUP_RANK environment
    varible A number between 0 and get_num_groups() (See [https://pytorch.org/docs/stable/elastic/run.html](https://pytorch.org/docs/stable/elastic/run.html))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Gets the local rank of the local processes (see [https://pytorch.org/docs/stable/elastic/run.html](https://pytorch.org/docs/stable/elastic/run.html))
    This is usually the rank of the worker on its node
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Gets the number of worker groups. Usually equivalent to max_nnodes (See [https://pytorch.org/docs/stable/elastic/run.html](https://pytorch.org/docs/stable/elastic/run.html))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Creates sub process groups (intra and cross node)  ## torchrec.distributed.comm_ops[](#module-torchrec.distributed.comm_ops
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The data class that collects the attributes when calling the alltoall_dense
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The data class that collects the attributes when calling the alltoall_pooled
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: batch size in each rank
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: number of features (sum of dimensions) of the embedding in each rank.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: the tensor version of dim_sum_per_rank, this is only used by the fast kernel
    of _recat_pooled_embedding_grad_out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[Tensor]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: cumulative sum of dim_sum_per_rank, this is only used by the fast kernel of
    _recat_pooled_embedding_grad_out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[Tensor]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: quantized communication codecs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[[QuantizedCommCodecs](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The data class that collects the attributes when calling the alltoall_sequence
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: embedding dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: lengths of sparse features after AlltoAll.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: recat tensor for forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[Tensor]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: recat tensor for backward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: input splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: output splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: whether variable batch size is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: bool
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: quantized communication codecs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[[QuantizedCommCodecs](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: lengths of sparse features before AlltoAll.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[Tensor]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The data class that collects the attributes when calling the alltoallv operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: number of features (sum of dimensions) of the embedding in each rank.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: global batch size for each rank.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: local batch size before scattering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '(List[int]): local batch sizes for each embedding table locally (in my current
    rank).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: embedding dimension of each embedding table locally (in my current rank).
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The input split sizes for each rank, this remembers how to split the input when
    doing the all_to_all_single operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The output split sizes for each rank, this remembers how to fill the output
    when doing the all_to_all_single operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.backward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Req.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.backward
    "torchrec.distributed.comm_ops.All2All_Pooled_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Pooled_Wait.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.forward "torchrec.distributed.comm_ops.All2All_Seq_Req.forward")
    returned (None will be passed in for non tensor outputs of the forward function),
    and it should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req.forward"). Each argument is the
    gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.backward
    "torchrec.distributed.comm_ops.All2All_Seq_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.backward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward
    "torchrec.distributed.comm_ops.All2All_Seq_Req_Wait.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Req.forward "torchrec.distributed.comm_ops.All2Allv_Req.forward")
    returned (None will be passed in for non tensor outputs of the forward function),
    and it should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Req.forward
    "torchrec.distributed.comm_ops.All2Allv_Req.forward"). Each argument is the gradient
    w.r.t the given output, and each returned value should be the gradient w.r.t.
    the corresponding input. If an input is not a Tensor or is a Tensor not requiring
    grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2Allv_Req.backward
    "torchrec.distributed.comm_ops.All2Allv_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Req.forward
    "torchrec.distributed.comm_ops.All2Allv_Req.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.forward "torchrec.distributed.comm_ops.All2Allv_Wait.forward")
    returned (None will be passed in for non tensor outputs of the forward function),
    and it should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.forward
    "torchrec.distributed.comm_ops.All2Allv_Wait.forward"). Each argument is the gradient
    w.r.t the given output, and each returned value should be the gradient w.r.t.
    the corresponding input. If an input is not a Tensor or is a Tensor not requiring
    grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.backward
    "torchrec.distributed.comm_ops.All2Allv_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.All2Allv_Wait.forward
    "torchrec.distributed.comm_ops.All2Allv_Wait.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The data class that collects the attributes when calling the all_gatther_base_pooled
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: the size of the input tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Req.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Req.forward"). Each argument is the
    gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.backward
    "torchrec.distributed.comm_ops.AllGatherBase_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Req.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Req.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.backward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.AllGatherBase_Wait.forward
    "torchrec.distributed.comm_ops.AllGatherBase_Wait.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The data class that collects the attributes when calling the reduce_scatter_base_pooled
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: the sizes of the input flatten tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: torch.Size
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward") returned (None
    will be passed in for non tensor outputs of the forward function), and it should
    return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward"). Each argument
    is the gradient w.r.t the given output, and each returned value should be the
    gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor
    not requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.backward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Req.forward") needs gradient
    computed w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward") returned (None
    will be passed in for non tensor outputs of the forward function), and it should
    return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward"). Each argument
    is the gradient w.r.t the given output, and each returned value should be the
    gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor
    not requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.backward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterBase_Wait.forward") needs gradient
    computed w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The data class that collects the attributes when calling the reduce_scatter_pooled
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: the sizes of the input tensors. This remembers the sizes of the input tensors
    when running the backward pass and producing the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[torch.Size]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The data class that collects the attributes when calling the reduce_scatter_v_pooled
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: the sizes of the input tensors. This saves the sizes of the input tensors when
    running the backward pass and producing the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[torch.Size]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: the splits of the input tensors along dim 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '(List[int]): total input size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.backward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Req.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.backward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatterV_Wait.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.forward"). Each argument is the
    gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.backward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Req.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Req.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.forward") returned (None will
    be passed in for non tensor outputs of the forward function), and it should return
    as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.forward"). Each argument is
    the gradient w.r.t the given output, and each returned value should be the gradient
    w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not
    requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.backward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.backward") will have `ctx.needs_input_grad[0]
    = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.ReduceScatter_Wait.forward
    "torchrec.distributed.comm_ops.ReduceScatter_Wait.forward") needs gradient computed
    w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`W`]'
  prefs: []
  type: TYPE_NORMAL
- en: Defines a collective operation request for a process group on a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – The process group the request is for.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The data class that collects the attributes when calling the variable_batch_alltoall_pooled
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: batch size per rank per feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[List[int]]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: local batch size before scattering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[int]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: embedding dimension per rank per feature
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[List[int]]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: quantized communication codecs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[[QuantizedCommCodecs](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: input splits of tensor all to all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[List[int]]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: output splits of tensor all to all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[List[int]]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward") returned
    (None will be passed in for non tensor outputs of the forward function), and it
    should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward"). Each
    argument is the gradient w.r.t the given output, and each returned value should
    be the gradient w.r.t. the corresponding input. If an input is not a Tensor or
    is a Tensor not requiring grads, you can just pass None as a gradient for that
    input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.backward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.backward") will
    have `ctx.needs_input_grad[0] = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Req.forward") needs
    gradient computed w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward") returned
    (None will be passed in for non tensor outputs of the forward function), and it
    should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward"). Each
    argument is the gradient w.r.t the given output, and each returned value should
    be the gradient w.r.t. the corresponding input. If an input is not a Tensor or
    is a Tensor not requiring grads, you can just pass None as a gradient for that
    input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.backward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.backward") will
    have `ctx.needs_input_grad[0] = True` if the first input to [`forward()`](#torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward
    "torchrec.distributed.comm_ops.Variable_Batch_All2All_Pooled_Wait.forward") needs
    gradient computed w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: All-gathers tensors from all processes in a group to form a flattened pooled
    embeddings tensor. Input tensor is of size output_tensor_size / world_size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Tensor*) – tensor to gather.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: all_gather_base_pooled is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoAll operation for a single pooled embedding tensor. Each process
    splits the input pooled embeddings tensor based on the world size, and then scatters
    the split list to all processes in the group. Then concatenates the received tensors
    from all processes in the group and returns a single output tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**a2a_pooled_embs_tensor** (*Tensor*) – input pooled embeddings. Must be pooled
    together before passing into this function. Its shape is B x D_local_sum, where
    D_local_sum is the dimension sum of all the local embedding tables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_rank** (*List**[**int**]*) – batch size in each rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dim_sum_per_rank** (*List**[**int**]*) – number of features (sum of dimensions)
    of the embedding in each rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dim_sum_per_rank_tensor** (*Optional**[**Tensor**]*) – the tensor version
    of dim_sum_per_rank, this is only used by the fast kernel of _recat_pooled_embedding_grad_out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cumsum_dim_sum_per_rank_tensor** (*Optional**[**Tensor**]*) – cumulative
    sum of dim_sum_per_rank, this is only used by the fast kernel of _recat_pooled_embedding_grad_out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: alltoall_pooled is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoAll operation for sequence embeddings. Each process splits the
    input tensor based on the world size, and then scatters the split list to all
    processes in the group. Then concatenates the received tensors from all processes
    in the group and returns a single output tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: AlltoAll operator for Sequence embedding tensors. Does not support mixed dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**a2a_sequence_embs_tensor** (*Tensor*) – input embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**forward_recat_tensor** (*Tensor*) – recat tensor for forward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**backward_recat_tensor** (*Tensor*) – recat tensor for backward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lengths_after_sparse_data_all2all** (*Tensor*) – lengths of sparse features
    after AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_splits** (*List**[**int**]*) – input splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_splits** (*List**[**int**]*) – output splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**variable_batch_size** (*bool*) – whether variable batch size is enabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[List[Tensor]]'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: alltoall_sequence is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: Performs alltoallv operation for a list of input embeddings. Each process scatters
    the list to all processes in the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**inputs** (*List**[**Tensor**]*) – list of tensors to scatter, one per rank.
    The tensors in the list usually have different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**out_split** (*Optional**[**List**[**int**]**]*) – output split sizes (or
    dim_sum_per_rank), if not specified, we will use per_rank_split_lengths to construct
    a output split with the assumption that all the embs have the same dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**per_rank_split_lengths** (*Optional**[**List**[**int**]**]*) – split lengths
    per rank. If not specified, the out_split must be specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    list of tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[List[Tensor]]'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: alltoallv is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: Reduces then scatters a flattened pooled embeddings tensor to all processes
    in a group. Input tensor is of size output_tensor_size * world_size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Tensor*) – flattened tensor to scatter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: reduce_scatter_base_pooled is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce-scatter operation for a pooled embeddings tensor split into
    world size number of chunks. The result of the reduce operation gets scattered
    to all processes in the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**inputs** (*List**[**Tensor**]*) – list of tensors to scatter, one per rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: reduce_scatter_pooled is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce-scatter-v operation for a 1-d pooled embeddings tensor of variable
    batch size per feature split unevenly into world size number of chunks. The result
    of the reduce operation gets scattered to all processes in the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Tensor*) – tensors to scatter, one per rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – batch
    size per rank per feature used to determine input splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**embedding_dims** (*List**[**int**]*) – embedding dimensions per feature used
    to determine input splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – The process group to work
    on. If None, the default process group will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: reduce_scatter_v_per_feature_pooled is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce-scatter-v operation for a pooled embeddings tensor split unevenly
    into world size number of chunks. The result of the reduce operation gets scattered
    to all processes in the group according to input_splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** (*Tensor*) – tensor to scatter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_splits** (*List**[**int**]*) – input splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*Optional**[**dist.ProcessGroup**]*) – the process group to work
    on. If None, the default process group will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: async work handle (Awaitable), which can be wait() later to get the resulting
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: reduce_scatter_v_pooled is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE185]  ## torchrec.distributed.dist_data[](#module-torchrec.distributed.dist_data
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Merges the pooled/sequence embedding tensor on each device into single tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which buffer will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cat_dim** (*int*) – which dimension you would like to concatenate on. For
    pooled embedding it is 1; for sequence embedding it is 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoOne operation on pooled/sequence embeddings tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of embedding tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of the merged embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Merges the pooled embedding tensor on each device into single tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which buffer will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoOne operation with Reduce on pooled embeddings tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of embedding tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of the reduced embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes KeyedJaggedTensor to a ProcessGroup according to splits.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation utilizes AlltoAll collective as part of torch.distributed.
  prefs: []
  type: TYPE_NORMAL
- en: The input provides the necessary tensors and input splits to distribute. The
    first collective call in KJTAllToAllSplitsAwaitable will transmit output splits
    (to allocate correct space for tensors) and batch size per rank. The following
    collective calls in KJTAllToAllTensorsAwaitable will transmit the actual tensors
    asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**splits** (*List**[**int**]*) – List of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stagger** (*int*) – stagger value to apply to recat tensor, see _get_recat
    function for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: Sends input to relevant ProcessGroup ranks.
  prefs: []
  type: TYPE_NORMAL
- en: The first wait will get the output splits for the provided tensors and issue
    tensors AlltoAll. The second wait will get the tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – KeyedJaggedTensor of values
    to distribute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of a KJTAllToAllTensorsAwaitable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[KJTAllToAllTensorsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`KJTAllToAllTensorsAwaitable`](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable for KJT tensors splits AlltoAll.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – input KJT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**splits** (*List**[**int**]*) – list of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tensor_splits** (*Dict**[**str**,* *List**[**int**]**]*) – tensor splits
    provided by input KJT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – provided KJT tensors (ie.
    lengths, values) to redistribute according to splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keys** (*List**[**str**]*) – KJT keys after AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which buffers will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stagger** (*int*) – stagger value to apply to recat tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable for KJT tensors AlltoAll.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – input KJT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**splits** (*List**[**int**]*) – list of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_splits** (*List**[**List**[**int**]**]*) – input splits (number of
    values each rank will get) for each tensor in AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_splits** (*List**[**List**[**int**]**]*) – output splits (number of
    values per rank in output) for each tensor in AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – provided KJT tensors (ie.
    lengths, values) to redistribute according to splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (*List**[**str**]*) – labels for each provided tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keys** (*List**[**str**]*) – KJT keys after AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which buffers will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stagger** (*int*) – stagger value to apply to recat tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stride_per_rank** (*Optional**[**List**[**int**]**]*) – stride per rank in
    the non variable batch per feature case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes KeyedJaggedTensor to all devices.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation utilizes OnetoAll function, which essentially P2P copies the
    feature to the devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**splits** (*List**[**int**]*) – lengths of features to split the KeyJaggedTensor
    features into before copying them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – the device on which the KJTs will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: Splits features first and then sends the slices to the corresponding devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kjt** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – the input features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of KeyedJaggedTensor splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[List[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The module class that wraps the all-gather communication primitive for pooled
    embedding communication.
  prefs: []
  type: TYPE_NORMAL
- en: Provided a local input tensor with a layout of [batch_size, dimension], we want
    to gather input tensors from all ranks into a flattened output tensor.
  prefs: []
  type: TYPE_NORMAL
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    all-gather is only available for NCCL backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – the process group that the all-gather communication
    happens within.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce scatter operation on pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_emb** (*torch.Tensor*) – tensor of shape [num_buckets x batch_size,
    dimension].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Shards batches and collects keys of tensor with a ProcessGroup according to
    dim_sum_per_rank.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation utilizes alltoall_pooled operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dim_sum_per_rank** (*List**[**int**]*) – number of features (sum of dimensions)
    of the embedding in each rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    – callback functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoAll pooled operation on pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_rank** (*Optional**[**List**[**int**]**]*) – batch size per
    rank, to support variable batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable for pooled embeddings after collective operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor_awaitable** ([*Awaitable*](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*)
    – awaitable of concatenated tensors from all the processes in the group after
    collective.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The module class that wraps reduce-scatter communication primitives for pooled
    embedding communication in row-wise and twrw sharding.
  prefs: []
  type: TYPE_NORMAL
- en: For pooled embeddings, we have a local model-parallel output tensor with a layout
    of [num_buckets x batch_size, dimension]. We need to sum over num_buckets dimension
    across batches. We split the tensor along the first dimension into unequal chunks
    (tensor slices of different buckets) according to input_splits and reduce them
    into the output tensor and scatter the results for corresponding ranks.
  prefs: []
  type: TYPE_NORMAL
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    reduce-scatter-v operation is only available for NCCL backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – the process group that the reduce-scatter communication
    happens within.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** – quantized communication codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce scatter operation on pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – tensor of shape [num_buckets * batch_size,
    dimension].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_splits** (*Optional**[**List**[**int**]**]*) – list of splits for local_embs
    dim 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Merges the pooled/sequence embedding tensor on each device into single tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device on which buffer will be allocated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**world_size** (*int*) – number of devices in the topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cat_dim** (*int*) – which dimension you like to concate on. For pooled embedding
    it is 1; for sequence embedding it is 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoOne operation on pooled embeddings tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of pooled embedding tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of the merged pooled embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE220]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE221]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Redistributes sequence embedding to a ProcessGroup according to splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – the process group that the AlltoAll communication
    happens within.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**features_per_rank** (*List**[**int**]*) – list of number of features per
    rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE223]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoAll operation on sequence embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – input embeddings tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lengths** (*torch.Tensor*) – lengths of sparse features after AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_splits** (*List**[**int**]*) – input splits of AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_splits** (*List**[**int**]*) – output splits of AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unbucketize_permute_tensor** (*Optional**[**torch.Tensor**]*) – stores the
    permute order of the KJT bucketize (for row-wise sharding only).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_rank** – (Optional[List[int]]): batch size per rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sparse_features_recat** (*Optional**[**torch.Tensor**]*) – recat tensor used
    for sparse feature input dist. Must be provided if using variable batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of sequence embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[SequenceEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable
    "torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable for sequence embeddings after collective operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor_awaitable** ([*Awaitable*](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*)
    – awaitable of concatenated tensors from all the processes in the group after
    collective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unbucketize_permute_tensor** (*Optional**[**torch.Tensor**]*) – stores the
    permute order of KJT bucketize (for row-wise sharding only).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**embedding_dim** (*int*) – embedding dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[`List`[`int`]]]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable for splits AlltoAll.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – tensor of splits to redistribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE227]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Shards batches and collects keys of tensor with a ProcessGroup according to
    dim_sum_per_rank.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation utilizes variable_batch_alltoall_pooled operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**emb_dim_per_rank_per_feature** (*List**[**List**[**int**]**]*) – embedding
    dimensions per rank per feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    – callback functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE228]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: Performs AlltoAll pooled operation with variable batch size per feature on a
    pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – batch
    size per rank per feature, post a2a. Used to get the input splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_feature_pre_a2a** (*List**[**int**]*) – local batch size before
    scattering, used to get the output splits. Ordered by rank_0 feature, rank_1 feature,
    …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The module class that wraps reduce-scatter communication primitives for pooled
    embedding communication of variable batch in rw and twrw sharding.
  prefs: []
  type: TYPE_NORMAL
- en: For variable batch per feature pooled embeddings, we have a local model-parallel
    output tensor with a 1d layout of the total sum of batch sizes per rank per feature
    multiplied by corresponding embedding dim [batch_size_r0_f0 * emb_dim_f0 + …)].
    We split the tensor into unequal chunks by rank according to batch_size_per_rank_per_feature
    and corresponding embedding_dims and reduce them into the output tensor and scatter
    the results for corresponding ranks.
  prefs: []
  type: TYPE_NORMAL
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    reduce-scatter-v operation is only available for NCCL backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pg** (*dist.ProcessGroup*) – the process group that the reduce-scatter communication
    happens within.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**codecs** – quantized communication codecs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE233]'
  prefs: []
  type: TYPE_PRE
- en: Performs reduce scatter operation on pooled embeddings tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**local_embs** (*torch.Tensor*) – tensor of shape [num_buckets * batch_size,
    dimension].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – batch
    size per rank per feature used to determine input splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**embedding_dims** (*List**[**int**]*) – embedding dimensions per feature used
    to determine input splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PooledEmbeddingsAwaitable](torchrec.distributed.sharding.html#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE234]  ## torchrec.distributed.embedding[](#module-torchrec.distributed.embedding
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[`Dict`[`str`,
    [`JaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor
    "torchrec.sparse.jagged_tensor.JaggedTensor")]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE236]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Multistreamable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE237]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE238]'
  prefs: []
  type: TYPE_PRE
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[[`EmbeddingCollection`](torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingCollection
    "torchrec.modules.embedding_modules.EmbeddingCollection")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE243]'
  prefs: []
  type: TYPE_PRE
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  prefs: []
  type: TYPE_NORMAL
- en: Default implementation is data-parallel replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*M*) – module to shard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – compute device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: sharded module implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE244]'
  prefs: []
  type: TYPE_PRE
- en: List of parameters that can be sharded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE245]'
  prefs: []
  type: TYPE_PRE
- en: List of supported sharding types. See ShardingType for well-known examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ShardedEmbeddingModule`](#torchrec.distributed.embedding_types.ShardedEmbeddingModule
    "torchrec.distributed.embedding_types.ShardedEmbeddingModule")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Dict`[`str`,
    [`JaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor
    "torchrec.sparse.jagged_tensor.JaggedTensor")], [`EmbeddingCollectionContext`](#torchrec.distributed.embedding.EmbeddingCollectionContext
    "torchrec.distributed.embedding.EmbeddingCollectionContext")], [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")'
  prefs: []
  type: TYPE_NORMAL
- en: Sharded implementation of EmbeddingCollection. This is part of the public API
    to allow for manual data dist pipelining.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE248]'
  prefs: []
  type: TYPE_PRE
- en: In case of multiple output distributions it makes sense to override this method
    and initiate the output distibution as soon as the corresponding compute completes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE249]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE250]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE251]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE252]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE253]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE254]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE255]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE256]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE257]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE258]  ## torchrec.distributed.embedding_lookup[](#module-torchrec.distributed.embedding_lookup
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE259]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Function`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE260]'
  prefs: []
  type: TYPE_PRE
- en: Define a formula for differentiating the operation with backward mode automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: This function is to be overridden by all subclasses. (Defining this function
    is equivalent to defining the `vjp` function.)
  prefs: []
  type: TYPE_NORMAL
- en: It must accept a context `ctx` as the first argument, followed by as many outputs
    as the [`forward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward") returned
    (None will be passed in for non tensor outputs of the forward function), and it
    should return as many tensors, as there were inputs to [`forward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward"). Each argument
    is the gradient w.r.t the given output, and each returned value should be the
    gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor
    not requiring grads, you can just pass None as a gradient for that input.
  prefs: []
  type: TYPE_NORMAL
- en: The context can be used to retrieve tensors saved during the forward pass. It
    also has an attribute `ctx.needs_input_grad` as a tuple of booleans representing
    whether each input needs gradient. E.g., [`backward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.backward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.backward") will have
    `ctx.needs_input_grad[0] = True` if the first input to [`forward()`](#torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward
    "torchrec.distributed.embedding_lookup.CommOpGradientScaling.forward") needs gradient
    computed w.r.t. the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE261]'
  prefs: []
  type: TYPE_PRE
- en: Define the forward of the custom autograd Function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is to be overridden by all subclasses. There are two ways to
    define forward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage 1 (Combined forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE262]'
  prefs: []
  type: TYPE_PRE
- en: It must accept a context ctx as the first argument, followed by any number of
    arguments (tensors or other types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See combining-forward-context for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage 2 (Separate forward and ctx):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE263]'
  prefs: []
  type: TYPE_PRE
- en: The forward no longer accepts a ctx argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, you must also override the `torch.autograd.Function.setup_context()`
    staticmethod to handle setting up the `ctx` object. `output` is the output of
    the forward, `inputs` are a Tuple of inputs to the forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See extending-autograd for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context can be used to store arbitrary data that can be then retrieved during
    the backward pass. Tensors should not be stored directly on ctx (though this is
    not currently enforced for backward compatibility). Instead, tensors should be
    saved either with `ctx.save_for_backward()` if they are intended to be used in
    `backward` (equivalently, `vjp`) or `ctx.save_for_forward()` if they are intended
    to be used for in `jvp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE264]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Lookup modules for Sequence embeddings (i.e Embeddings)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE265]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE266]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE267]'
  prefs: []
  type: TYPE_PRE
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict") into
    this module and its descendants.
  prefs: []
  type: TYPE_NORMAL
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict") must
    exactly match the keys returned by this module’s `state_dict()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict") match
    the keys returned by this module’s `state_dict()` function. Default: `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**missing_keys** is a list of str containing the missing keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.state_dict"), [`load_state_dict()`](#torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedEmbeddingsLookup.load_state_dict")
    will raise a `RuntimeError`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE268]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE269]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE270]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE271]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE272]'
  prefs: []
  type: TYPE_PRE
- en: Like named_parameters(), but yields table_name and embedding_weights which are
    wrapped in TableBatchedEmbeddingSlice. For a single table with multiple shards
    (i.e CW) these are combined into one table/weight. Used in composability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE273]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE274]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE275]'
  prefs: []
  type: TYPE_PRE
- en: Return a dictionary containing references to the whole state of the module.
  prefs: []
  type: TYPE_NORMAL
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary containing a whole state of the module
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: dict
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE276]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE277]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE278]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: Lookup modules for Pooled embeddings (i.e EmbeddingBags)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE279]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE280]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE281]'
  prefs: []
  type: TYPE_PRE
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict")
    into this module and its descendants.
  prefs: []
  type: TYPE_NORMAL
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict")
    must exactly match the keys returned by this module’s `state_dict()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict")
    match the keys returned by this module’s `state_dict()` function. Default: `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**missing_keys** is a list of str containing the missing keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.state_dict"),
    [`load_state_dict()`](#torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.GroupedPooledEmbeddingsLookup.load_state_dict")
    will raise a `RuntimeError`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE282]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE283]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE284]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE285]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE286]'
  prefs: []
  type: TYPE_PRE
- en: Like named_parameters(), but yields table_name and embedding_weights which are
    wrapped in TableBatchedEmbeddingSlice. For a single table with multiple shards
    (i.e CW) these are combined into one table/weight. Used in composability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE287]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE288]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE289]'
  prefs: []
  type: TYPE_PRE
- en: Return a dictionary containing references to the whole state of the module.
  prefs: []
  type: TYPE_NORMAL
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary containing a whole state of the module
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: dict
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE290]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE291]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE292]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`InferGroupedLookupMixin`](#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin
    "torchrec.distributed.embedding_lookup.InferGroupedLookupMixin"), [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`]], `TBEToRegisterMixIn`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE293]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE294]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE295]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE296]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE297]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE298]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE299]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE300]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE301]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`InferGroupedLookupMixin`](#torchrec.distributed.embedding_lookup.InferGroupedLookupMixin
    "torchrec.distributed.embedding_lookup.InferGroupedLookupMixin"), [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`]], `TBEToRegisterMixIn`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE302]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE303]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE304]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`], `TBEToRegisterMixIn`'
  prefs: []
  type: TYPE_NORMAL
- en: meta embedding lookup module for inference since inference lookup has references
    for multiple TBE ops over all gpu workers. inference grouped embedding lookup
    module contains meta modules allocated over gpu workers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE305]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE306]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE307]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE308]'
  prefs: []
  type: TYPE_PRE
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict")
    into this module and its descendants.
  prefs: []
  type: TYPE_NORMAL
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict")
    must exactly match the keys returned by this module’s `state_dict()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict")
    match the keys returned by this module’s `state_dict()` function. Default: `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**missing_keys** is a list of str containing the missing keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.state_dict"),
    [`load_state_dict()`](#torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedEmbeddingsLookup.load_state_dict")
    will raise a `RuntimeError`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE309]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE310]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE311]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE312]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE313]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE314]'
  prefs: []
  type: TYPE_PRE
- en: Return a dictionary containing references to the whole state of the module.
  prefs: []
  type: TYPE_NORMAL
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary containing a whole state of the module
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: dict
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE315]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE316]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE317]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingLookup`](#torchrec.distributed.embedding_types.BaseEmbeddingLookup
    "torchrec.distributed.embedding_types.BaseEmbeddingLookup")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`], `TBEToRegisterMixIn`'
  prefs: []
  type: TYPE_NORMAL
- en: meta embedding bag lookup module for inference since inference lookup has references
    for multiple TBE ops over all gpu workers. inference grouped embedding bag lookup
    module contains meta modules allocated over gpu workers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE318]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE319]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE320]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE321]'
  prefs: []
  type: TYPE_PRE
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict")
    into this module and its descendants.
  prefs: []
  type: TYPE_NORMAL
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict")
    must exactly match the keys returned by this module’s `state_dict()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict")
    match the keys returned by this module’s `state_dict()` function. Default: `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**missing_keys** is a list of str containing the missing keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.state_dict"),
    [`load_state_dict()`](#torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict
    "torchrec.distributed.embedding_lookup.MetaInferGroupedPooledEmbeddingsLookup.load_state_dict")
    will raise a `RuntimeError`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE322]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE323]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE324]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE325]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE326]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE327]'
  prefs: []
  type: TYPE_PRE
- en: Return a dictionary containing references to the whole state of the module.
  prefs: []
  type: TYPE_NORMAL
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary containing a whole state of the module
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: dict
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE328]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE329]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE330]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE331]  ## torchrec.distributed.embedding_sharding[](#module-torchrec.distributed.embedding_sharding
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE332]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`, `Module`, `Generic`[`C`, `T`, `W`]'
  prefs: []
  type: TYPE_NORMAL
- en: Converts output of EmbeddingLookup from model-parallel to data-parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE333]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE334]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE335]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`, `Module`, `Generic`[`F`]'
  prefs: []
  type: TYPE_NORMAL
- en: Converts input from data-parallel to model-parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE336]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE337]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE338]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`, `Generic`[`C`, `F`, `T`, `W`], [`FeatureShardingMixIn`](#torchrec.distributed.embedding_types.FeatureShardingMixIn
    "torchrec.distributed.embedding_types.FeatureShardingMixIn")'
  prefs: []
  type: TYPE_NORMAL
- en: Used to implement different sharding types for EmbeddingBagCollection, e.g.
    table_wise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE339]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE340]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE341]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE342]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE343]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE344]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE345]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE346]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE347]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE348]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE349]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE350]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Multistreamable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE351]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE352]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE353]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE354]'
  prefs: []
  type: TYPE_PRE
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE355]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE356]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE357]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE358]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE359]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE360]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE361]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[[`KJTListAwaitable`](#torchrec.distributed.embedding_sharding.KJTListAwaitable
    "torchrec.distributed.embedding_sharding.KJTListAwaitable")]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE362]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable of KJTList.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")*]**]*) – list of Awaitable
    of sparse features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ctx** (*C*) – sharding context to save the batch size info from the KJT for
    the embedding AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE363]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`Awaitable`](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]], `Generic`[`C`]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable of Awaitable of KJTList.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")*]**]**]*) – result from calling
    forward on KJTAllToAll with sparse features to redistribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ctx** (*C*) – sharding context to save the metadata from the input dist to
    for the embedding AlltoAll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE364]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE365]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE366]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE367]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE368]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE369]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE370]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE371]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE372]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE373]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE374]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE375]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`ListOfKJTList`](#torchrec.distributed.embedding_types.ListOfKJTList
    "torchrec.distributed.embedding_types.ListOfKJTList")]'
  prefs: []
  type: TYPE_NORMAL
- en: This module handles the tables-wise sharding input features distribution for
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KJTList*](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")*]**]*) – list of Awaitable of
    KJTList.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE376]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[`Awaitable`](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`ListOfKJTList`](#torchrec.distributed.embedding_types.ListOfKJTList
    "torchrec.distributed.embedding_types.ListOfKJTList")]]'
  prefs: []
  type: TYPE_NORMAL
- en: Awaitable of Awaitable of ListOfKJTList.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**awaitables** (*List**[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*Awaitable*](#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[*[*KJTList*](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")*]**]**]*) – list of Awaitable
    of Awaitable of sparse features list.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE377]'
  prefs: []
  type: TYPE_PRE
- en: Bucketizes the values in KeyedJaggedTensor into num_buckets buckets, lengths
    are readjusted based on the bucketization results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: This function should be used only for row-wise sharding before calling
    KJTAllToAll.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**num_buckets** (*int*) – number of buckets to bucketize the values into.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**block_sizes** – (torch.Tensor): bucket sizes for the keyed dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_permute** (*bool*) – output the memory location mapping from the unbucketized
    values to bucketized values or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bucketize_pos** (*bool*) – output the changed position of the bucketized
    values or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**block_bucketize_row_pos** (*Optional**[**List**[**torch.Tensor**]**]*) –
    The offsets of shard size for each feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: the bucketized KeyedJaggedTensor and the optional permute mapping from the unbucketized
    values to bucketized value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Tuple[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), Optional[torch.Tensor]]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE378]'
  prefs: []
  type: TYPE_PRE
- en: Groups tables by DataType, PoolingType, and EmbeddingComputeKernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tables_per_rank** (*List**[**List**[*[*ShardedEmbeddingTable*](#torchrec.distributed.embedding_types.ShardedEmbeddingTable
    "torchrec.distributed.embedding_types.ShardedEmbeddingTable")*]**]*) – list of
    sharded embedding tables per rank with consistent weightedness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: per rank list of GroupedEmbeddingConfig for features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: 'List[List[[GroupedEmbeddingConfig](#torchrec.distributed.embedding_types.GroupedEmbeddingConfig
    "torchrec.distributed.embedding_types.GroupedEmbeddingConfig")]]  ## torchrec.distributed.embedding_types[](#module-torchrec.distributed.embedding_types
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE379]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`, `Module`, `Generic`[`F`, `T`]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interface implemented by different embedding implementations: e.g. one, which
    relies on nn.EmbeddingBag or table-batched one, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE380]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE381]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE382]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ModuleSharder`](#torchrec.distributed.types.ModuleSharder "torchrec.distributed.types.ModuleSharder")[`M`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE383]'
  prefs: []
  type: TYPE_PRE
- en: List of supported compute kernels for a given sharding type and compute device.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE384]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE385]'
  prefs: []
  type: TYPE_PRE
- en: List of supported sharding types. See ShardingType for well-known examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE386]'
  prefs: []
  type: TYPE_PRE
- en: List of system resources and corresponding usage given a compute device and
    compute kernel
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE387]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract base class for grouped feature processor
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE388]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE389]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE390]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ModuleSharder`](#torchrec.distributed.types.ModuleSharder "torchrec.distributed.types.ModuleSharder")[`M`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE391]'
  prefs: []
  type: TYPE_PRE
- en: List of supported compute kernels for a given sharding type and compute device.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE392]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE393]'
  prefs: []
  type: TYPE_PRE
- en: List of parameters that can be sharded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE394]'
  prefs: []
  type: TYPE_PRE
- en: List of supported sharding types. See ShardingType for well-known examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE395]'
  prefs: []
  type: TYPE_PRE
- en: List of system resources and corresponding usage given a compute device and
    compute kernel
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE396]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE397]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE398]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Enum`'
  prefs: []
  type: TYPE_NORMAL
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE399]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE400]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE401]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE402]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE403]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE404]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE405]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE406]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: Feature Sharding Interface to provide sharding-aware feature metadata.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE407]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE408]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE409]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE410]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE411]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE412]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE413]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE414]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE415]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE416]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE417]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE418]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE419]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE420]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE421]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE422]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE423]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE424]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE425]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE426]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Multistreamable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE427]'
  prefs: []
  type: TYPE_PRE
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE428]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Multistreamable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE429]'
  prefs: []
  type: TYPE_PRE
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE430]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The interface to access a sharded module’s sharding scheme.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE431]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE432]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Enum`'
  prefs: []
  type: TYPE_NORMAL
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE433]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE434]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE435]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE436]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE437]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE438]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE439]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE440]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE441]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE442]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE443]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE444]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE445]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE446]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE447]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE448]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ShardedModule`](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[`CompIn`,
    `DistOut`, `Out`, `ShrdCtx`], [`ModuleShardingMixIn`](#torchrec.distributed.embedding_types.ModuleShardingMixIn
    "torchrec.distributed.embedding_types.ModuleShardingMixIn")'
  prefs: []
  type: TYPE_NORMAL
- en: All model-parallel embedding modules implement this interface. Inputs and outputs
    are data-parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Args::'
  prefs: []
  type: TYPE_NORMAL
- en: 'qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]) : Mapping
    of CommOp name to QuantizedCommCodecs'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE449]'
  prefs: []
  type: TYPE_PRE
- en: Pretty prints representation of the module’s lookup modules, input_dists and
    output_dists
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE450]'
  prefs: []
  type: TYPE_PRE
- en: Prefetch input features for each lookup module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE451]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE452]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ShardedMetaConfig`](#torchrec.distributed.embedding_types.ShardedMetaConfig
    "torchrec.distributed.embedding_types.ShardedMetaConfig"), [`EmbeddingAttributes`](#torchrec.distributed.embedding_types.EmbeddingAttributes
    "torchrec.distributed.embedding_types.EmbeddingAttributes"), [`EmbeddingTableConfig`](torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingTableConfig
    "torchrec.modules.embedding_configs.EmbeddingTableConfig")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE453]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE454]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ShardedConfig`](#torchrec.distributed.embedding_types.ShardedConfig
    "torchrec.distributed.embedding_types.ShardedConfig")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE455]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE456]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE457]  ## torchrec.distributed.embeddingbag[](#module-torchrec.distributed.embeddingbag
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE458]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[`Tensor`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE459]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[[`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE460]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Multistreamable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE461]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE462]'
  prefs: []
  type: TYPE_PRE
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE463]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE464]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE465]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[[`EmbeddingBagCollection`](torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection
    "torchrec.modules.embedding_modules.EmbeddingBagCollection")]'
  prefs: []
  type: TYPE_NORMAL
- en: This implementation uses non-fused EmbeddingBagCollection
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE466]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE467]'
  prefs: []
  type: TYPE_PRE
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  prefs: []
  type: TYPE_NORMAL
- en: Default implementation is data-parallel replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*M*) – module to shard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – compute device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: sharded module implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE468]'
  prefs: []
  type: TYPE_PRE
- en: List of parameters that can be sharded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE469]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[`EmbeddingBag`]'
  prefs: []
  type: TYPE_NORMAL
- en: This implementation uses non-fused nn.EmbeddingBag
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE470]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE471]'
  prefs: []
  type: TYPE_PRE
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  prefs: []
  type: TYPE_NORMAL
- en: Default implementation is data-parallel replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*M*) – module to shard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – compute device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: sharded module implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE472]'
  prefs: []
  type: TYPE_PRE
- en: List of parameters that can be sharded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE473]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ShardedEmbeddingModule`](#torchrec.distributed.embedding_types.ShardedEmbeddingModule
    "torchrec.distributed.embedding_types.ShardedEmbeddingModule")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`, [`NullShardedModuleContext`](#torchrec.distributed.types.NullShardedModuleContext
    "torchrec.distributed.types.NullShardedModuleContext")], [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")'
  prefs: []
  type: TYPE_NORMAL
- en: Sharded implementation of nn.EmbeddingBag. This is part of the public API to
    allow for manual data dist pipelining.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE474]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE475]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE476]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE477]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE478]'
  prefs: []
  type: TYPE_PRE
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict") into this
    module and its descendants.
  prefs: []
  type: TYPE_NORMAL
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict") must exactly
    match the keys returned by this module’s `state_dict()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict") match the
    keys returned by this module’s `state_dict()` function. Default: `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**missing_keys** is a list of str containing the missing keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.state_dict"), [`load_state_dict()`](#torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict
    "torchrec.distributed.embeddingbag.ShardedEmbeddingBag.load_state_dict") will
    raise a `RuntimeError`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE479]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE480]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE481]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over all modules in the network, yielding both the name of
    the module as well as the module itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**memo** – a memo to store the set of modules already added to the result'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefix** – a prefix that will be added to the name of the module'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** – whether to remove the duplicated module instances in
    the result or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, Module)* – Tuple of name and module'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate modules are returned only once. In the following example, `l` will
    be returned only once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE482]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE483]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE484]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE485]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE486]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE487]'
  prefs: []
  type: TYPE_PRE
- en: Return a dictionary containing references to the whole state of the module.
  prefs: []
  type: TYPE_NORMAL
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary containing a whole state of the module
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: dict
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE488]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE489]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE490]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ShardedEmbeddingModule`](#torchrec.distributed.embedding_types.ShardedEmbeddingModule
    "torchrec.distributed.embedding_types.ShardedEmbeddingModule")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], [`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor"), [`EmbeddingBagCollectionContext`](#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext
    "torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext")], [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")'
  prefs: []
  type: TYPE_NORMAL
- en: Sharded implementation of EmbeddingBagCollection. This is part of the public
    API to allow for manual data dist pipelining.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE491]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE492]'
  prefs: []
  type: TYPE_PRE
- en: In case of multiple output distributions it makes sense to override this method
    and initiate the output distibution as soon as the corresponding compute completes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE493]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE494]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE495]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE496]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE497]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE498]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE499]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[[`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE500]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE501]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE502]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE503]'
  prefs: []
  type: TYPE_PRE
- en: 'Placement device and tensor device could be unmatched in some scenarios, e.g.
    passing meta device to DMP and passing cuda to EmbeddingShardingPlanner. We need
    to make device consistent after getting sharding planner.  ## torchrec.distributed.grouped_position_weighted[](#module-torchrec.distributed.grouped_position_weighted
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE504]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseGroupedFeatureProcessor`](#torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor
    "torchrec.distributed.embedding_types.BaseGroupedFeatureProcessor")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE505]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE506]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE507]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE508]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE509]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE510]'
  prefs: []
  type: TYPE_PRE
- en: Return a dictionary containing references to the whole state of the module.
  prefs: []
  type: TYPE_NORMAL
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary containing a whole state of the module
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: dict
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE511]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE512]  ## torchrec.distributed.model_parallel[](#module-torchrec.distributed.model_parallel
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE513]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`'
  prefs: []
  type: TYPE_NORMAL
- en: Interface implemented by custom data parallel wrappers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE514]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE515]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`DataParallelWrapper`](#torchrec.distributed.model_parallel.DataParallelWrapper
    "torchrec.distributed.model_parallel.DataParallelWrapper")'
  prefs: []
  type: TYPE_NORMAL
- en: Default data parallel wrapper, which applies data parallel to all unsharded
    modules.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE516]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE517]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`, [`FusedOptimizerModule`](torchrec.optim.html#torchrec.optim.fused.FusedOptimizerModule
    "torchrec.optim.fused.FusedOptimizerModule")'
  prefs: []
  type: TYPE_NORMAL
- en: Entry point to model parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*nn.Module*) – module to wrap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env** (*Optional**[*[*ShardingEnv*](#torchrec.distributed.types.ShardingEnv
    "torchrec.distributed.types.ShardingEnv")*]*) – sharding environment that has
    the process group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*Optional**[**torch.device**]*) – compute device, defaults to cpu.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**plan** (*Optional**[*[*ShardingPlan*](#torchrec.distributed.types.ShardingPlan
    "torchrec.distributed.types.ShardingPlan")*]*) – plan to use when sharding, defaults
    to EmbeddingShardingPlanner.collective_plan().'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sharders** (*Optional**[**List**[*[*ModuleSharder*](#torchrec.distributed.types.ModuleSharder
    "torchrec.distributed.types.ModuleSharder")*[**nn.Module**]**]**]*) – ModuleSharders
    available to shard with, defaults to EmbeddingBagCollectionSharder().'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**init_data_parallel** (*bool*) – data-parallel modules can be lazy, i.e. they
    delay parameter initialization until the first forward pass. Pass True to delay
    initialization of data parallel modules. Do first forward pass and then call DistributedModelParallel.init_data_parallel().'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**init_parameters** (*bool*) – initialize parameters for modules still on meta
    device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**data_parallel_wrapper** (*Optional**[*[*DataParallelWrapper*](#torchrec.distributed.model_parallel.DataParallelWrapper
    "torchrec.distributed.model_parallel.DataParallelWrapper")*]*) – custom wrapper
    for data parallel modules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE518]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE519]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE520]'
  prefs: []
  type: TYPE_PRE
- en: Recursively copy submodules to new device by calling per-module customized copy
    process, since some modules needs to use the original references (like ShardedModule
    for inference).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE521]'
  prefs: []
  type: TYPE_PRE
- en: Define the computation performed at every call.
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden by all subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE522]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE523]'
  prefs: []
  type: TYPE_PRE
- en: See init_data_parallel c-tor argument for usage. It’s safe to call this method
    multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE524]'
  prefs: []
  type: TYPE_PRE
- en: Copy parameters and buffers from [`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict") into
    this module and its descendants.
  prefs: []
  type: TYPE_NORMAL
- en: If `strict` is `True`, then the keys of [`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict") must
    exactly match the keys returned by this module’s `state_dict()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: If `assign` is `True` the optimizer must be created after the call to [`load_state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**state_dict** (*dict*) – a dict containing parameters and persistent buffers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**strict** (*bool**,* *optional*) – whether to strictly enforce that the keys
    in [`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict") match
    the keys returned by this module’s `state_dict()` function. Default: `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**assign** (*bool**,* *optional*) – whether to assign items in the state dictionary
    to their corresponding keys in the module instead of copying them inplace into
    the module’s current parameters and buffers. When `False`, the properties of the
    tensors in the current module are preserved while when `True`, the properties
    of the Tensors in the state dict are preserved. Default: `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**missing_keys** is a list of str containing the missing keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unexpected_keys** is a list of str containing the unexpected keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NamedTuple` with `missing_keys` and `unexpected_keys` fields'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a parameter or buffer is registered as `None` and its corresponding key exists
    in [`state_dict`](#torchrec.distributed.model_parallel.DistributedModelParallel.state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.state_dict"), [`load_state_dict()`](#torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict
    "torchrec.distributed.model_parallel.DistributedModelParallel.load_state_dict")
    will raise a `RuntimeError`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE525]'
  prefs: []
  type: TYPE_PRE
- en: Property to directly access sharded module, which will not be wrapped in DDP,
    FSDP, DMP, or any other parallelism wrappers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE526]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module buffers, yielding both the name of the buffer
    as well as the buffer itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all buffer names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool**,* *optional*) – if True, then yields buffers of this module
    and all submodules. Otherwise, yields only buffers that are direct members of
    this module. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    buffers in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, torch.Tensor)* – Tuple containing the name and buffer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE527]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE528]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module parameters, yielding both the name of the parameter
    as well as the parameter itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – prefix to prepend to all parameter names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**recurse** (*bool*) – if True, then yields parameters of this module and all
    submodules. Otherwise, yields only parameters that are direct members of this
    module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_duplicate** (*bool**,* *optional*) – whether to remove the duplicated
    parameters in the result. Defaults to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(str, Parameter)* – Tuple containing the name and parameter'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE529]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE530]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE531]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE532]'
  prefs: []
  type: TYPE_PRE
- en: Return a dictionary containing references to the whole state of the module.
  prefs: []
  type: TYPE_NORMAL
- en: Both parameters and persistent buffers (e.g. running averages) are included.
    Keys are corresponding parameter and buffer names. Parameters and buffers set
    to `None` are not included.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The returned object is a shallow copy. It contains references to the module’s
    parameters and buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Currently `state_dict()` also accepts positional arguments for `destination`,
    `prefix` and `keep_vars` in order. However, this is being deprecated and keyword
    arguments will be enforced in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Please avoid the use of argument `destination` as it is not designed for end-users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**destination** (*dict**,* *optional*) – If provided, the state of module will
    be updated into the dict and the same object is returned. Otherwise, an `OrderedDict`
    will be created and returned. Default: `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefix** (*str**,* *optional*) – a prefix added to parameter and buffer names
    to compose the keys in state_dict. Default: `''''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keep_vars** (*bool**,* *optional*) – by default the `Tensor` s returned in
    the state dict are detached from autograd. If it’s set to `True`, detaching will
    not be performed. Default: `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary containing a whole state of the module
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: dict
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE533]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE534]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE535]'
  prefs: []
  type: TYPE_PRE
- en: Unwraps DMP module.
  prefs: []
  type: TYPE_NORMAL
- en: Does not unwrap data parallel wrappers (i.e. DDP/FSDP), so overriding implementations
    by the wrappers can be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE536]'
  prefs: []
  type: TYPE_PRE
- en: 'Unwraps module wrapped by DMP, DDP, or FSDP.  ## torchrec.distributed.quant_embeddingbag[](#module-torchrec.distributed.quant_embeddingbag
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE537]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseQuantEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder")[[`EmbeddingBagCollection`](torchrec.quant.html#torchrec.quant.embedding_modules.EmbeddingBagCollection
    "torchrec.quant.embedding_modules.EmbeddingBagCollection")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE538]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE539]'
  prefs: []
  type: TYPE_PRE
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  prefs: []
  type: TYPE_NORMAL
- en: Default implementation is data-parallel replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*M*) – module to shard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – compute device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: sharded module implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE540]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseQuantEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseQuantEmbeddingSharder")[[`FeatureProcessedEmbeddingBagCollection`](torchrec.quant.html#torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection
    "torchrec.quant.embedding_modules.FeatureProcessedEmbeddingBagCollection")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE541]'
  prefs: []
  type: TYPE_PRE
- en: List of supported compute kernels for a given sharding type and compute device.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE542]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE543]'
  prefs: []
  type: TYPE_PRE
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  prefs: []
  type: TYPE_NORMAL
- en: Default implementation is data-parallel replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*M*) – module to shard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – compute device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: sharded module implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE544]'
  prefs: []
  type: TYPE_PRE
- en: List of supported sharding types. See ShardingType for well-known examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE545]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ShardedQuantEmbeddingModuleState`[[`ListOfKJTList`](#torchrec.distributed.embedding_types.ListOfKJTList
    "torchrec.distributed.embedding_types.ListOfKJTList"), `List`[`List`[`Tensor`]],
    [`KeyedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedTensor
    "torchrec.sparse.jagged_tensor.KeyedTensor"), [`NullShardedModuleContext`](#torchrec.distributed.types.NullShardedModuleContext
    "torchrec.distributed.types.NullShardedModuleContext")]'
  prefs: []
  type: TYPE_NORMAL
- en: Sharded implementation of EmbeddingBagCollection. This is part of the public
    API to allow for manual data dist pipelining.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE546]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE547]'
  prefs: []
  type: TYPE_PRE
- en: In case of multiple output distributions it makes sense to override this method
    and initiate the output distibution as soon as the corresponding compute completes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE548]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE549]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE550]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE551]'
  prefs: []
  type: TYPE_PRE
- en: Executes the input dist, compute, and output dist steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '***input** – input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****kwargs** – keyword arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of output from output dist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[LazyAwaitable](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[Out]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE552]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE553]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE554]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE555]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE556]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE557]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE558]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ShardedQuantEmbeddingBagCollection`](#torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection
    "torchrec.distributed.quant_embeddingbag.ShardedQuantEmbeddingBagCollection")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE559]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE560]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE561]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE562]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE563]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE564]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE565]  ## torchrec.distributed.train_pipeline[](#module-torchrec.distributed.train_pipeline
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: Due to an internal packaging issue, train_pipeline.py must be compatible
    with older versions of TorchRec. Importing new modules from other files may break
    model publishing flows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE566]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: Representation of args from a node.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE567]'
  prefs: []
  type: TYPE_PRE
- en: attributes of input batch, e.g. batch.attr1.attr2 will produce [“attr1”, “attr2”].
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[str]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE568]'
  prefs: []
  type: TYPE_PRE
- en: batch[attr1].attr2 will produce [True, False].
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[bool]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE569]'
  prefs: []
  type: TYPE_PRE
- en: name for kwarg of pipelined forward() call or None for a positional arg.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[str]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE570]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE571]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE572]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE573]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE574]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE575]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE576]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[[`KJTListAwaitable`](#torchrec.distributed.embedding_sharding.KJTListAwaitable
    "torchrec.distributed.embedding_sharding.KJTListAwaitable")]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE577]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE578]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE579]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE580]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE581]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE582]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE583]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE584]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE585]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE586]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE587]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE588]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseForward`](#torchrec.distributed.train_pipeline.BaseForward "torchrec.distributed.train_pipeline.BaseForward")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE589]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseForward`](#torchrec.distributed.train_pipeline.BaseForward "torchrec.distributed.train_pipeline.BaseForward")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE590]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`TrainPipelineContext`](#torchrec.distributed.train_pipeline.TrainPipelineContext
    "torchrec.distributed.train_pipeline.TrainPipelineContext")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE591]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE592]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE593]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`TrainPipelineSparseDist`](#torchrec.distributed.train_pipeline.TrainPipelineSparseDist
    "torchrec.distributed.train_pipeline.TrainPipelineSparseDist")[`In`, `Out`]'
  prefs: []
  type: TYPE_NORMAL
- en: This pipeline overlaps device transfer, ShardedModule.input_dist(), and cache
    prefetching with forward and backward. This helps hide the all2all latency while
    preserving the training forward / backward ordering.
  prefs: []
  type: TYPE_NORMAL
- en: 'stage 4: forward, backward - uses default CUDA stream stage 3: prefetch - uses
    prefetch CUDA stream stage 2: ShardedModule.input_dist() - uses data_dist CUDA
    stream stage 1: device transfer - uses memcpy CUDA stream'
  prefs: []
  type: TYPE_NORMAL
- en: ShardedModule.input_dist() is only done for top-level modules in the call graph.
    To be considered a top-level module, a module can only depend on ‘getattr’ calls
    on input.
  prefs: []
  type: TYPE_NORMAL
- en: Input model must be symbolically traceable with the exception of ShardedModule
    and DistributedDataParallel modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**model** (*torch.nn.Module*) – model to pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optimizer** (*torch.optim.Optimizer*) – optimizer to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device where device transfer, sparse data dist,
    prefetch, and forward/backward pass will happen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**execute_all_batches** (*bool*) – executes remaining batches in pipeline after
    exhausting dataloader iterator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**apply_jit** (*bool*) – apply torch.jit.script to non-pipelined (unsharded)
    modules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE594]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE595]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`List`[`List`[`int`]]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE596]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Tracer`'
  prefs: []
  type: TYPE_NORMAL
- en: Disables proxying buffers during tracing. Ideally, proxying buffers would be
    disabled, but some models are currently mutating buffer values, which causes errors
    during tracing. If those models can be rewritten to not do that, we can likely
    remove this line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE597]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE598]'
  prefs: []
  type: TYPE_PRE
- en: A method to specify whether a given `nn.Module` is a “leaf” module.
  prefs: []
  type: TYPE_NORMAL
- en: Leaf modules are the atomic units that appear in the IR, referenced by `call_module`
    calls. By default, Modules in the PyTorch standard library namespace (torch.nn)
    are leaf modules. All other modules are traced through and their constituent ops
    are recorded, unless specified otherwise via this parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**m** (*Module*) – The module being queried about'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**module_qualified_name** (*str*) – The path to root of this module. For example,
    if you have a module hierarchy where submodule `foo` contains submodule `bar`,
    which contains submodule `baz`, that module will appear with the qualified name
    `foo.bar.baz` here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Backwards-compatibility for this API is guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE599]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE600]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE601]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE602]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE603]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`, `Generic`[`In`, `Out`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE604]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE605]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`TrainPipeline`](#torchrec.distributed.train_pipeline.TrainPipeline
    "torchrec.distributed.train_pipeline.TrainPipeline")[`In`, `Out`]'
  prefs: []
  type: TYPE_NORMAL
- en: This class runs training iterations using a pipeline of two stages, each as
    a CUDA stream, namely, the current (default) stream and self._memcpy_stream. For
    each iteration, self._memcpy_stream moves the input from host (CPU) memory to
    GPU memory, and the default stream runs forward, backward, and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE606]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE607]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: Context information for a TrainPipelineSparseDist instance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE608]'
  prefs: []
  type: TYPE_PRE
- en: Stores input dist requests in the splits awaitable stage, which occurs after
    starting the input dist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Dict[str, [Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Any]]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE609]'
  prefs: []
  type: TYPE_PRE
- en: Stores input dist requests in the tensors awaitable stage, which occurs after
    calling wait() on the splits awaitable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Dict[str, [Awaitable](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[Any]]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE610]'
  prefs: []
  type: TYPE_PRE
- en: Stores module contexts from the input dist for the current batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Dict[str, Multistreamable]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE611]'
  prefs: []
  type: TYPE_PRE
- en: Stores module contexts from the input dist for the next batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Dict[str, Multistreamable]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE612]'
  prefs: []
  type: TYPE_PRE
- en: List of fused splits input dist awaitable and the corresponding module names
    of each awaitable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[Tuple[List[str], [FusedKJTListSplitsAwaitable](#torchrec.distributed.train_pipeline.FusedKJTListSplitsAwaitable
    "torchrec.distributed.train_pipeline.FusedKJTListSplitsAwaitable")]]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE613]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE614]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE615]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE616]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE617]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE618]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`TrainPipeline`](#torchrec.distributed.train_pipeline.TrainPipeline
    "torchrec.distributed.train_pipeline.TrainPipeline")[`In`, `Out`]'
  prefs: []
  type: TYPE_NORMAL
- en: This pipeline overlaps device transfer, and ShardedModule.input_dist() with
    forward and backward. This helps hide the all2all latency while preserving the
    training forward / backward ordering.
  prefs: []
  type: TYPE_NORMAL
- en: 'stage 3: forward, backward - uses default CUDA stream stage 2: ShardedModule.input_dist()
    - uses data_dist CUDA stream stage 1: device transfer - uses memcpy CUDA stream'
  prefs: []
  type: TYPE_NORMAL
- en: ShardedModule.input_dist() is only done for top-level modules in the call graph.
    To be considered a top-level module, a module can only depend on ‘getattr’ calls
    on input.
  prefs: []
  type: TYPE_NORMAL
- en: Input model must be symbolically traceable with the exception of ShardedModule
    and DistributedDataParallel modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**model** (*torch.nn.Module*) – model to pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optimizer** (*torch.optim.Optimizer*) – optimizer to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – device where device transfer, sparse data dist,
    and forward/backward pass will happen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**execute_all_batches** (*bool*) – executes remaining batches in pipeline after
    exhausting dataloader iterator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**apply_jit** (*bool*) – apply torch.jit.script to non-pipelined (unsharded)
    modules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE619]  ## torchrec.distributed.types[](#module-torchrec.distributed.types
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE620]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`, `Generic`[`W`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE621]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE622]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE623]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE624]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE625]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE626]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE627]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE628]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE629]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE630]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE631]'
  prefs: []
  type: TYPE_PRE
- en: Summarized measure of the difficulty to cache a dataset that is independent
    of cache size. A score of 0 means the dataset is very cacheable (e.g. high locality
    between accesses), a score of 1 is very difficult to cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE632]'
  prefs: []
  type: TYPE_PRE
- en: Number of expected cache lookups per training step.
  prefs: []
  type: TYPE_NORMAL
- en: This is the expected number of distinct values in a global training batch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE633]'
  prefs: []
  type: TYPE_PRE
- en: Expected cache lookup miss rate for a given cache size.
  prefs: []
  type: TYPE_NORMAL
- en: When clf (cache load factor) is 0, returns 1.0 (100% miss). When clf is 1.0,
    returns 0 (100% hit). For values of clf between these extremes, returns the estimated
    miss rate of the cache, e.g. based on knowledge of the statistical properties
    of the training data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE634]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Enum`'
  prefs: []
  type: TYPE_NORMAL
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE635]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE636]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE637]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE638]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Enum`'
  prefs: []
  type: TYPE_NORMAL
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE639]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE640]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ModuleShardingPlan`](#torchrec.distributed.types.ModuleShardingPlan
    "torchrec.distributed.types.ModuleShardingPlan"), `Dict`[`str`, [`ParameterSharding`](#torchrec.distributed.types.ParameterSharding
    "torchrec.distributed.types.ParameterSharding")]'
  prefs: []
  type: TYPE_NORMAL
- en: Map of ParameterSharding per parameter (usually a table). This describes the
    sharding plan for a torchrec module (e.g. EmbeddingBagCollection)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE641]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `type`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE642]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`W`]'
  prefs: []
  type: TYPE_NORMAL
- en: The LazyAwaitable type which exposes a wait() API, concrete types can control
    how to initialize and how the wait() behavior should be in order to achieve specific
    async operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This base LazyAwaitable type is a “lazy” async type, which means it will delay
    wait() as late as possible, see details in __torch_function__ below. This could
    help the model automatically enable computation and communication overlap, model
    author doesn’t need to manually call wait() if the results is used by a pytorch
    function, or by other python operations (NOTE: need to implement corresponding
    magic methods like __getattr__ below)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: This works with Pytorch functions, but not any generic method, if you would
    like to do arbitary python operations, you need to implement the corresponding
    magic methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case that one function have two or more arguments are LazyAwaitable,
    the lazy wait mechanism can’t ensure perfect computation/communication overlap
    (i.e. quickly waited the first one but long wait on the second)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE643]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[`W`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE644]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`, `Generic`[`M`]'
  prefs: []
  type: TYPE_NORMAL
- en: ModuleSharder is per each module, which supports sharding, e.g. EmbeddingBagCollection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Args::'
  prefs: []
  type: TYPE_NORMAL
- en: 'qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]) : Mapping
    of CommOp name to QuantizedCommCodecs'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE645]'
  prefs: []
  type: TYPE_PRE
- en: List of supported compute kernels for a given sharding type and compute device.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE646]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE647]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE648]'
  prefs: []
  type: TYPE_PRE
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  prefs: []
  type: TYPE_NORMAL
- en: Default implementation is data-parallel replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*M*) – module to shard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – compute device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: sharded module implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE649]'
  prefs: []
  type: TYPE_PRE
- en: List of parameters that can be sharded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE650]'
  prefs: []
  type: TYPE_PRE
- en: List of supported sharding types. See ShardingType for well-known examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE651]'
  prefs: []
  type: TYPE_PRE
- en: List of system resources and corresponding usage given a compute device and
    compute kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE652]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE653]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Generic`[`QuantizationContext`]'
  prefs: []
  type: TYPE_NORMAL
- en: Default No-Op implementation of QuantizedCommCodec
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE654]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE655]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE656]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE657]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE658]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE659]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`Awaitable`](#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[`W`]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE660]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Multistreamable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE661]'
  prefs: []
  type: TYPE_PRE
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE662]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Multistreamable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE663]'
  prefs: []
  type: TYPE_PRE
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE664]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: Describes the sharding of the parameter.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'sharding_type (str): how this parameter is sharded. See ShardingType for well-known'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: types.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'compute_kernel (str): compute kernel to be used by this parameter. ranks (Optional[List[int]]):
    rank of each shard. sharding_spec (Optional[ShardingSpec]): list of ShardMetadata
    for each shard. cache_params (Optional[CacheParams]): cache params for embedding
    lookup. enforce_hbm (Optional[bool]): whether to use HBM. stochastic_rounding
    (Optional[bool]): whether to use stochastic rounding. bounds_check_mode (Optional[BoundsCheckMode]):
    bounds check mode.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: ShardingType.TABLE_WISE - rank where this embedding is placed ShardingType.COLUMN_WISE
    - rank where the embedding shards are placed, seen as individual tables ShardingType.TABLE_ROW_WISE
    - first rank when this embedding is placed ShardingType.ROW_WISE, ShardingType.DATA_PARALLEL
    - unused
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE665]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE666]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE667]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE668]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE669]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE670]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE671]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE672]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE673]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Enum`'
  prefs: []
  type: TYPE_NORMAL
- en: Well-known physical resources, which can be used as constraints by ShardingPlanner.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE674]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE675]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE676]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Generic`[`QuantizationContext`]'
  prefs: []
  type: TYPE_NORMAL
- en: Provide an implementation to quantized, or apply mixed precision, to the tensors
    used in collective calls (pooled_all_to_all, reduce_scatter, etc). The dtype is
    the dtype of the tensor called from encode.
  prefs: []
  type: TYPE_NORMAL
- en: This makes the assumption that the input tensor has type torch.float32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE677]'
  prefs: []
  type: TYPE_PRE
- en: torch.assert_close(input_tensors, output_tensor)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE678]'
  prefs: []
  type: TYPE_PRE
- en: Given the length of input tensor, returns the length of tensor after quantization.
    Used by INT8 codecs where the quantized tensor have some additional parameters.
    For other cases, the quantized tensor should have the same length with input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE679]'
  prefs: []
  type: TYPE_PRE
- en: Create a context object that can be used to carry session-based parameters between
    encoder and decoder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE680]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE681]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE682]'
  prefs: []
  type: TYPE_PRE
- en: tensor.dtype of the resultant encode(input_tensor)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE683]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: The quantization codecs to use for the forward and backward pass respectively
    of a comm op (e.g. pooled_all_to_all, reduce_scatter, sequence_all_to_all).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE684]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE685]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE686]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`, `Module`, `Generic`[`CompIn`, `DistOut`, `Out`, `ShrdCtx`], `ModuleNoCopyMixin`'
  prefs: []
  type: TYPE_NORMAL
- en: All model-parallel modules implement this interface. Inputs and outputs are
    data-parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Args::'
  prefs: []
  type: TYPE_NORMAL
- en: 'qcomm_codecs_registry (Optional[Dict[str, QuantizedCommCodecs]]) : Mapping
    of CommOp name to QuantizedCommCodecs'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: ‘input_dist’ / ‘output_dist’ are responsible of transforming inputs / outputs
    from data-parallel to model parallel and vise-versa.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE687]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE688]'
  prefs: []
  type: TYPE_PRE
- en: In case of multiple output distributions it makes sense to override this method
    and initiate the output distibution as soon as the corresponding compute completes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE689]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE690]'
  prefs: []
  type: TYPE_PRE
- en: Executes the input dist, compute, and output dist steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '***input** – input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****kwargs** – keyword arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: awaitable of output from output dist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[LazyAwaitable](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[Out]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE691]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE692]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE693]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE694]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE695]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE696]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: Provides an abstraction over torch.distributed.ProcessGroup, which practically
    enables DistributedModelParallel to be used during inference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE697]'
  prefs: []
  type: TYPE_PRE
- en: Creates a local host-based sharding environment.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Typically used during single host inference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE698]'
  prefs: []
  type: TYPE_PRE
- en: Creates ProcessGroup-based sharding environment.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Typically used during training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE699]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: Representation of sharding plan. This uses the FQN of the larger wrapped model
    (i.e the model that is wrapped using DistributedModelParallel) EmbeddingModuleShardingPlan
    should be used when TorchRec composability is desired.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE700]'
  prefs: []
  type: TYPE_PRE
- en: dict keyed by module path of dict of parameter sharding specs keyed by parameter
    name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  prefs: []
  type: TYPE_NORMAL
- en: Dict[str, [EmbeddingModuleShardingPlan](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE701]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module_path** (*str*) –'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: dict of parameter sharding specs keyed by parameter name. None if sharding specs
    do not exist for given module_path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: Optional[[ModuleShardingPlan](#torchrec.distributed.types.ModuleShardingPlan
    "torchrec.distributed.types.ModuleShardingPlan")]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE702]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE703]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`'
  prefs: []
  type: TYPE_NORMAL
- en: Plans sharding. This plan can be saved and re-used to ensure sharding stability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE704]'
  prefs: []
  type: TYPE_PRE
- en: Calls self.plan(…) on rank 0 and broadcasts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*nn.Module*) – module that sharding is planned for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sharders** (*List**[*[*ModuleSharder*](#torchrec.distributed.types.ModuleSharder
    "torchrec.distributed.types.ModuleSharder")*[**nn.Module**]**]*) – provided sharders
    for module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: the computed sharding plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardingPlan](#torchrec.distributed.types.ShardingPlan "torchrec.distributed.types.ShardingPlan")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE705]'
  prefs: []
  type: TYPE_PRE
- en: Plans sharding for provided module and given sharders.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*nn.Module*) – module that sharding is planned for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sharders** (*List**[*[*ModuleSharder*](#torchrec.distributed.types.ModuleSharder
    "torchrec.distributed.types.ModuleSharder")*[**nn.Module**]**]*) – provided sharders
    for module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: the computed sharding plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardingPlan](#torchrec.distributed.types.ShardingPlan "torchrec.distributed.types.ShardingPlan")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE706]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Enum`'
  prefs: []
  type: TYPE_NORMAL
- en: Well-known sharding types, used by inter-module optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE707]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE708]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE709]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE710]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE711]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE712]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE713]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE714]  ## torchrec.distributed.utils[](#module-torchrec.distributed.utils
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE715]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Module`'
  prefs: []
  type: TYPE_NORMAL
- en: Allows copying of module to a target device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE716]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**device** – torch.device to copy to'
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: nn.Module on new device
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE717]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE718]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE719]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE720]'
  prefs: []
  type: TYPE_PRE
- en: Extract params from parameter sharding and then add them to fused_params.
  prefs: []
  type: TYPE_NORMAL
- en: Params from parameter sharding will override the ones in fused_params if they
    exist already.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**fused_params** (*Optional**[**Dict**[**str**,* *Any**]**]*) – the existing
    fused_params'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**parameter_sharding** ([*ParameterSharding*](#torchrec.distributed.types.ParameterSharding
    "torchrec.distributed.types.ParameterSharding")) – the parameter sharding to use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: the fused_params dictionary with params from parameter sharding added.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dict[str, Any]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE721]'
  prefs: []
  type: TYPE_PRE
- en: Adds prefix to all keys in state dict, in place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**state_dict** (*Dict**[**str**,* *Any**]*) – input state dict to update.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefix** (*str*) – name to filter from state dict keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: None.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE722]'
  prefs: []
  type: TYPE_PRE
- en: Appends provided prefix to provided name.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE723]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE724]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE725]'
  prefs: []
  type: TYPE_PRE
- en: Filters state dict for keys that start with provided name. Strips provided name
    from beginning of key in the resulting state dict.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**state_dict** (*OrderedDict**[**str**,* *torch.Tensor**]*) – input state dict
    to filter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**name** (*str*) – name to filter from state dict keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: filtered state dict.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: OrderedDict[str, torch.Tensor]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE726]'
  prefs: []
  type: TYPE_PRE
- en: Retrieves names of top level modules that do not contain any sharded sub-modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**model** (*torch.nn.Module*) – model to retrieve unsharded module names from.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: list of names of modules that don’t have sharded sub-modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: List[str]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE727]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE728]'
  prefs: []
  type: TYPE_PRE
- en: Configure the fused_params including cache_precision if the value is not preset.
  prefs: []
  type: TYPE_NORMAL
- en: Values set in table_level_fused_params take precidence over the global fused_params
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**fused_params** (*Optional**[**Dict**[**str**,* *Any**]**]*) – the original
    fused_params'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**grouped_fused_params** –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: a non-null configured fused_params dictionary to be used to configure the embedding
    lookup kernel
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dict[str, Any]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE729]'
  prefs: []
  type: TYPE_PRE
- en: Convert an optional to its value. Raises an AssertionError if the value is None
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE730]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE731]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: Allows copying of DistributedModelParallel module to a target device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE732]'
  prefs: []
  type: TYPE_PRE
- en: torchrec.distributed.mc_modules[](#torchrec-distributed-mc-modules "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE733]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`LazyAwaitable`](#torchrec.distributed.types.LazyAwaitable "torchrec.distributed.types.LazyAwaitable")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE734]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`EmbeddingCollectionContext`](#torchrec.distributed.embedding.EmbeddingCollectionContext
    "torchrec.distributed.embedding.EmbeddingCollectionContext")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE735]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE736]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE737]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE738]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`BaseEmbeddingSharder`](#torchrec.distributed.embedding_types.BaseEmbeddingSharder
    "torchrec.distributed.embedding_types.BaseEmbeddingSharder")[[`ManagedCollisionCollection`](torchrec.modules.html#torchrec.modules.mc_modules.ManagedCollisionCollection
    "torchrec.modules.mc_modules.ManagedCollisionCollection")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE739]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE740]'
  prefs: []
  type: TYPE_PRE
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  prefs: []
  type: TYPE_NORMAL
- en: Default implementation is data-parallel replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*M*) – module to shard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – compute device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: sharded module implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE741]'
  prefs: []
  type: TYPE_PRE
- en: List of parameters that can be sharded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE742]'
  prefs: []
  type: TYPE_PRE
- en: List of supported sharding types. See ShardingType for well-known examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE743]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`ShardedModule`](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[[`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), [`KJTList`](#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), [`ManagedCollisionCollectionContext`](#torchrec.distributed.mc_modules.ManagedCollisionCollectionContext
    "torchrec.distributed.mc_modules.ManagedCollisionCollectionContext")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE744]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE745]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE746]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE747]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE748]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE749]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE750]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE751]'
  prefs: []
  type: TYPE_PRE
- en: torchrec.distributed.mc_embeddingbag[](#torchrec-distributed-mc-embeddingbag
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE752]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`EmbeddingBagCollectionContext`](#torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext
    "torchrec.distributed.embeddingbag.EmbeddingBagCollectionContext")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE753]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE754]'
  prefs: []
  type: TYPE_PRE
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE755]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE756]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `BaseManagedCollisionEmbeddingCollectionSharder`[[`ManagedCollisionEmbeddingBagCollection`](torchrec.modules.html#torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingBagCollection
    "torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingBagCollection")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE757]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE758]'
  prefs: []
  type: TYPE_PRE
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  prefs: []
  type: TYPE_NORMAL
- en: Default implementation is data-parallel replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*M*) – module to shard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – compute device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: sharded module implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE759]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `BaseShardedManagedCollisionEmbeddingCollection`[[`ManagedCollisionEmbeddingBagCollectionContext`](#torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext
    "torchrec.distributed.mc_embeddingbag.ManagedCollisionEmbeddingBagCollectionContext")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE760]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE761]'
  prefs: []
  type: TYPE_PRE
- en: torchrec.distributed.mc_embedding[](#torchrec-distributed-mc-embedding "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE762]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`EmbeddingCollectionContext`](#torchrec.distributed.embedding.EmbeddingCollectionContext
    "torchrec.distributed.embedding.EmbeddingCollectionContext")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE763]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE764]'
  prefs: []
  type: TYPE_PRE
- en: See [https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html](https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE765]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE766]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `BaseManagedCollisionEmbeddingCollectionSharder`[[`ManagedCollisionEmbeddingCollection`](torchrec.modules.html#torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingCollection
    "torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingCollection")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE767]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE768]'
  prefs: []
  type: TYPE_PRE
- en: Does the actual sharding. It will allocate parameters on the requested locations
    as specified by corresponding ParameterSharding.
  prefs: []
  type: TYPE_NORMAL
- en: Default implementation is data-parallel replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**module** (*M*) – module to shard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params** ([*EmbeddingModuleShardingPlan*](#torchrec.distributed.types.EmbeddingModuleShardingPlan
    "torchrec.distributed.types.EmbeddingModuleShardingPlan")) – dict of fully qualified
    parameter names (module path + parameter name, ‘.’-separated) to its sharding
    spec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**env** ([*ShardingEnv*](#torchrec.distributed.types.ShardingEnv "torchrec.distributed.types.ShardingEnv"))
    – sharding environment that has the process group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (*torch.device*) – compute device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: sharded module implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ShardedModule](#torchrec.distributed.types.ShardedModule "torchrec.distributed.types.ShardedModule")[Any,
    Any, Any]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE769]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `BaseShardedManagedCollisionEmbeddingCollection`[[`ManagedCollisionEmbeddingCollectionContext`](#torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionContext
    "torchrec.distributed.mc_embedding.ManagedCollisionEmbeddingCollectionContext")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE770]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE771]'
  prefs: []
  type: TYPE_PRE
