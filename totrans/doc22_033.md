# torch.nn.functional

> 原文：[`pytorch.org/docs/stable/nn.functional.html`](https://pytorch.org/docs/stable/nn.functional.html)

## 卷积函数

| `conv1d` | 对由多个输入平面组成的输入信号应用 1D 卷积。 |
| --- | --- |
| `conv2d` | 对由多个输入平面组成的输入图像应用 2D 卷积。 |
| `conv3d` | 对由多个输入平面组成的输入图像应用 3D 卷积。 |
| `conv_transpose1d` | 对由多个输入平面组成的输入信号应用 1D 转置卷积运算，有时也称为“反卷积”。 |
| `conv_transpose2d` | 对由多个输入平面组成的输入图像应用 2D 转置卷积运算，有时也称为“反卷积”。 |
| `conv_transpose3d` | 对由多个输入平面组成的输入图像应用 3D 转置卷积运算，有时也称为“反卷积” |
| `unfold` | 从批量输入张量中提取滑动的局部块。 |
| `fold` | 将滑动的局部块数组合并成一个包含大张量。 |

## 池化函数

| `avg_pool1d` | 对由多个输入平面组成的输入信号应用 1D 平均池化。 |
| --- | --- |
| `avg_pool2d` | 通过步长$sH \times sW$在$kH \times kW$区域内应用 2D 平均池化操作。 |
| `avg_pool3d` | 通过步长$sT \times sH \times sW$在$kT \times kH \times kW$区域内应用 3D 平均池化操作。 |
| `max_pool1d` | 对由多个输入平面组成的输入信号应用 1D 最大池化。 |
| `max_pool2d` | 对由多个输入平面组成的输入信号应用 2D 最大池化。 |
| `max_pool3d` | 对由多个输入平面组成的输入信号应用 3D 最大池化。 |
| `max_unpool1d` | 计算`MaxPool1d`的部分逆。 |
| `max_unpool2d` | 计算`MaxPool2d`的部分逆。 |
| `max_unpool3d` | 计算`MaxPool3d`的部分逆。 |
| `lp_pool1d` | 对由多个输入平面组成的输入信号应用 1D 幂平均池化。 |
| `lp_pool2d` | 对由多个输入平面组成的输入信号应用 2D 幂平均池化。 |
| `adaptive_max_pool1d` | 对由多个输入平面组成的输入信号应用 1D 自适应最大池化。 |
| `adaptive_max_pool2d` | 对由多个输入平面组成的输入信号应用 2D 自适应最大池化。 |
| `adaptive_max_pool3d` | 对由多个输入平面组成的输入信号应用 3D 自适应最大池化。 |
| `adaptive_avg_pool1d` | 对由多个输入平面组成的输入信号应用 1D 自适应平均池化。 |
| `adaptive_avg_pool2d` | 对由多个输入平面组成的输入信号应用 2D 自适应平均池化。 |
| `adaptive_avg_pool3d` | 对由多个输入平面组成的输入信号应用 3D 自适应平均池化。 |
| `fractional_max_pool2d` | 对由多个输入平面组成的输入信号应用 2D 分数最大池化。 |
| `fractional_max_pool3d` | 对由多个输入平面组成的输入信号应用 3D 分数最大池化。 |

## 注意力机制

| `scaled_dot_product_attention` | 在查询、键和值张量上计算缩放点积注意力，如果传递了可选的注意力掩码，则应用 dropout，如果指定了大于 0.0 的概率。 |
| --- | --- |

## 非线性激活函数[](#non-linear-activation-functions "跳转到此标题")

| `threshold` | 对输入张量的每个元素应用阈值。 |
| --- | --- |
| `threshold_` | `threshold()`的原地版本。 |
| `relu` | 逐元素应用修正线性单元函数。 |
| `relu_` | `relu()`的原地版本。 |
| `hardtanh` | 逐元素应用 HardTanh 函数。 |
| `hardtanh_` | `hardtanh()`的原地版本。 |
| `hardswish` | 应用硬 swish 函数，逐元素。 |
| `relu6` | 应用逐元素函数 $\text{ReLU6}(x) = \min(\max(0,x), 6)$。 |
| `elu` | 逐元素应用指数线性单元（ELU）函数。 |
| `elu_` | `elu()`的原地版本。 |
| `selu` | 逐元素应用，$\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))$，其中 $\alpha=1.6732632423543772848170429916717$ 和 $scale=1.0507009873554804934193349852946$。 |
| `celu` | 逐元素应用，$\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))$。 |
| `leaky_relu` | 逐元素应用，$\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)$。 |
| `leaky_relu_` | `leaky_relu()`的原地版本。 |
| `prelu` | 逐元素应用函数 $\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)$，其中 weight 是可学习参数。 |
| `rrelu` | 随机泄漏的 ReLU。 |
| `rrelu_` | `rrelu()`的原地版本。 |
| `glu` | 门控线性单元。 |
| `gelu` | 当近似参数为'none'时，逐元素应用函数 $\text{GELU}(x) = x * \Phi(x)$。 |
| `logsigmoid` | 逐元素应用 $\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)$。 |
| `hardshrink` | 逐元素应用硬收缩函数。 |
| `tanhshrink` | 应用函数$\text{Tanhshrink}(x) = x - \text{Tanh}(x)$ |
| `softsign` | 对每个元素应用函数$\text{SoftSign}(x) = \frac{x}{1 + | x | }$ |
| `softplus` | 对每个元素应用函数$\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))$ |
| `softmin` | 应用 softmin 函数。 |
| `softmax` | 应用 softmax 函数。 |
| `softshrink` | 对每个元素应用软收缩函数。 |
| `gumbel_softmax` | 从 Gumbel-Softmax 分布中采样，并可选择离散化。 |
| `log_softmax` | 应用 softmax 后再取对数。 |
| `tanh` | 对每个元素应用$\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}$ |
| `sigmoid` | 对每个元素应用函数$\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}$ |
| `hardsigmoid` | 对每个元素应用 Hardsigmoid 函数。 |
| `silu` | 对每个元素应用 Sigmoid 线性单元（SiLU）函数。 |
| `mish` | 对每个元素应用 Mish 函数。 |
| `batch_norm` | 对数据批次中的每个通道应用批次归一化。 |
| `group_norm` | 对最后若干维度应用组归一化。 |
| `instance_norm` | 对每个数据样本中的每个通道独立应用实例归一化。 |
| `layer_norm` | 对最后若干维度应用层归一化。 |
| `local_response_norm` | 对输入信号应用局部响应归一化。 |
| `normalize` | 在指定维度上对输入进行$L_p$规范化。 |

## 线性函数

| `linear` | 对传入数据应用线性变换：$y = xA^T + b$y=xAT+b。 |
| --- | --- |
| `bilinear` | 对传入数据应用双线性变换：$y = x_1^T A x_2 + b$y=x1T​Ax2​+b |

## Dropout 函数

| `dropout` | 在训练期间，以概率`p`随机将输入张量的一些元素置零。 |
| --- | --- |
| `alpha_dropout` | 对输入应用 alpha dropout。 |
| `feature_alpha_dropout` | 随机屏蔽整个通道（通道是一个特征图）。 |
| `dropout1d` | 随机将整个通道置零（通道是一个 1D 特征图）。 |
| `dropout2d` | 随机将整个通道置零（通道是一个 2D 特征图）。 |
| `dropout3d` | 随机将整个通道置零（通道是一个 3D 特征图）。 |

## 稀疏函数

| `embedding` | 生成一个简单的查找表，在固定字典和大小中查找嵌入。 |
| --- | --- |
| `embedding_bag` | 计算嵌入包的总和、平均值或最大值。 |
| `one_hot` | 接受形状为`(*)`的 LongTensor 索引值，并返回形状为`(*, num_classes)`的张量，除了最后一维的索引与输入张量的相应值匹配的地方为 1 外，其他地方都为零。 |

## 距离函数

| `pairwise_distance` | 详细信息请参见`torch.nn.PairwiseDistance` |
| --- | --- |
| `cosine_similarity` | 返回沿着维度计算的`x1`和`x2`之间的余弦相似度。 |
| `pdist` | 计算输入中每对行向量之间的 p-范数距离。 |

## 损失函数

| `binary_cross_entropy` | 计算目标和输入概率之间的二元交叉熵。 |
| --- | --- |
| `binary_cross_entropy_with_logits` | 计算目标和输入 logits 之间的二元交叉熵。 |
| `poisson_nll_loss` | 泊松负对数似然损失。 |
| `cosine_embedding_loss` | 详细信息请参阅`CosineEmbeddingLoss`。 |
| `cross_entropy` | 计算输入 logits 和目标之间的交叉熵损失。 |
| `ctc_loss` | 应用连接主义时间分类损失。 |
| `gaussian_nll_loss` | 高斯负对数似然损失。 |
| `hinge_embedding_loss` | 详细信息请参阅`HingeEmbeddingLoss`。 |
| `kl_div` | 计算 KL 散度损失。 |
| `l1_loss` | 计算元素间绝对值差的均值。 |
| `mse_loss` | 计算元素间均方误差。 |
| `margin_ranking_loss` | 详细信息请参阅`MarginRankingLoss`。 |
| `multilabel_margin_loss` | 详细信息请参阅`MultiLabelMarginLoss`。 |
| `multilabel_soft_margin_loss` | 详细信息请参阅`MultiLabelSoftMarginLoss`。 |
| `multi_margin_loss` | 详细信息请参阅`MultiMarginLoss`。 |
| `nll_loss` | 计算负对数似然损失。 |
| `huber_loss` | 计算 Huber 损失。 |
| `smooth_l1_loss` | 计算平滑 L1 损失。 |
| `soft_margin_loss` | 详细信息请参阅`SoftMarginLoss`。 |
| `triplet_margin_loss` | 计算给定输入张量之间的三元组损失，边距大于 0。 |
| `triplet_margin_with_distance_loss` | 使用自定义距离函数计算输入张量的三元组边距损失。 |

## Vision functions

| `pixel_shuffle` | 将形状为$(*, C \times r², H, W)$(∗,C×r2,H,W)的张量重新排列为形状为$(*, C, H \times r, W \times r)$(∗,C,H×r,W×r)的张量，其中 r 是`upscale_factor`。 |
| --- | --- |
| `pixel_unshuffle` | 通过将形状为$(*, C, H \times r, W \times r)$(∗,C,H×r,W×r)的张量重新排列为形状为$(*, C \times r², H, W)$(∗,C×r2,H,W)的张量，其中 r 是`downscale_factor`，来反转`PixelShuffle`操作。 |
| `pad` | 填充张量。 |
| `interpolate` | 对输入进行下采样/上采样。 |
| `upsample` | 上采样输入。 |
| `upsample_nearest` | 使用最近邻像素值对输入进行上采样。 |
| `upsample_bilinear` | 使用双线性上采样对输入进行上采样。 |
| `grid_sample` | 计算网格采样。 |
| `affine_grid` | 给定一批仿射矩阵`theta`，生成 2D 或 3D 流场（采样网格）。 |

## DataParallel functions (multi-GPU, distributed)[](#dataparallel-functions-multi-gpu-distributed "Permalink to this heading")

### data_parallel

| `torch.nn.parallel.data_parallel` | 在给定的 device_ids 上并行评估模块(input)。 |
| --- | --- |
