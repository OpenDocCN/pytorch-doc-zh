# torch.nn.functional

> 原文：[https://pytorch.org/docs/stable/nn.functional.html](https://pytorch.org/docs/stable/nn.functional.html)

## 卷积函数

| [`conv1d`](generated/torch.nn.functional.conv1d.html#torch.nn.functional.conv1d "torch.nn.functional.conv1d") | 对由多个输入平面组成的输入信号应用1D卷积。 |
| --- | --- |
| [`conv2d`](generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d "torch.nn.functional.conv2d") | 对由多个输入平面组成的输入图像应用2D卷积。 |
| [`conv3d`](generated/torch.nn.functional.conv3d.html#torch.nn.functional.conv3d "torch.nn.functional.conv3d") | 对由多个输入平面组成的输入图像应用3D卷积。 |
| [`conv_transpose1d`](generated/torch.nn.functional.conv_transpose1d.html#torch.nn.functional.conv_transpose1d "torch.nn.functional.conv_transpose1d") | 对由多个输入平面组成的输入信号应用1D转置卷积运算，有时也称为“反卷积”。 |
| [`conv_transpose2d`](generated/torch.nn.functional.conv_transpose2d.html#torch.nn.functional.conv_transpose2d "torch.nn.functional.conv_transpose2d") | 对由多个输入平面组成的输入图像应用2D转置卷积运算，有时也称为“反卷积”。 |
| [`conv_transpose3d`](generated/torch.nn.functional.conv_transpose3d.html#torch.nn.functional.conv_transpose3d "torch.nn.functional.conv_transpose3d") | 对由多个输入平面组成的输入图像应用3D转置卷积运算，有时也称为“反卷积” |
| [`unfold`](generated/torch.nn.functional.unfold.html#torch.nn.functional.unfold "torch.nn.functional.unfold") | 从批量输入张量中提取滑动的局部块。 |
| [`fold`](generated/torch.nn.functional.fold.html#torch.nn.functional.fold "torch.nn.functional.fold") | 将滑动的局部块数组合并成一个包含大张量。 |

## 池化函数

| [`avg_pool1d`](generated/torch.nn.functional.avg_pool1d.html#torch.nn.functional.avg_pool1d "torch.nn.functional.avg_pool1d") | 对由多个输入平面组成的输入信号应用1D平均池化。 |
| --- | --- |
| [`avg_pool2d`](generated/torch.nn.functional.avg_pool2d.html#torch.nn.functional.avg_pool2d "torch.nn.functional.avg_pool2d") | 通过步长$sH \times sW$在$kH \times kW$区域内应用2D平均池化操作。 |
| [`avg_pool3d`](generated/torch.nn.functional.avg_pool3d.html#torch.nn.functional.avg_pool3d "torch.nn.functional.avg_pool3d") | 通过步长$sT \times sH \times sW$在$kT \times kH \times kW$区域内应用3D平均池化操作。 |
| [`max_pool1d`](generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d "torch.nn.functional.max_pool1d") | 对由多个输入平面组成的输入信号应用1D最大池化。 |
| [`max_pool2d`](generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d "torch.nn.functional.max_pool2d") | 对由多个输入平面组成的输入信号应用2D最大池化。 |
| [`max_pool3d`](generated/torch.nn.functional.max_pool3d.html#torch.nn.functional.max_pool3d "torch.nn.functional.max_pool3d") | 对由多个输入平面组成的输入信号应用3D最大池化。 |
| [`max_unpool1d`](generated/torch.nn.functional.max_unpool1d.html#torch.nn.functional.max_unpool1d "torch.nn.functional.max_unpool1d") | 计算`MaxPool1d`的部分逆。 |
| [`max_unpool2d`](generated/torch.nn.functional.max_unpool2d.html#torch.nn.functional.max_unpool2d "torch.nn.functional.max_unpool2d") | 计算`MaxPool2d`的部分逆。 |
| [`max_unpool3d`](generated/torch.nn.functional.max_unpool3d.html#torch.nn.functional.max_unpool3d "torch.nn.functional.max_unpool3d") | 计算`MaxPool3d`的部分逆。 |
| [`lp_pool1d`](生成/torch.nn.functional.lp_pool1d.html#torch.nn.functional.lp_pool1d "torch.nn.functional.lp_pool1d") | 对由多个输入平面组成的输入信号应用1D幂平均池化。 |
| [`lp_pool2d`](生成/torch.nn.functional.lp_pool2d.html#torch.nn.functional.lp_pool2d "torch.nn.functional.lp_pool2d") | 对由多个输入平面组成的输入信号应用2D幂平均池化。 |
| [`adaptive_max_pool1d`](生成/torch.nn.functional.adaptive_max_pool1d.html#torch.nn.functional.adaptive_max_pool1d "torch.nn.functional.adaptive_max_pool1d") | 对由多个输入平面组成的输入信号应用1D自适应最大池化。 |
| [`adaptive_max_pool2d`](生成/torch.nn.functional.adaptive_max_pool2d.html#torch.nn.functional.adaptive_max_pool2d "torch.nn.functional.adaptive_max_pool2d") | 对由多个输入平面组成的输入信号应用2D自适应最大池化。 |
| [`adaptive_max_pool3d`](生成/torch.nn.functional.adaptive_max_pool3d.html#torch.nn.functional.adaptive_max_pool3d "torch.nn.functional.adaptive_max_pool3d") | 对由多个输入平面组成的输入信号应用3D自适应最大池化。 |
| [`adaptive_avg_pool1d`](生成/torch.nn.functional.adaptive_avg_pool1d.html#torch.nn.functional.adaptive_avg_pool1d "torch.nn.functional.adaptive_avg_pool1d") | 对由多个输入平面组成的输入信号应用1D自适应平均池化。 |
| [`adaptive_avg_pool2d`](生成/torch.nn.functional.adaptive_avg_pool2d.html#torch.nn.functional.adaptive_avg_pool2d "torch.nn.functional.adaptive_avg_pool2d") | 对由多个输入平面组成的输入信号应用2D自适应平均池化。 |
| [`adaptive_avg_pool3d`](生成/torch.nn.functional.adaptive_avg_pool3d.html#torch.nn.functional.adaptive_avg_pool3d "torch.nn.functional.adaptive_avg_pool3d") | 对由多个输入平面组成的输入信号应用3D自适应平均池化。 |
| [`fractional_max_pool2d`](生成/torch.nn.functional.fractional_max_pool2d.html#torch.nn.functional.fractional_max_pool2d "torch.nn.functional.fractional_max_pool2d") | 对由多个输入平面组成的输入信号应用2D分数最大池化。 |
| [`fractional_max_pool3d`](生成/torch.nn.functional.fractional_max_pool3d.html#torch.nn.functional.fractional_max_pool3d "torch.nn.functional.fractional_max_pool3d") | 对由多个输入平面组成的输入信号应用3D分数最大池化。 |

## 注意力机制

| [`scaled_dot_product_attention`](生成/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention "torch.nn.functional.scaled_dot_product_attention") | 在查询、键和值张量上计算缩放点积注意力，如果传递了可选的注意力掩码，则应用dropout，如果指定了大于0.0的概率。 |
| --- | --- |

## 非线性激活函数[](#non-linear-activation-functions "跳转到此标题")

| [`threshold`](生成/torch.nn.functional.threshold.html#torch.nn.functional.threshold "torch.nn.functional.threshold") | 对输入张量的每个元素应用阈值。 |
| --- | --- |
| [`threshold_`](生成/torch.nn.functional.threshold_.html#torch.nn.functional.threshold_ "torch.nn.functional.threshold_") | [`threshold()`](生成/torch.nn.functional.threshold.html#torch.nn.functional.threshold "torch.nn.functional.threshold")的原地版本。 |
| [`relu`](生成/torch.nn.functional.relu.html#torch.nn.functional.relu "torch.nn.functional.relu") | 逐元素应用修正线性单元函数。 |
| [`relu_`](生成/torch.nn.functional.relu_.html#torch.nn.functional.relu_ "torch.nn.functional.relu_") | [`relu()`](生成/torch.nn.functional.relu.html#torch.nn.functional.relu "torch.nn.functional.relu")的原地版本。 |
| [`hardtanh`](generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh "torch.nn.functional.hardtanh") | 逐元素应用HardTanh函数。 |
| [`hardtanh_`](generated/torch.nn.functional.hardtanh_.html#torch.nn.functional.hardtanh_ "torch.nn.functional.hardtanh_") | [`hardtanh()`](generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh "torch.nn.functional.hardtanh")的原地版本。 |
| [`hardswish`](generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish "torch.nn.functional.hardswish") | 应用硬swish函数，逐元素。 |
| [`relu6`](generated/torch.nn.functional.relu6.html#torch.nn.functional.relu6 "torch.nn.functional.relu6") | 应用逐元素函数 $\text{ReLU6}(x) = \min(\max(0,x), 6)$。 |
| [`elu`](generated/torch.nn.functional.elu.html#torch.nn.functional.elu "torch.nn.functional.elu") | 逐元素应用指数线性单元（ELU）函数。 |
| [`elu_`](generated/torch.nn.functional.elu_.html#torch.nn.functional.elu_ "torch.nn.functional.elu_") | [`elu()`](generated/torch.nn.functional.elu.html#torch.nn.functional.elu "torch.nn.functional.elu")的原地版本。 |
| [`selu`](generated/torch.nn.functional.selu.html#torch.nn.functional.selu "torch.nn.functional.selu") | 逐元素应用，$\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))$，其中 $\alpha=1.6732632423543772848170429916717$ 和 $scale=1.0507009873554804934193349852946$。 |
| [`celu`](generated/torch.nn.functional.celu.html#torch.nn.functional.celu "torch.nn.functional.celu") | 逐元素应用，$\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))$。 |
| [`leaky_relu`](generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu "torch.nn.functional.leaky_relu") | 逐元素应用，$\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)$。 |
| [`leaky_relu_`](generated/torch.nn.functional.leaky_relu_.html#torch.nn.functional.leaky_relu_ "torch.nn.functional.leaky_relu_") | [`leaky_relu()`](generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu "torch.nn.functional.leaky_relu")的原地版本。 |
| [`prelu`](generated/torch.nn.functional.prelu.html#torch.nn.functional.prelu "torch.nn.functional.prelu") | 逐元素应用函数 $\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)$，其中weight是可学习参数。 |
| [`rrelu`](generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu "torch.nn.functional.rrelu") | 随机泄漏的ReLU。 |
| [`rrelu_`](generated/torch.nn.functional.rrelu_.html#torch.nn.functional.rrelu_ "torch.nn.functional.rrelu_") | [`rrelu()`](generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu "torch.nn.functional.rrelu")的原地版本。 |
| [`glu`](generated/torch.nn.functional.glu.html#torch.nn.functional.glu "torch.nn.functional.glu") | 门控线性单元。 |
| [`gelu`](generated/torch.nn.functional.gelu.html#torch.nn.functional.gelu "torch.nn.functional.gelu") | 当近似参数为'none'时，逐元素应用函数 $\text{GELU}(x) = x * \Phi(x)$。 |
| [`logsigmoid`](generated/torch.nn.functional.logsigmoid.html#torch.nn.functional.logsigmoid "torch.nn.functional.logsigmoid") | 逐元素应用 $\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)$。 |
| [`hardshrink`](generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink "torch.nn.functional.hardshrink") | 逐元素应用硬收缩函数。 |
| [`tanhshrink`](generated/torch.nn.functional.tanhshrink.html#torch.nn.functional.tanhshrink "torch.nn.functional.tanhshrink") | 应用函数$\text{Tanhshrink}(x) = x - \text{Tanh}(x)$ |
| [`softsign`](generated/torch.nn.functional.softsign.html#torch.nn.functional.softsign "torch.nn.functional.softsign") | 对每个元素应用函数$\text{SoftSign}(x) = \frac{x}{1 + | x | }$ |
| [`softplus`](generated/torch.nn.functional.softplus.html#torch.nn.functional.softplus "torch.nn.functional.softplus") | 对每个元素应用函数$\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))$ |
| [`softmin`](generated/torch.nn.functional.softmin.html#torch.nn.functional.softmin "torch.nn.functional.softmin") | 应用softmin函数。 |
| [`softmax`](generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax "torch.nn.functional.softmax") | 应用softmax函数。 |
| [`softshrink`](generated/torch.nn.functional.softshrink.html#torch.nn.functional.softshrink "torch.nn.functional.softshrink") | 对每个元素应用软收缩函数。 |
| [`gumbel_softmax`](generated/torch.nn.functional.gumbel_softmax.html#torch.nn.functional.gumbel_softmax "torch.nn.functional.gumbel_softmax") | 从Gumbel-Softmax分布中采样，并可选择离散化。 |
| [`log_softmax`](generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax "torch.nn.functional.log_softmax") | 应用softmax后再取对数。 |
| [`tanh`](generated/torch.nn.functional.tanh.html#torch.nn.functional.tanh "torch.nn.functional.tanh") | 对每个元素应用$\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}$ |
| [`sigmoid`](generated/torch.nn.functional.sigmoid.html#torch.nn.functional.sigmoid "torch.nn.functional.sigmoid") | 对每个元素应用函数$\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}$ |
| [`hardsigmoid`](generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid "torch.nn.functional.hardsigmoid") | 对每个元素应用Hardsigmoid函数。 |
| [`silu`](generated/torch.nn.functional.silu.html#torch.nn.functional.silu "torch.nn.functional.silu") | 对每个元素应用Sigmoid线性单元（SiLU）函数。 |
| [`mish`](generated/torch.nn.functional.mish.html#torch.nn.functional.mish "torch.nn.functional.mish") | 对每个元素应用Mish函数。 |
| [`batch_norm`](generated/torch.nn.functional.batch_norm.html#torch.nn.functional.batch_norm "torch.nn.functional.batch_norm") | 对数据批次中的每个通道应用批次归一化。 |
| [`group_norm`](generated/torch.nn.functional.group_norm.html#torch.nn.functional.group_norm "torch.nn.functional.group_norm") | 对最后若干维度应用组归一化。 |
| [`instance_norm`](generated/torch.nn.functional.instance_norm.html#torch.nn.functional.instance_norm "torch.nn.functional.instance_norm") | 对每个数据样本中的每个通道独立应用实例归一化。 |
| [`layer_norm`](generated/torch.nn.functional.layer_norm.html#torch.nn.functional.layer_norm "torch.nn.functional.layer_norm") | 对最后若干维度应用层归一化。 |
| [`local_response_norm`](generated/torch.nn.functional.local_response_norm.html#torch.nn.functional.local_response_norm "torch.nn.functional.local_response_norm") | 对输入信号应用局部响应归一化。 |
| [`normalize`](generated/torch.nn.functional.normalize.html#torch.nn.functional.normalize "torch.nn.functional.normalize") | 在指定维度上对输入进行$L_p$规范化。 |

## 线性函数

| [`linear`](生成/torch.nn.functional.linear.html#torch.nn.functional.linear "torch.nn.functional.linear") | 对传入数据应用线性变换：$y = xA^T + b$y=xAT+b。 |
| --- | --- |
| [`bilinear`](生成/torch.nn.functional.bilinear.html#torch.nn.functional.bilinear "torch.nn.functional.bilinear") | 对传入数据应用双线性变换：$y = x_1^T A x_2 + b$y=x1T​Ax2​+b |

## Dropout函数

| [`dropout`](生成/torch.nn.functional.dropout.html#torch.nn.functional.dropout "torch.nn.functional.dropout") | 在训练期间，以概率`p`随机将输入张量的一些元素置零。 |
| --- | --- |
| [`alpha_dropout`](生成/torch.nn.functional.alpha_dropout.html#torch.nn.functional.alpha_dropout "torch.nn.functional.alpha_dropout") | 对输入应用alpha dropout。 |
| [`feature_alpha_dropout`](生成/torch.nn.functional.feature_alpha_dropout.html#torch.nn.functional.feature_alpha_dropout "torch.nn.functional.feature_alpha_dropout") | 随机屏蔽整个通道（通道是一个特征图）。 |
| [`dropout1d`](生成/torch.nn.functional.dropout1d.html#torch.nn.functional.dropout1d "torch.nn.functional.dropout1d") | 随机将整个通道置零（通道是一个1D特征图）。 |
| [`dropout2d`](生成/torch.nn.functional.dropout2d.html#torch.nn.functional.dropout2d "torch.nn.functional.dropout2d") | 随机将整个通道置零（通道是一个2D特征图）。 |
| [`dropout3d`](生成/torch.nn.functional.dropout3d.html#torch.nn.functional.dropout3d "torch.nn.functional.dropout3d") | 随机将整个通道置零（通道是一个3D特征图）。 |

## 稀疏函数

| [`embedding`](生成/torch.nn.functional.embedding.html#torch.nn.functional.embedding "torch.nn.functional.embedding") | 生成一个简单的查找表，在固定字典和大小中查找嵌入。 |
| --- | --- |
| [`embedding_bag`](生成/torch.nn.functional.embedding_bag.html#torch.nn.functional.embedding_bag "torch.nn.functional.embedding_bag") | 计算嵌入包的总和、平均值或最大值。 |
| [`one_hot`](生成/torch.nn.functional.one_hot.html#torch.nn.functional.one_hot "torch.nn.functional.one_hot") | 接受形状为`(*)`的LongTensor索引值，并返回形状为`(*, num_classes)`的张量，除了最后一维的索引与输入张量的相应值匹配的地方为1外，其他地方都为零。 |

## 距离函数

| [`pairwise_distance`](生成/torch.nn.functional.pairwise_distance.html#torch.nn.functional.pairwise_distance "torch.nn.functional.pairwise_distance") | 详细信息请参见[`torch.nn.PairwiseDistance`](生成/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance "torch.nn.PairwiseDistance") |
| --- | --- |
| [`cosine_similarity`](生成/torch.nn.functional.cosine_similarity.html#torch.nn.functional.cosine_similarity "torch.nn.functional.cosine_similarity") | 返回沿着维度计算的`x1`和`x2`之间的余弦相似度。 |
| [`pdist`](生成/torch.nn.functional.pdist.html#torch.nn.functional.pdist "torch.nn.functional.pdist") | 计算输入中每对行向量之间的p-范数距离。 |

## 损失函数

| [`binary_cross_entropy`](生成/torch.nn.functional.binary_cross_entropy.html#torch.nn.functional.binary_cross_entropy "torch.nn.functional.binary_cross_entropy") | 计算目标和输入概率之间的二元交叉熵。 |
| --- | --- |
| [`binary_cross_entropy_with_logits`](生成/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits "torch.nn.functional.binary_cross_entropy_with_logits") | 计算目标和输入logits之间的二元交叉熵。 |
| [`poisson_nll_loss`](生成/torch.nn.functional.poisson_nll_loss.html#torch.nn.functional.poisson_nll_loss "torch.nn.functional.poisson_nll_loss") | 泊松负对数似然损失。 |
| [`cosine_embedding_loss`](generated/torch.nn.functional.cosine_embedding_loss.html#torch.nn.functional.cosine_embedding_loss "torch.nn.functional.cosine_embedding_loss") | 详细信息请参阅[`CosineEmbeddingLoss`](generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss "torch.nn.CosineEmbeddingLoss")。 |
| [`cross_entropy`](generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy "torch.nn.functional.cross_entropy") | 计算输入logits和目标之间的交叉熵损失。 |
| [`ctc_loss`](generated/torch.nn.functional.ctc_loss.html#torch.nn.functional.ctc_loss "torch.nn.functional.ctc_loss") | 应用连接主义时间分类损失。 |
| [`gaussian_nll_loss`](generated/torch.nn.functional.gaussian_nll_loss.html#torch.nn.functional.gaussian_nll_loss "torch.nn.functional.gaussian_nll_loss") | 高斯负对数似然损失。 |
| [`hinge_embedding_loss`](generated/torch.nn.functional.hinge_embedding_loss.html#torch.nn.functional.hinge_embedding_loss "torch.nn.functional.hinge_embedding_loss") | 详细信息请参阅[`HingeEmbeddingLoss`](generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss "torch.nn.HingeEmbeddingLoss")。 |
| [`kl_div`](generated/torch.nn.functional.kl_div.html#torch.nn.functional.kl_div "torch.nn.functional.kl_div") | 计算KL散度损失。 |
| [`l1_loss`](generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss "torch.nn.functional.l1_loss") | 计算元素间绝对值差的均值。 |
| [`mse_loss`](generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss "torch.nn.functional.mse_loss") | 计算元素间均方误差。 |
| [`margin_ranking_loss`](generated/torch.nn.functional.margin_ranking_loss.html#torch.nn.functional.margin_ranking_loss "torch.nn.functional.margin_ranking_loss") | 详细信息请参阅[`MarginRankingLoss`](generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss "torch.nn.MarginRankingLoss")。 |
| [`multilabel_margin_loss`](generated/torch.nn.functional.multilabel_margin_loss.html#torch.nn.functional.multilabel_margin_loss "torch.nn.functional.multilabel_margin_loss") | 详细信息请参阅[`MultiLabelMarginLoss`](generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss "torch.nn.MultiLabelMarginLoss")。 |
| [`multilabel_soft_margin_loss`](generated/torch.nn.functional.multilabel_soft_margin_loss.html#torch.nn.functional.multilabel_soft_margin_loss "torch.nn.functional.multilabel_soft_margin_loss") | 详细信息请参阅[`MultiLabelSoftMarginLoss`](generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss "torch.nn.MultiLabelSoftMarginLoss")。 |
| [`multi_margin_loss`](generated/torch.nn.functional.multi_margin_loss.html#torch.nn.functional.multi_margin_loss "torch.nn.functional.multi_margin_loss") | 详细信息请参阅[`MultiMarginLoss`](generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss "torch.nn.MultiMarginLoss")。 |
| [`nll_loss`](generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss "torch.nn.functional.nll_loss") | 计算负对数似然损失。 |
| [`huber_loss`](generated/torch.nn.functional.huber_loss.html#torch.nn.functional.huber_loss "torch.nn.functional.huber_loss") | 计算Huber损失。 |
| [`smooth_l1_loss`](generated/torch.nn.functional.smooth_l1_loss.html#torch.nn.functional.smooth_l1_loss "torch.nn.functional.smooth_l1_loss") | 计算平滑L1损失。 |
| [`soft_margin_loss`](generated/torch.nn.functional.soft_margin_loss.html#torch.nn.functional.soft_margin_loss "torch.nn.functional.soft_margin_loss") | 详细信息请参阅[`SoftMarginLoss`](generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss "torch.nn.SoftMarginLoss")。 |
| [`triplet_margin_loss`](generated/torch.nn.functional.triplet_margin_loss.html#torch.nn.functional.triplet_margin_loss "torch.nn.functional.triplet_margin_loss") | 计算给定输入张量之间的三元组损失，边距大于0。 |
| [`triplet_margin_with_distance_loss`](generated/torch.nn.functional.triplet_margin_with_distance_loss.html#torch.nn.functional.triplet_margin_with_distance_loss "torch.nn.functional.triplet_margin_with_distance_loss") | 使用自定义距离函数计算输入张量的三元组边距损失。 |

## Vision functions

| [`pixel_shuffle`](generated/torch.nn.functional.pixel_shuffle.html#torch.nn.functional.pixel_shuffle "torch.nn.functional.pixel_shuffle") | 将形状为$(*, C \times r^2, H, W)$(∗,C×r2,H,W)的张量重新排列为形状为$(*, C, H \times r, W \times r)$(∗,C,H×r,W×r)的张量，其中r是`upscale_factor`。 |
| --- | --- |
| [`pixel_unshuffle`](generated/torch.nn.functional.pixel_unshuffle.html#torch.nn.functional.pixel_unshuffle "torch.nn.functional.pixel_unshuffle") | 通过将形状为$(*, C, H \times r, W \times r)$(∗,C,H×r,W×r)的张量重新排列为形状为$(*, C \times r^2, H, W)$(∗,C×r2,H,W)的张量，其中r是`downscale_factor`，来反转[`PixelShuffle`](generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle "torch.nn.PixelShuffle")操作。 |
| [`pad`](generated/torch.nn.functional.pad.html#torch.nn.functional.pad "torch.nn.functional.pad") | 填充张量。 |
| [`interpolate`](generated/torch.nn.functional.interpolate.html#torch.nn.functional.interpolate "torch.nn.functional.interpolate") | 对输入进行下采样/上采样。 |
| [`upsample`](generated/torch.nn.functional.upsample.html#torch.nn.functional.upsample "torch.nn.functional.upsample") | 上采样输入。 |
| [`upsample_nearest`](generated/torch.nn.functional.upsample_nearest.html#torch.nn.functional.upsample_nearest "torch.nn.functional.upsample_nearest") | 使用最近邻像素值对输入进行上采样。 |
| [`upsample_bilinear`](generated/torch.nn.functional.upsample_bilinear.html#torch.nn.functional.upsample_bilinear "torch.nn.functional.upsample_bilinear") | 使用双线性上采样对输入进行上采样。 |
| [`grid_sample`](generated/torch.nn.functional.grid_sample.html#torch.nn.functional.grid_sample "torch.nn.functional.grid_sample") | 计算网格采样。 |
| [`affine_grid`](generated/torch.nn.functional.affine_grid.html#torch.nn.functional.affine_grid "torch.nn.functional.affine_grid") | 给定一批仿射矩阵`theta`，生成2D或3D流场（采样网格）。 |

## DataParallel functions (multi-GPU, distributed)[](#dataparallel-functions-multi-gpu-distributed "Permalink to this heading")

### data_parallel

| `torch.nn.parallel.data_parallel` | 在给定的device_ids上并行评估模块(input)。 |
| --- | --- |
