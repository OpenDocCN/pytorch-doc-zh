["```py\n# install conda to make installying pytorch with cudatoolkit 11.3 easier.\n!sudo rm Miniconda3-py37_4.9.2-Linux-x86_64.sh Miniconda3-py37_4.9.2-Linux-x86_64.sh.*\n!sudo wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh\n!sudo chmod +x Miniconda3-py37_4.9.2-Linux-x86_64.sh\n!sudo bash ./Miniconda3-py37_4.9.2-Linux-x86_64.sh -b -f -p /usr/local \n```", "```py\n# install pytorch with cudatoolkit 11.3\n!sudo conda install pytorch cudatoolkit=11.3 -c pytorch-nightly -y \n```", "```py\n# install torchrec\n!pip3 install torchrec-nightly \n```", "```py\n!pip3 install multiprocess \n```", "```py\n!sudo cp /usr/local/lib/lib* /usr/lib/ \n```", "```py\nimport sys\nsys.path = ['', '/env/python', '/usr/local/lib/python37.zip', '/usr/local/lib/python3.7', '/usr/local/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/site-packages', './.local/lib/python3.7/site-packages'] \n```", "```py\nimport os\nimport torch\nimport torchrec\n\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"29500\" \n```", "```py\nfrom torchrec.distributed.planner.types import ParameterConstraints\nfrom torchrec.distributed.embedding_types import EmbeddingComputeKernel\nfrom torchrec.distributed.types import ShardingType\nfrom typing import Dict\n\nlarge_table_cnt = 2\nsmall_table_cnt = 2\nlarge_tables=[\n  torchrec.EmbeddingBagConfig(\n    name=\"large_table_\" + str(i),\n    embedding_dim=64,\n    num_embeddings=4096,\n    feature_names=[\"large_table_feature_\" + str(i)],\n    pooling=torchrec.PoolingType.SUM,\n  ) for i in range(large_table_cnt)\n]\nsmall_tables=[\n  torchrec.EmbeddingBagConfig(\n    name=\"small_table_\" + str(i),\n    embedding_dim=64,\n    num_embeddings=1024,\n    feature_names=[\"small_table_feature_\" + str(i)],\n    pooling=torchrec.PoolingType.SUM,\n  ) for i in range(small_table_cnt)\n]\n\ndef gen_constraints(sharding_type: ShardingType = ShardingType.TABLE_WISE) -> Dict[str, ParameterConstraints]:\n  large_table_constraints = {\n    \"large_table_\" + str(i): ParameterConstraints(\n      sharding_types=[sharding_type.value],\n    ) for i in range(large_table_cnt)\n  }\n  small_table_constraints = {\n    \"small_table_\" + str(i): ParameterConstraints(\n      sharding_types=[sharding_type.value],\n    ) for i in range(small_table_cnt)\n  }\n  constraints = {**large_table_constraints, **small_table_constraints}\n  return constraints \n```", "```py\nebc = torchrec.EmbeddingBagCollection(\n    device=\"cuda\",\n    tables=large_tables + small_tables\n) \n```", "```py\ndef single_rank_execution(\n    rank: int,\n    world_size: int,\n    constraints: Dict[str, ParameterConstraints],\n    module: torch.nn.Module,\n    backend: str,\n) -> None:\n    import os\n    import torch\n    import torch.distributed as dist\n    from torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder\n    from torchrec.distributed.model_parallel import DistributedModelParallel\n    from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology\n    from torchrec.distributed.types import ModuleSharder, ShardingEnv\n    from typing import cast\n\n    def init_distributed_single_host(\n        rank: int,\n        world_size: int,\n        backend: str,\n        # pyre-fixme[11]: Annotation `ProcessGroup` is not defined as a type.\n    ) -> dist.ProcessGroup:\n        os.environ[\"RANK\"] = f\"{rank}\"\n        os.environ[\"WORLD_SIZE\"] = f\"{world_size}\"\n        dist.init_process_group(rank=rank, world_size=world_size, backend=backend)\n        return dist.group.WORLD\n\n    if backend == \"nccl\":\n        device = torch.device(f\"cuda:{rank}\")\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n    topology = Topology(world_size=world_size, compute_device=\"cuda\")\n    pg = init_distributed_single_host(rank, world_size, backend)\n    planner = EmbeddingShardingPlanner(\n        topology=topology,\n        constraints=constraints,\n    )\n    sharders = [cast(ModuleSharder[torch.nn.Module], EmbeddingBagCollectionSharder())]\n    plan: ShardingPlan = planner.collective_plan(module, sharders, pg)\n\n    sharded_model = DistributedModelParallel(\n        module,\n        env=ShardingEnv.from_process_group(pg),\n        plan=plan,\n        sharders=sharders,\n        device=device,\n    )\n    print(f\"rank:{rank},sharding plan: {plan}\")\n    return sharded_model \n```", "```py\nimport multiprocess\n\ndef spmd_sharing_simulation(\n    sharding_type: ShardingType = ShardingType.TABLE_WISE,\n    world_size = 2,\n):\n  ctx = multiprocess.get_context(\"spawn\")\n  processes = []\n  for rank in range(world_size):\n      p = ctx.Process(\n          target=single_rank_execution,\n          args=(\n              rank,\n              world_size,\n              gen_constraints(sharding_type),\n              ebc,\n              \"nccl\"\n          ),\n      )\n      p.start()\n      processes.append(p)\n\n  for p in processes:\n      p.join()\n      assert 0 == p.exitcode \n```", "```py\nspmd_sharing_simulation(ShardingType.TABLE_WISE) \n```", "```py\nrank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:0/cuda:0)])), 'large_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:0/cuda:0)])), 'small_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:1/cuda:1)]))}}\nrank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:0/cuda:0)])), 'large_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:0/cuda:0)])), 'small_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:1/cuda:1)]))}} \n```", "```py\nspmd_sharing_simulation(ShardingType.ROW_WISE) \n```", "```py\nrank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)]))}}\nrank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)]))}} \n```", "```py\nspmd_sharing_simulation(ShardingType.COLUMN_WISE) \n```", "```py\nrank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)]))}}\nrank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)]))}} \n```", "```py\nspmd_sharing_simulation(ShardingType.DATA_PARALLEL) \n```", "```py\nrank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'large_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None)}}\nrank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'large_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None)}} \n```"]