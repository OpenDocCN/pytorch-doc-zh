["```py\n!pip  install  -U  portalocker>=2.0.0` \n```", "```py\nimport torch\nfrom torchtext.datasets import [AG_NEWS](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.AG_NEWS \"torchtext.datasets.AG_NEWS\")\n\ntrain_iter = iter([AG_NEWS](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.AG_NEWS \"torchtext.datasets.AG_NEWS\")(split=\"train\")) \n```", "```py\nnext(train_iter)\n>>>  (3,  \"Fears for T N pension after talks Unions representing workers at Turner\nNewall say they are 'disappointed' after talks with stricken parent firm Federal\nMogul.\")\n\nnext(train_iter)\n>>>  (4,  \"The Race is On: Second Private Team Sets Launch Date for Human\nSpaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\\\team of\nrocketeers competing for the  #36;10 million Ansari X Prize, a contest\nfor\\\\privately funded suborbital space flight, has officially announced\nthe first\\\\launch date for its manned rocket.\")\n\nnext(train_iter)\n>>>  (4,  'Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded\nby a chemistry researcher at the University of Louisville won a grant to develop\na method of producing better peptides, which are short chains of amino acids, the\nbuilding blocks of proteins.') \n```", "```py\nfrom torchtext.data.utils import [get_tokenizer](https://pytorch.org/text/stable/data_utils.html#torchtext.data.utils.get_tokenizer \"torchtext.data.utils.get_tokenizer\")\nfrom torchtext.vocab import [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")\n\ntokenizer = [get_tokenizer](https://pytorch.org/text/stable/data_utils.html#torchtext.data.utils.get_tokenizer \"torchtext.data.utils.get_tokenizer\")(\"basic_english\")\ntrain_iter = [AG_NEWS](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.AG_NEWS \"torchtext.datasets.AG_NEWS\")(split=\"train\")\n\ndef yield_tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n\n[vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\") = [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")(yield_tokens(train_iter), specials=[\"<unk>\"])\n[vocab.set_default_index](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.set_default_index \"torchtext.vocab.Vocab.set_default_index\")([vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\")[\"<unk>\"]) \n```", "```py\nvocab(['here',  'is',  'an',  'example'])\n>>>  [475,  21,  30,  5297] \n```", "```py\ntext_pipeline = lambda x: [vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\")(tokenizer(x))\nlabel_pipeline = lambda x: int(x) - 1 \n```", "```py\ntext_pipeline('here is the an example')\n>>>  [475,  21,  2,  30,  5297]\nlabel_pipeline('10')\n>>>  9 \n```", "```py\nfrom torch.utils.data import [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")\n\n[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")(\"cuda\" if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available \"torch.cuda.is_available\")() else \"cpu\")\n\ndef collate_batch(batch):\n    label_list, text_list, offsets = [], [], [0]\n    for _label, _text in batch:\n        label_list.append(label_pipeline(_label))\n        processed_text = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")(text_pipeline(_text), dtype=[torch.int64](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n        text_list.append(processed_text)\n        offsets.append(processed_text.size(0))\n    label_list = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")(label_list, dtype=[torch.int64](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n    offsets = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")(offsets[:-1]).cumsum(dim=0)\n    text_list = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")(text_list)\n    return label_list.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")), text_list.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")), offsets.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\ntrain_iter = [AG_NEWS](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.AG_NEWS \"torchtext.datasets.AG_NEWS\")(split=\"train\")\n[dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")(\n    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n) \n```", "```py\nfrom torch import nn\n\nclass TextClassificationModel([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super([TextClassificationModel](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.embedding = [nn.EmbeddingBag](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag \"torch.nn.EmbeddingBag\")(vocab_size, embed_dim, sparse=False)\n        self.fc = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(embed_dim, num_class)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded) \n```", "```py\n1  :  World\n2  :  Sports\n3  :  Business\n4  :  Sci/Tec \n```", "```py\ntrain_iter = [AG_NEWS](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.AG_NEWS \"torchtext.datasets.AG_NEWS\")(split=\"train\")\nnum_class = len(set([label for (label, text) in train_iter]))\nvocab_size = len([vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\"))\nemsize = 64\nmodel = [TextClassificationModel](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(vocab_size, emsize, num_class).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")) \n```", "```py\nimport time\n\ndef train([dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")):\n    [model.train](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train \"torch.nn.Module.train\")()\n    total_acc, total_count = 0, 0\n    log_interval = 500\n    start_time = time.time()\n\n    for idx, (label, text, offsets) in enumerate([dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")):\n        [optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.zero_grad \"torch.optim.SGD.zero_grad\")()\n        predicted_label = model(text, offsets)\n        loss = [criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")(predicted_label, label)\n        loss.backward()\n        [torch.nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_ \"torch.nn.utils.clip_grad_norm_\")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")(), 0.1)\n        [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\").step()\n        total_acc += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n        if idx % log_interval == 0 and idx > 0:\n            elapsed = time.time() - start_time\n            print(\n                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n                \"| accuracy {:8.3f}\".format(\n                    epoch, idx, len([dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")), total_acc / total_count\n                )\n            )\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\ndef evaluate([dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")):\n    [model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")()\n    total_acc, total_count = 0, 0\n\n    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n        for idx, (label, text, offsets) in enumerate([dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")):\n            predicted_label = model(text, offsets)\n            loss = [criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")(predicted_label, label)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    return total_acc / total_count \n```", "```py\nfrom torch.utils.data.dataset import [random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split \"torch.utils.data.random_split\")\nfrom torchtext.data.functional import [to_map_style_dataset](https://pytorch.org/text/stable/data_functional.html#torchtext.data.functional.to_map_style_dataset \"torchtext.data.functional.to_map_style_dataset\")\n\n# Hyperparameters\nEPOCHS = 10  # epoch\nLR = 5  # learning rate\nBATCH_SIZE = 64  # batch size for training\n\n[criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\") = [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\n[optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\") = [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")(), lr=LR)\n[scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\") = [torch.optim.lr_scheduler.StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\")([optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\"), 1.0, gamma=0.1)\ntotal_accu = None\ntrain_iter, test_iter = [AG_NEWS](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.AG_NEWS \"torchtext.datasets.AG_NEWS\")()\ntrain_dataset = [to_map_style_dataset](https://pytorch.org/text/stable/data_functional.html#torchtext.data.functional.to_map_style_dataset \"torchtext.data.functional.to_map_style_dataset\")(train_iter)\ntest_dataset = [to_map_style_dataset](https://pytorch.org/text/stable/data_functional.html#torchtext.data.functional.to_map_style_dataset \"torchtext.data.functional.to_map_style_dataset\")(test_iter)\nnum_train = int(len(train_dataset) * 0.95)\n[split_train_](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\"), [split_valid_](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\") = [random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split \"torch.utils.data.random_split\")(\n    train_dataset, [num_train, len(train_dataset) - num_train]\n)\n\n[train_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")(\n    [split_train_](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\"), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n)\n[valid_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")(\n    [split_valid_](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\"), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n)\n[test_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")(\n    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n)\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_start_time = time.time()\n    train([train_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"))\n    accu_val = evaluate([valid_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"))\n    if total_accu is not None and total_accu > accu_val:\n        [scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\").step()\n    else:\n        total_accu = accu_val\n    print(\"-\" * 59)\n    print(\n        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n        \"valid accuracy {:8.3f} \".format(\n            epoch, time.time() - epoch_start_time, accu_val\n        )\n    )\n    print(\"-\" * 59) \n```", "```py\n| epoch   1 |   500/ 1782 batches | accuracy    0.694\n| epoch   1 |  1000/ 1782 batches | accuracy    0.856\n| epoch   1 |  1500/ 1782 batches | accuracy    0.877\n-----------------------------------------------------------\n| end of epoch   1 | time: 11.29s | valid accuracy    0.886\n-----------------------------------------------------------\n| epoch   2 |   500/ 1782 batches | accuracy    0.898\n| epoch   2 |  1000/ 1782 batches | accuracy    0.899\n| epoch   2 |  1500/ 1782 batches | accuracy    0.906\n-----------------------------------------------------------\n| end of epoch   2 | time: 10.99s | valid accuracy    0.895\n-----------------------------------------------------------\n| epoch   3 |   500/ 1782 batches | accuracy    0.916\n| epoch   3 |  1000/ 1782 batches | accuracy    0.913\n| epoch   3 |  1500/ 1782 batches | accuracy    0.915\n-----------------------------------------------------------\n| end of epoch   3 | time: 10.97s | valid accuracy    0.894\n-----------------------------------------------------------\n| epoch   4 |   500/ 1782 batches | accuracy    0.930\n| epoch   4 |  1000/ 1782 batches | accuracy    0.932\n| epoch   4 |  1500/ 1782 batches | accuracy    0.929\n-----------------------------------------------------------\n| end of epoch   4 | time: 10.97s | valid accuracy    0.902\n-----------------------------------------------------------\n| epoch   5 |   500/ 1782 batches | accuracy    0.932\n| epoch   5 |  1000/ 1782 batches | accuracy    0.933\n| epoch   5 |  1500/ 1782 batches | accuracy    0.931\n-----------------------------------------------------------\n| end of epoch   5 | time: 10.92s | valid accuracy    0.902\n-----------------------------------------------------------\n| epoch   6 |   500/ 1782 batches | accuracy    0.933\n| epoch   6 |  1000/ 1782 batches | accuracy    0.932\n| epoch   6 |  1500/ 1782 batches | accuracy    0.935\n-----------------------------------------------------------\n| end of epoch   6 | time: 10.91s | valid accuracy    0.903\n-----------------------------------------------------------\n| epoch   7 |   500/ 1782 batches | accuracy    0.934\n| epoch   7 |  1000/ 1782 batches | accuracy    0.933\n| epoch   7 |  1500/ 1782 batches | accuracy    0.935\n-----------------------------------------------------------\n| end of epoch   7 | time: 10.90s | valid accuracy    0.903\n-----------------------------------------------------------\n| epoch   8 |   500/ 1782 batches | accuracy    0.935\n| epoch   8 |  1000/ 1782 batches | accuracy    0.933\n| epoch   8 |  1500/ 1782 batches | accuracy    0.935\n-----------------------------------------------------------\n| end of epoch   8 | time: 10.91s | valid accuracy    0.904\n-----------------------------------------------------------\n| epoch   9 |   500/ 1782 batches | accuracy    0.934\n| epoch   9 |  1000/ 1782 batches | accuracy    0.934\n| epoch   9 |  1500/ 1782 batches | accuracy    0.934\n-----------------------------------------------------------\n| end of epoch   9 | time: 10.90s | valid accuracy    0.904\n-----------------------------------------------------------\n| epoch  10 |   500/ 1782 batches | accuracy    0.934\n| epoch  10 |  1000/ 1782 batches | accuracy    0.936\n| epoch  10 |  1500/ 1782 batches | accuracy    0.933\n-----------------------------------------------------------\n| end of epoch  10 | time: 10.91s | valid accuracy    0.905\n----------------------------------------------------------- \n```", "```py\nprint(\"Checking the results of test dataset.\")\naccu_test = evaluate([test_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"))\nprint(\"test accuracy {:8.3f}\".format(accu_test)) \n```", "```py\nChecking the results of test dataset.\ntest accuracy    0.907 \n```", "```py\nag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n\ndef predict(text, text_pipeline):\n    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n        text = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")(text_pipeline(text))\n        output = model(text, [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")([0]))\n        return output.argmax(1).item() + 1\n\nex_text_str = \"MEMPHIS, Tenn. \u2013 Four days ago, Jon Rahm was \\\n enduring the season\u2019s worst weather conditions on Sunday at The \\\n Open on his way to a closing 75 at Royal Portrush, which \\\n considering the wind and the rain was a respectable showing. \\\n Thursday\u2019s first round at the WGC-FedEx St. Jude Invitational \\\n was another story. With temperatures in the mid-80s and hardly any \\\n wind, the Spaniard was 13 strokes better in a flawless round. \\\n Thanks to his best putting performance on the PGA Tour, Rahm \\\n finished with an 8-under 62 for a three-stroke lead, which \\\n was even more impressive considering he\u2019d never played the \\\n front nine at TPC Southwind.\"\n\nmodel = [model.to](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to \"torch.nn.Module.to\")(\"cpu\")\n\nprint(\"This is a %s news\" % ag_news_label[predict(ex_text_str, text_pipeline)]) \n```", "```py\nThis is a Sports news \n```"]