- en: Distributed Pipeline Parallelism Using RPC
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RPC进行分布式管道并行
- en: 原文：[https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html](https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html](https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html)
- en: '**Author**: [Shen Li](https://mrshenli.github.io/)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：[Shen Li](https://mrshenli.github.io/)
- en: Note
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '![edit](../Images/a8aa37bcc5edbf2ba5fcf18dba1e55f9.png) View and edit this
    tutorial in [github](https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_pipeline_parallel_tutorial.rst).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '![edit](../Images/a8aa37bcc5edbf2ba5fcf18dba1e55f9.png) 在[github](https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_pipeline_parallel_tutorial.rst)中查看并编辑本教程。'
- en: 'Prerequisites:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 先决条件：
- en: '[PyTorch Distributed Overview](../beginner/dist_overview.html)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch分布式概述](../beginner/dist_overview.html)'
- en: '[Single-Machine Model Parallel Best Practices](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[单机模型并行最佳实践](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)'
- en: '[Getting started with Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用分布式RPC框架](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)'
- en: 'RRef helper functions: [RRef.rpc_sync()](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.RRef.rpc_sync),
    [RRef.rpc_async()](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.RRef.rpc_async),
    and [RRef.remote()](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.RRef.remote)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RRef辅助函数：[RRef.rpc_sync()](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.RRef.rpc_sync)、[RRef.rpc_async()](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.RRef.rpc_async)和[RRef.remote()](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.RRef.remote)
- en: This tutorial uses a Resnet50 model to demonstrate implementing distributed
    pipeline parallelism with [torch.distributed.rpc](https://pytorch.org/docs/master/rpc.html)
    APIs. This can be viewed as the distributed counterpart of the multi-GPU pipeline
    parallelism discussed in [Single-Machine Model Parallel Best Practices](model_parallel_tutorial.html).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程使用Resnet50模型演示了如何使用[torch.distributed.rpc](https://pytorch.org/docs/master/rpc.html)
    API实现分布式管道并行。这可以看作是[单机模型并行最佳实践](model_parallel_tutorial.html)中讨论的多GPU管道并行的分布式对应。
- en: Note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This tutorial requires PyTorch v1.6.0 or above.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程要求使用PyTorch v1.6.0或更高版本。
- en: Note
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Full source code of this tutorial can be found at [pytorch/examples](https://github.com/pytorch/examples/tree/master/distributed/rpc/pipeline).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的完整源代码可以在[pytorch/examples](https://github.com/pytorch/examples/tree/master/distributed/rpc/pipeline)找到。
- en: Basics
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础知识
- en: The previous tutorial, [Getting Started with Distributed RPC Framework](rpc_tutorial.html)
    shows how to use [torch.distributed.rpc](https://pytorch.org/docs/master/rpc.html)
    to implement distributed model parallelism for an RNN model. That tutorial uses
    one GPU to host the `EmbeddingTable`, and the provided code works fine. However,
    if a model lives on multiple GPUs, it would require some extra steps to increase
    the amortized utilization of all GPUs. Pipeline parallelism is one type of paradigm
    that can help in this case.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的教程[开始使用分布式RPC框架](rpc_tutorial.html)展示了如何使用[torch.distributed.rpc](https://pytorch.org/docs/master/rpc.html)为RNN模型实现分布式模型并行。该教程使用一个GPU来托管`EmbeddingTable`，提供的代码可以正常工作。但是，如果一个模型存在于多个GPU上，就需要一些额外的步骤来增加所有GPU的摊销利用率。管道并行是一种可以在这种情况下有所帮助的范式之一。
- en: In this tutorial, we use `ResNet50` as an example model which is also used by
    the [Single-Machine Model Parallel Best Practices](model_parallel_tutorial.html)
    tutorial. Similarly, the `ResNet50` model is divided into two shards and the input
    batch is partitioned into multiple splits and fed into the two model shards in
    a pipelined fashion. The difference is that, instead of parallelizing the execution
    using CUDA streams, this tutorial invokes asynchronous RPCs. So, the solution
    presented in this tutorial also works across machine boundaries. The remainder
    of this tutorial presents the implementation in four steps.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们以`ResNet50`作为示例模型，该模型也被[单机模型并行最佳实践](model_parallel_tutorial.html)教程使用。类似地，`ResNet50`模型被分成两个分片，并且输入批次被分成多个部分并以流水线方式馈送到两个模型分片中。不同之处在于，本教程使用异步RPC调用来并行执行，而不是使用CUDA流来并行执行。因此，本教程中提出的解决方案也适用于跨机器边界。本教程的其余部分将以四个步骤呈现实现。
- en: 'Step 1: Partition ResNet50 Model'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：对ResNet50模型进行分区
- en: This is the preparation step which implements `ResNet50` in two model shards.
    The code below is borrowed from the [ResNet implementation in torchvision](https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L124).
    The `ResNetBase` module contains the common building blocks and attributes for
    the two ResNet shards.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是准备步骤，实现了在两个模型分片中的`ResNet50`。下面的代码是从[torchvision中的ResNet实现](https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L124)借用的。`ResNetBase`模块包含了两个ResNet分片的共同构建块和属性。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, we are ready to define the two model shards. For the constructor, we simply
    split all ResNet50 layers into two parts and move each part into the provided
    device. The `forward` functions of both shards take an `RRef` of the input data,
    fetch the data locally, and then move it to the expected device. After applying
    all layers to the input, it moves the output to CPU and returns. It is because
    the RPC API requires tensors to reside on CPU to avoid invalid device errors when
    the numbers of devices in the caller and the callee do not match.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备定义两个模型分片。对于构造函数，我们简单地将所有ResNet50层分成两部分，并将每部分移动到提供的设备上。这两个分片的`forward`函数接受输入数据的`RRef`，在本地获取数据，然后将其移动到预期的设备上。在将所有层应用于输入后，将输出移动到CPU并返回。这是因为RPC
    API要求张量驻留在CPU上，以避免在调用方和被调用方的设备数量不匹配时出现无效设备错误。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 2: Stitch ResNet50 Model Shards Into One Module'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：将ResNet50模型分片拼接成一个模块
- en: Then, we create a `DistResNet50` module to assemble the two shards and implement
    the pipeline parallel logic. In the constructor, we use two `rpc.remote` calls
    to put the two shards on two different RPC workers respectively and hold on to
    the `RRef` to the two model parts so that they can be referenced in the forward
    pass. The `forward` function splits the input batch into multiple micro-batches,
    and feeds these micro-batches to the two model parts in a pipelined fashion. It
    first uses an `rpc.remote` call to apply the first shard to a micro-batch and
    then forwards the returned intermediate output `RRef` to the second model shard.
    After that, it collects the `Future` of all micro-outputs, and waits for all of
    them after the loop. Note that both `remote()` and `rpc_async()` return immediately
    and run asynchronously. Therefore, the entire loop is non-blocking, and will launch
    multiple RPCs concurrently. The execution order of one micro-batch on two model
    parts are preserved by intermediate output `y_rref`. The execution order across
    micro-batches does not matter. In the end, the forward function concatenates outputs
    of all micro-batches into one single output tensor and returns. The `parameter_rrefs`
    function is a helper to simplify distributed optimizer construction, which will
    be used later.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个`DistResNet50`模块来组装两个分片并实现管道并行逻辑。在构造函数中，我们使用两个`rpc.remote`调用分别将两个分片放在两个不同的RPC工作进程上，并保留两个模型部分的`RRef`，以便它们可以在前向传递中引用。`forward`函数将输入批次分成多个微批次，并以管道方式将这些微批次馈送到两个模型部分。它首先使用`rpc.remote`调用将第一个分片应用于微批次，然后将返回的中间输出`RRef`转发到第二个模型分片。之后，它收集所有微输出的`Future`，并在循环后等待所有微输出。请注意，`remote()`和`rpc_async()`都会立即返回并异步运行。因此，整个循环是非阻塞的，并且将同时启动多个RPC。通过中间输出`y_rref`保留了两个模型部分上一个微批次的执行顺序。跨微批次的执行顺序并不重要。最后，forward函数将所有微批次的输出连接成一个单一的输出张量并返回。`parameter_rrefs`函数是一个辅助函数，用于简化分布式优化器的构建，稍后将使用它。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 3: Define The Training Loop'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：定义训练循环
- en: After defining the model, let us implement the training loop. We use a dedicated
    “master” worker to prepare random inputs and labels, and control the distributed
    backward pass and distributed optimizer step. It first creates an instance of
    the `DistResNet50` module. It specifies the number of micro-batches for each batch,
    and also provides the name of the two RPC workers (i.e., “worker1”, and “worker2”).
    Then it defines the loss function and creates a `DistributedOptimizer` using the
    `parameter_rrefs()` helper to acquire a list of parameter `RRefs`. Then, the main
    training loop is very similar to regular local training, except that it uses `dist_autograd`
    to launch backward and provides the `context_id` for both backward and optimizer
    `step()`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义模型之后，让我们实现训练循环。我们使用一个专用的“主”工作进程来准备随机输入和标签，并控制分布式反向传递和分布式优化器步骤。首先创建一个`DistResNet50`模块的实例。它指定每个批次的微批次数量，并提供两个RPC工作进程的名称（即“worker1”和“worker2”）。然后定义损失函数，并使用`parameter_rrefs()`助手创建一个`DistributedOptimizer`来获取参数`RRefs`的列表。然后，主要训练循环与常规本地训练非常相似，只是它使用`dist_autograd`来启动反向传递，并为反向传递和优化器`step()`提供`context_id`。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 4: Launch RPC Processes'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4：启动RPC进程
- en: Finally, the code below shows the target function for all processes. The main
    logic is defined in `run_master`. The workers passively waiting for commands from
    the master, and hence simply runs `init_rpc` and `shutdown`, where the `shutdown`
    by default will block until all RPC participants finish.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，下面的代码展示了所有进程的目标函数。主要逻辑在`run_master`中定义。工作进程 passively 等待来自主进程的命令，因此只需运行`init_rpc`和`shutdown`，其中`shutdown`默认情况下将阻塞，直到所有RPC参与者完成。
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
