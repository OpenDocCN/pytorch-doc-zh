# 可复制性

> 原文：[https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html)

在PyTorch发布、单个提交或不同平台之间不能保证完全可复制的结果。此外，即使使用相同的种子，在CPU和GPU执行之间的结果也可能无法复制。

然而，您可以采取一些步骤来限制特定平台、设备和PyTorch发布的不确定行为源的数量。首先，您可以控制可能导致应用程序多次执行时行为不同的随机性源。其次，您可以配置PyTorch以避免对某些操作使用非确定性算法，以便对这些操作进行多次调用时，给定相同的输入，将产生相同的结果。

警告

确定性操作通常比非确定性操作慢，因此您的模型的单次运行性能可能会降低。然而，确定性可能会通过促进实验、调试和回归测试来节省开发时间。

## 控制随机性源[](#controlling-sources-of-randomness "Permalink to this heading")

### PyTorch随机数生成器[](#pytorch-random-number-generator "Permalink to this heading")

您可以使用[`torch.manual_seed()`](../generated/torch.manual_seed.html#torch.manual_seed "torch.manual_seed")为所有设备（CPU和CUDA）设置RNG的种子：

```py
import torch
torch.manual_seed(0) 
```

一些PyTorch操作可能在内部使用随机数。例如，[`torch.svd_lowrank()`](../generated/torch.svd_lowrank.html#torch.svd_lowrank "torch.svd_lowrank")就是这样。因此，连续多次使用相同的输入参数调用它可能会产生不同的结果。然而，只要在应用程序开头将[`torch.manual_seed()`](../generated/torch.manual_seed.html#torch.manual_seed "torch.manual_seed")设置为常量，并且已经消除了所有其他不确定性源，每次在相同环境中运行应用程序时都会生成相同系列的随机数。

通过在连续调用之间将[`torch.manual_seed()`](../generated/torch.manual_seed.html#torch.manual_seed "torch.manual_seed")设置为相同的值，也可以从使用随机数的操作中获得相同的结果。

### Python

对于自定义操作符，您可能还需要设置python种子：

```py
import random
random.seed(0) 
```

### 其他库中的随机数生成器[](#random-number-generators-in-other-libraries "Permalink to this heading")

如果您或您正在使用的任何库依赖于NumPy，您可以使用以下方法为全局NumPy RNG设置种子：

```py
import numpy as np
np.random.seed(0) 
```

然而，一些应用程序和库可能使用NumPy随机生成器对象，而不是全局RNG ([https://numpy.org/doc/stable/reference/random/generator.html](https://numpy.org/doc/stable/reference/random/generator.html))，这些对象也需要一致地设置种子。

如果您正在使用任何其他使用随机数生成器的库，请参考这些库的文档，看看如何为它们设置一致的种子。

### CUDA卷积基准测试[](#cuda-convolution-benchmarking "Permalink to this heading")

由CUDA卷积操作使用的cuDNN库可能是应用程序多次执行中的不确定性源。当使用新的大小参数集调用cuDNN卷积时，一个可选功能可以运行多个卷积算法，并对它们进行基准测试以找到最快的算法。然后，在接下来的过程中，将始终使用最快的算法来处理相应的大小参数集。由于基准测试噪声和不同的硬件，基准测试可能会在后续运行中选择不同的算法，即使在同一台机器上也是如此。

通过使用`torch.backends.cudnn.benchmark = False`禁用基准测试功能，可以使cuDNN确定性地选择算法，可能会以降低性能为代价。

然而，如果您不需要在应用程序的多次执行之间实现可重现性，则启用基准测试功能可能会提高性能，方法是使用`torch.backends.cudnn.benchmark = True`。

请注意，此设置与下面讨论的`torch.backends.cudnn.deterministic`设置不同。

## 避免非确定性算法[](#avoiding-nondeterministic-algorithms "跳转到此标题")

[`torch.use_deterministic_algorithms()`](../generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms "torch.use_deterministic_algorithms")允许您配置PyTorch使用确定性算法，而不是非确定性算法（如果有的话），并且如果已知某个操作是非确定性的（且没有确定性替代方案），则会引发错误。

请查看[`torch.use_deterministic_algorithms()`](../generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms "torch.use_deterministic_algorithms")文档，获取受影响操作的完整列表。如果某个操作未按照文档正确执行，或者您需要一个没有确定性实现的操作的确定性实现，请提交一个问题：[https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22](https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22)

例如，运行[`torch.Tensor.index_add_()`](../generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_ "torch.Tensor.index_add_")的非确定性CUDA实现将引发错误：

```py
>>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set
'torch.use_deterministic_algorithms(True)'. ... 
```

当使用稀疏-稠密CUDA张量调用[`torch.bmm()`](../generated/torch.bmm.html#torch.bmm "torch.bmm")时，通常会使用一个非确定性算法，但当打开确定性标志时，将使用其备用确定性实现：

```py
>>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())
tensor([[[ 1.1900, -2.3409],
 [ 0.4796,  0.8003]],
 [[ 0.1509,  1.8027],
 [ 0.0333, -1.1444]]], device='cuda:0') 
```

此外，如果您正在使用CUDA张量，并且CUDA版本为10.2或更高，则应根据CUDA文档设置环境变量CUBLAS_WORKSPACE_CONFIG：[https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility](https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility)

### CUDA卷积确定性性[](#cuda-convolution-determinism "跳转到此标题")

虽然禁用CUDA卷积基准测试（如上所述）确保CUDA每次运行应用程序时选择相同的算法，但该算法本身可能是非确定性的，除非设置`torch.use_deterministic_algorithms(True)`或`torch.backends.cudnn.deterministic = True`。后者仅控制此行为，不像[`torch.use_deterministic_algorithms()`](../generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms "torch.use_deterministic_algorithms")会使其他PyTorch操作也表现出确定性。

### CUDA RNN和LSTM

在某些版本的CUDA中，RNN和LSTM网络可能具有非确定性行为。有关详细信息和解决方法，请参阅[`torch.nn.RNN()`](../generated/torch.nn.RNN.html#torch.nn.RNN "torch.nn.RNN")和[`torch.nn.LSTM()`](../generated/torch.nn.LSTM.html#torch.nn.LSTM "torch.nn.LSTM")。

### 填充未初始化内存[](#filling-uninitialized-memory "跳转到此标题")

像[`torch.empty()`](../generated/torch.empty.html#torch.empty "torch.empty")和[`torch.Tensor.resize_()`](../generated/torch.Tensor.resize_.html#torch.Tensor.resize_ "torch.Tensor.resize_")这样的操作可能返回具有未初始化内存的张量，其中包含未定义的值。如果需要确定性，将这样的张量用作另一个操作的输入是无效的，因为输出将是不确定的。但实际上没有任何东西可以阻止运行这种无效代码。因此，为了安全起见，默认情况下将[`torch.utils.deterministic.fill_uninitialized_memory`](../deterministic.html#torch.utils.deterministic.fill_uninitialized_memory "torch.utils.deterministic.fill_uninitialized_memory")设置为`True`，如果设置了`torch.use_deterministic_algorithms(True)`，则会使用已知值填充未初始化的内存。这将防止这种非确定性行为的可能性。

然而，填充未初始化内存对性能有害。因此，如果您的程序有效且不将未初始化内存用作操作的输入，则可以关闭此设置以获得更好的性能。

## DataLoader

DataLoader将根据[多进程数据加载中的随机性](../data.html#data-loading-randomness)算法重新播种工作进程。使用`worker_init_fn()`和生成器来保持可重现性：

```py
def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    numpy.random.seed(worker_seed)
    random.seed(worker_seed)

g = torch.Generator()
g.manual_seed(0)

DataLoader(
    train_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    worker_init_fn=seed_worker,
    generator=g,
) 
```
