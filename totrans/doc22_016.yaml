- en: Frequently Asked Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/notes/faq.html](https://pytorch.org/docs/stable/notes/faq.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'My model reports “cuda runtime error(2): out of memory”[](#my-model-reports-cuda-runtime-error-2-out-of-memory
    "Permalink to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the error message suggests, you have run out of memory on your GPU. Since
    we often deal with large amounts of data in PyTorch, small mistakes can rapidly
    cause your program to use up all of your GPU; fortunately, the fixes in these
    cases are often simple. Here are a few common things to check:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Don’t accumulate history across your training loop.** By default, computations
    involving variables that require gradients will keep history. This means that
    you should avoid using such variables in computations which will live beyond your
    training loops, e.g., when tracking statistics. Instead, you should detach the
    variable or access its underlying data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, it can be non-obvious when differentiable variables can occur. Consider
    the following training loop (abridged from [source](https://discuss.pytorch.org/t/high-memory-usage-while-training/162)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, `total_loss` is accumulating history across your training loop, since
    `loss` is a differentiable variable with autograd history. You can fix this by
    writing total_loss += float(loss) instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other instances of this problem: [1](https://discuss.pytorch.org/t/resolved-gpu-out-of-memory-error-with-batch-size-1/3719).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Don’t hold onto tensors and variables you don’t need.** If you assign a Tensor
    or Variable to a local, Python will not deallocate until the local goes out of
    scope. You can free this reference by using `del x`. Similarly, if you assign
    a Tensor or Variable to a member variable of an object, it will not deallocate
    until the object goes out of scope. You will get the best memory usage if you
    don’t hold onto temporaries you don’t need.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The scopes of locals can be larger than you expect. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, `intermediate` remains live even while `h` is executing, because its scope
    extrudes past the end of the loop. To free it earlier, you should `del intermediate`
    when you are done with it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Avoid running RNNs on sequences that are too large.** The amount of memory
    required to backpropagate through an RNN scales linearly with the length of the
    RNN input; thus, you will run out of memory if you try to feed an RNN a sequence
    that is too long.'
  prefs: []
  type: TYPE_NORMAL
- en: The technical term for this phenomenon is [backpropagation through time](https://en.wikipedia.org/wiki/Backpropagation_through_time),
    and there are plenty of references for how to implement truncated BPTT, including
    in the [word language model](https://github.com/pytorch/examples/tree/master/word_language_model)
    example; truncation is handled by the `repackage` function as described in [this
    forum post](https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226).
  prefs: []
  type: TYPE_NORMAL
- en: '**Don’t use linear layers that are too large.** A linear layer `nn.Linear(m,
    n)` uses <math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>m</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nm)</annotation></semantics></math>O(nm)
    memory: that is to say, the memory requirements of the weights scales quadratically
    with the number of features. It is very easy to [blow through your memory](https://github.com/pytorch/pytorch/issues/958)
    this way (and remember that you will need at least twice the size of the weights,
    since you also need to store the gradients.)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consider checkpointing.** You can trade-off memory for compute by using [checkpoint](https://pytorch.org/docs/stable/checkpoint.html).'
  prefs: []
  type: TYPE_NORMAL
- en: My GPU memory isn’t freed properly[](#my-gpu-memory-isn-t-freed-properly "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch uses a caching memory allocator to speed up memory allocations. As a
    result, the values shown in `nvidia-smi` usually don’t reflect the true memory
    usage. See [Memory management](cuda.html#cuda-memory-management) for more details
    about GPU memory management.
  prefs: []
  type: TYPE_NORMAL
- en: If your GPU memory isn’t freed even after Python quits, it is very likely that
    some Python subprocesses are still alive. You may find them via `ps -elf | grep
    python` and manually kill them with `kill -9 [pid]`.
  prefs: []
  type: TYPE_NORMAL
- en: My out of memory exception handler can’t allocate memory[](#my-out-of-memory-exception-handler-can-t-allocate-memory
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have some code that tries to recover from out of memory errors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: But find that when you do run out of memory, your recovery code can’t allocate
    either. That’s because the python exception object holds a reference to the stack
    frame where the error was raised. Which prevents the original tensor objects from
    being freed. The solution is to move you OOM recovery code outside of the `except`
    clause.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '## My data loader workers return identical random numbers[](#my-data-loader-workers-return-identical-random-numbers
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are likely using other libraries to generate random numbers in the dataset
    and worker subprocesses are started via `fork`. See [`torch.utils.data.DataLoader`](../data.html#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader")’s documentation for how to properly set up random
    seeds in workers with its `worker_init_fn` option.  ## My recurrent network doesn’t
    work with data parallelism[](#my-recurrent-network-doesn-t-work-with-data-parallelism
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a subtlety in using the `pack sequence -> recurrent network -> unpack
    sequence` pattern in a [`Module`](../generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module") with [`DataParallel`](../generated/torch.nn.DataParallel.html#torch.nn.DataParallel
    "torch.nn.DataParallel") or [`data_parallel()`](../nn.html#module-torch.nn.parallel.data_parallel
    "torch.nn.parallel.data_parallel"). Input to each the `forward()` on each device
    will only be part of the entire input. Because the unpack operation [`torch.nn.utils.rnn.pad_packed_sequence()`](../generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence
    "torch.nn.utils.rnn.pad_packed_sequence") by default only pads up to the longest
    input it sees, i.e., the longest on that particular device, size mismatches will
    happen when results are gathered together. Therefore, you can instead take advantage
    of the `total_length` argument of [`pad_packed_sequence()`](../generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence
    "torch.nn.utils.rnn.pad_packed_sequence") to make sure that the `forward()` calls
    return sequences of same length. For example, you can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, extra care needs to be taken when batch dimension is dim `1` (i.e.,
    `batch_first=False`) with data parallelism. In this case, the first argument of
    pack_padded_sequence `padding_input` will be of shape `[T x B x *]` and should
    be scattered along dim `1`, but the second argument `input_lengths` will be of
    shape `[B]` and should be scattered along dim `0`. Extra code to manipulate the
    tensor shapes will be needed.
  prefs: []
  type: TYPE_NORMAL
