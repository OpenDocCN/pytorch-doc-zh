- en: torch.onnx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/onnx.html](https://pytorch.org/docs/stable/onnx.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Open Neural Network eXchange (ONNX)](https://onnx.ai/) is an open standard
    format for representing machine learning models. The `torch.onnx` module captures
    the computation graph from a native PyTorch [`torch.nn.Module`](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module") model and converts it into an [ONNX graph](https://github.com/onnx/onnx/blob/main/docs/IR.md).'
  prefs: []
  type: TYPE_NORMAL
- en: The exported model can be consumed by any of the many [runtimes that support
    ONNX](https://onnx.ai/supported-tools.html#deployModel), including Microsoft’s
    [ONNX Runtime](https://www.onnxruntime.ai).
  prefs: []
  type: TYPE_NORMAL
- en: '**There are two flavors of ONNX exporter API that you can use, as listed below:**'
  prefs: []
  type: TYPE_NORMAL
- en: TorchDynamo-based ONNX Exporter[](#torchdynamo-based-onnx-exporter "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*The TorchDynamo-based ONNX exporter is the newest (and Beta) exporter for
    PyTorch 2.0 and newer*'
  prefs: []
  type: TYPE_NORMAL
- en: TorchDynamo engine is leveraged to hook into Python’s frame evaluation API and
    dynamically rewrite its bytecode into an FX Graph. The resulting FX Graph is then
    polished before it is finally translated into an ONNX graph.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of this approach is that the [FX graph](https://pytorch.org/docs/stable/fx.html)
    is captured using bytecode analysis that preserves the dynamic nature of the model
    instead of using traditional static tracing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '[Learn more about the TorchDynamo-based ONNX Exporter](onnx_dynamo.html)'
  prefs: []
  type: TYPE_NORMAL
- en: TorchScript-based ONNX Exporter[](#torchscript-based-onnx-exporter "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*The TorchScript-based ONNX exporter is available since PyTorch 1.2.0*'
  prefs: []
  type: TYPE_NORMAL
- en: '[TorchScript](https://pytorch.org/docs/stable/jit.html) is leveraged to trace
    (through [`torch.jit.trace()`](generated/torch.jit.trace.html#torch.jit.trace
    "torch.jit.trace")) the model and capture a static computation graph.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a consequence, the resulting graph has a couple limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: It does not record any control-flow, like if-statements or loops;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not handle nuances between `training` and `eval` mode;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not truly handle dynamic inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As an attempt to support the static tracing limitations, the exporter also supports
    TorchScript scripting (through [`torch.jit.script()`](generated/torch.jit.script.html#torch.jit.script
    "torch.jit.script")), which adds support for data-dependent control-flow, for
    example. However, TorchScript itself is a subset of the Python language, so not
    all features in Python are supported, such as in-place operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[Learn more about the TorchScript-based ONNX Exporter](onnx_torchscript.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Contributing / Developing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ONNX exporter is a community project and we welcome contributions. We follow
    the [PyTorch guidelines for contributions](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md),
    but you might also be interested in reading our [development wiki](https://github.com/pytorch/pytorch/wiki/PyTorch-ONNX-exporter).
  prefs: []
  type: TYPE_NORMAL
