["```py\nimport torch\nfrom torch.autograd.function import once_differentiable\nimport torch.nn.functional as F\n\ndef convolution_backward(grad_out, [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n    grad_input = [F.conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d \"torch.nn.functional.conv2d\")([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").transpose(0, 1), grad_out.transpose(0, 1)).transpose(0, 1)\n    grad_X = [F.conv_transpose2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose2d.html#torch.nn.functional.conv_transpose2d \"torch.nn.functional.conv_transpose2d\")(grad_out, [weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    return grad_X, grad_input\n\nclass Conv2D([torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function \"torch.autograd.Function\")):\n    @staticmethod\n    def forward(ctx, [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n        ctx.save_for_backward([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n        return [F.conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d \"torch.nn.functional.conv2d\")([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\n    # Use @once_differentiable by default unless we intend to double backward\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_out):\n        [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = ctx.saved_tensors\n        return convolution_backward(grad_out, [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) \n```", "```py\n[weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(5, 3, 3, 3, requires_grad=True, dtype=[torch.double](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n[X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(10, 3, 7, 7, requires_grad=True, dtype=[torch.double](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n[torch.autograd.gradcheck](https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.gradcheck \"torch.autograd.gradcheck\")(Conv2D.apply, ([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))) \n```", "```py\nTrue \n```", "```py\ndef unsqueeze_all(t):\n    # Helper function to ``unsqueeze`` all the dimensions that we reduce over\n    return t[None, :, None, None]\n\ndef batch_norm_backward(grad_out, [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), sum, sqrt_var, N, eps):\n    # We use the formula: ``out = (X - mean(X)) / (sqrt(var(X)) + eps)``\n    # in batch norm 2D forward. To simplify our derivation, we follow the\n    # chain rule and compute the gradients as follows before accumulating\n    # them all into a final grad_input.\n    #  1) ``grad of out wrt var(X)`` * ``grad of var(X) wrt X``\n    #  2) ``grad of out wrt mean(X)`` * ``grad of mean(X) wrt X``\n    #  3) ``grad of out wrt X in the numerator`` * ``grad of X wrt X``\n    # We then rewrite the formulas to use as few extra buffers as possible\n    tmp = (([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") - unsqueeze_all(sum) / N) * grad_out).sum(dim=(0, 2, 3))\n    tmp *= -1\n    d_denom = tmp / (sqrt_var + eps)**2  # ``d_denom = -num / denom**2``\n    # It is useful to delete tensors when you no longer need them with ``del``\n    # For example, we could've done ``del tmp`` here because we won't use it later\n    # In this case, it's not a big difference because ``tmp`` only has size of (C,)\n    # The important thing is avoid allocating NCHW-sized tensors unnecessarily\n    d_var = d_denom / (2 * sqrt_var)  # ``denom = torch.sqrt(var) + eps``\n    # Compute ``d_mean_dx`` before allocating the final NCHW-sized grad_input buffer\n    d_mean_dx = grad_out / unsqueeze_all(sqrt_var + eps)\n    d_mean_dx = unsqueeze_all(-d_mean_dx.sum(dim=(0, 2, 3)) / N)\n    # ``d_mean_dx`` has already been reassigned to a C-sized buffer so no need to worry\n\n    # ``(1) unbiased_var(x) = ((X - unsqueeze_all(mean))**2).sum(dim=(0, 2, 3)) / (N - 1)``\n    grad_input = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") * unsqueeze_all(d_var * N)\n    grad_input += unsqueeze_all(-d_var * sum)\n    grad_input *= 2 / ((N - 1) * N)\n    # (2) mean (see above)\n    grad_input += d_mean_dx\n    # (3) Add 'grad_out / <factor>' without allocating an extra buffer\n    grad_input *= unsqueeze_all(sqrt_var + eps)\n    grad_input += grad_out\n    grad_input /= unsqueeze_all(sqrt_var + eps)  # ``sqrt_var + eps > 0!``\n    return grad_input\n\nclass BatchNorm([torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function \"torch.autograd.Function\")):\n    @staticmethod\n    def forward(ctx, [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), eps=1e-3):\n        # Don't save ``keepdim`` values for backward\n        sum = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").sum(dim=(0, 2, 3))\n        var = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").var(unbiased=True, dim=(0, 2, 3))\n        N = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").numel() / [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").size(1)\n        sqrt_var = [torch.sqrt](https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt \"torch.sqrt\")(var)\n        ctx.save_for_backward([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n        ctx.eps = eps\n        ctx.sum = sum\n        ctx.N = N\n        ctx.sqrt_var = sqrt_var\n        mean = sum / N\n        denom = sqrt_var + eps\n        out = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") - unsqueeze_all(mean)\n        out /= unsqueeze_all(denom)\n        return out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_out):\n        [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), = ctx.saved_tensors\n        return batch_norm_backward(grad_out, [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), ctx.sum, ctx.sqrt_var, ctx.N, ctx.eps) \n```", "```py\n[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(1, 2, 3, 4, requires_grad=True, dtype=[torch.double](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n[torch.autograd.gradcheck](https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.gradcheck \"torch.autograd.gradcheck\")(BatchNorm.apply, ([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),), fast_mode=False) \n```", "```py\nTrue \n```", "```py\nclass FusedConvBN2DFunction([torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function \"torch.autograd.Function\")):\n    @staticmethod\n    def forward(ctx, [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), conv_weight, eps=1e-3):\n        assert [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").ndim == 4  # N, C, H, W\n        # (1) Only need to save this single buffer for backward!\n        ctx.save_for_backward([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), conv_weight)\n\n        # (2) Exact same Conv2D forward from example above\n        [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [F.conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d \"torch.nn.functional.conv2d\")([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), conv_weight)\n        # (3) Exact same BatchNorm2D forward from example above\n        sum = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").sum(dim=(0, 2, 3))\n        var = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").var(unbiased=True, dim=(0, 2, 3))\n        N = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").numel() / [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").size(1)\n        sqrt_var = [torch.sqrt](https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt \"torch.sqrt\")(var)\n        ctx.eps = eps\n        ctx.sum = sum\n        ctx.N = N\n        ctx.sqrt_var = sqrt_var\n        mean = sum / N\n        denom = sqrt_var + eps\n        # Try to do as many things in-place as possible\n        # Instead of `out = (X - a) / b`, doing `out = X - a; out /= b`\n        # avoids allocating one extra NCHW-sized buffer here\n        out = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") - unsqueeze_all(mean)\n        out /= unsqueeze_all(denom)\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), conv_weight, = ctx.saved_tensors\n        # (4) Batch norm backward\n        # (5) We need to recompute conv\n        X_conv_out = [F.conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d \"torch.nn.functional.conv2d\")([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), conv_weight)\n        grad_out = batch_norm_backward(grad_out, X_conv_out, ctx.sum, ctx.sqrt_var,\n                                       ctx.N, ctx.eps)\n        # (6) Conv2d backward\n        grad_X, grad_input = convolution_backward(grad_out, [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), conv_weight)\n        return grad_X, grad_input, None, None, None, None, None \n```", "```py\nimport torch.nn as nn\nimport math\n\nclass FusedConvBN([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, in_channels, out_channels, kernel_size, exp_avg_factor=0.1,\n                 eps=1e-3, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")=None, dtype=None):\n        super([FusedConvBN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        factory_kwargs = {'device': [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"), 'dtype': dtype}\n        # Conv parameters\n        weight_shape = (out_channels, in_channels, kernel_size, kernel_size)\n        self.conv_weight = [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\")([torch.empty](https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty \"torch.empty\")(*weight_shape, **factory_kwargs))\n        # Batch norm parameters\n        num_features = out_channels\n        self.num_features = num_features\n        self.eps = eps\n        # Initialize\n        self.reset_parameters()\n\n    def forward(self, [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n        return FusedConvBN2DFunction.apply([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), self.conv_weight, self.eps)\n\n    def reset_parameters(self) -> None:\n        [nn.init.kaiming_uniform_](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_ \"torch.nn.init.kaiming_uniform_\")(self.conv_weight, [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")=math.sqrt(5)) \n```", "```py\n[weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(5, 3, 3, 3, requires_grad=True, dtype=[torch.double](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n[X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(2, 3, 4, 4, requires_grad=True, dtype=[torch.double](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n[torch.autograd.gradcheck](https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.gradcheck \"torch.autograd.gradcheck\")(FusedConvBN2DFunction.apply, ([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))) \n```", "```py\nTrue \n```", "```py\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\")\n\n# Record memory allocated at the end of the forward pass\nmemory_allocated = [[],[]]\n\nclass Net([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, fused=True):\n        super([Net](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.fused = fused\n        if fused:\n            self.convbn1 = [FusedConvBN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(1, 32, 3)\n            self.convbn2 = [FusedConvBN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(32, 64, 3)\n        else:\n            self.conv1 = [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(1, 32, 3, 1, bias=False)\n            self.bn1 = [nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d \"torch.nn.BatchNorm2d\")(32, affine=False, track_running_stats=False)\n            self.conv2 = [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(32, 64, 3, 1, bias=False)\n            self.bn2 = [nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d \"torch.nn.BatchNorm2d\")(64, affine=False, track_running_stats=False)\n        self.fc1 = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(9216, 128)\n        self.dropout = [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(0.5)\n        self.fc2 = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(128, 10)\n\n    def forward(self, x):\n        if self.fused:\n            x = self.convbn1(x)\n        else:\n            x = self.conv1(x)\n            x = self.bn1(x)\n        [F.relu_](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu_.html#torch.nn.functional.relu_ \"torch.nn.functional.relu_\")(x)\n        if self.fused:\n            x = self.convbn2(x)\n        else:\n            x = self.conv2(x)\n            x = self.bn2(x)\n        [F.relu_](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu_.html#torch.nn.functional.relu_ \"torch.nn.functional.relu_\")(x)\n        x = [F.max_pool2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d \"torch.nn.functional.max_pool2d\")(x, 2)\n        [F.relu_](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu_.html#torch.nn.functional.relu_ \"torch.nn.functional.relu_\")(x)\n        x = x.flatten(1)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        [F.relu_](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu_.html#torch.nn.functional.relu_ \"torch.nn.functional.relu_\")(x)\n        x = self.fc2(x)\n        output = [F.log_softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax \"torch.nn.functional.log_softmax\")(x, dim=1)\n        if fused:\n            memory_allocated[0].append([torch.cuda.memory_allocated](https://pytorch.org/docs/stable/generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated \"torch.cuda.memory_allocated\")())\n        else:\n            memory_allocated[1].append([torch.cuda.memory_allocated](https://pytorch.org/docs/stable/generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated \"torch.cuda.memory_allocated\")())\n        return output\n\ndef train(model, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"), [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta \"torch.optim.Adadelta\"), epoch):\n    [model.train](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train \"torch.nn.Module.train\")()\n    for batch_idx, (data, target) in enumerate([train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")):\n        data, target = data.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")), target.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n        [optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta.zero_grad \"torch.optim.Adadelta.zero_grad\")()\n        output = model(data)\n        loss = [F.nll_loss](https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss \"torch.nn.functional.nll_loss\")(output, target)\n        loss.backward()\n        [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta \"torch.optim.Adadelta\").step()\n        if batch_idx % 2 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len([train_loader.dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST \"torchvision.datasets.MNIST\")),\n                100. * batch_idx / len([train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")), loss.item()))\n\ndef test(model, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"), [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")):\n    [model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")()\n    test_loss = 0\n    correct = 0\n    # Use inference mode instead of no_grad, for free improved test-time performance\n    with [torch.inference_mode](https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode \"torch.inference_mode\")():\n        for data, target in [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"):\n            data, target = data.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")), target.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n            output = model(data)\n            # sum up batch loss\n            test_loss += [F.nll_loss](https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss \"torch.nn.functional.nll_loss\")(output, target, reduction='sum').item()\n            # get the index of the max log-probability\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len([test_loader.dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST \"torchvision.datasets.MNIST\"))\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len([test_loader.dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST \"torchvision.datasets.MNIST\")),\n        100. * correct / len([test_loader.dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST \"torchvision.datasets.MNIST\"))))\n\nuse_cuda = [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available \"torch.cuda.is_available\")()\n[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")(\"cuda\" if use_cuda else \"cpu\")\ntrain_kwargs = {'batch_size': 2048}\ntest_kwargs = {'batch_size': 2048}\n\nif use_cuda:\n    cuda_kwargs = {'num_workers': 1,\n                   'pin_memory': True,\n                   'shuffle': True}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n[transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\") = [transforms.Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\")([\n    [transforms.ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor \"torchvision.transforms.ToTensor\")(),\n    [transforms.Normalize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize \"torchvision.transforms.Normalize\")((0.1307,), (0.3081,))\n])\n[dataset1](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST \"torchvision.datasets.MNIST\") = [datasets.MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST \"torchvision.datasets.MNIST\")('../data', train=True, download=True,\n                          [transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\")=[transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\"))\n[dataset2](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST \"torchvision.datasets.MNIST\") = [datasets.MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST \"torchvision.datasets.MNIST\")('../data', train=False,\n                          [transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\")=[transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\"))\n[train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")([dataset1](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST \"torchvision.datasets.MNIST\"), **train_kwargs)\n[test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")([dataset2](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST \"torchvision.datasets.MNIST\"), **test_kwargs) \n```", "```py\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/9912422 [00:00<?, ?it/s]\n100%|##########| 9912422/9912422 [00:00<00:00, 503661080.89it/s]\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/28881 [00:00<?, ?it/s]\n100%|##########| 28881/28881 [00:00<00:00, 115697892.86it/s]\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/1648877 [00:00<?, ?it/s]\n100%|##########| 1648877/1648877 [00:00<00:00, 346140710.54it/s]\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/4542 [00:00<?, ?it/s]\n100%|##########| 4542/4542 [00:00<00:00, 33421980.29it/s]\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw \n```", "```py\nfrom statistics import mean\n\n[torch.backends.cudnn.enabled](https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.enabled \"torch.backends.cudnn.enabled\") = True\n\nif use_cuda:\n    peak_memory_allocated = []\n\n    for fused in (True, False):\n        [torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed \"torch.manual_seed\")(123456)\n\n        model = [Net](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(fused=fused).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n        [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta \"torch.optim.Adadelta\") = [optim.Adadelta](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta \"torch.optim.Adadelta\")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")(), lr=1.0)\n        [scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\") = [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\")([optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta \"torch.optim.Adadelta\"), step_size=1, gamma=0.7)\n\n        for epoch in range(1):\n            train(model, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"), [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta \"torch.optim.Adadelta\"), epoch)\n            test(model, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"), [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"))\n            [scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\").step()\n        peak_memory_allocated.append([torch.cuda.max_memory_allocated](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated \"torch.cuda.max_memory_allocated\")())\n        [torch.cuda.reset_peak_memory_stats](https://pytorch.org/docs/stable/generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats \"torch.cuda.reset_peak_memory_stats\")()\n    print(\"cuDNN version:\", [torch.backends.cudnn.version](https://pytorch.org/docs/stable/backends.html#torch.backends.cudnn.version \"torch.backends.cudnn.version\")())\n    print()\n    print(\"Peak memory allocated:\")\n    print(f\"fused: {peak_memory_allocated[0]/1024**3:.2f}GB, unfused: {peak_memory_allocated[1]/1024**3:.2f}GB\")\n    print(\"Memory allocated at end of forward pass:\")\n    print(f\"fused: {mean(memory_allocated[0])/1024**3:.2f}GB, unfused: {mean(memory_allocated[1])/1024**3:.2f}GB\") \n```", "```py\nTrain Epoch: 0 [0/60000 (0%)]   Loss: 2.348735\nTrain Epoch: 0 [4096/60000 (7%)]        Loss: 7.435781\nTrain Epoch: 0 [8192/60000 (13%)]       Loss: 5.540894\nTrain Epoch: 0 [12288/60000 (20%)]      Loss: 2.274223\nTrain Epoch: 0 [16384/60000 (27%)]      Loss: 1.618885\nTrain Epoch: 0 [20480/60000 (33%)]      Loss: 1.515203\nTrain Epoch: 0 [24576/60000 (40%)]      Loss: 1.329276\nTrain Epoch: 0 [28672/60000 (47%)]      Loss: 1.184942\nTrain Epoch: 0 [32768/60000 (53%)]      Loss: 1.140154\nTrain Epoch: 0 [36864/60000 (60%)]      Loss: 1.174118\nTrain Epoch: 0 [40960/60000 (67%)]      Loss: 1.057965\nTrain Epoch: 0 [45056/60000 (73%)]      Loss: 0.976334\nTrain Epoch: 0 [49152/60000 (80%)]      Loss: 0.842555\nTrain Epoch: 0 [53248/60000 (87%)]      Loss: 0.690169\nTrain Epoch: 0 [57344/60000 (93%)]      Loss: 0.656998\n\nTest set: Average loss: 0.4197, Accuracy: 8681/10000 (87%)\n\nTrain Epoch: 0 [0/60000 (0%)]   Loss: 2.349030\nTrain Epoch: 0 [4096/60000 (7%)]        Loss: 7.435157\nTrain Epoch: 0 [8192/60000 (13%)]       Loss: 5.443537\nTrain Epoch: 0 [12288/60000 (20%)]      Loss: 2.457860\nTrain Epoch: 0 [16384/60000 (27%)]      Loss: 1.739216\nTrain Epoch: 0 [20480/60000 (33%)]      Loss: 1.448296\nTrain Epoch: 0 [24576/60000 (40%)]      Loss: 1.312144\nTrain Epoch: 0 [28672/60000 (47%)]      Loss: 1.145347\nTrain Epoch: 0 [32768/60000 (53%)]      Loss: 1.495082\nTrain Epoch: 0 [36864/60000 (60%)]      Loss: 1.251163\nTrain Epoch: 0 [40960/60000 (67%)]      Loss: 1.066768\nTrain Epoch: 0 [45056/60000 (73%)]      Loss: 0.883593\nTrain Epoch: 0 [49152/60000 (80%)]      Loss: 0.830817\nTrain Epoch: 0 [53248/60000 (87%)]      Loss: 0.727264\nTrain Epoch: 0 [57344/60000 (93%)]      Loss: 0.774158\n\nTest set: Average loss: 0.4437, Accuracy: 8710/10000 (87%)\n\ncuDNN version: 8902\n\nPeak memory allocated:\nfused: 3.08GB, unfused: 1.77GB\nMemory allocated at end of forward pass:\nfused: 0.59GB, unfused: 0.96GB \n```"]