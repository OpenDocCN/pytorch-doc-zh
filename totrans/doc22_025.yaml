- en: Serialization semantics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/notes/serialization.html](https://pytorch.org/docs/stable/notes/serialization.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This note describes how you can save and load PyTorch tensors and module states
    in Python, and how to serialize Python modules so they can be loaded in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs: []
  type: TYPE_NORMAL
- en: '[Serialization semantics](#serialization-semantics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Saving and loading tensors](#saving-and-loading-tensors)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Saving and loading tensors preserves views](#saving-and-loading-tensors-preserves-views)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Saving and loading torch.nn.Modules](#saving-and-loading-torch-nn-modules)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Serializing torch.nn.Modules and loading them in C++](#serializing-torch-nn-modules-and-loading-them-in-c)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Saving and loading ScriptModules across PyTorch versions](#saving-and-loading-scriptmodules-across-pytorch-versions)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[torch.div performing integer division](#torch-div-performing-integer-division)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[torch.full always inferring a float dtype](#torch-full-always-inferring-a-float-dtype)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Utility functions](#utility-functions)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '## [Saving and loading tensors](#id3)[](#saving-and-loading-tensors "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[`torch.save()`](../generated/torch.save.html#torch.save "torch.save") and
    [`torch.load()`](../generated/torch.load.html#torch.load "torch.load") let you
    easily save and load tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: By convention, PyTorch files are typically written with a ‘.pt’ or ‘.pth’ extension.
  prefs: []
  type: TYPE_NORMAL
- en: '[`torch.save()`](../generated/torch.save.html#torch.save "torch.save") and
    [`torch.load()`](../generated/torch.load.html#torch.load "torch.load") use Python’s
    pickle by default, so you can also save multiple tensors as part of Python objects
    like tuples, lists, and dicts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Custom data structures that include PyTorch tensors can also be saved if the
    data structure is pickle-able.  ## [Saving and loading tensors preserves views](#id4)[](#saving-and-loading-tensors-preserves-views
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Saving tensors preserves their view relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Behind the scenes, these tensors share the same “storage.” See [Tensor Views](https://pytorch.org/docs/main/tensor_view.html)
    for more on views and storage.
  prefs: []
  type: TYPE_NORMAL
- en: When PyTorch saves tensors it saves their storage objects and tensor metadata
    separately. This is an implementation detail that may change in the future, but
    it typically saves space and lets PyTorch easily reconstruct the view relationships
    between the loaded tensors. In the above snippet, for example, only a single storage
    is written to ‘tensors.pt’.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, however, saving the current storage objects may be unnecessary
    and create prohibitively large files. In the following snippet a storage much
    larger than the saved tensor is written to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Instead of saving only the five values in the small tensor to ‘small.pt,’ the
    999 values in the storage it shares with large were saved and loaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'When saving tensors with fewer elements than their storage objects, the size
    of the saved file can be reduced by first cloning the tensors. Cloning a tensor
    produces a new tensor with a new storage object containing only the values in
    the tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the cloned tensors are independent of each other, however, they have
    none of the view relationships the original tensors did. If both file size and
    view relationships are important when saving tensors smaller than their storage
    objects, then care must be taken to construct new tensors that minimize the size
    of their storage objects but still have the desired view relationships before
    saving.  ## [Saving and loading torch.nn.Modules](#id5)[](#saving-and-loading-torch-nn-modules
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'See also: [Tutorial: Saving and loading modules](https://pytorch.org/tutorials/beginner/saving_loading_models.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In PyTorch, a module’s state is frequently serialized using a ‘state dict.’
    A module’s state dict contains all of its parameters and persistent buffers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of saving a module directly, for compatibility reasons it is recommended
    to instead save only its state dict. Python modules even have a function, [`load_state_dict()`](../generated/torch.nn.Module.html#torch.nn.Module.load_state_dict
    "torch.nn.Module.load_state_dict"), to restore their states from a state dict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that the state dict is first loaded from its file with [`torch.load()`](../generated/torch.load.html#torch.load
    "torch.load") and the state then restored with [`load_state_dict()`](../generated/torch.nn.Module.html#torch.nn.Module.load_state_dict
    "torch.nn.Module.load_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: 'Even custom modules and modules containing other modules have state dicts and
    can use this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]  ## [Serializing torch.nn.Modules and loading them in C++](#id6)[](#serializing-torch-nn-modules-and-loading-them-in-c
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'See also: [Tutorial: Loading a TorchScript Model in C++](https://pytorch.org/tutorials/advanced/cpp_export.html)'
  prefs: []
  type: TYPE_NORMAL
- en: ScriptModules can be serialized as a TorchScript program and loaded using [`torch.jit.load()`](../generated/torch.jit.load.html#torch.jit.load
    "torch.jit.load"). This serialization encodes all the modules’ methods, submodules,
    parameters, and attributes, and it allows the serialized program to be loaded
    in C++ (i.e. without Python).
  prefs: []
  type: TYPE_NORMAL
- en: The distinction between [`torch.jit.save()`](../generated/torch.jit.save.html#torch.jit.save
    "torch.jit.save") and [`torch.save()`](../generated/torch.save.html#torch.save
    "torch.save") may not be immediately clear. [`torch.save()`](../generated/torch.save.html#torch.save
    "torch.save") saves Python objects with pickle. This is especially useful for
    prototyping, researching, and training. [`torch.jit.save()`](../generated/torch.jit.save.html#torch.jit.save
    "torch.jit.save"), on the other hand, serializes ScriptModules to a format that
    can be loaded in Python or C++. This is useful when saving and loading C++ modules
    or for running modules trained in Python with C++, a common practice when deploying
    PyTorch models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To script, serialize and load a module in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Traced modules can also be saved with [`torch.jit.save()`](../generated/torch.jit.save.html#torch.jit.save
    "torch.jit.save"), with the caveat that only the traced code path is serialized.
    The following example demonstrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The above module has an if statement that is not triggered by the traced inputs,
    and so is not part of the traced module and not serialized with it. The scripted
    module, however, contains the if statement and is serialized with it. See the
    [TorchScript documentation](https://pytorch.org/docs/stable/jit.html) for more
    on scripting and tracing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to load the module in C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'See the [PyTorch C++ API documentation](https://pytorch.org/cppdocs/) for details
    about how to use PyTorch modules in C++.  ## [Saving and loading ScriptModules
    across PyTorch versions](#id7)[](#saving-and-loading-scriptmodules-across-pytorch-versions
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch Team recommends saving and loading modules with the same version
    of PyTorch. Older versions of PyTorch may not support newer modules, and newer
    versions may have removed or modified older behavior. These changes are explicitly
    described in PyTorch’s [release notes](https://github.com/pytorch/pytorch/releases),
    and modules relying on functionality that has changed may need to be updated to
    continue working properly. In limited cases, detailed below, PyTorch will preserve
    the historic behavior of serialized ScriptModules so they do not require an update.
  prefs: []
  type: TYPE_NORMAL
- en: '[torch.div performing integer division](#id8)[](#torch-div-performing-integer-division
    "Permalink to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In PyTorch 1.5 and earlier [`torch.div()`](../generated/torch.div.html#torch.div
    "torch.div") would perform floor division when given two integer inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In PyTorch 1.7, however, [`torch.div()`](../generated/torch.div.html#torch.div
    "torch.div") will always perform a true division of its inputs, just like division
    in Python 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The behavior of [`torch.div()`](../generated/torch.div.html#torch.div "torch.div")
    is preserved in serialized ScriptModules. That is, ScriptModules serialized with
    versions of PyTorch before 1.6 will continue to see [`torch.div()`](../generated/torch.div.html#torch.div
    "torch.div") perform floor division when given two integer inputs even when loaded
    with newer versions of PyTorch. ScriptModules using [`torch.div()`](../generated/torch.div.html#torch.div
    "torch.div") and serialized on PyTorch 1.6 and later cannot be loaded in earlier
    versions of PyTorch, however, since those earlier versions do not understand the
    new behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '[torch.full always inferring a float dtype](#id9)[](#torch-full-always-inferring-a-float-dtype
    "Permalink to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In PyTorch 1.5 and earlier [`torch.full()`](../generated/torch.full.html#torch.full
    "torch.full") always returned a float tensor, regardless of the fill value it’s
    given:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In PyTorch 1.7, however, [`torch.full()`](../generated/torch.full.html#torch.full
    "torch.full") will infer the returned tensor’s dtype from the fill value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The behavior of [`torch.full()`](../generated/torch.full.html#torch.full "torch.full")
    is preserved in serialized ScriptModules. That is, ScriptModules serialized with
    versions of PyTorch before 1.6 will continue to see torch.full return float tensors
    by default, even when given bool or integer fill values. ScriptModules using [`torch.full()`](../generated/torch.full.html#torch.full
    "torch.full") and serialized on PyTorch 1.6 and later cannot be loaded in earlier
    versions of PyTorch, however, since those earlier versions do not understand the
    new behavior.  ## [Utility functions](#id10)[](#utility-functions "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following utility functions are related to serialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Registers callables for tagging and deserializing storage objects with an associated
    priority. Tagging associates a device with a storage object at save time while
    deserializing moves a storage object to an appropriate device at load time. `tagger`
    and `deserializer` are run in the order given by their `priority` until a tagger/deserializer
    returns a value that is not None.
  prefs: []
  type: TYPE_NORMAL
- en: To override the deserialization behavior for a device in the global registry,
    one can register a tagger with a higher priority than the existing tagger.
  prefs: []
  type: TYPE_NORMAL
- en: This function can also be used to register a tagger and deserializer for new
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**priority** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – Indicates the priority associated with the tagger and
    deserializer, where a lower value indicates higher priority.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tagger** ([*Callable*](https://docs.python.org/3/library/typing.html#typing.Callable
    "(in Python v3.12)")*[**[*[*Union*](https://docs.python.org/3/library/typing.html#typing.Union
    "(in Python v3.12)")*[**Storage**,* [*TypedStorage*](../storage.html#torch.TypedStorage
    "torch.storage.TypedStorage")*,* [*UntypedStorage*](../storage.html#torch.UntypedStorage
    "torch.storage.UntypedStorage")*]**]**,* [*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**]*) – Callable that takes in a storage object and returns
    its tagged device as a string or None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**deserializer** ([*Callable*](https://docs.python.org/3/library/typing.html#typing.Callable
    "(in Python v3.12)")*[**[*[*Union*](https://docs.python.org/3/library/typing.html#typing.Union
    "(in Python v3.12)")*[**Storage**,* [*TypedStorage*](../storage.html#torch.TypedStorage
    "torch.storage.TypedStorage")*,* [*UntypedStorage*](../storage.html#torch.UntypedStorage
    "torch.storage.UntypedStorage")*]**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*]**,* [*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Union*](https://docs.python.org/3/library/typing.html#typing.Union
    "(in Python v3.12)")*[**Storage**,* [*TypedStorage*](../storage.html#torch.TypedStorage
    "torch.storage.TypedStorage")*,* [*UntypedStorage*](../storage.html#torch.UntypedStorage
    "torch.storage.UntypedStorage")*]**]**]*) – Callable that takes in storage object
    and a device string and returns a storage object on the appropriate device or
    None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: None
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Get fallback byte order for loading files
  prefs: []
  type: TYPE_NORMAL
- en: If byteorder mark is not present in saved checkpoint, this byte order is used
    as fallback. By default, it’s “native” byte order.
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Optional[LoadEndianness]
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: default_load_endian
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Set fallback byte order for loading files
  prefs: []
  type: TYPE_NORMAL
- en: If byteorder mark is not present in saved checkpoint, this byte order is used
    as fallback. By default, it’s “native” byte order.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**endianness** – the new fallback byte order'
  prefs: []
  type: TYPE_NORMAL
