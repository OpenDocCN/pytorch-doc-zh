["```py\nimport torch\n\ndef foo(x, y):\n    return 2 * x + y\n\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n@torch.jit.script\ndef bar(x):\n    return traced_foo(x, x) \n```", "```py\nimport torch\n\n@torch.jit.script\ndef foo(x, y):\n    if x.max() > y.max():\n        r = x\n    else:\n        r = y\n    return r\n\ndef bar(x, y, z):\n    return foo(x, y) + z\n\ntraced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3))) \n```", "```py\nimport torch\nimport torchvision\n\nclass MyScriptModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])\n                                        .resize_(1, 3, 1, 1))\n        self.resnet = torch.jit.trace(torchvision.models.resnet18(),\n                                      torch.rand(1, 3, 224, 224))\n\n    def forward(self, input):\n        return self.resnet(input - self.means)\n\nmy_script_module = torch.jit.script(MyScriptModule()) \n```", "```py\n@torch.jit.script\ndef scripted_fn(x : torch.Tensor):\n    for i in range(12):\n        x = x + x\n    return x\n\ndef fn(x):\n    x = torch.neg(x)\n    import pdb; pdb.set_trace()\n    return scripted_fn(x)\n\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\ntraced_fn(torch.rand(3, 4)) \n```", "```py\n$ PYTORCH_JIT=0 python disable_jit_example.py \n```", "```py\n@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.code) \n```", "```py\ndef foo(len: int) -> Tensor:\n    rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\n    rv0 = rv\n    for i in range(len):\n        if torch.lt(i, 10):\n            rv1 = torch.sub(rv0, 1., 1)\n        else:\n            rv1 = torch.add(rv0, 1., 1)\n        rv0 = rv1\n    return rv0 \n```", "```py\n@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.graph) \n```", "```py\ngraph(%len.1 : int):\n  %24 : int = prim::Constant[value=1]()\n  %17 : bool = prim::Constant[value=1]() # test.py:10:5\n  %12 : bool? = prim::Constant()\n  %10 : Device? = prim::Constant()\n  %6 : int? = prim::Constant()\n  %1 : int = prim::Constant[value=3]() # test.py:9:22\n  %2 : int = prim::Constant[value=4]() # test.py:9:25\n  %20 : int = prim::Constant[value=10]() # test.py:11:16\n  %23 : float = prim::Constant[value=1]() # test.py:12:23\n  %4 : int[] = prim::ListConstruct(%1, %2)\n  %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10\n  %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5\n    block0(%i.1 : int, %rv.14 : Tensor):\n      %21 : bool = aten::lt(%i.1, %20) # test.py:11:12\n      %rv.13 : Tensor = prim::If(%21) # test.py:11:9\n        block0():\n          %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18\n          -> (%rv.3)\n        block1():\n          %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18\n          -> (%rv.6)\n      -> (%17, %rv.13)\n  return (%rv) \n```", "```py\ndef loop_in_traced_fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\ntraced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs) \n```", "```py\nERROR: Graphs differed across invocations!\nGraph diff:\n\n            graph(%x : Tensor) {\n            %1 : int = prim::Constant[value=0]()\n            %2 : int = prim::Constant[value=0]()\n            %result.1 : Tensor = aten::select(%x, %1, %2)\n            %4 : int = prim::Constant[value=0]()\n            %5 : int = prim::Constant[value=0]()\n            %6 : Tensor = aten::select(%x, %4, %5)\n            %result.2 : Tensor = aten::mul(%result.1, %6)\n            %8 : int = prim::Constant[value=0]()\n            %9 : int = prim::Constant[value=1]()\n            %10 : Tensor = aten::select(%x, %8, %9)\n        -   %result : Tensor = aten::mul(%result.2, %10)\n        +   %result.3 : Tensor = aten::mul(%result.2, %10)\n        ?          ++\n            %12 : int = prim::Constant[value=0]()\n            %13 : int = prim::Constant[value=2]()\n            %14 : Tensor = aten::select(%x, %12, %13)\n        +   %result : Tensor = aten::mul(%result.3, %14)\n        +   %16 : int = prim::Constant[value=0]()\n        +   %17 : int = prim::Constant[value=3]()\n        +   %18 : Tensor = aten::select(%x, %16, %17)\n        -   %15 : Tensor = aten::mul(%result, %14)\n        ?     ^                                 ^\n        +   %19 : Tensor = aten::mul(%result, %18)\n        ?     ^                                 ^\n        -   return (%15);\n        ?             ^\n        +   return (%19);\n        ?             ^\n            } \n```", "```py\ndef fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\nscripted_fn = torch.jit.script(fn)\nprint(scripted_fn.graph)\n#print(str(scripted_fn.graph).strip())\n\nfor input_tuple in [inputs] + check_inputs:\n    torch.testing.assert_close(fn(*input_tuple), scripted_fn(*input_tuple)) \n```", "```py\ngraph(%x : Tensor) {\n    %5 : bool = prim::Constant[value=1]()\n    %1 : int = prim::Constant[value=0]()\n    %result.1 : Tensor = aten::select(%x, %1, %1)\n    %4 : int = aten::size(%x, %1)\n    %result : Tensor = prim::Loop(%4, %5, %result.1)\n    block0(%i : int, %7 : Tensor) {\n        %10 : Tensor = aten::select(%x, %1, %i)\n        %result.2 : Tensor = aten::mul(%7, %10)\n        -> (%5, %result.2)\n    }\n    return (%result);\n} \n```", "```py\ndef fill_row_zero(x):\n    x[0] = torch.rand(*x.shape[1:2])\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph) \n```", "```py\nfill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n    x[0] = torch.rand(*x.shape[1:2])\nfill_row_zero.py:6: TracerWarning: Output nr 1\\. of the traced function does not match the corresponding output of the Python function. Detailed error:\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)\n    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\ngraph(%0 : Float(3, 4)) {\n    return (%0);\n} \n```", "```py\ndef fill_row_zero(x):\n    x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0)\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph) \n```", "```py\n> cpu_model = gpu_model.cpu()\n> sample_input_cpu = sample_input_gpu.cpu()\n> traced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\n> torch.jit.save(traced_cpu, \"cpu.pt\")\n> \n> traced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\n> torch.jit.save(traced_gpu, \"gpu.pt\")\n> \n> # ... later, when using the model:\n> \n> if use_gpu:\n>   model = torch.jit.load(\"gpu.pt\")\n> else:\n>   model = torch.jit.load(\"cpu.pt\")\n> \n> model(input) \n> ```", "```py\n> import torch\n> \n> class Model(torch.nn.Module):\n>     def __init__(self):\n>         super().__init__()\n>         self.x = 2\n> \n>     def forward(self):\n>         return self.x\n> \n> m = torch.jit.script(Model()) \n> ```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\nmy_model = Model()\nmy_scripted_model = torch.jit.script(my_model) \n```", "```py\ntorch.jit.export(fn)\u00b6\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    def implicitly_compiled_method(self, x):\n        return x + 99\n\n    # `forward` is implicitly decorated with `@torch.jit.export`,\n    # so adding it here would have no effect\n    def forward(self, x):\n        return x + 10\n\n    @torch.jit.export\n    def another_forward(self, x):\n        # When the compiler sees this call, it will compile\n        # `implicitly_compiled_method`\n        return self.implicitly_compiled_method(x)\n\n    def unused_method(self, x):\n        return x - 20\n\n# `m` will contain compiled methods:\n#     `forward`\n#     `another_forward`\n#     `implicitly_compiled_method`\n# `unused_method` will not be compiled since it was not called from\n# any compiled methods and wasn't decorated with `@torch.jit.export`\nm = torch.jit.script(MyModule()) \n```", "```py\n# Same behavior as pre-PyTorch 1.2\n@torch.jit.script\ndef some_fn():\n    return 2\n\n# Marks a function as ignored, if nothing\n# ever calls it then this has no effect\n@torch.jit.ignore\ndef some_fn2():\n    return 2\n\n# As with ignore, if nothing calls it then it has no effect.\n# If it is called in script it is replaced with an exception.\n@torch.jit.unused\ndef some_fn3():\n  import pdb; pdb.set_trace()\n  return 4\n\n# Doesn't do anything, this function is already\n# the main entry point\n@torch.jit.export\ndef some_fn4():\n    return 2 \n```", "```py\nfrom typing import Dict\nimport torch\n\nclass MyModule(torch.jit.ScriptModule):\n    def __init__(self):\n        super().__init__()\n        self.my_dict = torch.jit.Attribute({}, Dict[str, int])\n        self.my_int = torch.jit.Attribute(20, int)\n\nm = MyModule() \n```", "```py\nfrom typing import Dict\n\nclass MyModule(torch.nn.Module):\n    my_dict: Dict[str, int]\n\n    def __init__(self):\n        super().__init__()\n        # This type cannot be inferred and must be specified\n        self.my_dict = {}\n\n        # The attribute type here is inferred to be `int`\n        self.my_int = 20\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule()) \n```", "```py\nclass MyModule(torch.jit.ScriptModule):\n    __constants__ = ['my_constant']\n\n    def __init__(self):\n        super().__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\nm = MyModule() \n```", "```py\nfrom typing import Final\n\nclass MyModule(torch.nn.Module):\n\n    my_constant: Final[int]\n\n    def __init__(self):\n        super().__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule()) \n```", "```py\nimport torch\nfrom typing import Dict, Optional\n\n@torch.jit.script\ndef make_dict(flag: bool):\n    x: Dict[str, int] = {}\n    x['hi'] = 2\n    b: Optional[int] = None\n    if flag:\n        b = 2\n    return x, b \n```"]