["```py\nfor iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward() \n```", "```py\nclass torch.autograd.Function(*args, **kwargs)\u00b6\n```", "```py\n>>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input) \n```", "```py\nclass torch.autograd.profiler.profile(enabled=True, *, use_cuda=False, use_device=None, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, with_modules=False, use_kineto=False, use_cpu=True, use_mtia=False, experimental_config=None)\u00b6\n```", "```py\n>>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>>         y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  --------------- \n```", "```py\nclass torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False)\u00b6\n```", "```py\nnvprof --profile-from-start off -o trace_name.prof -- <regular command here> \n```", "```py\n>>> with torch.cuda.profiler.profile():\n...     model(x)  # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x) \n```", "```py\nclass torch.autograd.profiler.emit_itt(enabled=True, record_shapes=False)\u00b6\n```", "```py\nvtune <--vtune-flags> <regular command here> \n```", "```py\n>>> with torch.autograd.profiler.emit_itt():\n...     model(x) \n```", "```py\nclass torch.autograd.detect_anomaly(check_nan=True)\u00b6\n```", "```py\n>>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n Traceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\n File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n torch.autograd.backward(self, gradient, retain_graph, create_graph)\n File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n allow_unreachable=True)  # allow_unreachable flag\n File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n return self._forward_cls.backward(self, *args)\n File \"<stdin>\", line 8, in backward\n RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n Traceback of forward call that caused the error:\n File \"tmp.py\", line 53, in <module>\n out = run_fn(inp)\n File \"tmp.py\", line 44, in run_fn\n out = MyFunc.apply(a)\n Traceback (most recent call last):\n File \"<stdin>\", line 4, in <module>\n File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n torch.autograd.backward(self, gradient, retain_graph, create_graph)\n File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n allow_unreachable=True)  # allow_unreachable flag\n File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n return self._forward_cls.backward(self, *args)\n File \"<stdin>\", line 8, in backward\n RuntimeError: Some error in backward \n```", "```py\nclass torch.autograd.set_detect_anomaly(mode, check_nan=True)\u00b6\n```", "```py\n>>> a = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> b = a.exp()\n>>> print(isinstance(b.grad_fn, torch.autograd.graph.Node))\nTrue\n>>> print(dir(b.grad_fn))\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']\n>>> print(torch.allclose(b.grad_fn._saved_result, b))\nTrue \n```", "```py\nclass torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook)\u00b6\n```", "```py\n>>> def pack_hook(x):\n...     print(\"Packing\", x)\n...     return x\n>>>\n>>> def unpack_hook(x):\n...     print(\"Unpacking\", x)\n...     return x\n>>>\n>>> a = torch.ones(5, requires_grad=True)\n>>> b = torch.ones(5, requires_grad=True) * 2\n>>> with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n...     y = a * b\nPacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nPacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n>>> y.sum().backward()\nUnpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nUnpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>) \n```", "```py\nclass torch.autograd.graph.save_on_cpu(pin_memory=False, device_type='cuda')\u00b6\n```", "```py\n>>> a = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>> b = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>> c = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>>\n>>> def f(a, b, c):\n...     prod_1 = a * b           # a and b are saved on GPU\n...     with torch.autograd.graph.save_on_cpu():\n...         prod_2 = prod_1 * c  # prod_1 and c are saved on CPU\n...     y = prod_2 * a           # prod_2 and a are saved on GPU\n...     return y\n>>>\n>>> y = f(a, b, c)\n>>> del a, b, c  # for illustration only\n>>> # the content of a, b, and prod_2 are still alive on GPU\n>>> # the content of prod_1 and c only live on CPU\n>>> y.sum().backward()  # all CPU tensors are moved back to GPU, for backward\n>>> # all intermediary tensors are released (deleted) after the call to backward \n```", "```py\nclass torch.autograd.graph.disable_saved_tensors_hooks(error_message)\u00b6\n```", "```py\n>>> message = \"saved tensors default hooks are disabled\"\n>>> with torch.autograd.graph.disable_saved_tensors_hooks(message):\n...     # Raises RuntimeError: saved tensors default hooks are disabled\n...     with torch.autograd.graph.save_on_cpu():\n...         pass \n```", "```py\nclass torch.autograd.graph.register_multi_grad_hook(tensors, fn)\u00b6\n```", "```py\n>>> import torch\n>>>\n>>> a = torch.rand(2, 3, requires_grad=True)\n>>> b = torch.rand(2, 3, requires_grad=True)\n>>> c = a * b\n>>> d = a * b\n>>>\n>>> def fn(grads):\n...     print([g is not None for g in grads])\n...\n>>> torch.autograd.graph.register_multi_grad_hook((a, b, c, d), fn)\n>>>\n>>> c.sum().backward(retain_graph=True)\n[True, True, True, False]\n>>> c.sum().backward(inputs=(a,), retain_graph=True)\n[True, False, True, False]\n>>> \n```", "```py\nclass torch.autograd.graph.allow_mutation_on_saved_tensors\u00b6\n```", "```py\n>>> import torch\n>>> with torch.autograd.graph.allow_mutation_on_saved_tensors():\n...     # forward\n...     a = torch.ones(2, 3, requires_grad=True)\n...     b = a.clone()\n...     out = (b**2).sum()\n...     b.sin_()\n...     # backward\n...     out.sum().backward()\n...\ntensor([[0.8415, 0.8415, 0.8415],\n [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>) \n```", "```py\nclass torch.autograd.graph.GradientEdge(node, output_nr)\u00b6\n```", "```py\ntorch.autograd.graph.get_gradient_edge(tensor)\u00b6\n```"]