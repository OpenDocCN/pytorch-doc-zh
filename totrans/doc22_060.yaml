- en: TorchScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/jit.html](https://pytorch.org/docs/stable/jit.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[TorchScript Language Reference](jit_language_reference_v2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Creating TorchScript Code](#creating-torchscript-code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mixing Tracing and Scripting](#mixing-tracing-and-scripting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TorchScript Language](#torchscript-language)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Built-in Functions and Modules](#built-in-functions-and-modules)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyTorch Functions and Modules](#pytorch-functions-and-modules)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Functions and Modules](#python-functions-and-modules)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Language Reference Comparison](#python-language-reference-comparison)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Debugging](#debugging)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Disable JIT for Debugging](#disable-jit-for-debugging)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Inspecting Code](#inspecting-code)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpreting Graphs](#interpreting-graphs)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tracer](#tracer)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Frequently Asked Questions](#frequently-asked-questions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Known Issues](#known-issues)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Appendix](#appendix)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Migrating to PyTorch 1.2 Recursive Scripting API](#migrating-to-pytorch-1-2-recursive-scripting-api)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fusion Backends](#fusion-backends)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[References](#references)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TorchScript is a way to create serializable and optimizable models from PyTorch
    code. Any TorchScript program can be saved from a Python process and loaded in
    a process where there is no Python dependency.
  prefs: []
  type: TYPE_NORMAL
- en: We provide tools to incrementally transition a model from a pure Python program
    to a TorchScript program that can be run independently from Python, such as in
    a standalone C++ program. This makes it possible to train models in PyTorch using
    familiar tools in Python and then export the model via TorchScript to a production
    environment where Python programs may be disadvantageous for performance and multi-threading
    reasons.
  prefs: []
  type: TYPE_NORMAL
- en: For a gentle introduction to TorchScript, see the [Introduction to TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)
    tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: For an end-to-end example of converting a PyTorch model to TorchScript and running
    it in C++, see the [Loading a PyTorch Model in C++](https://pytorch.org/tutorials/advanced/cpp_export.html)
    tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: '[Creating TorchScript Code](#id4)[](#creating-torchscript-code "Permalink
    to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`script`](generated/torch.jit.script.html#torch.jit.script "torch.jit.script")
    | Script the function. |'
  prefs: []
  type: TYPE_TB
- en: '| [`trace`](generated/torch.jit.trace.html#torch.jit.trace "torch.jit.trace")
    | Trace a function and return an executable or [`ScriptFunction`](generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction
    "torch.jit.ScriptFunction") that will be optimized using just-in-time compilation.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`script_if_tracing`](generated/torch.jit.script_if_tracing.html#torch.jit.script_if_tracing
    "torch.jit.script_if_tracing") | Compiles `fn` when it is first called during
    tracing. |'
  prefs: []
  type: TYPE_TB
- en: '| [`trace_module`](generated/torch.jit.trace_module.html#torch.jit.trace_module
    "torch.jit.trace_module") | Trace a module and return an executable [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") that will be optimized using just-in-time compilation.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`fork`](generated/torch.jit.fork.html#torch.jit.fork "torch.jit.fork") |
    Create an asynchronous task executing func and a reference to the value of the
    result of this execution. |'
  prefs: []
  type: TYPE_TB
- en: '| [`wait`](generated/torch.jit.wait.html#torch.jit.wait "torch.jit.wait") |
    Force completion of a torch.jit.Future[T] asynchronous task, returning the result
    of the task. |'
  prefs: []
  type: TYPE_TB
- en: '| [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") | Wrapper for C++ torch::jit::Module with methods, attributes,
    and parameters. |'
  prefs: []
  type: TYPE_TB
- en: '| [`ScriptFunction`](generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction
    "torch.jit.ScriptFunction") | Functionally equivalent to a [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule"), but represents a single function and does not have
    any attributes or Parameters. |'
  prefs: []
  type: TYPE_TB
- en: '| [`freeze`](generated/torch.jit.freeze.html#torch.jit.freeze "torch.jit.freeze")
    | Freeze ScriptModule, inline submodules, and attributes as constants. |'
  prefs: []
  type: TYPE_TB
- en: '| [`optimize_for_inference`](generated/torch.jit.optimize_for_inference.html#torch.jit.optimize_for_inference
    "torch.jit.optimize_for_inference") | Perform a set of optimization passes to
    optimize a model for the purposes of inference. |'
  prefs: []
  type: TYPE_TB
- en: '| [`enable_onednn_fusion`](generated/torch.jit.enable_onednn_fusion.html#torch.jit.enable_onednn_fusion
    "torch.jit.enable_onednn_fusion") | Enable or disables onednn JIT fusion based
    on the parameter enabled. |'
  prefs: []
  type: TYPE_TB
- en: '| [`onednn_fusion_enabled`](generated/torch.jit.onednn_fusion_enabled.html#torch.jit.onednn_fusion_enabled
    "torch.jit.onednn_fusion_enabled") | Return whether onednn JIT fusion is enabled.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`set_fusion_strategy`](generated/torch.jit.set_fusion_strategy.html#torch.jit.set_fusion_strategy
    "torch.jit.set_fusion_strategy") | Set the type and number of specializations
    that can occur during fusion. |'
  prefs: []
  type: TYPE_TB
- en: '| [`strict_fusion`](generated/torch.jit.strict_fusion.html#torch.jit.strict_fusion
    "torch.jit.strict_fusion") | Give errors if not all nodes have been fused in inference,
    or symbolically differentiated in training. |'
  prefs: []
  type: TYPE_TB
- en: '| [`save`](generated/torch.jit.save.html#torch.jit.save "torch.jit.save") |
    Save an offline version of this module for use in a separate process. |'
  prefs: []
  type: TYPE_TB
- en: '| [`load`](generated/torch.jit.load.html#torch.jit.load "torch.jit.load") |
    Load a [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") or [`ScriptFunction`](generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction
    "torch.jit.ScriptFunction") previously saved with [`torch.jit.save`](generated/torch.jit.save.html#torch.jit.save
    "torch.jit.save"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`ignore`](generated/torch.jit.ignore.html#torch.jit.ignore "torch.jit.ignore")
    | This decorator indicates to the compiler that a function or method should be
    ignored and left as a Python function. |'
  prefs: []
  type: TYPE_TB
- en: '| [`unused`](generated/torch.jit.unused.html#torch.jit.unused "torch.jit.unused")
    | This decorator indicates to the compiler that a function or method should be
    ignored and replaced with the raising of an exception. |'
  prefs: []
  type: TYPE_TB
- en: '| [`interface`](generated/torch.jit.interface.html#torch.jit.interface "torch.jit.interface")
    | Decorate to annotate classes or modules of different types. |'
  prefs: []
  type: TYPE_TB
- en: '| [`isinstance`](generated/torch.jit.isinstance.html#torch.jit.isinstance "torch.jit.isinstance")
    | Provide container type refinement in TorchScript. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Attribute`](generated/torch.jit.Attribute.html#torch.jit.Attribute "torch.jit.Attribute")
    | This method is a pass-through function that returns value, mostly used to indicate
    to the TorchScript compiler that the left-hand side expression is a class instance
    attribute with type of type. |'
  prefs: []
  type: TYPE_TB
- en: '| [`annotate`](generated/torch.jit.annotate.html#torch.jit.annotate "torch.jit.annotate")
    | Use to give type of the_value in TorchScript compiler. |'
  prefs: []
  type: TYPE_TB
- en: '[Mixing Tracing and Scripting](#id5)[](#mixing-tracing-and-scripting "Permalink
    to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many cases either tracing or scripting is an easier approach for converting
    a model to TorchScript. Tracing and scripting can be composed to suit the particular
    requirements of a part of a model.
  prefs: []
  type: TYPE_NORMAL
- en: Scripted functions can call traced functions. This is particularly useful when
    you need to use control-flow around a simple feed-forward model. For instance
    the beam search of a sequence to sequence model will typically be written in script
    but can call an encoder module generated using tracing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example (calling a traced function in script):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Traced functions can call script functions. This is useful when a small part
    of a model requires some control-flow even though most of the model is just a
    feed-forward network. Control-flow inside of a script function called by a traced
    function is preserved correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example (calling a script function in a traced function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This composition also works for `nn.Module`s as well, where it can be used to
    generate a submodule using tracing that can be called from the methods of a script
    module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example (using a traced module):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[TorchScript Language](#id6)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TorchScript is a statically typed subset of Python, so many Python features
    apply directly to TorchScript. See the full [TorchScript Language Reference](jit_language_reference.html#language-reference)
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: '## [Built-in Functions and Modules](#id7)[](#built-in-functions-and-modules
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: TorchScript supports the use of most PyTorch functions and many Python built-ins.
    See [TorchScript Builtins](jit_builtin_functions.html#builtin-functions) for a
    full reference of supported functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PyTorch Functions and Modules](#id8)[](#pytorch-functions-and-modules "Permalink
    to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TorchScript supports a subset of the tensor and neural network functions that
    PyTorch provides. Most methods on Tensor as well as functions in the `torch` namespace,
    all functions in `torch.nn.functional` and most modules from `torch.nn` are supported
    in TorchScript.
  prefs: []
  type: TYPE_NORMAL
- en: See [TorchScript Unsupported PyTorch Constructs](jit_unsupported.html#jit-unsupported)
    for a list of unsupported PyTorch functions and modules.
  prefs: []
  type: TYPE_NORMAL
- en: '[Python Functions and Modules](#id9)[](#python-functions-and-modules "Permalink
    to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many of Python’s [built-in functions](https://docs.python.org/3/library/functions.html)
    are supported in TorchScript. The [`math`](https://docs.python.org/3/library/math.html#module-math
    "(in Python v3.12)") module is also supported (see [math Module](jit_builtin_functions.html#math-module)
    for details), but no other Python modules (built-in or third party) are supported.
  prefs: []
  type: TYPE_NORMAL
- en: '[Python Language Reference Comparison](#id10)[](#python-language-reference-comparison
    "Permalink to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a full listing of supported Python features, see [Python Language Reference
    Coverage](jit_python_reference.html#python-language-reference).
  prefs: []
  type: TYPE_NORMAL
- en: '[Debugging](#id11)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### [Disable JIT for Debugging](#id12)[](#disable-jit-for-debugging "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: PYTORCH_JIT
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the environment variable `PYTORCH_JIT=0` will disable all script and
    tracing annotations. If there is hard-to-debug error in one of your TorchScript
    models, you can use this flag to force everything to run using native Python.
    Since TorchScript (scripting and tracing) is disabled with this flag, you can
    use tools like `pdb` to debug the model code. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Debugging this script with `pdb` works except for when we invoke the [`@torch.jit.script`](generated/torch.jit.script.html#torch.jit.script
    "torch.jit.script") function. We can globally disable JIT, so that we can call
    the [`@torch.jit.script`](generated/torch.jit.script.html#torch.jit.script "torch.jit.script")
    function as a normal Python function and not compile it. If the above script is
    called `disable_jit_example.py`, we can invoke it like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'and we will be able to step into the [`@torch.jit.script`](generated/torch.jit.script.html#torch.jit.script
    "torch.jit.script") function as a normal Python function. To disable the TorchScript
    compiler for a specific function, see [`@torch.jit.ignore`](generated/torch.jit.ignore.html#torch.jit.ignore
    "torch.jit.ignore").  ### [Inspecting Code](#id13)[](#inspecting-code "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'TorchScript provides a code pretty-printer for all [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") instances. This pretty-printer gives an interpretation
    of the script method’s code as valid Python syntax. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") with a single `forward` method will have an attribute
    `code`, which you can use to inspect the [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule")’s code. If the [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") has more than one method, you will need to access `.code`
    on the method itself and not the module. We can inspect the code of a method named
    `foo` on a [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") by accessing `.foo.code`. The example above produces
    this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is TorchScript’s compilation of the code for the `forward` method. You
    can use this to ensure TorchScript (tracing or scripting) has captured your model
    code correctly.  ### [Interpreting Graphs](#id14)[](#interpreting-graphs "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: TorchScript also has a representation at a lower level than the code pretty-
    printer, in the form of IR graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'TorchScript uses a static single assignment (SSA) intermediate representation
    (IR) to represent computation. The instructions in this format consist of ATen
    (the C++ backend of PyTorch) operators and other primitive operators, including
    control flow operators for loops and conditionals. As an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`graph` follows the same rules described in the [Inspecting Code](#inspecting-code)
    section with regard to `forward` method lookup.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The example script above produces the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Take the instruction `%rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) #
    test.py:9:10` for example.'
  prefs: []
  type: TYPE_NORMAL
- en: '`%rv.1 : Tensor` means we assign the output to a (unique) value named `rv.1`,
    that value is of `Tensor` type and that we do not know its concrete shape.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aten::zeros` is the operator (equivalent to `torch.zeros`) and the input list
    `(%4, %6, %6, %10, %12)` specifies which values in scope should be passed as inputs.
    The schema for built-in functions like `aten::zeros` can be found at [Builtin
    Functions](#builtin-functions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`# test.py:9:10` is the location in the original source file that generated
    this instruction. In this case, it is a file named test.py, on line 9, and at
    character 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that operators can also have associated `blocks`, namely the `prim::Loop`
    and `prim::If` operators. In the graph print-out, these operators are formatted
    to reflect their equivalent source code forms to facilitate easy debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs can be inspected as shown to confirm that the computation described by
    a [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") is correct, in both automated and manual fashion, as
    described below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Tracer](#id15)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tracing Edge Cases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are some edge cases that exist where the trace of a given Python function/module
    will not be representative of the underlying code. These cases can include:'
  prefs: []
  type: TYPE_NORMAL
- en: Tracing of control flow that is dependent on inputs (e.g. tensor shapes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracing of in-place operations of tensor views (e.g. indexing on the left-hand
    side of an assignment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that these cases may in fact be traceable in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic Trace Checking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One way to automatically catch many errors in traces is by using `check_inputs`
    on the `torch.jit.trace()` API. `check_inputs` takes a list of tuples of inputs
    that will be used to re-trace the computation and verify the results. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Gives us the following diagnostic information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This message indicates to us that the computation differed between when we first
    traced it and when we traced it with the `check_inputs`. Indeed, the loop within
    the body of `loop_in_traced_fn` depends on the shape of the input `x`, and thus
    when we try another `x` with a different shape, the trace differs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, data-dependent control flow like this can be captured using [`torch.jit.script()`](generated/torch.jit.script.html#torch.jit.script
    "torch.jit.script") instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Which produces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Tracer Warnings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The tracer produces warnings for several problematic patterns in traced computation.
    As an example, take a trace of a function that contains an in-place assignment
    on a slice (a view) of a Tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Produces several warnings and a graph which simply returns the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can fix this by modifying the code to not use the in-place update, but rather
    build up the result tensor out-of-place with `torch.cat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[Frequently Asked Questions](#id16)[](#frequently-asked-questions "Permalink
    to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Q: I would like to train a model on GPU and do inference on CPU. What are the
    best practices?'
  prefs: []
  type: TYPE_NORMAL
- en: 'First convert your model from GPU to CPU and then save it, like so:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is recommended because the tracer may witness tensor creation on a specific
    device, so casting an already-loaded model may have unexpected effects. Casting
    the model *before* saving it ensures that the tracer has the correct device information.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Q: How do I store attributes on a [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule")?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we have a model like:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If `Model` is instantiated it will result in a compilation error since the
    compiler doesn’t know about `x`. There are 4 ways to inform the compiler of attributes
    on [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule"):'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. `nn.Parameter` - Values wrapped in `nn.Parameter` will work as they do on
    `nn.Module`s
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. `register_buffer` - Values wrapped in `register_buffer` will work as they
    do on `nn.Module`s. This is equivalent to an attribute (see 4) of type `Tensor`.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. Constants - Annotating a class member as `Final` (or adding it to a list
    called `__constants__` at the class definition level) will mark the contained
    names as constants. Constants are saved directly in the code of the model. See
    builtin-constants for details.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4\. Attributes - Values that are a supported type can be added as mutable attributes.
    Most types can be inferred but some may need to be specified, see module attributes
    for details.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Q: I would like to trace module’s method but I keep getting this error:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider
    making it a parameter or input, or detaching the gradient`'
  prefs: []
  type: TYPE_NORMAL
- en: This error usually means that the method you are tracing uses a module’s parameters
    and you are passing the module’s method instead of the module instance (e.g. `my_module_instance.forward`
    vs `my_module_instance`).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Invoking `trace` with a module’s method captures module parameters (which may
    require gradients) as **constants**.
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: On the other hand, invoking `trace` with module’s instance (e.g. `my_module`)
    creates a new module and correctly copies parameters into the new module, so they
    can accumulate gradients if required.
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To trace a specific method on a module, see [`torch.jit.trace_module`](generated/torch.jit.trace_module.html#torch.jit.trace_module
    "torch.jit.trace_module")
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Known Issues](#id17)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re using `Sequential` with TorchScript, the inputs of some of the `Sequential`
    submodules may be falsely inferred to be `Tensor`, even if they’re annotated otherwise.
    The canonical solution is to subclass `nn.Sequential` and redeclare `forward`
    with the input typed correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[Appendix](#id18)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Migrating to PyTorch 1.2 Recursive Scripting API](#id19)[](#migrating-to-pytorch-1-2-recursive-scripting-api
    "Permalink to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section details the changes to TorchScript in PyTorch 1.2\. If you are
    new to TorchScript you can skip this section. There are two main changes to the
    TorchScript API with PyTorch 1.2.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. [`torch.jit.script`](generated/torch.jit.script.html#torch.jit.script "torch.jit.script")
    will now attempt to recursively compile functions, methods, and classes that it
    encounters. Once you call `torch.jit.script`, compilation is “opt-out”, rather
    than “opt-in”.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. `torch.jit.script(nn_module_instance)` is now the preferred way to create
    [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule")s, instead of inheriting from `torch.jit.ScriptModule`.
    These changes combine to provide a simpler, easier-to-use API for converting your
    `nn.Module`s into [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule")s, ready to be optimized and executed in a non-Python
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new usage looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The module’s `forward` is compiled by default. Methods called from `forward`
    are lazily compiled in the order they are used in `forward`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To compile a method other than `forward` that is not called from `forward`,
    add `@torch.jit.export`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To stop the compiler from compiling a method, add [`@torch.jit.ignore`](generated/torch.jit.ignore.html#torch.jit.ignore
    "torch.jit.ignore") or [`@torch.jit.unused`](generated/torch.jit.unused.html#torch.jit.unused
    "torch.jit.unused"). `@ignore` leaves the
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: method as a call to python, and `@unused` replaces it with an exception. `@ignored`
    cannot be exported; `@unused` can.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most attribute types can be inferred, so `torch.jit.Attribute` is not necessary.
    For empty container types, annotate their types using [PEP 526-style](https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations)
    class annotations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constants can be marked with a `Final` class annotation instead of adding the
    name of the member to `__constants__`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3 type hints can be used in place of `torch.jit.annotate`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a result of these changes, the following items are considered deprecated
    and should not appear in new code:'
  prefs: []
  type: TYPE_NORMAL
- en: The `@torch.jit.script_method` decorator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classes that inherit from `torch.jit.ScriptModule`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `torch.jit.Attribute` wrapper class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `__constants__` array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `torch.jit.annotate` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modules
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The [`@torch.jit.ignore`](generated/torch.jit.ignore.html#torch.jit.ignore "torch.jit.ignore")
    annotation’s behavior changes in PyTorch 1.2\. Before PyTorch 1.2 the @ignore
    decorator was used to make a function or method callable from code that is exported.
    To get this functionality back, use `@torch.jit.unused()`. `@torch.jit.ignore`
    is now equivalent to `@torch.jit.ignore(drop=False)`. See [`@torch.jit.ignore`](generated/torch.jit.ignore.html#torch.jit.ignore
    "torch.jit.ignore") and [`@torch.jit.unused`](generated/torch.jit.unused.html#torch.jit.unused
    "torch.jit.unused") for details.
  prefs: []
  type: TYPE_NORMAL
- en: When passed to the [`torch.jit.script`](generated/torch.jit.script.html#torch.jit.script
    "torch.jit.script") function, a `torch.nn.Module`'s data is copied to a [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") and the TorchScript compiler compiles the module. The
    module’s `forward` is compiled by default. Methods called from `forward` are lazily
    compiled in the order they are used in `forward`, as well as any `@torch.jit.export`
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This decorator indicates that a method on an `nn.Module` is used as an entry
    point into a [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") and should be compiled.
  prefs: []
  type: TYPE_NORMAL
- en: '`forward` implicitly is assumed to be an entry point, so it does not need this
    decorator. Functions and methods called from `forward` are compiled as they are
    seen by the compiler, so they do not need this decorator either.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example (using `@torch.jit.export` on a method):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Functions don’t change much, they can be decorated with [`@torch.jit.ignore`](generated/torch.jit.ignore.html#torch.jit.ignore
    "torch.jit.ignore") or [`torch.jit.unused`](generated/torch.jit.unused.html#torch.jit.unused
    "torch.jit.unused") if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: TorchScript Classes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: TorchScript class support is experimental. Currently it is best suited for simple
    record-like types (think a `NamedTuple` with methods attached).
  prefs: []
  type: TYPE_NORMAL
- en: Everything in a user defined [TorchScript Class](torchscript-class) is exported
    by default, functions can be decorated with [`@torch.jit.ignore`](generated/torch.jit.ignore.html#torch.jit.ignore
    "torch.jit.ignore") if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Attributes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The TorchScript compiler needs to know the types of module attributes. Most
    types can be inferred from the value of the member. Empty lists and dicts cannot
    have their types inferred and must have their types annotated with [PEP 526-style](https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations)
    class annotations. If a type cannot be inferred and is not explicitly annotated,
    it will not be added as an attribute to the resulting [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule")
  prefs: []
  type: TYPE_NORMAL
- en: 'Old API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'New API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Constants
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `Final` type constructor can be used to mark members as constant. If members
    are not marked constant, they will be copied to the resulting [`ScriptModule`](generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule
    "torch.jit.ScriptModule") as an attribute. Using `Final` opens opportunities for
    optimization if the value is known to be fixed and gives additional type safety.
  prefs: []
  type: TYPE_NORMAL
- en: 'Old API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'New API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#### Variables'
  prefs: []
  type: TYPE_NORMAL
- en: Containers are assumed to have type `Tensor` and be non-optional (see Default
    Types for more information). Previously, `torch.jit.annotate` was used to tell
    the TorchScript compiler what the type should be. Python 3 style type hints are
    now supported.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[Fusion Backends](#id20)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a couple of fusion backends available to optimize TorchScript execution.
    The default fuser on CPUs is NNC, which can perform fusions for both CPUs and
    GPUs. The default fuser on GPUs is NVFuser, which supports a wider range of operators
    and has demonstrated generated kernels with improved throughput. See the [NVFuser
    documentation](https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md)
    for more details on usage and debugging.
  prefs: []
  type: TYPE_NORMAL
- en: '[References](#id21)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Python Language Reference Coverage](jit_python_reference.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TorchScript Unsupported PyTorch Constructs](jit_unsupported.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
