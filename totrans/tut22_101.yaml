- en: (beta) Static Quantization with Eager Mode in PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Author**: [Raghuraman Krishnamoorthi](https://github.com/raghuramank100)
    **Edited by**: [Seth Weidman](https://github.com/SethHWeidman/), [Jerry Zhang](https:github.com/jerryzh168)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'This tutorial shows how to do post-training static quantization, as well as
    illustrating two more advanced techniques - per-channel quantization and quantization-aware
    training - to further improve the model’s accuracy. Note that quantization is
    currently only supported for CPUs, so we will not be utilizing GPUs / CUDA in
    this tutorial. By the end of this tutorial, you will see how quantization in PyTorch
    can result in significant decreases in model size while increasing speed. Furthermore,
    you’ll see how to easily apply some advanced quantization techniques shown [here](https://arxiv.org/abs/1806.08342)
    so that your quantized models take much less of an accuracy hit than they would
    otherwise. Warning: we use a lot of boilerplate code from other PyTorch repos
    to, for example, define the `MobileNetV2` model architecture, define data loaders,
    and so on. We of course encourage you to read it; but if you want to get to the
    quantization features, feel free to skip to the “4\. Post-training static quantization”
    section. We’ll start by doing the necessary imports:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 1\. Model architecture
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first define the MobileNetV2 model architecture, with several notable modifications
    to enable quantization:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Replacing addition with `nn.quantized.FloatFunctional`
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insert `QuantStub` and `DeQuantStub` at the beginning and end of the network.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace ReLU6 with ReLU
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: this code is taken from [here](https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 2\. Helper functions
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We next define several helper functions to help with model evaluation. These
    mostly come from [here](https://github.com/pytorch/examples/blob/master/imagenet/main.py).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 3\. Define dataset and data loaders
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As our last major setup step, we define our dataloaders for our training and
    testing set.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: ImageNet Data
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To run the code in this tutorial using the entire ImageNet dataset, first download
    imagenet by following the instructions at here [ImageNet Data](http://www.image-net.org/download).
    Unzip the downloaded file into the ‘data_path’ folder.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: With the data downloaded, we show functions below that define dataloaders we’ll
    use to read in this data. These functions mostly come from [here](https://github.com/pytorch/vision/blob/master/references/detection/train.py).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we’ll load in the pre-trained MobileNetV2 model. We provide the URL to
    download the model [here](https://download.pytorch.org/models/mobilenet_v2-b0353104.pth).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally to get a “baseline” accuracy, let’s see the accuracy of our un-quantized
    model with fused modules
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: On the entire model, we get an accuracy of 71.9% on the eval dataset of 50,000
    images.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: This will be our baseline to compare to. Next, let’s try different quantization
    methods
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Post-training static quantization
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post-training static quantization involves not just converting the weights from
    float to int, as in dynamic quantization, but also performing the additional step
    of first feeding batches of data through the network and computing the resulting
    distributions of the different activations (specifically, this is done by inserting
    observer modules at different points that record this data). These distributions
    are then used to determine how the specifically the different activations should
    be quantized at inference time (a simple technique would be to simply divide the
    entire range of activations into 256 levels, but we support more sophisticated
    methods as well). Importantly, this additional step allows us to pass quantized
    values between operations instead of converting these values to floats - and then
    back to ints - between every operation, resulting in a significant speed-up.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 静态后训练量化不仅涉及将权重从浮点转换为整数，如动态量化那样，还包括首先通过网络传递数据批次并计算不同激活的结果分布的额外步骤（具体来说，这是通过在不同点插入观察器模块记录这些数据来完成的）。然后使用这些分布来确定如何在推断时量化不同的激活（一个简单的技术是将整个激活范围分成256个级别，但我们也支持更复杂的方法）。重要的是，这个额外的步骤允许我们在操作之间传递量化值，而不是在每个操作之间将这些值转换为浮点数
    - 然后再转换为整数，从而实现显著的加速。
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For this quantized model, we see an accuracy of 56.7% on the eval dataset. This
    is because we used a simple min/max observer to determine quantization parameters.
    Nevertheless, we did reduce the size of our model down to just under 3.6 MB, almost
    a 4x decrease.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个量化模型，在评估数据集上看到准确率为56.7%。这是因为我们使用简单的最小/最大观察器来确定量化参数。尽管如此，我们将模型的大小减小到了将近3.6
    MB，几乎减少了4倍。
- en: 'In addition, we can significantly improve on the accuracy simply by using a
    different quantization configuration. We repeat the same exercise with the recommended
    configuration for quantizing for x86 architectures. This configuration does the
    following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过使用不同的量化配置显着提高准确性。我们使用为x86架构量化推荐的配置重复相同的练习。此配置执行以下操作：
- en: Quantizes weights on a per-channel basis
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按通道对权重进行量化
- en: Uses a histogram observer that collects a histogram of activations and then
    picks quantization parameters in an optimal manner.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用直方图观察器收集激活的直方图，然后以最佳方式选择量化参数。
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Changing just this quantization configuration method resulted in an increase
    of the accuracy to over 67.3%! Still, this is 4% worse than the baseline of 71.9%
    achieved above. So lets try quantization aware training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 仅改变这种量化配置方法就使准确率提高到67.3%以上！但是，这仍然比上面实现的71.9%的基准差4%。所以让我们尝试量化感知训练。
- en: 5\. Quantization-aware training
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 量化感知训练
- en: 'Quantization-aware training (QAT) is the quantization method that typically
    results in the highest accuracy. With QAT, all weights and activations are “fake
    quantized” during both the forward and backward passes of training: that is, float
    values are rounded to mimic int8 values, but all computations are still done with
    floating point numbers. Thus, all the weight adjustments during training are made
    while “aware” of the fact that the model will ultimately be quantized; after quantizing,
    therefore, this method will usually yield higher accuracy than either dynamic
    quantization or post-training static quantization.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 量化感知训练（QAT）通常是产生最高准确率的量化方法。使用QAT时，在训练的前向和后向传递中，所有权重和激活都被“伪量化”：即，浮点值被四舍五入以模拟int8值，但所有计算仍然使用浮点数。因此，在训练期间进行的所有权重调整都是在“意识到”模型最终将被量化的情况下进行的；因此，在量化之后，这种方法通常会产生比动态量化或静态后训练量化更高的准确率。
- en: 'The overall workflow for actually performing QAT is very similar to before:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实际执行QAT的整体工作流程与以前非常相似：
- en: 'We can use the same model as before: there is no additional preparation needed
    for quantization-aware training.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用与以前相同的模型：对于量化感知训练，不需要额外的准备工作。
- en: We need to use a `qconfig` specifying what kind of fake-quantization is to be
    inserted after weights and activations, instead of specifying observers
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要使用一个`qconfig`指定在权重和激活之后要插入什么样的伪量化，而不是指定观察器
- en: 'We first define a training function:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个训练函数：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We fuse modules as before
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样融合模块
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Finally, `prepare_qat` performs the “fake quantization”, preparing the model
    for quantization-aware training
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`prepare_qat`执行“伪量化”，为量化感知训练准备模型
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Training a quantized model with high accuracy requires accurate modeling of
    numerics at inference. For quantization aware training, therefore, we modify the
    training loop by:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个准确率高的量化模型需要准确地模拟推断时的数值。因此，对于量化感知训练，我们通过修改训练循环来进行：
- en: Switch batch norm to use running mean and variance towards the end of training
    to better match inference numerics.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练结束时切换批量归一化以使用运行时均值和方差，以更好地匹配推断数值。
- en: We also freeze the quantizer parameters (scale and zero-point) and fine tune
    the weights.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还会冻结量化器参数（比例和零点）并微调权重。
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Quantization-aware training yields an accuracy of over 71.5% on the entire imagenet
    dataset, which is close to the floating point accuracy of 71.9%.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 量化感知训练在整个imagenet数据集上的准确率超过71.5%，接近71.9%的浮点准确率。
- en: 'More on quantization-aware training:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于量化感知训练：
- en: QAT is a super-set of post training quant techniques that allows for more debugging.
    For example, we can analyze if the accuracy of the model is limited by weight
    or activation quantization.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QAT是一种超集，包含了更多的调试后量化技术。例如，我们可以分析模型的准确性是否受到权重或激活量化的限制。
- en: We can also simulate the accuracy of a quantized model in floating point since
    we are using fake-quantization to model the numerics of actual quantized arithmetic.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们使用伪量化来模拟实际量化算术的数值，因此我们还可以模拟量化模型在浮点数上的准确性。
- en: We can mimic post training quantization easily too.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们也可以轻松地模拟后训练量化。
- en: Speedup from quantization
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从量化中加速
- en: 'Finally, let’s confirm something we alluded to above: do our quantized models
    actually perform inference faster? Let’s test:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们确认我们上面提到的一点：我们的量化模型是否确实执行推断更快？让我们测试一下：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Running this locally on a MacBook pro yielded 61 ms for the regular model, and
    just 20 ms for the quantized model, illustrating the typical 2-4x speedup we see
    for quantized models compared to floating point ones.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在MacBook Pro上本地运行，常规模型的运行时间为61毫秒，量化模型仅为20毫秒，显示了与浮点模型相比，我们通常看到的2-4倍加速。
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this tutorial, we showed two quantization methods - post-training static
    quantization, and quantization-aware training - describing what they do “under
    the hood” and how to use them in PyTorch.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们展示了两种量化方法 - 后训练静态量化和量化感知训练 - 描述了它们在PyTorch中的使用方法。
- en: Thanks for reading! As always, we welcome any feedback, so please create an
    issue [here](https://github.com/pytorch/pytorch/issues) if you have any.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！我们始终欢迎任何反馈意见，如果您有任何问题，请在[这里](https://github.com/pytorch/pytorch/issues)创建一个问题。
