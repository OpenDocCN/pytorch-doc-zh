- en: (beta) Static Quantization with Eager Mode in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Author**: [Raghuraman Krishnamoorthi](https://github.com/raghuramank100)
    **Edited by**: [Seth Weidman](https://github.com/SethHWeidman/), [Jerry Zhang](https:github.com/jerryzh168)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This tutorial shows how to do post-training static quantization, as well as
    illustrating two more advanced techniques - per-channel quantization and quantization-aware
    training - to further improve the model’s accuracy. Note that quantization is
    currently only supported for CPUs, so we will not be utilizing GPUs / CUDA in
    this tutorial. By the end of this tutorial, you will see how quantization in PyTorch
    can result in significant decreases in model size while increasing speed. Furthermore,
    you’ll see how to easily apply some advanced quantization techniques shown [here](https://arxiv.org/abs/1806.08342)
    so that your quantized models take much less of an accuracy hit than they would
    otherwise. Warning: we use a lot of boilerplate code from other PyTorch repos
    to, for example, define the `MobileNetV2` model architecture, define data loaders,
    and so on. We of course encourage you to read it; but if you want to get to the
    quantization features, feel free to skip to the “4\. Post-training static quantization”
    section. We’ll start by doing the necessary imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 1\. Model architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first define the MobileNetV2 model architecture, with several notable modifications
    to enable quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: Replacing addition with `nn.quantized.FloatFunctional`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insert `QuantStub` and `DeQuantStub` at the beginning and end of the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace ReLU6 with ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: this code is taken from [here](https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Helper functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We next define several helper functions to help with model evaluation. These
    mostly come from [here](https://github.com/pytorch/examples/blob/master/imagenet/main.py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Define dataset and data loaders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As our last major setup step, we define our dataloaders for our training and
    testing set.
  prefs: []
  type: TYPE_NORMAL
- en: ImageNet Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To run the code in this tutorial using the entire ImageNet dataset, first download
    imagenet by following the instructions at here [ImageNet Data](http://www.image-net.org/download).
    Unzip the downloaded file into the ‘data_path’ folder.
  prefs: []
  type: TYPE_NORMAL
- en: With the data downloaded, we show functions below that define dataloaders we’ll
    use to read in this data. These functions mostly come from [here](https://github.com/pytorch/vision/blob/master/references/detection/train.py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll load in the pre-trained MobileNetV2 model. We provide the URL to
    download the model [here](https://download.pytorch.org/models/mobilenet_v2-b0353104.pth).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Finally to get a “baseline” accuracy, let’s see the accuracy of our un-quantized
    model with fused modules
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: On the entire model, we get an accuracy of 71.9% on the eval dataset of 50,000
    images.
  prefs: []
  type: TYPE_NORMAL
- en: This will be our baseline to compare to. Next, let’s try different quantization
    methods
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Post-training static quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post-training static quantization involves not just converting the weights from
    float to int, as in dynamic quantization, but also performing the additional step
    of first feeding batches of data through the network and computing the resulting
    distributions of the different activations (specifically, this is done by inserting
    observer modules at different points that record this data). These distributions
    are then used to determine how the specifically the different activations should
    be quantized at inference time (a simple technique would be to simply divide the
    entire range of activations into 256 levels, but we support more sophisticated
    methods as well). Importantly, this additional step allows us to pass quantized
    values between operations instead of converting these values to floats - and then
    back to ints - between every operation, resulting in a significant speed-up.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For this quantized model, we see an accuracy of 56.7% on the eval dataset. This
    is because we used a simple min/max observer to determine quantization parameters.
    Nevertheless, we did reduce the size of our model down to just under 3.6 MB, almost
    a 4x decrease.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we can significantly improve on the accuracy simply by using a
    different quantization configuration. We repeat the same exercise with the recommended
    configuration for quantizing for x86 architectures. This configuration does the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Quantizes weights on a per-channel basis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses a histogram observer that collects a histogram of activations and then
    picks quantization parameters in an optimal manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Changing just this quantization configuration method resulted in an increase
    of the accuracy to over 67.3%! Still, this is 4% worse than the baseline of 71.9%
    achieved above. So lets try quantization aware training.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Quantization-aware training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Quantization-aware training (QAT) is the quantization method that typically
    results in the highest accuracy. With QAT, all weights and activations are “fake
    quantized” during both the forward and backward passes of training: that is, float
    values are rounded to mimic int8 values, but all computations are still done with
    floating point numbers. Thus, all the weight adjustments during training are made
    while “aware” of the fact that the model will ultimately be quantized; after quantizing,
    therefore, this method will usually yield higher accuracy than either dynamic
    quantization or post-training static quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall workflow for actually performing QAT is very similar to before:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the same model as before: there is no additional preparation needed
    for quantization-aware training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to use a `qconfig` specifying what kind of fake-quantization is to be
    inserted after weights and activations, instead of specifying observers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We first define a training function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We fuse modules as before
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Finally, `prepare_qat` performs the “fake quantization”, preparing the model
    for quantization-aware training
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Training a quantized model with high accuracy requires accurate modeling of
    numerics at inference. For quantization aware training, therefore, we modify the
    training loop by:'
  prefs: []
  type: TYPE_NORMAL
- en: Switch batch norm to use running mean and variance towards the end of training
    to better match inference numerics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also freeze the quantizer parameters (scale and zero-point) and fine tune
    the weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Quantization-aware training yields an accuracy of over 71.5% on the entire imagenet
    dataset, which is close to the floating point accuracy of 71.9%.
  prefs: []
  type: TYPE_NORMAL
- en: 'More on quantization-aware training:'
  prefs: []
  type: TYPE_NORMAL
- en: QAT is a super-set of post training quant techniques that allows for more debugging.
    For example, we can analyze if the accuracy of the model is limited by weight
    or activation quantization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also simulate the accuracy of a quantized model in floating point since
    we are using fake-quantization to model the numerics of actual quantized arithmetic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can mimic post training quantization easily too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speedup from quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, let’s confirm something we alluded to above: do our quantized models
    actually perform inference faster? Let’s test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Running this locally on a MacBook pro yielded 61 ms for the regular model, and
    just 20 ms for the quantized model, illustrating the typical 2-4x speedup we see
    for quantized models compared to floating point ones.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we showed two quantization methods - post-training static
    quantization, and quantization-aware training - describing what they do “under
    the hood” and how to use them in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! As always, we welcome any feedback, so please create an
    issue [here](https://github.com/pytorch/pytorch/issues) if you have any.
  prefs: []
  type: TYPE_NORMAL
