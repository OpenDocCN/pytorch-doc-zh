# torch.Tensor

> 原文：[https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)

[`torch.Tensor`](#torch.Tensor "torch.Tensor") 是包含单一数据类型元素的多维矩阵。

## 数据类型

Torch定义了10种张量类型，包括CPU和GPU变体，如下所示：

| 数据类型 | dtype | CPU张量 | GPU张量 |
| --- | --- | --- | --- |
| 32位浮点数 | `torch.float32` 或 `torch.float` | `torch.FloatTensor` | `torch.cuda.FloatTensor` |
| 64位浮点数 | `torch.float64` 或 `torch.double` | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |
| 16位浮点数 [1](#id4) | `torch.float16` 或 `torch.half` | `torch.HalfTensor` | `torch.cuda.HalfTensor` |
| 16位浮点数 [2](#id5) | `torch.bfloat16` | `torch.BFloat16Tensor` | `torch.cuda.BFloat16Tensor` |
| 32位复数 | `torch.complex32` 或 `torch.chalf` |  |  |
| 64位复数 | `torch.complex64` 或 `torch.cfloat` |  |  |
| 128位复数 | `torch.complex128` 或 `torch.cdouble` |  |  |
| 8位整数（无符号） | `torch.uint8` | `torch.ByteTensor` | `torch.cuda.ByteTensor` |
| 8位整数（有符号） | `torch.int8` | `torch.CharTensor` | `torch.cuda.CharTensor` |
| 16位整数（有符号） | `torch.int16` 或 `torch.short` | `torch.ShortTensor` | `torch.cuda.ShortTensor` |
| 32位整数（有符号） | `torch.int32` 或 `torch.int` | `torch.IntTensor` | `torch.cuda.IntTensor` |
| 64位整数（有符号） | `torch.int64` 或 `torch.long` | `torch.LongTensor` | `torch.cuda.LongTensor` |
| 布尔值 | `torch.bool` | `torch.BoolTensor` | `torch.cuda.BoolTensor` |
| 量化的8位整数（无符号） | `torch.quint8` | `torch.ByteTensor` | / |
| 量化的8位整数（有符号） | `torch.qint8` | `torch.CharTensor` | / |
| 量化的32位整数（有符号） | `torch.qint32` | `torch.IntTensor` | / |
| 量化的4位整数（无符号）[3](#id6) | `torch.quint4x2` | `torch.ByteTensor` | / |

[1](#id1)

有时被称为binary16：使用1个符号位，5个指数位和10个有效位。当精度重要时很有用，但会牺牲范围。

[2](#id2)

有时被称为Brain Floating Point：使用1个符号位，8个指数位和7个有效位。当范围重要时很有用，因为它具有与`float32`相同数量的指数位。

[3](#id3)

量化的4位整数存储为8位有符号整数。目前仅在EmbeddingBag运算符中支持。

[`torch.Tensor`](#torch.Tensor "torch.Tensor") 是默认张量类型（`torch.FloatTensor`）的别名。

## 初始化和基本操作[](#initializing-and-basic-operations "跳转到此标题的永久链接")

可以使用[`torch.tensor()`](generated/torch.tensor.html#torch.tensor "torch.tensor")构造来自Python [`list`](https://docs.python.org/3/library/stdtypes.html#list "(在Python v3.12中)") 或序列的张量：

```py
>>> torch.tensor([[1., -1.], [1., -1.]])
tensor([[ 1.0000, -1.0000],
 [ 1.0000, -1.0000]])
>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))
tensor([[ 1,  2,  3],
 [ 4,  5,  6]]) 
```

警告

[`torch.tensor()`](generated/torch.tensor.html#torch.tensor "torch.tensor") 总是复制`data`。如果您有一个张量`data`，只想改变其`requires_grad`标志，请使用[`requires_grad_()`](generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_ "torch.Tensor.requires_grad_")或[`detach()`](generated/torch.Tensor.detach.html#torch.Tensor.detach "torch.Tensor.detach")来避免复制。如果您有一个numpy数组并想避免复制，请使用[`torch.as_tensor()`](generated/torch.as_tensor.html#torch.as_tensor "torch.as_tensor")。

通过将[`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype")和/或[`torch.device`](tensor_attributes.html#torch.device "torch.device")传递给构造函数或张量创建操作，可以构造特定数据类型的张量：

```py
>>> torch.zeros([2, 4], dtype=torch.int32)
tensor([[ 0,  0,  0,  0],
 [ 0,  0,  0,  0]], dtype=torch.int32)
>>> cuda0 = torch.device('cuda:0')
>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)
tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
 [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0') 
```

有关构建张量的更多信息，请参阅[Creation Ops](torch.html#tensor-creation-ops)

可以使用Python的索引和切片表示法访问和修改张量的内容：

```py
>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])
>>> print(x[1][2])
tensor(6)
>>> x[0][1] = 8
>>> print(x)
tensor([[ 1,  8,  3],
 [ 4,  5,  6]]) 
```

使用 [`torch.Tensor.item()`](generated/torch.Tensor.item.html#torch.Tensor.item "torch.Tensor.item") 从包含单个值的张量中获取一个Python数字：

```py
>>> x = torch.tensor([[1]])
>>> x
tensor([[ 1]])
>>> x.item()
1
>>> x = torch.tensor(2.5)
>>> x
tensor(2.5000)
>>> x.item()
2.5 
```

有关索引的更多信息，请参见 [Indexing, Slicing, Joining, Mutating Ops](torch.html#indexing-slicing-joining)。

可以创建一个带有 `requires_grad=True` 的张量，以便 [`torch.autograd`](autograd.html#module-torch.autograd "torch.autograd") 记录对它们的操作以进行自动微分。

```py
>>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)
>>> out = x.pow(2).sum()
>>> out.backward()
>>> x.grad
tensor([[ 2.0000, -2.0000],
 [ 2.0000,  2.0000]]) 
```

每个张量都有一个关联的 `torch.Storage`，用于保存其数据。张量类还提供了对存储的多维、[分步](https://en.wikipedia.org/wiki/Stride_of_an_array)视图，并在其上定义了数值操作。

注意

有关张量视图的更多信息，请参见 [Tensor Views](tensor_view.html#tensor-view-doc)。

注意

有关 [`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype")、[`torch.device`](tensor_attributes.html#torch.device "torch.device") 和 [`torch.layout`](tensor_attributes.html#torch.layout "torch.layout") 属性的更多信息，请参见 [`torch.Tensor`](#torch.Tensor "torch.Tensor") 的 [Tensor Attributes](tensor_attributes.html#tensor-attributes-doc)。

注意

会改变张量的方法会带有下划线后缀。例如，`torch.FloatTensor.abs_()` 在原地计算绝对值并返回修改后的张量，而 `torch.FloatTensor.abs()` 在新张量中计算结果。

注意

要更改现有张量的 [`torch.device`](tensor_attributes.html#torch.device "torch.device") 和/或 [`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype")，请考虑在张量上使用 [`to()`](generated/torch.Tensor.to.html#torch.Tensor.to "torch.Tensor.to") 方法。

警告

当前的 [`torch.Tensor`](#torch.Tensor "torch.Tensor") 实现引入了内存开销，因此在具有许多小张量的应用程序中可能导致意外高的内存使用量。如果这是您的情况，请考虑使用一个大结构。

## 张量类参考

```py
class torch.Tensor¶
```

有几种主要方法可以创建张量，取决于您的用例。

+   要使用预先存在的数据创建张量，请使用 [`torch.tensor()`](generated/torch.tensor.html#torch.tensor "torch.tensor")。

+   要创建特定大小的张量，请使用 `torch.*` 张量创建操作（参见 [Creation Ops](torch.html#tensor-creation-ops)）。

+   要创建一个与另一个张量相同大小（和相似类型）的张量，请使用 `torch.*_like` 张量创建操作（参见 [Creation Ops](torch.html#tensor-creation-ops)）。

+   要创建一个与另一个张量相似类型但不同大小的张量，请使用 `tensor.new_*` 创建操作。

```py
Tensor.T¶
```

返回一个维度被颠倒的张量视图。

如果 `x` 中有 `n` 个维度，`x.T` 等同于 `x.permute(n-1, n-2, ..., 0)`。

警告

在维度不为2的张量上使用 [`Tensor.T()`](#torch.Tensor.T "torch.Tensor.T") 来颠倒它们的形状已被弃用，并且在将来的版本中会引发错误。考虑使用 [`mT`](#torch.Tensor.mT "torch.Tensor.mT") 来转置矩阵批次或者使用 x.permute(*torch.arange(x.ndim - 1, -1, -1)) 来颠倒张量的维度。

```py
Tensor.H¶
```

返回一个共轭和转置的矩阵（2-D张量）视图。

对于复杂矩阵，`x.H` 等同于 `x.transpose(0, 1).conj()`，对于实矩阵，`x.H` 等同于 `x.transpose(0, 1)`。

另请参阅

[`mH`](#torch.Tensor.mH "torch.Tensor.mH")：也适用于矩阵批次的属性。

```py
Tensor.mT¶
```

返回一个最后两个维度被转置的张量视图。

`x.mT` 等同于 `x.transpose(-2, -1)`。

```py
Tensor.mH¶
```

访问此属性等同于调用 [`adjoint()`](generated/torch.adjoint.html#torch.adjoint "torch.adjoint")。

| [`Tensor.new_tensor`](generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor "torch.Tensor.new_tensor") | 返回一个以 `data` 为张量数据的新张量。 |
| --- | --- |
| [`Tensor.new_full`](generated/torch.Tensor.new_full.html#torch.Tensor.new_full "torch.Tensor.new_full") | 返回一个大小为`size`且填充为`fill_value`的张量。 |
| [`Tensor.new_empty`](generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty "torch.Tensor.new_empty") | 返回一个大小为`size`且填充为未初始化数据的张量。 |
| [`Tensor.new_ones`](generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones "torch.Tensor.new_ones") | 返回一个大小为`size`且填充为`1`的张量。 |
| [`Tensor.new_zeros`](generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros "torch.Tensor.new_zeros") | 返回一个大小为`size`且填充为`0`的张量。 |
| [`Tensor.is_cuda`](generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda "torch.Tensor.is_cuda") | 如果张量存储在GPU上，则为`True`，否则为`False`。 |
| [`Tensor.is_quantized`](generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized "torch.Tensor.is_quantized") | 如果张量是量化的，则为`True`，否则为`False`。 |
| [`Tensor.is_meta`](generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta "torch.Tensor.is_meta") | 如果张量是元张量，则为`True`，否则为`False`。 |
| [`Tensor.device`](generated/torch.Tensor.device.html#torch.Tensor.device "torch.Tensor.device") | 此张量所在的[`torch.device`](tensor_attributes.html#torch.device "torch.device")。 |
| [`Tensor.grad`](generated/torch.Tensor.grad.html#torch.Tensor.grad "torch.Tensor.grad") | 默认情况下，此属性为`None`，第一次调用`backward()`计算`self`的梯度时会变成一个张量。 |
| [`Tensor.ndim`](generated/torch.Tensor.ndim.html#torch.Tensor.ndim "torch.Tensor.ndim") | [`dim()`](generated/torch.Tensor.dim.html#torch.Tensor.dim "torch.Tensor.dim")的别名 |
| [`Tensor.real`](generated/torch.Tensor.real.html#torch.Tensor.real "torch.Tensor.real") | 返回一个包含复值输入张量`self`的实部值的新张量。 |
| [`Tensor.imag`](generated/torch.Tensor.imag.html#torch.Tensor.imag "torch.Tensor.imag") | 返回一个包含`self`张量的虚部值的新张量。 |
| [`Tensor.nbytes`](generated/torch.Tensor.nbytes.html#torch.Tensor.nbytes "torch.Tensor.nbytes") | 如果张量不使用稀疏存储布局，则返回张量元素“视图”消耗的字节数。 |
| [`Tensor.itemsize`](generated/torch.Tensor.itemsize.html#torch.Tensor.itemsize "torch.Tensor.itemsize") | [`element_size()`](generated/torch.Tensor.element_size.html#torch.Tensor.element_size "torch.Tensor.element_size")的别名 |
| [`Tensor.abs`](generated/torch.Tensor.abs.html#torch.Tensor.abs "torch.Tensor.abs") | 参见[`torch.abs()`](generated/torch.abs.html#torch.abs "torch.abs") |
| [`Tensor.abs_`](generated/torch.Tensor.abs_.html#torch.Tensor.abs_ "torch.Tensor.abs_") | [`abs()`](generated/torch.Tensor.abs.html#torch.Tensor.abs "torch.Tensor.abs")的原地版本 |
| [`Tensor.absolute`](generated/torch.Tensor.absolute.html#torch.Tensor.absolute "torch.Tensor.absolute") | [`abs()`](generated/torch.abs.html#torch.abs "torch.abs")的别名 |
| [`Tensor.absolute_`](generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_ "torch.Tensor.absolute_") | [`absolute()`](generated/torch.Tensor.absolute.html#torch.Tensor.absolute "torch.Tensor.absolute")的原地版本，别名为`abs_()` |
| [`Tensor.acos`](generated/torch.Tensor.acos.html#torch.Tensor.acos "torch.Tensor.acos") | 参见[`torch.acos()`](generated/torch.acos.html#torch.acos "torch.acos") |
| [`Tensor.acos_`](generated/torch.Tensor.acos_.html#torch.Tensor.acos_ "torch.Tensor.acos_") | [`acos()`](generated/torch.Tensor.acos.html#torch.Tensor.acos "torch.Tensor.acos")的原地版本 |
| [`Tensor.arccos`](generated/torch.Tensor.arccos.html#torch.Tensor.arccos "torch.Tensor.arccos") | 参见[`torch.arccos()`](generated/torch.arccos.html#torch.arccos "torch.arccos") |
| [`Tensor.arccos_`](generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_ "torch.Tensor.arccos_") | [`arccos()`](generated/torch.Tensor.arccos.html#torch.Tensor.arccos "torch.Tensor.arccos") 的原地版本 |
| [`Tensor.add`](generated/torch.Tensor.add.html#torch.Tensor.add "torch.Tensor.add") | 将标量或张量添加到 `self` 张量中。 |
| [`Tensor.add_`](generated/torch.Tensor.add_.html#torch.Tensor.add_ "torch.Tensor.add_") | [`add()`](generated/torch.Tensor.add.html#torch.Tensor.add "torch.Tensor.add") 的原地版本 |
| [`Tensor.addbmm`](generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm "torch.Tensor.addbmm") | 查看 [`torch.addbmm()`](generated/torch.addbmm.html#torch.addbmm "torch.addbmm") |
| [`Tensor.addbmm_`](generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_ "torch.Tensor.addbmm_") | [`addbmm()`](generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm "torch.Tensor.addbmm") 的原地版本 |
| [`Tensor.addcdiv`](generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv "torch.Tensor.addcdiv") | 查看 [`torch.addcdiv()`](generated/torch.addcdiv.html#torch.addcdiv "torch.addcdiv") |
| [`Tensor.addcdiv_`](generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_ "torch.Tensor.addcdiv_") | [`addcdiv()`](generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv "torch.Tensor.addcdiv") 的原地版本 |
| [`Tensor.addcmul`](generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul "torch.Tensor.addcmul") | 查看 [`torch.addcmul()`](generated/torch.addcmul.html#torch.addcmul "torch.addcmul") |
| [`Tensor.addcmul_`](generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_ "torch.Tensor.addcmul_") | [`addcmul()`](generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul "torch.Tensor.addcmul") 的原地版本 |
| [`Tensor.addmm`](generated/torch.Tensor.addmm.html#torch.Tensor.addmm "torch.Tensor.addmm") | 查看 [`torch.addmm()`](generated/torch.addmm.html#torch.addmm "torch.addmm") |
| [`Tensor.addmm_`](generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_ "torch.Tensor.addmm_") | [`addmm()`](generated/torch.Tensor.addmm.html#torch.Tensor.addmm "torch.Tensor.addmm") 的原地版本 |
| [`Tensor.sspaddmm`](generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm "torch.Tensor.sspaddmm") | 查看 [`torch.sspaddmm()`](generated/torch.sspaddmm.html#torch.sspaddmm "torch.sspaddmm") |
| [`Tensor.addmv`](generated/torch.Tensor.addmv.html#torch.Tensor.addmv "torch.Tensor.addmv") | 查看 [`torch.addmv()`](generated/torch.addmv.html#torch.addmv "torch.addmv") |
| [`Tensor.addmv_`](generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_ "torch.Tensor.addmv_") | [`addmv()`](generated/torch.Tensor.addmv.html#torch.Tensor.addmv "torch.Tensor.addmv") 的原地版本 |
| [`Tensor.addr`](generated/torch.Tensor.addr.html#torch.Tensor.addr "torch.Tensor.addr") | 查看 [`torch.addr()`](generated/torch.addr.html#torch.addr "torch.addr") |
| [`Tensor.addr_`](generated/torch.Tensor.addr_.html#torch.Tensor.addr_ "torch.Tensor.addr_") | [`addr()`](generated/torch.Tensor.addr.html#torch.Tensor.addr "torch.Tensor.addr") 的原地版本 |
| [`Tensor.adjoint`](generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint "torch.Tensor.adjoint") | [`adjoint()`](generated/torch.adjoint.html#torch.adjoint "torch.adjoint") 的别名 |
| [`Tensor.allclose`](generated/torch.Tensor.allclose.html#torch.Tensor.allclose "torch.Tensor.allclose") | 查看 [`torch.allclose()`](generated/torch.allclose.html#torch.allclose "torch.allclose") |
| [`Tensor.amax`](generated/torch.Tensor.amax.html#torch.Tensor.amax "torch.Tensor.amax") | 查看 [`torch.amax()`](generated/torch.amax.html#torch.amax "torch.amax") |
| [`Tensor.amin`](generated/torch.Tensor.amin.html#torch.Tensor.amin "torch.Tensor.amin") | 查看 [`torch.amin()`](generated/torch.amin.html#torch.amin "torch.amin") |
| [`Tensor.aminmax`](generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax "torch.Tensor.aminmax") | 参见 [`torch.aminmax()`](generated/torch.aminmax.html#torch.aminmax "torch.aminmax") |
| [`Tensor.angle`](generated/torch.Tensor.angle.html#torch.Tensor.angle "torch.Tensor.angle") | 参见 [`torch.angle()`](generated/torch.angle.html#torch.angle "torch.angle") |
| [`Tensor.apply_`](generated/torch.Tensor.apply_.html#torch.Tensor.apply_ "torch.Tensor.apply_") | 将函数 `callable` 应用于张量中的每个元素，用 `callable` 返回的值替换每个元素。 |
| [`Tensor.argmax`](generated/torch.Tensor.argmax.html#torch.Tensor.argmax "torch.Tensor.argmax") | 参见 [`torch.argmax()`](generated/torch.argmax.html#torch.argmax "torch.argmax") |
| [`Tensor.argmin`](generated/torch.Tensor.argmin.html#torch.Tensor.argmin "torch.Tensor.argmin") | 参见 [`torch.argmin()`](generated/torch.argmin.html#torch.argmin "torch.argmin") |
| [`Tensor.argsort`](generated/torch.Tensor.argsort.html#torch.Tensor.argsort "torch.Tensor.argsort") | 参见 [`torch.argsort()`](generated/torch.argsort.html#torch.argsort "torch.argsort") |
| [`Tensor.argwhere`](generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere "torch.Tensor.argwhere") | 参见 [`torch.argwhere()`](generated/torch.argwhere.html#torch.argwhere "torch.argwhere") |
| [`Tensor.asin`](generated/torch.Tensor.asin.html#torch.Tensor.asin "torch.Tensor.asin") | 参见 [`torch.asin()`](generated/torch.asin.html#torch.asin "torch.asin") |
| [`Tensor.asin_`](generated/torch.Tensor.asin_.html#torch.Tensor.asin_ "torch.Tensor.asin_") | [`asin()`](generated/torch.Tensor.asin.html#torch.Tensor.asin "torch.Tensor.asin") 的原地版本 |
| [`Tensor.arcsin`](generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin "torch.Tensor.arcsin") | 参见 [`torch.arcsin()`](generated/torch.arcsin.html#torch.arcsin "torch.arcsin") |
| [`Tensor.arcsin_`](generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_ "torch.Tensor.arcsin_") | [`arcsin()`](generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin "torch.Tensor.arcsin") 的原地版本 |
| [`Tensor.as_strided`](generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided "torch.Tensor.as_strided") | 参见 [`torch.as_strided()`](generated/torch.as_strided.html#torch.as_strided "torch.as_strided") |
| [`Tensor.atan`](generated/torch.Tensor.atan.html#torch.Tensor.atan "torch.Tensor.atan") | 参见 [`torch.atan()`](generated/torch.atan.html#torch.atan "torch.atan") |
| [`Tensor.atan_`](generated/torch.Tensor.atan_.html#torch.Tensor.atan_ "torch.Tensor.atan_") | [`atan()`](generated/torch.Tensor.atan.html#torch.Tensor.atan "torch.Tensor.atan") 的原地版本 |
| [`Tensor.arctan`](generated/torch.Tensor.arctan.html#torch.Tensor.arctan "torch.Tensor.arctan") | 参见 [`torch.arctan()`](generated/torch.arctan.html#torch.arctan "torch.arctan") |
| [`Tensor.arctan_`](generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_ "torch.Tensor.arctan_") | [`arctan()`](generated/torch.Tensor.arctan.html#torch.Tensor.arctan "torch.Tensor.arctan") 的原地版本 |
| [`Tensor.atan2`](generated/torch.Tensor.atan2.html#torch.Tensor.atan2 "torch.Tensor.atan2") | 参见 [`torch.atan2()`](generated/torch.atan2.html#torch.atan2 "torch.atan2") |
| [`Tensor.atan2_`](generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_ "torch.Tensor.atan2_") | [`atan2()`](generated/torch.Tensor.atan2.html#torch.Tensor.atan2 "torch.Tensor.atan2") 的原地版本 |
| [`Tensor.arctan2`](generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2 "torch.Tensor.arctan2") | 参见 [`torch.arctan2()`](generated/torch.arctan2.html#torch.arctan2 "torch.arctan2") |
| [`Tensor.arctan2_`](generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_ "torch.Tensor.arctan2_") | atan2_(other) -> Tensor |
| [`Tensor.all`](generated/torch.Tensor.all.html#torch.Tensor.all "torch.Tensor.all") | 参见 [`torch.all()`](generated/torch.all.html#torch.all "torch.all") |
| [`Tensor.any`](generated/torch.Tensor.any.html#torch.Tensor.any "torch.Tensor.any") | 查看 [`torch.any()`](generated/torch.any.html#torch.any "torch.any") |
| [`Tensor.backward`](generated/torch.Tensor.backward.html#torch.Tensor.backward "torch.Tensor.backward") | 计算当前张量相对于图中叶子节点的梯度。 |
| [`Tensor.baddbmm`](generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm "torch.Tensor.baddbmm") | 查看 [`torch.baddbmm()`](generated/torch.baddbmm.html#torch.baddbmm "torch.baddbmm") |
| [`Tensor.baddbmm_`](generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_ "torch.Tensor.baddbmm_") | [`baddbmm()`](generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm "torch.Tensor.baddbmm") 的原地版本 |
| [`Tensor.bernoulli`](generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli "torch.Tensor.bernoulli") | 返回一个结果张量，其中每个 $\texttt{result[i]}$ 从 $\text{Bernoulli}(\texttt{self[i]})$ 独立采样。 |
| [`Tensor.bernoulli_`](generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_ "torch.Tensor.bernoulli_") | 用来自 $\text{Bernoulli}(\texttt{p})$ 的独立样本填充 `self` 的每个位置。 |
| [`Tensor.bfloat16`](generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16 "torch.Tensor.bfloat16") | `self.bfloat16()` 等同于 `self.to(torch.bfloat16)`。 |
| [`Tensor.bincount`](generated/torch.Tensor.bincount.html#torch.Tensor.bincount "torch.Tensor.bincount") | 查看 [`torch.bincount()`](generated/torch.bincount.html#torch.bincount "torch.bincount") |
| [`Tensor.bitwise_not`](generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not "torch.Tensor.bitwise_not") | 查看 [`torch.bitwise_not()`](generated/torch.bitwise_not.html#torch.bitwise_not "torch.bitwise_not") |
| [`Tensor.bitwise_not_`](generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_ "torch.Tensor.bitwise_not_") | [`bitwise_not()`](generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not "torch.Tensor.bitwise_not") 的原地版本 |
| [`Tensor.bitwise_and`](generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and "torch.Tensor.bitwise_and") | 查看 [`torch.bitwise_and()`](generated/torch.bitwise_and.html#torch.bitwise_and "torch.bitwise_and") |
| [`Tensor.bitwise_and_`](generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_ "torch.Tensor.bitwise_and_") | [`bitwise_and()`](generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and "torch.Tensor.bitwise_and") 的原地版本 |
| [`Tensor.bitwise_or`](generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or "torch.Tensor.bitwise_or") | 查看 [`torch.bitwise_or()`](generated/torch.bitwise_or.html#torch.bitwise_or "torch.bitwise_or") |
| [`Tensor.bitwise_or_`](generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_ "torch.Tensor.bitwise_or_") | [`bitwise_or()`](generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or "torch.Tensor.bitwise_or") 的原地版本 |
| [`Tensor.bitwise_xor`](generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor "torch.Tensor.bitwise_xor") | 查看 [`torch.bitwise_xor()`](generated/torch.bitwise_xor.html#torch.bitwise_xor "torch.bitwise_xor") |
| [`Tensor.bitwise_xor_`](generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_ "torch.Tensor.bitwise_xor_") | [`bitwise_xor()`](generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor "torch.Tensor.bitwise_xor") 的原地版本 |
| [`Tensor.bitwise_left_shift`](generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift "torch.Tensor.bitwise_left_shift") | 查看 [`torch.bitwise_left_shift()`](generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift "torch.bitwise_left_shift") |
| [`Tensor.bitwise_left_shift_`](generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_ "torch.Tensor.bitwise_left_shift_") | [`bitwise_left_shift()`](generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift "torch.Tensor.bitwise_left_shift") 的原地版本 |
| [`Tensor.bitwise_right_shift`](generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift "torch.Tensor.bitwise_right_shift") | 参见 [`torch.bitwise_right_shift()`](generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift "torch.bitwise_right_shift") |
| [`Tensor.bitwise_right_shift_`](generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_ "torch.Tensor.bitwise_right_shift_") | [`bitwise_right_shift()`](generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift "torch.Tensor.bitwise_right_shift") 的原地版本 |
| [`Tensor.bmm`](generated/torch.Tensor.bmm.html#torch.Tensor.bmm "torch.Tensor.bmm") | 参见 [`torch.bmm()`](generated/torch.bmm.html#torch.bmm "torch.bmm") |
| [`Tensor.bool`](generated/torch.Tensor.bool.html#torch.Tensor.bool "torch.Tensor.bool") | `self.bool()` 等同于 `self.to(torch.bool)`。 |
| [`Tensor.byte`](generated/torch.Tensor.byte.html#torch.Tensor.byte "torch.Tensor.byte") | `self.byte()` 等同于 `self.to(torch.uint8)`。 |
| [`Tensor.broadcast_to`](generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to "torch.Tensor.broadcast_to") | 参见 [`torch.broadcast_to()`](generated/torch.broadcast_to.html#torch.broadcast_to "torch.broadcast_to"). |
| [`Tensor.cauchy_`](generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_ "torch.Tensor.cauchy_") | 用从 Cauchy 分布中抽取的数字填充张量： |
| [`Tensor.ceil`](generated/torch.Tensor.ceil.html#torch.Tensor.ceil "torch.Tensor.ceil") | 参见 [`torch.ceil()`](generated/torch.ceil.html#torch.ceil "torch.ceil") |
| [`Tensor.ceil_`](generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_ "torch.Tensor.ceil_") | [`ceil()`](generated/torch.Tensor.ceil.html#torch.Tensor.ceil "torch.Tensor.ceil") 的原地版本 |
| [`Tensor.char`](generated/torch.Tensor.char.html#torch.Tensor.char "torch.Tensor.char") | `self.char()` 等同于 `self.to(torch.int8)`。 |
| [`Tensor.cholesky`](generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky "torch.Tensor.cholesky") | 参见 [`torch.cholesky()`](generated/torch.cholesky.html#torch.cholesky "torch.cholesky") |
| [`Tensor.cholesky_inverse`](generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse "torch.Tensor.cholesky_inverse") | 参见 [`torch.cholesky_inverse()`](generated/torch.cholesky_inverse.html#torch.cholesky_inverse "torch.cholesky_inverse") |
| [`Tensor.cholesky_solve`](generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve "torch.Tensor.cholesky_solve") | 参见 [`torch.cholesky_solve()`](generated/torch.cholesky_solve.html#torch.cholesky_solve "torch.cholesky_solve") |
| [`Tensor.chunk`](generated/torch.Tensor.chunk.html#torch.Tensor.chunk "torch.Tensor.chunk") | 参见 [`torch.chunk()`](generated/torch.chunk.html#torch.chunk "torch.chunk") |
| [`Tensor.clamp`](generated/torch.Tensor.clamp.html#torch.Tensor.clamp "torch.Tensor.clamp") | 参见 [`torch.clamp()`](generated/torch.clamp.html#torch.clamp "torch.clamp") |
| [`Tensor.clamp_`](generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_ "torch.Tensor.clamp_") | [`clamp()`](generated/torch.Tensor.clamp.html#torch.Tensor.clamp "torch.Tensor.clamp") 的原地版本 |
| [`Tensor.clip`](generated/torch.Tensor.clip.html#torch.Tensor.clip "torch.Tensor.clip") | [`clamp()`](generated/torch.Tensor.clamp.html#torch.Tensor.clamp "torch.Tensor.clamp") 的别名 |
| [`Tensor.clip_`](generated/torch.Tensor.clip_.html#torch.Tensor.clip_ "torch.Tensor.clip_") | [`clamp_()`](generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_ "torch.Tensor.clamp_") 的别名。 |
| [`Tensor.clone`](generated/torch.Tensor.clone.html#torch.Tensor.clone "torch.Tensor.clone") | 查看 [`torch.clone()`](generated/torch.clone.html#torch.clone "torch.clone") |
| [`Tensor.contiguous`](generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous "torch.Tensor.contiguous") | 返回一个包含与 `self` 张量相同数据的内存连续张量 |
| [`Tensor.copy_`](generated/torch.Tensor.copy_.html#torch.Tensor.copy_ "torch.Tensor.copy_") | 将 `src` 中的元素复制到 `self` 张量中并返回 `self` |
| [`Tensor.conj`](generated/torch.Tensor.conj.html#torch.Tensor.conj "torch.Tensor.conj") | 查看 [`torch.conj()`](generated/torch.conj.html#torch.conj "torch.conj") |
| [`Tensor.conj_physical`](generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical "torch.Tensor.conj_physical") | 查看 [`torch.conj_physical()`](generated/torch.conj_physical.html#torch.conj_physical "torch.conj_physical") |
| [`Tensor.conj_physical_`](generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_ "torch.Tensor.conj_physical_") | [`conj_physical()`](generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical "torch.Tensor.conj_physical") 的原地版本 |
| [`Tensor.resolve_conj`](generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj "torch.Tensor.resolve_conj") | 查看 [`torch.resolve_conj()`](generated/torch.resolve_conj.html#torch.resolve_conj "torch.resolve_conj") |
| [`Tensor.resolve_neg`](generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg "torch.Tensor.resolve_neg") | 查看 [`torch.resolve_neg()`](generated/torch.resolve_neg.html#torch.resolve_neg "torch.resolve_neg") |
| [`Tensor.copysign`](generated/torch.Tensor.copysign.html#torch.Tensor.copysign "torch.Tensor.copysign") | 查看 [`torch.copysign()`](generated/torch.copysign.html#torch.copysign "torch.copysign") |
| [`Tensor.copysign_`](generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_ "torch.Tensor.copysign_") | [`copysign()`](generated/torch.Tensor.copysign.html#torch.Tensor.copysign "torch.Tensor.copysign") 的原地版本 |
| [`Tensor.cos`](generated/torch.Tensor.cos.html#torch.Tensor.cos "torch.Tensor.cos") | 查看 [`torch.cos()`](generated/torch.cos.html#torch.cos "torch.cos") |
| [`Tensor.cos_`](generated/torch.Tensor.cos_.html#torch.Tensor.cos_ "torch.Tensor.cos_") | [`cos()`](generated/torch.Tensor.cos.html#torch.Tensor.cos "torch.Tensor.cos") 的原地版本 |
| [`Tensor.cosh`](generated/torch.Tensor.cosh.html#torch.Tensor.cosh "torch.Tensor.cosh") | 查看 [`torch.cosh()`](generated/torch.cosh.html#torch.cosh "torch.cosh") |
| [`Tensor.cosh_`](generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_ "torch.Tensor.cosh_") | [`cosh()`](generated/torch.Tensor.cosh.html#torch.Tensor.cosh "torch.Tensor.cosh") 的原地版本 |
| [`Tensor.corrcoef`](generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef "torch.Tensor.corrcoef") | 查看 [`torch.corrcoef()`](generated/torch.corrcoef.html#torch.corrcoef "torch.corrcoef") |
| [`Tensor.count_nonzero`](generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero "torch.Tensor.count_nonzero") | 查看 [`torch.count_nonzero()`](generated/torch.count_nonzero.html#torch.count_nonzero "torch.count_nonzero") |
| [`Tensor.cov`](generated/torch.Tensor.cov.html#torch.Tensor.cov "torch.Tensor.cov") | 查看 [`torch.cov()`](generated/torch.cov.html#torch.cov "torch.cov") |
| [`Tensor.acosh`](generated/torch.Tensor.acosh.html#torch.Tensor.acosh "torch.Tensor.acosh") | 查看 [`torch.acosh()`](generated/torch.acosh.html#torch.acosh "torch.acosh") |
| [`Tensor.acosh_`](generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_ "torch.Tensor.acosh_") | [`acosh()`](generated/torch.Tensor.acosh.html#torch.Tensor.acosh "torch.Tensor.acosh") 的原地版本 |
| [`Tensor.arccosh`](generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh "torch.Tensor.arccosh") | acosh() -> Tensor |
| [`Tensor.arccosh_`](generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_ "torch.Tensor.arccosh_") | acosh_() -> Tensor |
| [`Tensor.cpu`](generated/torch.Tensor.cpu.html#torch.Tensor.cpu "torch.Tensor.cpu") | 返回此对象在CPU内存中的副本 |
| [`Tensor.cross`](generated/torch.Tensor.cross.html#torch.Tensor.cross "torch.Tensor.cross") | 参见`torch.cross()` |
| [`Tensor.cuda`](generated/torch.Tensor.cuda.html#torch.Tensor.cuda "torch.Tensor.cuda") | 返回此对象在CUDA内存中的副本 |
| [`Tensor.logcumsumexp`](generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp "torch.Tensor.logcumsumexp") | 参见`torch.logcumsumexp()` |
| [`Tensor.cummax`](generated/torch.Tensor.cummax.html#torch.Tensor.cummax "torch.Tensor.cummax") | 参见`torch.cummax()` |
| [`Tensor.cummin`](generated/torch.Tensor.cummin.html#torch.Tensor.cummin "torch.Tensor.cummin") | 参见`torch.cummin()` |
| [`Tensor.cumprod`](generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod "torch.Tensor.cumprod") | 参见`torch.cumprod()` |
| [`Tensor.cumprod_`](generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_ "torch.Tensor.cumprod_") | `cumprod()`的原位版本 |
| [`Tensor.cumsum`](generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum "torch.Tensor.cumsum") | 参见`torch.cumsum()` |
| [`Tensor.cumsum_`](generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_ "torch.Tensor.cumsum_") | `cumsum()`的原位版本 |
| [`Tensor.chalf`](generated/torch.Tensor.chalf.html#torch.Tensor.chalf "torch.Tensor.chalf") | `self.chalf()`等同于`self.to(torch.complex32)` |
| [`Tensor.cfloat`](generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat "torch.Tensor.cfloat") | `self.cfloat()`等同于`self.to(torch.complex64)` |
| [`Tensor.cdouble`](generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble "torch.Tensor.cdouble") | `self.cdouble()`等同于`self.to(torch.complex128)` |
| [`Tensor.data_ptr`](generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr "torch.Tensor.data_ptr") | 返回`self`张量的第一个元素的地址 |
| [`Tensor.deg2rad`](generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad "torch.Tensor.deg2rad") | 参见`torch.deg2rad()` |
| [`Tensor.dequantize`](generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize "torch.Tensor.dequantize") | 给定一个量化张量，对其进行去量化并返回去量化的浮点张量 |
| [`Tensor.det`](generated/torch.Tensor.det.html#torch.Tensor.det "torch.Tensor.det") | 参见`torch.det()` |
| [`Tensor.dense_dim`](generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim "torch.Tensor.dense_dim") | 返回[稀疏张量](sparse.html#sparse-docs) `self` 中的密集维度数 |
| [`Tensor.detach`](generated/torch.Tensor.detach.html#torch.Tensor.detach "torch.Tensor.detach") | 返回一个从当前图中分离出来的新张量 |
| [`Tensor.detach_`](generated/torch.Tensor.detach_.html#torch.Tensor.detach_ "torch.Tensor.detach_") | 将张量从创建它的图中分离出来，使其成为叶子节点 |
| [`Tensor.diag`](generated/torch.Tensor.diag.html#torch.Tensor.diag "torch.Tensor.diag") | 参见`torch.diag()` |
| [`Tensor.diag_embed`](generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed "torch.Tensor.diag_embed") | 查看 [`torch.diag_embed()`](generated/torch.diag_embed.html#torch.diag_embed "torch.diag_embed") |
| [`Tensor.diagflat`](generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat "torch.Tensor.diagflat") | 查看 [`torch.diagflat()`](generated/torch.diagflat.html#torch.diagflat "torch.diagflat") |
| [`Tensor.diagonal`](generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal "torch.Tensor.diagonal") | 查看 [`torch.diagonal()`](generated/torch.diagonal.html#torch.diagonal "torch.diagonal") |
| [`Tensor.diagonal_scatter`](generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter "torch.Tensor.diagonal_scatter") | 查看 [`torch.diagonal_scatter()`](generated/torch.diagonal_scatter.html#torch.diagonal_scatter "torch.diagonal_scatter") |
| [`Tensor.fill_diagonal_`](generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_ "torch.Tensor.fill_diagonal_") | 填充至少为2维的张量的主对角线。 |
| [`Tensor.fmax`](generated/torch.Tensor.fmax.html#torch.Tensor.fmax "torch.Tensor.fmax") | 查看 [`torch.fmax()`](generated/torch.fmax.html#torch.fmax "torch.fmax") |
| [`Tensor.fmin`](generated/torch.Tensor.fmin.html#torch.Tensor.fmin "torch.Tensor.fmin") | 查看 [`torch.fmin()`](generated/torch.fmin.html#torch.fmin "torch.fmin") |
| [`Tensor.diff`](generated/torch.Tensor.diff.html#torch.Tensor.diff "torch.Tensor.diff") | 查看 [`torch.diff()`](generated/torch.diff.html#torch.diff "torch.diff") |
| [`Tensor.digamma`](generated/torch.Tensor.digamma.html#torch.Tensor.digamma "torch.Tensor.digamma") | 查看 [`torch.digamma()`](generated/torch.digamma.html#torch.digamma "torch.digamma") |
| [`Tensor.digamma_`](generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_ "torch.Tensor.digamma_") | [`digamma()`](generated/torch.Tensor.digamma.html#torch.Tensor.digamma "torch.Tensor.digamma") 的原地版本 |
| [`Tensor.dim`](generated/torch.Tensor.dim.html#torch.Tensor.dim "torch.Tensor.dim") | 返回 `self` 张量的维度数量。 |
| [`Tensor.dim_order`](generated/torch.Tensor.dim_order.html#torch.Tensor.dim_order "torch.Tensor.dim_order") | 返回一个描述 `self` 张量维度顺序或物理布局的整数元组。 |
| [`Tensor.dist`](generated/torch.Tensor.dist.html#torch.Tensor.dist "torch.Tensor.dist") | 查看 [`torch.dist()`](generated/torch.dist.html#torch.dist "torch.dist") |
| [`Tensor.div`](generated/torch.Tensor.div.html#torch.Tensor.div "torch.Tensor.div") | 查看 [`torch.div()`](generated/torch.div.html#torch.div "torch.div") |
| [`Tensor.div_`](generated/torch.Tensor.div_.html#torch.Tensor.div_ "torch.Tensor.div_") | [`div()`](generated/torch.Tensor.div.html#torch.Tensor.div "torch.Tensor.div") 的原地版本 |
| [`Tensor.divide`](generated/torch.Tensor.divide.html#torch.Tensor.divide "torch.Tensor.divide") | 查看 [`torch.divide()`](generated/torch.divide.html#torch.divide "torch.divide") |
| [`Tensor.divide_`](generated/torch.Tensor.divide_.html#torch.Tensor.divide_ "torch.Tensor.divide_") | [`divide()`](generated/torch.Tensor.divide.html#torch.Tensor.divide "torch.Tensor.divide") 的原地版本 |
| [`Tensor.dot`](generated/torch.Tensor.dot.html#torch.Tensor.dot "torch.Tensor.dot") | 查看 [`torch.dot()`](generated/torch.dot.html#torch.dot "torch.dot") |
| [`Tensor.double`](generated/torch.Tensor.double.html#torch.Tensor.double "torch.Tensor.double") | `self.double()` 等同于 `self.to(torch.float64)`。 |
| [`Tensor.dsplit`](generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit "torch.Tensor.dsplit") | 查看 [`torch.dsplit()`](generated/torch.dsplit.html#torch.dsplit "torch.dsplit") |
| [`Tensor.element_size`](generated/torch.Tensor.element_size.html#torch.Tensor.element_size "torch.Tensor.element_size") | 返回单个元素的字节大小。 |
| [`Tensor.eq`](generated/torch.Tensor.eq.html#torch.Tensor.eq "torch.Tensor.eq") | 查看 [`torch.eq()`](generated/torch.eq.html#torch.eq "torch.eq") |
| [`Tensor.eq_`](generated/torch.Tensor.eq_.html#torch.Tensor.eq_ "torch.Tensor.eq_") | [`eq()`](generated/torch.Tensor.eq.html#torch.Tensor.eq "torch.Tensor.eq") 的原地版本 |
| [`Tensor.equal`](generated/torch.Tensor.equal.html#torch.Tensor.equal "torch.Tensor.equal") | 查看 [`torch.equal()`](generated/torch.equal.html#torch.equal "torch.equal") |
| [`Tensor.erf`](generated/torch.Tensor.erf.html#torch.Tensor.erf "torch.Tensor.erf") | 查看 [`torch.erf()`](generated/torch.erf.html#torch.erf "torch.erf") |
| [`Tensor.erf_`](generated/torch.Tensor.erf_.html#torch.Tensor.erf_ "torch.Tensor.erf_") | [`erf()`](generated/torch.Tensor.erf.html#torch.Tensor.erf "torch.Tensor.erf") 的原地版本 |
| [`Tensor.erfc`](generated/torch.Tensor.erfc.html#torch.Tensor.erfc "torch.Tensor.erfc") | 查看 [`torch.erfc()`](generated/torch.erfc.html#torch.erfc "torch.erfc") |
| [`Tensor.erfc_`](generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_ "torch.Tensor.erfc_") | [`erfc()`](generated/torch.Tensor.erfc.html#torch.Tensor.erfc "torch.Tensor.erfc") 的原地版本 |
| [`Tensor.erfinv`](generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv "torch.Tensor.erfinv") | 查看 [`torch.erfinv()`](generated/torch.erfinv.html#torch.erfinv "torch.erfinv") |
| [`Tensor.erfinv_`](generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_ "torch.Tensor.erfinv_") | [`erfinv()`](generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv "torch.Tensor.erfinv") 的原地版本 |
| [`Tensor.exp`](generated/torch.Tensor.exp.html#torch.Tensor.exp "torch.Tensor.exp") | 查看 [`torch.exp()`](generated/torch.exp.html#torch.exp "torch.exp") |
| [`Tensor.exp_`](generated/torch.Tensor.exp_.html#torch.Tensor.exp_ "torch.Tensor.exp_") | [`exp()`](generated/torch.Tensor.exp.html#torch.Tensor.exp "torch.Tensor.exp") 的原地版本 |
| [`Tensor.expm1`](generated/torch.Tensor.expm1.html#torch.Tensor.expm1 "torch.Tensor.expm1") | 查看 [`torch.expm1()`](generated/torch.expm1.html#torch.expm1 "torch.expm1") |
| [`Tensor.expm1_`](generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_ "torch.Tensor.expm1_") | [`expm1()`](generated/torch.Tensor.expm1.html#torch.Tensor.expm1 "torch.Tensor.expm1") 的原地版本 |
| [`Tensor.expand`](generated/torch.Tensor.expand.html#torch.Tensor.expand "torch.Tensor.expand") | 返回一个新的视图，将 `self` 张量中的单例维度扩展到更大的大小 |
| [`Tensor.expand_as`](generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as "torch.Tensor.expand_as") | 将此张量扩展到与 `other` 相同的大小 |
| [`Tensor.exponential_`](generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_ "torch.Tensor.exponential_") | 用从概率密度函数中抽取的元素填充 `self` 张量 |
| [`Tensor.fix`](generated/torch.Tensor.fix.html#torch.Tensor.fix "torch.Tensor.fix") | 查看 [`torch.fix()`](generated/torch.fix.html#torch.fix "torch.fix") |
| [`Tensor.fix_`](generated/torch.Tensor.fix_.html#torch.Tensor.fix_ "torch.Tensor.fix_") | [`fix()`](generated/torch.Tensor.fix.html#torch.Tensor.fix "torch.Tensor.fix") 的原地版本 |
| [`Tensor.fill_`](generated/torch.Tensor.fill_.html#torch.Tensor.fill_ "torch.Tensor.fill_") | 用指定值填充 `self` 张量 |
| [`Tensor.flatten`](generated/torch.Tensor.flatten.html#torch.Tensor.flatten "torch.Tensor.flatten") | 查看 [`torch.flatten()`](generated/torch.flatten.html#torch.flatten "torch.flatten") |
| [`Tensor.flip`](generated/torch.Tensor.flip.html#torch.Tensor.flip "torch.Tensor.flip") | 查看 [`torch.flip()`](generated/torch.flip.html#torch.flip "torch.flip") |
| [`Tensor.fliplr`](generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr "torch.Tensor.fliplr") | 查看 [`torch.fliplr()`](generated/torch.fliplr.html#torch.fliplr "torch.fliplr") |
| [`Tensor.flipud`](generated/torch.Tensor.flipud.html#torch.Tensor.flipud "torch.Tensor.flipud") | 查看 [`torch.flipud()`](generated/torch.flipud.html#torch.flipud "torch.flipud") |
| [`Tensor.float`](generated/torch.Tensor.float.html#torch.Tensor.float "torch.Tensor.float") | `self.float()` 等同于 `self.to(torch.float32)` |
| [`Tensor.float_power`](generated/torch.Tensor.float_power.html#torch.Tensor.float_power "torch.Tensor.float_power") | 查看 [`torch.float_power()`](generated/torch.float_power.html#torch.float_power "torch.float_power") |
| [`Tensor.float_power_`](generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_ "torch.Tensor.float_power_") | [`float_power()`](generated/torch.Tensor.float_power.html#torch.Tensor.float_power "torch.Tensor.float_power") 的原地版本 |
| [`Tensor.floor`](generated/torch.Tensor.floor.html#torch.Tensor.floor "torch.Tensor.floor") | 查看 [`torch.floor()`](generated/torch.floor.html#torch.floor "torch.floor") |
| [`Tensor.floor_`](generated/torch.Tensor.floor_.html#torch.Tensor.floor_ "torch.Tensor.floor_") | [`floor()`](generated/torch.Tensor.floor.html#torch.Tensor.floor "torch.Tensor.floor") 的原地版本 |
| [`Tensor.floor_divide`](generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide "torch.Tensor.floor_divide") | 查看 [`torch.floor_divide()`](generated/torch.floor_divide.html#torch.floor_divide "torch.floor_divide") |
| [`Tensor.floor_divide_`](generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_ "torch.Tensor.floor_divide_") | [`floor_divide()`](generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide "torch.Tensor.floor_divide") 的原地版本 |
| [`Tensor.fmod`](generated/torch.Tensor.fmod.html#torch.Tensor.fmod "torch.Tensor.fmod") | 查看 [`torch.fmod()`](generated/torch.fmod.html#torch.fmod "torch.fmod") |
| [`Tensor.fmod_`](generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_ "torch.Tensor.fmod_") | [`fmod()`](generated/torch.Tensor.fmod.html#torch.Tensor.fmod "torch.Tensor.fmod") 的原地版本 |
| [`Tensor.frac`](generated/torch.Tensor.frac.html#torch.Tensor.frac "torch.Tensor.frac") | 查看 [`torch.frac()`](generated/torch.frac.html#torch.frac "torch.frac") |
| [`Tensor.frac_`](generated/torch.Tensor.frac_.html#torch.Tensor.frac_ "torch.Tensor.frac_") | [`frac()`](generated/torch.Tensor.frac.html#torch.Tensor.frac "torch.Tensor.frac") 的原地版本 |
| [`Tensor.frexp`](generated/torch.Tensor.frexp.html#torch.Tensor.frexp "torch.Tensor.frexp") | 查看 [`torch.frexp()`](generated/torch.frexp.html#torch.frexp "torch.frexp") |
| [`Tensor.gather`](generated/torch.Tensor.gather.html#torch.Tensor.gather "torch.Tensor.gather") | 查看 [`torch.gather()`](generated/torch.gather.html#torch.gather "torch.gather") |
| [`Tensor.gcd`](generated/torch.Tensor.gcd.html#torch.Tensor.gcd "torch.Tensor.gcd") | 查看 [`torch.gcd()`](generated/torch.gcd.html#torch.gcd "torch.gcd") |
| [`Tensor.gcd_`](generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_ "torch.Tensor.gcd_") | [`gcd()`](generated/torch.Tensor.gcd.html#torch.Tensor.gcd "torch.Tensor.gcd") 的原地版本 |
| [`Tensor.ge`](generated/torch.Tensor.ge.html#torch.Tensor.ge "torch.Tensor.ge") | 查看 [`torch.ge()`](generated/torch.ge.html#torch.ge "torch.ge") |
| [`Tensor.ge_`](generated/torch.Tensor.ge_.html#torch.Tensor.ge_ "torch.Tensor.ge_") | [`ge()`](generated/torch.Tensor.ge.html#torch.Tensor.ge "torch.Tensor.ge") 的原地版本 |
| [`Tensor.greater_equal`](generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal "torch.Tensor.greater_equal") | 查看 [`torch.greater_equal()`](generated/torch.greater_equal.html#torch.greater_equal "torch.greater_equal") |
| [`Tensor.greater_equal_`](generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_ "torch.Tensor.greater_equal_") | [`greater_equal()`](generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal "torch.Tensor.greater_equal") 的原地版本 |
| [`Tensor.geometric_`](generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_ "torch.Tensor.geometric_") | 用几何分布中的元素填充 `self` 张量： |
| [`Tensor.geqrf`](generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf "torch.Tensor.geqrf") | 参见 [`torch.geqrf()`](generated/torch.geqrf.html#torch.geqrf "torch.geqrf") |
| [`Tensor.ger`](generated/torch.Tensor.ger.html#torch.Tensor.ger "torch.Tensor.ger") | 参见 [`torch.ger()`](generated/torch.ger.html#torch.ger "torch.ger") |
| [`Tensor.get_device`](generated/torch.Tensor.get_device.html#torch.Tensor.get_device "torch.Tensor.get_device") | 对于 CUDA 张量，此函数返回张量所在 GPU 的设备序数。 |
| [`Tensor.gt`](generated/torch.Tensor.gt.html#torch.Tensor.gt "torch.Tensor.gt") | 参见 [`torch.gt()`](generated/torch.gt.html#torch.gt "torch.gt") |
| [`Tensor.gt_`](generated/torch.Tensor.gt_.html#torch.Tensor.gt_ "torch.Tensor.gt_") | [`gt()`](generated/torch.Tensor.gt.html#torch.Tensor.gt "torch.Tensor.gt") 的原地版本。 |
| [`Tensor.greater`](generated/torch.Tensor.greater.html#torch.Tensor.greater "torch.Tensor.greater") | 参见 [`torch.greater()`](generated/torch.greater.html#torch.greater "torch.greater") |
| [`Tensor.greater_`](generated/torch.Tensor.greater_.html#torch.Tensor.greater_ "torch.Tensor.greater_") | [`greater()`](generated/torch.Tensor.greater.html#torch.Tensor.greater "torch.Tensor.greater") 的原地版本。 |
| [`Tensor.half`](generated/torch.Tensor.half.html#torch.Tensor.half "torch.Tensor.half") | `self.half()` 等同于 `self.to(torch.float16)`。 |
| [`Tensor.hardshrink`](generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink "torch.Tensor.hardshrink") | 参见 [`torch.nn.functional.hardshrink()`](generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink "torch.nn.functional.hardshrink") |
| [`Tensor.heaviside`](generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside "torch.Tensor.heaviside") | 参见 [`torch.heaviside()`](generated/torch.heaviside.html#torch.heaviside "torch.heaviside") |
| [`Tensor.histc`](generated/torch.Tensor.histc.html#torch.Tensor.histc "torch.Tensor.histc") | 参见 [`torch.histc()`](generated/torch.histc.html#torch.histc "torch.histc") |
| [`Tensor.histogram`](generated/torch.Tensor.histogram.html#torch.Tensor.histogram "torch.Tensor.histogram") | 参见 [`torch.histogram()`](generated/torch.histogram.html#torch.histogram "torch.histogram") |
| [`Tensor.hsplit`](generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit "torch.Tensor.hsplit") | 参见 [`torch.hsplit()`](generated/torch.hsplit.html#torch.hsplit "torch.hsplit") |
| [`Tensor.hypot`](generated/torch.Tensor.hypot.html#torch.Tensor.hypot "torch.Tensor.hypot") | 参见 [`torch.hypot()`](generated/torch.hypot.html#torch.hypot "torch.hypot") |
| [`Tensor.hypot_`](generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_ "torch.Tensor.hypot_") | [`hypot()`](generated/torch.Tensor.hypot.html#torch.Tensor.hypot "torch.Tensor.hypot") 的原地版本。 |
| [`Tensor.i0`](generated/torch.Tensor.i0.html#torch.Tensor.i0 "torch.Tensor.i0") | 参见 [`torch.i0()`](generated/torch.i0.html#torch.i0 "torch.i0") |
| [`Tensor.i0_`](generated/torch.Tensor.i0_.html#torch.Tensor.i0_ "torch.Tensor.i0_") | [`i0()`](generated/torch.Tensor.i0.html#torch.Tensor.i0 "torch.Tensor.i0") 的原地版本。 |
| [`Tensor.igamma`](generated/torch.Tensor.igamma.html#torch.Tensor.igamma "torch.Tensor.igamma") | 参见 [`torch.igamma()`](generated/torch.igamma.html#torch.igamma "torch.igamma") |
| [`Tensor.igamma_`](generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_ "torch.Tensor.igamma_") | [`igamma()`](generated/torch.Tensor.igamma.html#torch.Tensor.igamma "torch.Tensor.igamma") 的原地版本。 |
| [`Tensor.igammac`](generated/torch.Tensor.igammac.html#torch.Tensor.igammac "torch.Tensor.igammac") | 参见 [`torch.igammac()`](generated/torch.igammac.html#torch.igammac "torch.igammac") |
| [`Tensor.igammac_`](generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_ "torch.Tensor.igammac_") | [`igammac()`](generated/torch.Tensor.igammac.html#torch.Tensor.igammac "torch.Tensor.igammac")的原地版本。 |
| [`Tensor.index_add_`](generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_ "torch.Tensor.index_add_") | 通过将`alpha`倍的`source`元素累加到`self`张量中，按照`index`中给定的顺序添加到索引中。 |
| [`Tensor.index_add`](generated/torch.Tensor.index_add.html#torch.Tensor.index_add "torch.Tensor.index_add") | [`torch.Tensor.index_add_()`](generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_ "torch.Tensor.index_add_")的非原地版本。 |
| [`Tensor.index_copy_`](generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_ "torch.Tensor.index_copy_") | 通过按照`index`中给定的顺序选择的索引，将[`tensor`](generated/torch.tensor.html#torch.tensor "torch.tensor")的元素复制到`self`张量中。 |
| [`Tensor.index_copy`](generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy "torch.Tensor.index_copy") | [`torch.Tensor.index_copy_()`](generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_ "torch.Tensor.index_copy_")的非原地版本。 |
| [`Tensor.index_fill_`](generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_ "torch.Tensor.index_fill_") | 通过按照`index`中给定的顺序选择的索引，用值`value`填充`self`张量的元素。 |
| [`Tensor.index_fill`](generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill "torch.Tensor.index_fill") | [`torch.Tensor.index_fill_()`](generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_ "torch.Tensor.index_fill_")的非原地版本。 |
| [`Tensor.index_put_`](generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_ "torch.Tensor.index_put_") | 使用`indices`中指定的索引（一个张量元组）将张量`values`中的值放入张量`self`中。 |
| [`Tensor.index_put`](generated/torch.Tensor.index_put.html#torch.Tensor.index_put "torch.Tensor.index_put") | [`index_put_()`](generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_ "torch.Tensor.index_put_")的非原地版本。 |
| [`Tensor.index_reduce_`](generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_ "torch.Tensor.index_reduce_") | 通过使用`reduce`参数给定的减少方式，按照`index`中给定的顺序将`source`元素累加到`self`张量中。 |
| [`Tensor.index_reduce`](generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce "torch.Tensor.index_reduce") |  |
| [`Tensor.index_select`](generated/torch.Tensor.index_select.html#torch.Tensor.index_select "torch.Tensor.index_select") | 参见[`torch.index_select()`](generated/torch.index_select.html#torch.index_select "torch.index_select") |
| [`Tensor.indices`](generated/torch.Tensor.indices.html#torch.Tensor.indices "torch.Tensor.indices") | 返回[稀疏COO张量](sparse.html#sparse-coo-docs)的索引张量。 |
| [`Tensor.inner`](generated/torch.Tensor.inner.html#torch.Tensor.inner "torch.Tensor.inner") | 参见[`torch.inner()`](generated/torch.inner.html#torch.inner "torch.inner")。 |
| [`Tensor.int`](generated/torch.Tensor.int.html#torch.Tensor.int "torch.Tensor.int") | `self.int()`等同于`self.to(torch.int32)`。 |
| [`Tensor.int_repr`](generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr "torch.Tensor.int_repr") | 给定一个量化张量，`self.int_repr()`返回一个CPU张量，数据类型为uint8_t，存储给定张量的底层uint8_t值。 |
| [`Tensor.inverse`](generated/torch.Tensor.inverse.html#torch.Tensor.inverse "torch.Tensor.inverse") | 参见[`torch.inverse()`](generated/torch.inverse.html#torch.inverse "torch.inverse") |
| [`Tensor.isclose`](generated/torch.Tensor.isclose.html#torch.Tensor.isclose "torch.Tensor.isclose") | 参见[`torch.isclose()`](generated/torch.isclose.html#torch.isclose "torch.isclose") |
| [`Tensor.isfinite`](generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite "torch.Tensor.isfinite") | 参见[`torch.isfinite()`](generated/torch.isfinite.html#torch.isfinite "torch.isfinite") |
| [`Tensor.isinf`](generated/torch.Tensor.isinf.html#torch.Tensor.isinf "torch.Tensor.isinf") | 参见[`torch.isinf()`](generated/torch.isinf.html#torch.isinf "torch.isinf") |
| [`Tensor.isposinf`](generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf "torch.Tensor.isposinf") | 参见[`torch.isposinf()`](generated/torch.isposinf.html#torch.isposinf "torch.isposinf") |
| [`Tensor.isneginf`](generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf "torch.Tensor.isneginf") | 参见[`torch.isneginf()`](generated/torch.isneginf.html#torch.isneginf "torch.isneginf") |
| [`Tensor.isnan`](generated/torch.Tensor.isnan.html#torch.Tensor.isnan "torch.Tensor.isnan") | 参见[`torch.isnan()`](generated/torch.isnan.html#torch.isnan "torch.isnan") |
| [`Tensor.is_contiguous`](generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous "torch.Tensor.is_contiguous") | 如果`self`张量在内存中按照内存格式指定的顺序是连续的，则返回True。 |
| [`Tensor.is_complex`](generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex "torch.Tensor.is_complex") | 如果`self`的数据类型是复数数据类型，则返回True。 |
| [`Tensor.is_conj`](generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj "torch.Tensor.is_conj") | 如果`self`的共轭位设置为true，则返回True。 |
| [`Tensor.is_floating_point`](generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point "torch.Tensor.is_floating_point") | 如果`self`的数据类型是浮点数据类型，则返回True。 |
| [`Tensor.is_inference`](generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference "torch.Tensor.is_inference") | 参见`torch.is_inference()` |
| [`Tensor.is_leaf`](generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf "torch.Tensor.is_leaf") | 所有`requires_grad`为`False`的张量按照惯例都将是叶张量。 |
| [`Tensor.is_pinned`](generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned "torch.Tensor.is_pinned") | 如果此张量驻留在固定内存中，则返回true。 |
| [`Tensor.is_set_to`](generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to "torch.Tensor.is_set_to") | 如果两个张量指向完全相同的内存（相同的存储、偏移、大小和步幅），则返回True。 |
| [`Tensor.is_shared`](generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared "torch.Tensor.is_shared") | 检查张量是否在共享内存中。 |
| [`Tensor.is_signed`](generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed "torch.Tensor.is_signed") | 如果`self`的数据类型是有符号数据类型，则返回True。 |
| [`Tensor.is_sparse`](generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse "torch.Tensor.is_sparse") | 如果张量使用稀疏COO存储布局，则为`True`，否则为`False`。 |
| [`Tensor.istft`](generated/torch.Tensor.istft.html#torch.Tensor.istft "torch.Tensor.istft") | 参见[`torch.istft()`](generated/torch.istft.html#torch.istft "torch.istft") |
| [`Tensor.isreal`](generated/torch.Tensor.isreal.html#torch.Tensor.isreal "torch.Tensor.isreal") | 参见[`torch.isreal()`](generated/torch.isreal.html#torch.isreal "torch.isreal") |
| [`Tensor.item`](generated/torch.Tensor.item.html#torch.Tensor.item "torch.Tensor.item") | 将此张量的值作为标准Python数字返回。 |
| [`Tensor.kthvalue`](generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue "torch.Tensor.kthvalue") | 参见[`torch.kthvalue()`](generated/torch.kthvalue.html#torch.kthvalue "torch.kthvalue") |
| [`Tensor.lcm`](generated/torch.Tensor.lcm.html#torch.Tensor.lcm "torch.Tensor.lcm") | 查看 [`torch.lcm()`](generated/torch.lcm.html#torch.lcm "torch.lcm") |
| [`Tensor.lcm_`](generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_ "torch.Tensor.lcm_") | [`lcm()`](generated/torch.Tensor.lcm.html#torch.Tensor.lcm "torch.Tensor.lcm") 的原地版本。 |
| [`Tensor.ldexp`](generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp "torch.Tensor.ldexp") | 查看 [`torch.ldexp()`](generated/torch.ldexp.html#torch.ldexp "torch.ldexp") |
| [`Tensor.ldexp_`](generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_ "torch.Tensor.ldexp_") | [`ldexp()`](generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp "torch.Tensor.ldexp") 的原地版本。 |
| [`Tensor.le`](generated/torch.Tensor.le.html#torch.Tensor.le "torch.Tensor.le") | 查看 [`torch.le()`](generated/torch.le.html#torch.le "torch.le")。 |
| [`Tensor.le_`](generated/torch.Tensor.le_.html#torch.Tensor.le_ "torch.Tensor.le_") | [`le()`](generated/torch.Tensor.le.html#torch.Tensor.le "torch.Tensor.le") 的原地版本。 |
| [`Tensor.less_equal`](generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal "torch.Tensor.less_equal") | 查看 [`torch.less_equal()`](generated/torch.less_equal.html#torch.less_equal "torch.less_equal")。 |
| [`Tensor.less_equal_`](generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_ "torch.Tensor.less_equal_") | [`less_equal()`](generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal "torch.Tensor.less_equal") 的原地版本。 |
| [`Tensor.lerp`](generated/torch.Tensor.lerp.html#torch.Tensor.lerp "torch.Tensor.lerp") | 查看 [`torch.lerp()`](generated/torch.lerp.html#torch.lerp "torch.lerp") |
| [`Tensor.lerp_`](generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_ "torch.Tensor.lerp_") | [`lerp()`](generated/torch.Tensor.lerp.html#torch.Tensor.lerp "torch.Tensor.lerp") 的原地版本。 |
| [`Tensor.lgamma`](generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma "torch.Tensor.lgamma") | 查看 [`torch.lgamma()`](generated/torch.lgamma.html#torch.lgamma "torch.lgamma") |
| [`Tensor.lgamma_`](generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_ "torch.Tensor.lgamma_") | [`lgamma()`](generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma "torch.Tensor.lgamma") 的原地版本。 |
| [`Tensor.log`](generated/torch.Tensor.log.html#torch.Tensor.log "torch.Tensor.log") | 查看 [`torch.log()`](generated/torch.log.html#torch.log "torch.log") |
| [`Tensor.log_`](generated/torch.Tensor.log_.html#torch.Tensor.log_ "torch.Tensor.log_") | [`log()`](generated/torch.Tensor.log.html#torch.Tensor.log "torch.Tensor.log") 的原地版本。 |
| [`Tensor.logdet`](generated/torch.Tensor.logdet.html#torch.Tensor.logdet "torch.Tensor.logdet") | 查看 [`torch.logdet()`](generated/torch.logdet.html#torch.logdet "torch.logdet") |
| [`Tensor.log10`](generated/torch.Tensor.log10.html#torch.Tensor.log10 "torch.Tensor.log10") | 查看 [`torch.log10()`](generated/torch.log10.html#torch.log10 "torch.log10") |
| [`Tensor.log10_`](generated/torch.Tensor.log10_.html#torch.Tensor.log10_ "torch.Tensor.log10_") | [`log10()`](generated/torch.Tensor.log10.html#torch.Tensor.log10 "torch.Tensor.log10") 的原地版本。 |
| [`Tensor.log1p`](generated/torch.Tensor.log1p.html#torch.Tensor.log1p "torch.Tensor.log1p") | 查看 [`torch.log1p()`](generated/torch.log1p.html#torch.log1p "torch.log1p") |
| [`Tensor.log1p_`](generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_ "torch.Tensor.log1p_") | [`log1p()`](generated/torch.Tensor.log1p.html#torch.Tensor.log1p "torch.Tensor.log1p") 的原地版本。 |
| [`Tensor.log2`](generated/torch.Tensor.log2.html#torch.Tensor.log2 "torch.Tensor.log2") | 查看 [`torch.log2()`](generated/torch.log2.html#torch.log2 "torch.log2") |
| [`Tensor.log2_`](generated/torch.Tensor.log2_.html#torch.Tensor.log2_ "torch.Tensor.log2_") | [`log2()`](generated/torch.Tensor.log2.html#torch.Tensor.log2 "torch.Tensor.log2") 的原地版本。 |
| [`Tensor.log_normal_`](generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_ "torch.Tensor.log_normal_") | 使用给定的均值 $\mu$ 和标准差 $\sigma$ 参数化的对数正态分布中的样本填充 `self` 张量 |
| [`Tensor.logaddexp`](generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp "torch.Tensor.logaddexp") | 参见 [`torch.logaddexp()`](generated/torch.logaddexp.html#torch.logaddexp "torch.logaddexp") |
| [`Tensor.logaddexp2`](generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2 "torch.Tensor.logaddexp2") | 参见 [`torch.logaddexp2()`](generated/torch.logaddexp2.html#torch.logaddexp2 "torch.logaddexp2") |
| [`Tensor.logsumexp`](generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp "torch.Tensor.logsumexp") | 参见 [`torch.logsumexp()`](generated/torch.logsumexp.html#torch.logsumexp "torch.logsumexp") |
| [`Tensor.logical_and`](generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and "torch.Tensor.logical_and") | 参见 [`torch.logical_and()`](generated/torch.logical_and.html#torch.logical_and "torch.logical_and") |
| [`Tensor.logical_and_`](generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_ "torch.Tensor.logical_and_") | [`logical_and()`](generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and "torch.Tensor.logical_and") 的原地版本 |
| [`Tensor.logical_not`](generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not "torch.Tensor.logical_not") | 参见 [`torch.logical_not()`](generated/torch.logical_not.html#torch.logical_not "torch.logical_not") |
| [`Tensor.logical_not_`](generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_ "torch.Tensor.logical_not_") | [`logical_not()`](generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not "torch.Tensor.logical_not") 的原地版本 |
| [`Tensor.logical_or`](generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or "torch.Tensor.logical_or") | 参见 [`torch.logical_or()`](generated/torch.logical_or.html#torch.logical_or "torch.logical_or") |
| [`Tensor.logical_or_`](generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_ "torch.Tensor.logical_or_") | [`logical_or()`](generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or "torch.Tensor.logical_or") 的原地版本 |
| [`Tensor.logical_xor`](generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor "torch.Tensor.logical_xor") | 参见 [`torch.logical_xor()`](generated/torch.logical_xor.html#torch.logical_xor "torch.logical_xor") |
| [`Tensor.logical_xor_`](generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_ "torch.Tensor.logical_xor_") | [`logical_xor()`](generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor "torch.Tensor.logical_xor") 的原地版本 |
| [`Tensor.logit`](generated/torch.Tensor.logit.html#torch.Tensor.logit "torch.Tensor.logit") | 参见 [`torch.logit()`](generated/torch.logit.html#torch.logit "torch.logit") |
| [`Tensor.logit_`](generated/torch.Tensor.logit_.html#torch.Tensor.logit_ "torch.Tensor.logit_") | [`logit()`](generated/torch.Tensor.logit.html#torch.Tensor.logit "torch.Tensor.logit") 的原地版本 |
| [`Tensor.long`](generated/torch.Tensor.long.html#torch.Tensor.long "torch.Tensor.long") | `self.long()` 等同于 `self.to(torch.int64)` |
| [`Tensor.lt`](generated/torch.Tensor.lt.html#torch.Tensor.lt "torch.Tensor.lt") | 参见 [`torch.lt()`](generated/torch.lt.html#torch.lt "torch.lt") |
| [`Tensor.lt_`](generated/torch.Tensor.lt_.html#torch.Tensor.lt_ "torch.Tensor.lt_") | [`lt()`](generated/torch.Tensor.lt.html#torch.Tensor.lt "torch.Tensor.lt") 的原地版本 |
| [`Tensor.less`](generated/torch.Tensor.less.html#torch.Tensor.less "torch.Tensor.less") | lt(other) -> Tensor |
| [`Tensor.less_`](generated/torch.Tensor.less_.html#torch.Tensor.less_ "torch.Tensor.less_") | [`less()`](generated/torch.Tensor.less.html#torch.Tensor.less "torch.Tensor.less") 的原地版本 |
| [`Tensor.lu`](generated/torch.Tensor.lu.html#torch.Tensor.lu "torch.Tensor.lu") | 查看 [`torch.lu()`](generated/torch.lu.html#torch.lu "torch.lu") |
| [`Tensor.lu_solve`](generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve "torch.Tensor.lu_solve") | 查看 [`torch.lu_solve()`](generated/torch.lu_solve.html#torch.lu_solve "torch.lu_solve") |
| [`Tensor.as_subclass`](generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass "torch.Tensor.as_subclass") | 创建一个具有与`self`相同数据指针的`cls`实例。 |
| [`Tensor.map_`](generated/torch.Tensor.map_.html#torch.Tensor.map_ "torch.Tensor.map_") | 对`self`张量中的每个元素应用`callable`，并将结果存储在`self`张量中。 |
| [`Tensor.masked_scatter_`](generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_ "torch.Tensor.masked_scatter_") | 将`source`中的元素复制到`self`张量中，其中`mask`为True。 |
| [`Tensor.masked_scatter`](generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter "torch.Tensor.masked_scatter") | [`torch.Tensor.masked_scatter_()`](generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_ "torch.Tensor.masked_scatter_")的非就地版本 |
| [`Tensor.masked_fill_`](generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_ "torch.Tensor.masked_fill_") | 在`mask`为True的位置，用`value`填充`self`张量的元素。 |
| [`Tensor.masked_fill`](generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill "torch.Tensor.masked_fill") | [`torch.Tensor.masked_fill_()`](generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_ "torch.Tensor.masked_fill_")的非就地版本 |
| [`Tensor.masked_select`](generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select "torch.Tensor.masked_select") | 查看 [`torch.masked_select()`](generated/torch.masked_select.html#torch.masked_select "torch.masked_select") |
| [`Tensor.matmul`](generated/torch.Tensor.matmul.html#torch.Tensor.matmul "torch.Tensor.matmul") | 查看 [`torch.matmul()`](generated/torch.matmul.html#torch.matmul "torch.matmul") |
| [`Tensor.matrix_power`](generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power "torch.Tensor.matrix_power") |

注意

[`matrix_power()`](generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power "torch.Tensor.matrix_power")已弃用，请使用[`torch.linalg.matrix_power()`](generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power "torch.linalg.matrix_power")代替。

|

| [`Tensor.matrix_exp`](generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp "torch.Tensor.matrix_exp") | 查看 [`torch.matrix_exp()`](generated/torch.matrix_exp.html#torch.matrix_exp "torch.matrix_exp") |
| --- | --- |
| [`Tensor.max`](generated/torch.Tensor.max.html#torch.Tensor.max "torch.Tensor.max") | 查看 [`torch.max()`](generated/torch.max.html#torch.max "torch.max") |
| [`Tensor.maximum`](generated/torch.Tensor.maximum.html#torch.Tensor.maximum "torch.Tensor.maximum") | 查看 [`torch.maximum()`](generated/torch.maximum.html#torch.maximum "torch.maximum") |
| [`Tensor.mean`](generated/torch.Tensor.mean.html#torch.Tensor.mean "torch.Tensor.mean") | 查看 [`torch.mean()`](generated/torch.mean.html#torch.mean "torch.mean") |
| [`Tensor.nanmean`](generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean "torch.Tensor.nanmean") | 查看 [`torch.nanmean()`](generated/torch.nanmean.html#torch.nanmean "torch.nanmean") |
| [`Tensor.median`](generated/torch.Tensor.median.html#torch.Tensor.median "torch.Tensor.median") | 查看 [`torch.median()`](generated/torch.median.html#torch.median "torch.median") |
| [`Tensor.nanmedian`](generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian "torch.Tensor.nanmedian") | 查看 [`torch.nanmedian()`](generated/torch.nanmedian.html#torch.nanmedian "torch.nanmedian") |
| [`Tensor.min`](generated/torch.Tensor.min.html#torch.Tensor.min "torch.Tensor.min") | 查看 [`torch.min()`](generated/torch.min.html#torch.min "torch.min") |
| [`Tensor.minimum`](generated/torch.Tensor.minimum.html#torch.Tensor.minimum "torch.Tensor.minimum") | 查看 [`torch.minimum()`](generated/torch.minimum.html#torch.minimum "torch.minimum") |
| [`Tensor.mm`](generated/torch.Tensor.mm.html#torch.Tensor.mm "torch.Tensor.mm") | 查看 [`torch.mm()`](generated/torch.mm.html#torch.mm "torch.mm") |
| [`Tensor.smm`](generated/torch.Tensor.smm.html#torch.Tensor.smm "torch.Tensor.smm") | 查看 [`torch.smm()`](generated/torch.smm.html#torch.smm "torch.smm") |
| [`Tensor.mode`](generated/torch.Tensor.mode.html#torch.Tensor.mode "torch.Tensor.mode") | 查看 [`torch.mode()`](generated/torch.mode.html#torch.mode "torch.mode") |
| [`Tensor.movedim`](generated/torch.Tensor.movedim.html#torch.Tensor.movedim "torch.Tensor.movedim") | 查看 [`torch.movedim()`](generated/torch.movedim.html#torch.movedim "torch.movedim") |
| [`Tensor.moveaxis`](generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis "torch.Tensor.moveaxis") | 查看 [`torch.moveaxis()`](generated/torch.moveaxis.html#torch.moveaxis "torch.moveaxis") |
| [`Tensor.msort`](generated/torch.Tensor.msort.html#torch.Tensor.msort "torch.Tensor.msort") | 查看 [`torch.msort()`](generated/torch.msort.html#torch.msort "torch.msort") |
| [`Tensor.mul`](generated/torch.Tensor.mul.html#torch.Tensor.mul "torch.Tensor.mul") | 查看 [`torch.mul()`](generated/torch.mul.html#torch.mul "torch.mul") |
| [`Tensor.mul_`](generated/torch.Tensor.mul_.html#torch.Tensor.mul_ "torch.Tensor.mul_") | [`mul()`](generated/torch.Tensor.mul.html#torch.Tensor.mul "torch.Tensor.mul") 的原地版本 |
| [`Tensor.multiply`](generated/torch.Tensor.multiply.html#torch.Tensor.multiply "torch.Tensor.multiply") | 查看 [`torch.multiply()`](generated/torch.multiply.html#torch.multiply "torch.multiply") |
| [`Tensor.multiply_`](generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_ "torch.Tensor.multiply_") | [`multiply()`](generated/torch.Tensor.multiply.html#torch.Tensor.multiply "torch.Tensor.multiply") 的原地版本 |
| [`Tensor.multinomial`](generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial "torch.Tensor.multinomial") | 查看 [`torch.multinomial()`](generated/torch.multinomial.html#torch.multinomial "torch.multinomial") |
| [`Tensor.mv`](generated/torch.Tensor.mv.html#torch.Tensor.mv "torch.Tensor.mv") | 查看 [`torch.mv()`](generated/torch.mv.html#torch.mv "torch.mv") |
| [`Tensor.mvlgamma`](generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma "torch.Tensor.mvlgamma") | 查看 [`torch.mvlgamma()`](generated/torch.mvlgamma.html#torch.mvlgamma "torch.mvlgamma") |
| [`Tensor.mvlgamma_`](generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_ "torch.Tensor.mvlgamma_") | [`mvlgamma()`](generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma "torch.Tensor.mvlgamma") 的原地版本 |
| [`Tensor.nansum`](generated/torch.Tensor.nansum.html#torch.Tensor.nansum "torch.Tensor.nansum") | 查看 [`torch.nansum()`](generated/torch.nansum.html#torch.nansum "torch.nansum") |
| [`Tensor.narrow`](generated/torch.Tensor.narrow.html#torch.Tensor.narrow "torch.Tensor.narrow") | 查看 [`torch.narrow()`](generated/torch.narrow.html#torch.narrow "torch.narrow") |
| [`Tensor.narrow_copy`](generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy "torch.Tensor.narrow_copy") | 查看 [`torch.narrow_copy()`](generated/torch.narrow_copy.html#torch.narrow_copy "torch.narrow_copy") |
| [`Tensor.ndimension`](generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension "torch.Tensor.ndimension") | [`dim()`](generated/torch.Tensor.dim.html#torch.Tensor.dim "torch.Tensor.dim") 的别名 |
| [`Tensor.nan_to_num`](generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num "torch.Tensor.nan_to_num") | 查看 [`torch.nan_to_num()`](generated/torch.nan_to_num.html#torch.nan_to_num "torch.nan_to_num") |
| [`Tensor.nan_to_num_`](generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_ "torch.Tensor.nan_to_num_") | [`nan_to_num()`](generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num "torch.Tensor.nan_to_num") 的原地版本。 |
| [`Tensor.ne`](generated/torch.Tensor.ne.html#torch.Tensor.ne "torch.Tensor.ne") | 参见 [`torch.ne()`](generated/torch.ne.html#torch.ne "torch.ne") |
| [`Tensor.ne_`](generated/torch.Tensor.ne_.html#torch.Tensor.ne_ "torch.Tensor.ne_") | [`ne()`](generated/torch.Tensor.ne.html#torch.Tensor.ne "torch.Tensor.ne") 的原地版本。 |
| [`Tensor.not_equal`](generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal "torch.Tensor.not_equal") | 参见 [`torch.not_equal()`](generated/torch.not_equal.html#torch.not_equal "torch.not_equal") |
| [`Tensor.not_equal_`](generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_ "torch.Tensor.not_equal_") | [`not_equal()`](generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal "torch.Tensor.not_equal") 的原地版本。 |
| [`Tensor.neg`](generated/torch.Tensor.neg.html#torch.Tensor.neg "torch.Tensor.neg") | 参见 [`torch.neg()`](generated/torch.neg.html#torch.neg "torch.neg") |
| [`Tensor.neg_`](generated/torch.Tensor.neg_.html#torch.Tensor.neg_ "torch.Tensor.neg_") | [`neg()`](generated/torch.Tensor.neg.html#torch.Tensor.neg "torch.Tensor.neg") 的原地版本。 |
| [`Tensor.negative`](generated/torch.Tensor.negative.html#torch.Tensor.negative "torch.Tensor.negative") | 参见 [`torch.negative()`](generated/torch.negative.html#torch.negative "torch.negative") |
| [`Tensor.negative_`](generated/torch.Tensor.negative_.html#torch.Tensor.negative_ "torch.Tensor.negative_") | [`negative()`](generated/torch.Tensor.negative.html#torch.Tensor.negative "torch.Tensor.negative") 的原地版本。 |
| [`Tensor.nelement`](generated/torch.Tensor.nelement.html#torch.Tensor.nelement "torch.Tensor.nelement") | [`numel()`](generated/torch.Tensor.numel.html#torch.Tensor.numel "torch.Tensor.numel") 的别名 |
| [`Tensor.nextafter`](generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter "torch.Tensor.nextafter") | 参见 [`torch.nextafter()`](generated/torch.nextafter.html#torch.nextafter "torch.nextafter") |
| [`Tensor.nextafter_`](generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_ "torch.Tensor.nextafter_") | [`nextafter()`](generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter "torch.Tensor.nextafter") 的原地版本。 |
| [`Tensor.nonzero`](generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero "torch.Tensor.nonzero") | 参见 [`torch.nonzero()`](generated/torch.nonzero.html#torch.nonzero "torch.nonzero") |
| [`Tensor.norm`](generated/torch.Tensor.norm.html#torch.Tensor.norm "torch.Tensor.norm") | 参见 [`torch.norm()`](generated/torch.norm.html#torch.norm "torch.norm") |
| [`Tensor.normal_`](generated/torch.Tensor.normal_.html#torch.Tensor.normal_ "torch.Tensor.normal_") | 使用由 [`mean`](generated/torch.mean.html#torch.mean "torch.mean") 和 [`std`](generated/torch.std.html#torch.std "torch.std") 参数化的正态分布样本填充 `self` 张量。 |
| [`Tensor.numel`](generated/torch.Tensor.numel.html#torch.Tensor.numel "torch.Tensor.numel") | 参见 [`torch.numel()`](generated/torch.numel.html#torch.numel "torch.numel") |
| [`Tensor.numpy`](generated/torch.Tensor.numpy.html#torch.Tensor.numpy "torch.Tensor.numpy") | 将张量返回为 NumPy `ndarray`。 |
| [`Tensor.orgqr`](generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr "torch.Tensor.orgqr") | 参见 [`torch.orgqr()`](generated/torch.orgqr.html#torch.orgqr "torch.orgqr") |
| [`Tensor.ormqr`](generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr "torch.Tensor.ormqr") | 参见 [`torch.ormqr()`](generated/torch.ormqr.html#torch.ormqr "torch.ormqr") |
| [`Tensor.outer`](generated/torch.Tensor.outer.html#torch.Tensor.outer "torch.Tensor.outer") | 参见 [`torch.outer()`](generated/torch.outer.html#torch.outer "torch.outer") |
| [`Tensor.permute`](generated/torch.Tensor.permute.html#torch.Tensor.permute "torch.Tensor.permute") | 查看 [`torch.permute()`](generated/torch.permute.html#torch.permute "torch.permute") |
| [`Tensor.pin_memory`](generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory "torch.Tensor.pin_memory") | 如果尚未固定，将张量复制到固定内存中。 |
| [`Tensor.pinverse`](generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse "torch.Tensor.pinverse") | 查看 [`torch.pinverse()`](generated/torch.pinverse.html#torch.pinverse "torch.pinverse") |
| [`Tensor.polygamma`](generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma "torch.Tensor.polygamma") | 查看 [`torch.polygamma()`](generated/torch.polygamma.html#torch.polygamma "torch.polygamma") |
| [`Tensor.polygamma_`](generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_ "torch.Tensor.polygamma_") | [`polygamma()`](generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma "torch.Tensor.polygamma") 的原地版本 |
| [`Tensor.positive`](generated/torch.Tensor.positive.html#torch.Tensor.positive "torch.Tensor.positive") | 查看 [`torch.positive()`](generated/torch.positive.html#torch.positive "torch.positive") |
| [`Tensor.pow`](generated/torch.Tensor.pow.html#torch.Tensor.pow "torch.Tensor.pow") | 查看 [`torch.pow()`](generated/torch.pow.html#torch.pow "torch.pow") |
| [`Tensor.pow_`](generated/torch.Tensor.pow_.html#torch.Tensor.pow_ "torch.Tensor.pow_") | [`pow()`](generated/torch.Tensor.pow.html#torch.Tensor.pow "torch.Tensor.pow") 的原地版本 |
| [`Tensor.prod`](generated/torch.Tensor.prod.html#torch.Tensor.prod "torch.Tensor.prod") | 查看 [`torch.prod()`](generated/torch.prod.html#torch.prod "torch.prod") |
| [`Tensor.put_`](generated/torch.Tensor.put_.html#torch.Tensor.put_ "torch.Tensor.put_") | 将 `source` 中的元素复制到由 `index` 指定的位置。 |
| [`Tensor.qr`](generated/torch.Tensor.qr.html#torch.Tensor.qr "torch.Tensor.qr") | 查看 [`torch.qr()`](generated/torch.qr.html#torch.qr "torch.qr") |
| [`Tensor.qscheme`](generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme "torch.Tensor.qscheme") | 返回给定 QTensor 的量化方案。 |
| [`Tensor.quantile`](generated/torch.Tensor.quantile.html#torch.Tensor.quantile "torch.Tensor.quantile") | 查看 [`torch.quantile()`](generated/torch.quantile.html#torch.quantile "torch.quantile") |
| [`Tensor.nanquantile`](generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile "torch.Tensor.nanquantile") | 查看 [`torch.nanquantile()`](generated/torch.nanquantile.html#torch.nanquantile "torch.nanquantile") |
| [`Tensor.q_scale`](generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale "torch.Tensor.q_scale") | 给定通过线性（仿射）量化的张量，返回底层量化器的比例。 |
| [`Tensor.q_zero_point`](generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point "torch.Tensor.q_zero_point") | 给定通过线性（仿射）量化的张量，返回底层量化器的零点。 |
| [`Tensor.q_per_channel_scales`](generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales "torch.Tensor.q_per_channel_scales") | 给定通过线性（仿射）逐通道量化的张量，返回底层量化器的比例张量。 |
| [`Tensor.q_per_channel_zero_points`](generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points "torch.Tensor.q_per_channel_zero_points") | 给定通过线性（仿射）逐通道量化的张量，返回底层量化器的零点张量。 |
| [`Tensor.q_per_channel_axis`](generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis "torch.Tensor.q_per_channel_axis") | 给定通过线性（仿射）逐通道量化的张量，返回应用逐通道量化的维度索引。 |
| [`Tensor.rad2deg`](generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg "torch.Tensor.rad2deg") | 参见[`torch.rad2deg()`](generated/torch.rad2deg.html#torch.rad2deg "torch.rad2deg") |
| [`Tensor.random_`](generated/torch.Tensor.random_.html#torch.Tensor.random_ "torch.Tensor.random_") | 用从离散均匀分布`[from, to - 1]`中抽样的数字填充`self`张量。 |
| [`Tensor.ravel`](generated/torch.Tensor.ravel.html#torch.Tensor.ravel "torch.Tensor.ravel") | 参见[`torch.ravel()`](generated/torch.ravel.html#torch.ravel "torch.ravel") |
| [`Tensor.reciprocal`](generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal "torch.Tensor.reciprocal") | 参见[`torch.reciprocal()`](generated/torch.reciprocal.html#torch.reciprocal "torch.reciprocal") |
| [`Tensor.reciprocal_`](generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_ "torch.Tensor.reciprocal_") | [`reciprocal()`](generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal "torch.Tensor.reciprocal")的原位版本 |
| [`Tensor.record_stream`](generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream "torch.Tensor.record_stream") | 将张量标记为此流程已使用。 |
| [`Tensor.register_hook`](generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook "torch.Tensor.register_hook") | 注册一个反向钩子。 |
| [`Tensor.register_post_accumulate_grad_hook`](generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook "torch.Tensor.register_post_accumulate_grad_hook") | 注册一个在梯度累积后运行的反向钩子。 |
| [`Tensor.remainder`](generated/torch.Tensor.remainder.html#torch.Tensor.remainder "torch.Tensor.remainder") | 参见[`torch.remainder()`](generated/torch.remainder.html#torch.remainder "torch.remainder") |
| [`Tensor.remainder_`](generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_ "torch.Tensor.remainder_") | [`remainder()`](generated/torch.Tensor.remainder.html#torch.Tensor.remainder "torch.Tensor.remainder")的原位版本 |
| [`Tensor.renorm`](generated/torch.Tensor.renorm.html#torch.Tensor.renorm "torch.Tensor.renorm") | 参见[`torch.renorm()`](generated/torch.renorm.html#torch.renorm "torch.renorm") |
| [`Tensor.renorm_`](generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_ "torch.Tensor.renorm_") | [`renorm()`](generated/torch.Tensor.renorm.html#torch.Tensor.renorm "torch.Tensor.renorm")的原位版本 |
| [`Tensor.repeat`](generated/torch.Tensor.repeat.html#torch.Tensor.repeat "torch.Tensor.repeat") | 沿指定维度重复此张量。 |
| [`Tensor.repeat_interleave`](generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave "torch.Tensor.repeat_interleave") | 参见[`torch.repeat_interleave()`](generated/torch.repeat_interleave.html#torch.repeat_interleave "torch.repeat_interleave")。 |
| [`Tensor.requires_grad`](generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad "torch.Tensor.requires_grad") | 如果需要为此张量计算梯度，则为`True`，否则为`False`。 |
| [`Tensor.requires_grad_`](generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_ "torch.Tensor.requires_grad_") | 更改是否应记录此张量上的操作的自动微分：就地设置此张量的`requires_grad`属性。 |
| [`Tensor.reshape`](generated/torch.Tensor.reshape.html#torch.Tensor.reshape "torch.Tensor.reshape") | 返回一个与`self`具有相同数据和元素数量但具有指定形状的张量。 |
| [`Tensor.reshape_as`](generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as "torch.Tensor.reshape_as") | 将此张量返回为与`other`相同形状的张量。 |
| [`Tensor.resize_`](generated/torch.Tensor.resize_.html#torch.Tensor.resize_ "torch.Tensor.resize_") | 将`self`张量调整为指定大小。 |
| [`Tensor.resize_as_`](generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_ "torch.Tensor.resize_as_") | 将 `self` 张量调整为与指定的 [`tensor`](generated/torch.tensor.html#torch.tensor "torch.tensor") 相同大小 |
| [`Tensor.retain_grad`](generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad "torch.Tensor.retain_grad") | 在 `backward()` 过程中使此张量的 `grad` 被填充 |
| [`Tensor.retains_grad`](generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad "torch.Tensor.retains_grad") | 如果此张量是非叶子节点且其 `grad` 已启用填充，则为 `True`，否则为 `False` |
| [`Tensor.roll`](generated/torch.Tensor.roll.html#torch.Tensor.roll "torch.Tensor.roll") | 查看 [`torch.roll()`](generated/torch.roll.html#torch.roll "torch.roll") |
| [`Tensor.rot90`](generated/torch.Tensor.rot90.html#torch.Tensor.rot90 "torch.Tensor.rot90") | 查看 [`torch.rot90()`](generated/torch.rot90.html#torch.rot90 "torch.rot90") |
| [`Tensor.round`](generated/torch.Tensor.round.html#torch.Tensor.round "torch.Tensor.round") | 查看 [`torch.round()`](generated/torch.round.html#torch.round "torch.round") |
| [`Tensor.round_`](generated/torch.Tensor.round_.html#torch.Tensor.round_ "torch.Tensor.round_") | [`round()`](generated/torch.Tensor.round.html#torch.Tensor.round "torch.Tensor.round") 的就地版本 |
| [`Tensor.rsqrt`](generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt "torch.Tensor.rsqrt") | 查看 [`torch.rsqrt()`](generated/torch.rsqrt.html#torch.rsqrt "torch.rsqrt") |
| [`Tensor.rsqrt_`](generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_ "torch.Tensor.rsqrt_") | [`rsqrt()`](generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt "torch.Tensor.rsqrt") 的就地版本 |
| [`Tensor.scatter`](generated/torch.Tensor.scatter.html#torch.Tensor.scatter "torch.Tensor.scatter") | [`torch.Tensor.scatter_()`](generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_ "torch.Tensor.scatter_") 的非就地版本 |
| [`Tensor.scatter_`](generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_ "torch.Tensor.scatter_") | 将张量 `src` 中的所有值写入到指定在 `index` 张量中的 `self` 中 |
| [`Tensor.scatter_add_`](generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_ "torch.Tensor.scatter_add_") | 将张量 `src` 中的所有值添加到 `index` 张量中指定的 `self` 中，类似于 [`scatter_()`](generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_ "torch.Tensor.scatter_") 的方式 |
| [`Tensor.scatter_add`](generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add "torch.Tensor.scatter_add") | [`torch.Tensor.scatter_add_()`](generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_ "torch.Tensor.scatter_add_") 的非就地版本 |
| [`Tensor.scatter_reduce_`](generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_ "torch.Tensor.scatter_reduce_") | 将 `src` 张量中的所有值按照应用的减少方式（`"sum"`、`"prod"`、`"mean"`、`"amax"`、`"amin"`）减少到 `index` 张量中指定的索引中的 `self` 张量中 |
| [`Tensor.scatter_reduce`](generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce "torch.Tensor.scatter_reduce") | [`torch.Tensor.scatter_reduce_()`](generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_ "torch.Tensor.scatter_reduce_") 的非就地版本 |
| [`Tensor.select`](generated/torch.Tensor.select.html#torch.Tensor.select "torch.Tensor.select") | 查看 [`torch.select()`](generated/torch.select.html#torch.select "torch.select") |
| [`Tensor.select_scatter`](generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter "torch.Tensor.select_scatter") | 查看 [`torch.select_scatter()`](generated/torch.select_scatter.html#torch.select_scatter "torch.select_scatter") |
| [`Tensor.set_`](generated/torch.Tensor.set_.html#torch.Tensor.set_ "torch.Tensor.set_") | 设置底层存储、大小和步幅。 |
| [`Tensor.share_memory_`](generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_ "torch.Tensor.share_memory_") | 将底层存储移动到共享内存。 |
| [`Tensor.short`](generated/torch.Tensor.short.html#torch.Tensor.short "torch.Tensor.short") | `self.short()` 等同于 `self.to(torch.int16)`。 |
| [`Tensor.sigmoid`](generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid "torch.Tensor.sigmoid") | 查看 [`torch.sigmoid()`](generated/torch.sigmoid.html#torch.sigmoid "torch.sigmoid") |
| [`Tensor.sigmoid_`](generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_ "torch.Tensor.sigmoid_") | [`sigmoid()`](generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid "torch.Tensor.sigmoid") 的原地版本 |
| [`Tensor.sign`](generated/torch.Tensor.sign.html#torch.Tensor.sign "torch.Tensor.sign") | 查看 [`torch.sign()`](generated/torch.sign.html#torch.sign "torch.sign") |
| [`Tensor.sign_`](generated/torch.Tensor.sign_.html#torch.Tensor.sign_ "torch.Tensor.sign_") | [`sign()`](generated/torch.Tensor.sign.html#torch.Tensor.sign "torch.Tensor.sign") 的原地版本 |
| [`Tensor.signbit`](generated/torch.Tensor.signbit.html#torch.Tensor.signbit "torch.Tensor.signbit") | 查看 [`torch.signbit()`](generated/torch.signbit.html#torch.signbit "torch.signbit") |
| [`Tensor.sgn`](generated/torch.Tensor.sgn.html#torch.Tensor.sgn "torch.Tensor.sgn") | 查看 [`torch.sgn()`](generated/torch.sgn.html#torch.sgn "torch.sgn") |
| [`Tensor.sgn_`](generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_ "torch.Tensor.sgn_") | [`sgn()`](generated/torch.Tensor.sgn.html#torch.Tensor.sgn "torch.Tensor.sgn") 的原地版本 |
| [`Tensor.sin`](generated/torch.Tensor.sin.html#torch.Tensor.sin "torch.Tensor.sin") | 查看 [`torch.sin()`](generated/torch.sin.html#torch.sin "torch.sin") |
| [`Tensor.sin_`](generated/torch.Tensor.sin_.html#torch.Tensor.sin_ "torch.Tensor.sin_") | [`sin()`](generated/torch.Tensor.sin.html#torch.Tensor.sin "torch.Tensor.sin") 的原地版本 |
| [`Tensor.sinc`](generated/torch.Tensor.sinc.html#torch.Tensor.sinc "torch.Tensor.sinc") | 查看 [`torch.sinc()`](generated/torch.sinc.html#torch.sinc "torch.sinc") |
| [`Tensor.sinc_`](generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_ "torch.Tensor.sinc_") | [`sinc()`](generated/torch.Tensor.sinc.html#torch.Tensor.sinc "torch.Tensor.sinc") 的原地版本 |
| [`Tensor.sinh`](generated/torch.Tensor.sinh.html#torch.Tensor.sinh "torch.Tensor.sinh") | 查看 [`torch.sinh()`](generated/torch.sinh.html#torch.sinh "torch.sinh") |
| [`Tensor.sinh_`](generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_ "torch.Tensor.sinh_") | [`sinh()`](generated/torch.Tensor.sinh.html#torch.Tensor.sinh "torch.Tensor.sinh") 的原地版本 |
| [`Tensor.asinh`](generated/torch.Tensor.asinh.html#torch.Tensor.asinh "torch.Tensor.asinh") | 查看 [`torch.asinh()`](generated/torch.asinh.html#torch.asinh "torch.asinh") |
| [`Tensor.asinh_`](generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_ "torch.Tensor.asinh_") | [`asinh()`](generated/torch.Tensor.asinh.html#torch.Tensor.asinh "torch.Tensor.asinh") 的原地版本 |
| [`Tensor.arcsinh`](generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh "torch.Tensor.arcsinh") | 查看 [`torch.arcsinh()`](generated/torch.arcsinh.html#torch.arcsinh "torch.arcsinh") |
| [`Tensor.arcsinh_`](generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_ "torch.Tensor.arcsinh_") | [`arcsinh()`](generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh "torch.Tensor.arcsinh") 的原地版本 |
| [`Tensor.shape`](generated/torch.Tensor.shape.html#torch.Tensor.shape "torch.Tensor.shape") | 返回 `self` 张量的大小。 |
| [`Tensor.size`](generated/torch.Tensor.size.html#torch.Tensor.size "torch.Tensor.size") | 返回 `self` 张量的大小。 |
| [`Tensor.slogdet`](generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet "torch.Tensor.slogdet") | 查看 [`torch.slogdet()`](generated/torch.slogdet.html#torch.slogdet "torch.slogdet") |
| [`Tensor.slice_scatter`](generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter "torch.Tensor.slice_scatter") | 查看 [`torch.slice_scatter()`](generated/torch.slice_scatter.html#torch.slice_scatter "torch.slice_scatter") |
| [`Tensor.softmax`](generated/torch.Tensor.softmax.html#torch.Tensor.softmax "torch.Tensor.softmax") | [`torch.nn.functional.softmax()`](generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax "torch.nn.functional.softmax") 的别名。 |
| [`Tensor.sort`](generated/torch.Tensor.sort.html#torch.Tensor.sort "torch.Tensor.sort") | 查看 [`torch.sort()`](generated/torch.sort.html#torch.sort "torch.sort") |
| [`Tensor.split`](generated/torch.Tensor.split.html#torch.Tensor.split "torch.Tensor.split") | 查看 [`torch.split()`](generated/torch.split.html#torch.split "torch.split") |
| [`Tensor.sparse_mask`](generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask "torch.Tensor.sparse_mask") | 使用稀疏张量 `mask` 的索引过滤来自分块张量 `self` 的值，返回一个新的 [稀疏张量](sparse.html#sparse-docs)。 |
| [`Tensor.sparse_dim`](generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim "torch.Tensor.sparse_dim") | 返回 [稀疏张量](sparse.html#sparse-docs) `self` 中稀疏维度的数量。 |
| [`Tensor.sqrt`](generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt "torch.Tensor.sqrt") | 查看 [`torch.sqrt()`](generated/torch.sqrt.html#torch.sqrt "torch.sqrt") |
| [`Tensor.sqrt_`](generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_ "torch.Tensor.sqrt_") | [`sqrt()`](generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt "torch.Tensor.sqrt") 的原地版本 |
| [`Tensor.square`](generated/torch.Tensor.square.html#torch.Tensor.square "torch.Tensor.square") | 查看 [`torch.square()`](generated/torch.square.html#torch.square "torch.square") |
| [`Tensor.square_`](generated/torch.Tensor.square_.html#torch.Tensor.square_ "torch.Tensor.square_") | [`square()`](generated/torch.Tensor.square.html#torch.Tensor.square "torch.Tensor.square") 的原地版本 |
| [`Tensor.squeeze`](generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze "torch.Tensor.squeeze") | 查看 [`torch.squeeze()`](generated/torch.squeeze.html#torch.squeeze "torch.squeeze") |
| [`Tensor.squeeze_`](generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_ "torch.Tensor.squeeze_") | [`squeeze()`](generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze "torch.Tensor.squeeze") 的原地版本 |
| [`Tensor.std`](generated/torch.Tensor.std.html#torch.Tensor.std "torch.Tensor.std") | 查看 [`torch.std()`](generated/torch.std.html#torch.std "torch.std") |
| [`Tensor.stft`](generated/torch.Tensor.stft.html#torch.Tensor.stft "torch.Tensor.stft") | 查看 [`torch.stft()`](generated/torch.stft.html#torch.stft "torch.stft") |
| [`Tensor.storage`](generated/torch.Tensor.storage.html#torch.Tensor.storage "torch.Tensor.storage") | 返回底层的 [`TypedStorage`](storage.html#torch.TypedStorage "torch.TypedStorage")。 |
| [`Tensor.untyped_storage`](generated/torch.Tensor.untyped_storage.html#torch.Tensor.untyped_storage "torch.Tensor.untyped_storage") | 返回底层的 [`UntypedStorage`](storage.html#torch.UntypedStorage "torch.UntypedStorage")。 |
| [`Tensor.storage_offset`](generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset "torch.Tensor.storage_offset") | 返回 `self` 张量在底层存储中的偏移量，以存储元素的数量表示（不是字节）。 |
| [`Tensor.storage_type`](generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type "torch.Tensor.storage_type") | 返回底层存储的类型。 |
| [`Tensor.stride`](generated/torch.Tensor.stride.html#torch.Tensor.stride "torch.Tensor.stride") | 返回 `self` 张量的步幅。 |
| [`Tensor.sub`](generated/torch.Tensor.sub.html#torch.Tensor.sub "torch.Tensor.sub") | 查看 [`torch.sub()`](generated/torch.sub.html#torch.sub "torch.sub") |
| [`Tensor.sub_`](generated/torch.Tensor.sub_.html#torch.Tensor.sub_ "torch.Tensor.sub_") | [`sub()`](generated/torch.Tensor.sub.html#torch.Tensor.sub "torch.Tensor.sub") 的原地版本 |
| [`Tensor.subtract`](generated/torch.Tensor.subtract.html#torch.Tensor.subtract "torch.Tensor.subtract") | 查看 [`torch.subtract()`](generated/torch.subtract.html#torch.subtract "torch.subtract") |
| [`Tensor.subtract_`](generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_ "torch.Tensor.subtract_") | [`subtract()`](generated/torch.Tensor.subtract.html#torch.Tensor.subtract "torch.Tensor.subtract") 的原地版本 |
| [`Tensor.sum`](generated/torch.Tensor.sum.html#torch.Tensor.sum "torch.Tensor.sum") | 查看 [`torch.sum()`](generated/torch.sum.html#torch.sum "torch.sum") |
| [`Tensor.sum_to_size`](generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size "torch.Tensor.sum_to_size") | 将 `this` 张量求和到 `size` |
| [`Tensor.svd`](generated/torch.Tensor.svd.html#torch.Tensor.svd "torch.Tensor.svd") | 查看 [`torch.svd()`](generated/torch.svd.html#torch.svd "torch.svd") |
| [`Tensor.swapaxes`](generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes "torch.Tensor.swapaxes") | 查看 [`torch.swapaxes()`](generated/torch.swapaxes.html#torch.swapaxes "torch.swapaxes") |
| [`Tensor.swapdims`](generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims "torch.Tensor.swapdims") | 查看 [`torch.swapdims()`](generated/torch.swapdims.html#torch.swapdims "torch.swapdims") |
| [`Tensor.t`](generated/torch.Tensor.t.html#torch.Tensor.t "torch.Tensor.t") | 查看 [`torch.t()`](generated/torch.t.html#torch.t "torch.t") |
| [`Tensor.t_`](generated/torch.Tensor.t_.html#torch.Tensor.t_ "torch.Tensor.t_") | [`t()`](generated/torch.Tensor.t.html#torch.Tensor.t "torch.Tensor.t") 的原地版本 |
| [`Tensor.tensor_split`](generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split "torch.Tensor.tensor_split") | 查看 [`torch.tensor_split()`](generated/torch.tensor_split.html#torch.tensor_split "torch.tensor_split") |
| [`Tensor.tile`](generated/torch.Tensor.tile.html#torch.Tensor.tile "torch.Tensor.tile") | 查看 [`torch.tile()`](generated/torch.tile.html#torch.tile "torch.tile") |
| [`Tensor.to`](generated/torch.Tensor.to.html#torch.Tensor.to "torch.Tensor.to") | 执行张量的数据类型和/或设备转换 |
| [`Tensor.to_mkldnn`](generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn "torch.Tensor.to_mkldnn") | 返回在 `torch.mkldnn` 布局中的张量的副本 |
| [`Tensor.take`](generated/torch.Tensor.take.html#torch.Tensor.take "torch.Tensor.take") | 查看 [`torch.take()`](generated/torch.take.html#torch.take "torch.take") |
| [`Tensor.take_along_dim`](generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim "torch.Tensor.take_along_dim") | 查看 [`torch.take_along_dim()`](generated/torch.take_along_dim.html#torch.take_along_dim "torch.take_along_dim") |
| [`Tensor.tan`](generated/torch.Tensor.tan.html#torch.Tensor.tan "torch.Tensor.tan") | 查看 [`torch.tan()`](generated/torch.tan.html#torch.tan "torch.tan") |
| [`Tensor.tan_`](generated/torch.Tensor.tan_.html#torch.Tensor.tan_ "torch.Tensor.tan_") | [`tan()`](generated/torch.Tensor.tan.html#torch.Tensor.tan "torch.Tensor.tan") 的原地版本 |
| [`Tensor.tanh`](generated/torch.Tensor.tanh.html#torch.Tensor.tanh "torch.Tensor.tanh") | 查看 [`torch.tanh()`](generated/torch.tanh.html#torch.tanh "torch.tanh") |
| [`Tensor.tanh_`](generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_ "torch.Tensor.tanh_") | [`tanh()`](generated/torch.Tensor.tanh.html#torch.Tensor.tanh "torch.Tensor.tanh") 的原地版本 |
| [`Tensor.atanh`](generated/torch.Tensor.atanh.html#torch.Tensor.atanh "torch.Tensor.atanh") | 查看 [`torch.atanh()`](generated/torch.atanh.html#torch.atanh "torch.atanh") |
| [`Tensor.atanh_`](generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_ "torch.Tensor.atanh_") | [`atanh()`](generated/torch.Tensor.atanh.html#torch.Tensor.atanh "torch.Tensor.atanh") 的原地版本 |
| [`Tensor.arctanh`](generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh "torch.Tensor.arctanh") | 参见 [`torch.arctanh()`](generated/torch.arctanh.html#torch.arctanh "torch.arctanh") |
| [`Tensor.arctanh_`](generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_ "torch.Tensor.arctanh_") | [`arctanh()`](generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh "torch.Tensor.arctanh") 的原地版本 |
| [`Tensor.tolist`](generated/torch.Tensor.tolist.html#torch.Tensor.tolist "torch.Tensor.tolist") | 将张量返回为（嵌套的）列表。 |
| [`Tensor.topk`](generated/torch.Tensor.topk.html#torch.Tensor.topk "torch.Tensor.topk") | 参见 [`torch.topk()`](generated/torch.topk.html#torch.topk "torch.topk") |
| [`Tensor.to_dense`](generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense "torch.Tensor.to_dense") | 如果 `self` 不是分块张量，则创建 `self` 的分块副本，否则返回 `self`。 |
| [`Tensor.to_sparse`](generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse "torch.Tensor.to_sparse") | 返回张量的稀疏副本。 |
| [`Tensor.to_sparse_csr`](generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr "torch.Tensor.to_sparse_csr") | 将张量转换为压缩行存储格式（CSR）。 |
| [`Tensor.to_sparse_csc`](generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc "torch.Tensor.to_sparse_csc") | 将张量转换为压缩列存储（CSC）格式。 |
| [`Tensor.to_sparse_bsr`](generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr "torch.Tensor.to_sparse_bsr") | 将张量转换为给定块大小的块稀疏行（BSR）存储格式。 |
| [`Tensor.to_sparse_bsc`](generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc "torch.Tensor.to_sparse_bsc") | 将张量转换为给定块大小的块稀疏列（BSC）存储格式。 |
| [`Tensor.trace`](generated/torch.Tensor.trace.html#torch.Tensor.trace "torch.Tensor.trace") | 参见 [`torch.trace()`](generated/torch.trace.html#torch.trace "torch.trace") |
| [`Tensor.transpose`](generated/torch.Tensor.transpose.html#torch.Tensor.transpose "torch.Tensor.transpose") | 参见 [`torch.transpose()`](generated/torch.transpose.html#torch.transpose "torch.transpose") |
| [`Tensor.transpose_`](generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_ "torch.Tensor.transpose_") | [`transpose()`](generated/torch.Tensor.transpose.html#torch.Tensor.transpose "torch.Tensor.transpose") 的原地版本 |
| [`Tensor.triangular_solve`](generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve "torch.Tensor.triangular_solve") | 参见 [`torch.triangular_solve()`](generated/torch.triangular_solve.html#torch.triangular_solve "torch.triangular_solve") |
| [`Tensor.tril`](generated/torch.Tensor.tril.html#torch.Tensor.tril "torch.Tensor.tril") | 参见 [`torch.tril()`](generated/torch.tril.html#torch.tril "torch.tril") |
| [`Tensor.tril_`](generated/torch.Tensor.tril_.html#torch.Tensor.tril_ "torch.Tensor.tril_") | [`tril()`](generated/torch.Tensor.tril.html#torch.Tensor.tril "torch.Tensor.tril") 的原地版本 |
| [`Tensor.triu`](generated/torch.Tensor.triu.html#torch.Tensor.triu "torch.Tensor.triu") | 参见 [`torch.triu()`](generated/torch.triu.html#torch.triu "torch.triu") |
| [`Tensor.triu_`](generated/torch.Tensor.triu_.html#torch.Tensor.triu_ "torch.Tensor.triu_") | [`triu()`](generated/torch.Tensor.triu.html#torch.Tensor.triu "torch.Tensor.triu") 的原地版本 |
| [`Tensor.true_divide`](generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide "torch.Tensor.true_divide") | 参见 [`torch.true_divide()`](generated/torch.true_divide.html#torch.true_divide "torch.true_divide") |
| [`Tensor.true_divide_`](generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_ "torch.Tensor.true_divide_") | `true_divide_()` 的原位版本 |
| [`Tensor.trunc`](generated/torch.Tensor.trunc.html#torch.Tensor.trunc "torch.Tensor.trunc") | 参见 `torch.trunc()` |
| [`Tensor.trunc_`](generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_ "torch.Tensor.trunc_") | `trunc()` 的原位版本 |
| [`Tensor.type`](generated/torch.Tensor.type.html#torch.Tensor.type "torch.Tensor.type") | 如果未提供 dtype，则返回类型，否则将此对象转换为指定类型 |
| [`Tensor.type_as`](generated/torch.Tensor.type_as.html#torch.Tensor.type_as "torch.Tensor.type_as") | 返回此张量转换为给定张量类型的结果 |
| [`Tensor.unbind`](generated/torch.Tensor.unbind.html#torch.Tensor.unbind "torch.Tensor.unbind") | 参见 `torch.unbind()` |
| [`Tensor.unflatten`](generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten "torch.Tensor.unflatten") | 参见 `torch.unflatten()` |
| [`Tensor.unfold`](generated/torch.Tensor.unfold.html#torch.Tensor.unfold "torch.Tensor.unfold") | 返回原始张量的视图，其中包含 `self` 张量在维度 `dimension` 中大小为 `size` 的所有切片 |
| [`Tensor.uniform_`](generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_ "torch.Tensor.uniform_") | 用从连续均匀分布中抽样的数字填充 `self` 张量 |
| [`Tensor.unique`](generated/torch.Tensor.unique.html#torch.Tensor.unique "torch.Tensor.unique") | 返回输入张量的唯一元素 |
| [`Tensor.unique_consecutive`](generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive "torch.Tensor.unique_consecutive") | 消除每个连续等价元素组的除第一个元素之外的所有元素 |
| [`Tensor.unsqueeze`](generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze "torch.Tensor.unsqueeze") | 参见 `torch.unsqueeze()` |
| [`Tensor.unsqueeze_`](generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_ "torch.Tensor.unsqueeze_") | `unsqueeze()` 的原位版本 |
| [`Tensor.values`](generated/torch.Tensor.values.html#torch.Tensor.values "torch.Tensor.values") | 返回 [稀疏 COO 张量](sparse.html#sparse-coo-docs) 的值张量 |
| [`Tensor.var`](generated/torch.Tensor.var.html#torch.Tensor.var "torch.Tensor.var") | 参见 `torch.var()` |
| [`Tensor.vdot`](generated/torch.Tensor.vdot.html#torch.Tensor.vdot "torch.Tensor.vdot") | 参见 `torch.vdot()` |
| [`Tensor.view`](generated/torch.Tensor.view.html#torch.Tensor.view "torch.Tensor.view") | 返回一个与 `self` 张量具有相同数据但不同 `shape` 的新张量 |
| [`Tensor.view_as`](generated/torch.Tensor.view_as.html#torch.Tensor.view_as "torch.Tensor.view_as") | 将此张量视为与 `other` 相同大小 |
| [`Tensor.vsplit`](generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit "torch.Tensor.vsplit") | 参见 `torch.vsplit()` |
| [`Tensor.where`](generated/torch.Tensor.where.html#torch.Tensor.where "torch.Tensor.where") | `self.where(condition, y)` 等同于 `torch.where(condition, self, y)` |
| [`Tensor.xlogy`](generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy "torch.Tensor.xlogy") | 查看 [`torch.xlogy()`](generated/torch.xlogy.html#torch.xlogy "torch.xlogy") |
| [`Tensor.xlogy_`](generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_ "torch.Tensor.xlogy_") | [`xlogy()`](generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy "torch.Tensor.xlogy") 的原地版本 |
| [`Tensor.zero_`](generated/torch.Tensor.zero_.html#torch.Tensor.zero_ "torch.Tensor.zero_") | 用零填充 `self` 张量。 |
