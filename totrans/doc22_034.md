# torch.Tensor

> 原文：[`pytorch.org/docs/stable/tensors.html`](https://pytorch.org/docs/stable/tensors.html)

`torch.Tensor` 是包含单一数据类型元素的多维矩阵。

## 数据类型

Torch 定义了 10 种张量类型，包括 CPU 和 GPU 变体，如下所示：

| 数据类型 | dtype | CPU 张量 | GPU 张量 |
| --- | --- | --- | --- |
| 32 位浮点数 | `torch.float32` 或 `torch.float` | `torch.FloatTensor` | `torch.cuda.FloatTensor` |
| 64 位浮点数 | `torch.float64` 或 `torch.double` | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |
| 16 位浮点数 1 | `torch.float16` 或 `torch.half` | `torch.HalfTensor` | `torch.cuda.HalfTensor` |
| 16 位浮点数 2 | `torch.bfloat16` | `torch.BFloat16Tensor` | `torch.cuda.BFloat16Tensor` |
| 32 位复数 | `torch.complex32` 或 `torch.chalf` |  |  |
| 64 位复数 | `torch.complex64` 或 `torch.cfloat` |  |  |
| 128 位复数 | `torch.complex128` 或 `torch.cdouble` |  |  |
| 8 位整数（无符号） | `torch.uint8` | `torch.ByteTensor` | `torch.cuda.ByteTensor` |
| 8 位整数（有符号） | `torch.int8` | `torch.CharTensor` | `torch.cuda.CharTensor` |
| 16 位整数（有符号） | `torch.int16` 或 `torch.short` | `torch.ShortTensor` | `torch.cuda.ShortTensor` |
| 32 位整数（有符号） | `torch.int32` 或 `torch.int` | `torch.IntTensor` | `torch.cuda.IntTensor` |
| 64 位整数（有符号） | `torch.int64` 或 `torch.long` | `torch.LongTensor` | `torch.cuda.LongTensor` |
| 布尔值 | `torch.bool` | `torch.BoolTensor` | `torch.cuda.BoolTensor` |
| 量化的 8 位整数（无符号） | `torch.quint8` | `torch.ByteTensor` | / |
| 量化的 8 位整数（有符号） | `torch.qint8` | `torch.CharTensor` | / |
| 量化的 32 位整数（有符号） | `torch.qint32` | `torch.IntTensor` | / |
| 量化的 4 位整数（无符号）3 | `torch.quint4x2` | `torch.ByteTensor` | / |

1

有时被称为 binary16：使用 1 个符号位，5 个指数位和 10 个有效位。当精度重要时很有用，但会牺牲范围。

2

有时被称为 Brain Floating Point：使用 1 个符号位，8 个指数位和 7 个有效位。当范围重要时很有用，因为它具有与`float32`相同数量的指数位。

3

量化的 4 位整数存储为 8 位有符号整数。目前仅在 EmbeddingBag 运算符中支持。

`torch.Tensor` 是默认张量类型（`torch.FloatTensor`）的别名。

## 初始化和基本操作[](#initializing-and-basic-operations "跳转到此标题的永久链接")

可以使用`torch.tensor()`构造来自 Python [`list`](https://docs.python.org/3/library/stdtypes.html#list "(在 Python v3.12 中)") 或序列的张量：

```py
>>> torch.tensor([[1., -1.], [1., -1.]])
tensor([[ 1.0000, -1.0000],
 [ 1.0000, -1.0000]])
>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))
tensor([[ 1,  2,  3],
 [ 4,  5,  6]]) 
```

警告

`torch.tensor()` 总是复制`data`。如果您有一个张量`data`，只想改变其`requires_grad`标志，请使用`requires_grad_()`或`detach()`来避免复制。如果您有一个 numpy 数组并想避免复制，请使用`torch.as_tensor()`。

通过将`torch.dtype`和/或`torch.device`传递给构造函数或张量创建操作，可以构造特定数据类型的张量：

```py
>>> torch.zeros([2, 4], dtype=torch.int32)
tensor([[ 0,  0,  0,  0],
 [ 0,  0,  0,  0]], dtype=torch.int32)
>>> cuda0 = torch.device('cuda:0')
>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)
tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
 [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0') 
```

有关构建张量的更多信息，请参阅 Creation Ops

可以使用 Python 的索引和切片表示法访问和修改张量的内容：

```py
>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])
>>> print(x[1][2])
tensor(6)
>>> x[0][1] = 8
>>> print(x)
tensor([[ 1,  8,  3],
 [ 4,  5,  6]]) 
```

使用 `torch.Tensor.item()` 从包含单个值的张量中获取一个 Python 数字：

```py
>>> x = torch.tensor([[1]])
>>> x
tensor([[ 1]])
>>> x.item()
1
>>> x = torch.tensor(2.5)
>>> x
tensor(2.5000)
>>> x.item()
2.5 
```

有关索引的更多信息，请参见 Indexing, Slicing, Joining, Mutating Ops。

可以创建一个带有 `requires_grad=True` 的张量，以便 `torch.autograd` 记录对它们的操作以进行自动微分。

```py
>>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)
>>> out = x.pow(2).sum()
>>> out.backward()
>>> x.grad
tensor([[ 2.0000, -2.0000],
 [ 2.0000,  2.0000]]) 
```

每个张量都有一个关联的 `torch.Storage`，用于保存其数据。张量类还提供了对存储的多维、[分步](https://en.wikipedia.org/wiki/Stride_of_an_array)视图，并在其上定义了数值操作。

注意

有关张量视图的更多信息，请参见 Tensor Views。

注意

有关 `torch.dtype`、`torch.device` 和 `torch.layout` 属性的更多信息，请参见 `torch.Tensor` 的 Tensor Attributes。

注意

会改变张量的方法会带有下划线后缀。例如，`torch.FloatTensor.abs_()` 在原地计算绝对值并返回修改后的张量，而 `torch.FloatTensor.abs()` 在新张量中计算结果。

注意

要更改现有张量的 `torch.device` 和/或 `torch.dtype`，请考虑在张量上使用 `to()` 方法。

警告

当前的 `torch.Tensor` 实现引入了内存开销，因此在具有许多小张量的应用程序中可能导致意外高的内存使用量。如果这是您的情况，请考虑使用一个大结构。

## 张量类参考

```py
class torch.Tensor
```

有几种主要方法可以创建张量，取决于您的用例。

+   要使用预先存在的数据创建张量，请使用 `torch.tensor()`。

+   要创建特定大小的张量，请使用 `torch.*` 张量创建操作（参见 Creation Ops）。

+   要创建一个与另一个张量相同大小（和相似类型）的张量，请使用 `torch.*_like` 张量创建操作（参见 Creation Ops）。

+   要创建一个与另一个张量相似类型但不同大小的张量，请使用 `tensor.new_*` 创建操作。

```py
Tensor.T
```

返回一个维度被颠倒的张量视图。

如果 `x` 中有 `n` 个维度，`x.T` 等同于 `x.permute(n-1, n-2, ..., 0)`。

警告

在维度不为 2 的张量上使用 `Tensor.T()` 来颠倒它们的形状已被弃用，并且在将来的版本中会引发错误。考虑使用 `mT` 来转置矩阵批次或者使用 x.permute(*torch.arange(x.ndim - 1, -1, -1)) 来颠倒张量的维度。

```py
Tensor.H
```

返回一个共轭和转置的矩阵（2-D 张量）视图。

对于复杂矩阵，`x.H` 等同于 `x.transpose(0, 1).conj()`，对于实矩阵，`x.H` 等同于 `x.transpose(0, 1)`。

另请参阅

`mH`：也适用于矩阵批次的属性。

```py
Tensor.mT
```

返回一个最后两个维度被转置的张量视图。

`x.mT` 等同于 `x.transpose(-2, -1)`。

```py
Tensor.mH
```

访问此属性等同于调用 `adjoint()`。

| `Tensor.new_tensor` | 返回一个以 `data` 为张量数据的新张量。 |
| --- | --- |
| `Tensor.new_full` | 返回一个大小为`size`且填充为`fill_value`的张量。 |
| `Tensor.new_empty` | 返回一个大小为`size`且填充为未初始化数据的张量。 |
| `Tensor.new_ones` | 返回一个大小为`size`且填充为`1`的张量。 |
| `Tensor.new_zeros` | 返回一个大小为`size`且填充为`0`的张量。 |
| `Tensor.is_cuda` | 如果张量存储在 GPU 上，则为`True`，否则为`False`。 |
| `Tensor.is_quantized` | 如果张量是量化的，则为`True`，否则为`False`。 |
| `Tensor.is_meta` | 如果张量是元张量，则为`True`，否则为`False`。 |
| `Tensor.device` | 此张量所在的`torch.device`。 |
| `Tensor.grad` | 默认情况下，此属性为`None`，第一次调用`backward()`计算`self`的梯度时会变成一个张量。 |
| `Tensor.ndim` | `dim()`的别名 |
| `Tensor.real` | 返回一个包含复值输入张量`self`的实部值的新张量。 |
| `Tensor.imag` | 返回一个包含`self`张量的虚部值的新张量。 |
| `Tensor.nbytes` | 如果张量不使用稀疏存储布局，则返回张量元素“视图”消耗的字节数。 |
| `Tensor.itemsize` | `element_size()`的别名 |
| `Tensor.abs` | 参见`torch.abs()` |
| `Tensor.abs_` | `abs()`的原地版本 |
| `Tensor.absolute` | `abs()`的别名 |
| `Tensor.absolute_` | `absolute()`的原地版本，别名为`abs_()` |
| `Tensor.acos` | 参见`torch.acos()` |
| `Tensor.acos_` | `acos()`的原地版本 |
| `Tensor.arccos` | 参见`torch.arccos()` |
| `Tensor.arccos_` | `arccos()` 的原地版本 |
| `Tensor.add` | 将标量或张量添加到 `self` 张量中。 |
| `Tensor.add_` | `add()` 的原地版本 |
| `Tensor.addbmm` | 查看 `torch.addbmm()` |
| `Tensor.addbmm_` | `addbmm()` 的原地版本 |
| `Tensor.addcdiv` | 查看 `torch.addcdiv()` |
| `Tensor.addcdiv_` | `addcdiv()` 的原地版本 |
| `Tensor.addcmul` | 查看 `torch.addcmul()` |
| `Tensor.addcmul_` | `addcmul()` 的原地版本 |
| `Tensor.addmm` | 查看 `torch.addmm()` |
| `Tensor.addmm_` | `addmm()` 的原地版本 |
| `Tensor.sspaddmm` | 查看 `torch.sspaddmm()` |
| `Tensor.addmv` | 查看 `torch.addmv()` |
| `Tensor.addmv_` | `addmv()` 的原地版本 |
| `Tensor.addr` | 查看 `torch.addr()` |
| `Tensor.addr_` | `addr()` 的原地版本 |
| `Tensor.adjoint` | `adjoint()` 的别名 |
| `Tensor.allclose` | 查看 `torch.allclose()` |
| `Tensor.amax` | 查看 `torch.amax()` |
| `Tensor.amin` | 查看 `torch.amin()` |
| `Tensor.aminmax` | 参见 `torch.aminmax()` |
| `Tensor.angle` | 参见 `torch.angle()` |
| `Tensor.apply_` | 将函数 `callable` 应用于张量中的每个元素，用 `callable` 返回的值替换每个元素。 |
| `Tensor.argmax` | 参见 `torch.argmax()` |
| `Tensor.argmin` | 参见 `torch.argmin()` |
| `Tensor.argsort` | 参见 `torch.argsort()` |
| `Tensor.argwhere` | 参见 `torch.argwhere()` |
| `Tensor.asin` | 参见 `torch.asin()` |
| `Tensor.asin_` | `asin()` 的原地版本 |
| `Tensor.arcsin` | 参见 `torch.arcsin()` |
| `Tensor.arcsin_` | `arcsin()` 的原地版本 |
| `Tensor.as_strided` | 参见 `torch.as_strided()` |
| `Tensor.atan` | 参见 `torch.atan()` |
| `Tensor.atan_` | `atan()` 的原地版本 |
| `Tensor.arctan` | 参见 `torch.arctan()` |
| `Tensor.arctan_` | `arctan()` 的原地版本 |
| `Tensor.atan2` | 参见 `torch.atan2()` |
| `Tensor.atan2_` | `atan2()` 的原地版本 |
| `Tensor.arctan2` | 参见 `torch.arctan2()` |
| `Tensor.arctan2_` | atan2_(other) -> Tensor |
| `Tensor.all` | 参见 `torch.all()` |
| `Tensor.any` | 查看 `torch.any()` |
| `Tensor.backward` | 计算当前张量相对于图中叶子节点的梯度。 |
| `Tensor.baddbmm` | 查看 `torch.baddbmm()` |
| `Tensor.baddbmm_` | `baddbmm()` 的原地版本 |
| `Tensor.bernoulli` | 返回一个结果张量，其中每个 $\texttt{result[i]}$ 从 $\text{Bernoulli}(\texttt{self[i]})$ 独立采样。 |
| `Tensor.bernoulli_` | 用来自 $\text{Bernoulli}(\texttt{p})$ 的独立样本填充 `self` 的每个位置。 |
| `Tensor.bfloat16` | `self.bfloat16()` 等同于 `self.to(torch.bfloat16)`。 |
| `Tensor.bincount` | 查看 `torch.bincount()` |
| `Tensor.bitwise_not` | 查看 `torch.bitwise_not()` |
| `Tensor.bitwise_not_` | `bitwise_not()` 的原地版本 |
| `Tensor.bitwise_and` | 查看 `torch.bitwise_and()` |
| `Tensor.bitwise_and_` | `bitwise_and()` 的原地版本 |
| `Tensor.bitwise_or` | 查看 `torch.bitwise_or()` |
| `Tensor.bitwise_or_` | `bitwise_or()` 的原地版本 |
| `Tensor.bitwise_xor` | 查看 `torch.bitwise_xor()` |
| `Tensor.bitwise_xor_` | `bitwise_xor()` 的原地版本 |
| `Tensor.bitwise_left_shift` | 查看 `torch.bitwise_left_shift()` |
| `Tensor.bitwise_left_shift_` | `bitwise_left_shift()` 的原地版本 |
| `Tensor.bitwise_right_shift` | 参见 `torch.bitwise_right_shift()` |
| `Tensor.bitwise_right_shift_` | `bitwise_right_shift()` 的原地版本 |
| `Tensor.bmm` | 参见 `torch.bmm()` |
| `Tensor.bool` | `self.bool()` 等同于 `self.to(torch.bool)`。 |
| `Tensor.byte` | `self.byte()` 等同于 `self.to(torch.uint8)`。 |
| `Tensor.broadcast_to` | 参见 `torch.broadcast_to()`. |
| `Tensor.cauchy_` | 用从 Cauchy 分布中抽取的数字填充张量： |
| `Tensor.ceil` | 参见 `torch.ceil()` |
| `Tensor.ceil_` | `ceil()` 的原地版本 |
| `Tensor.char` | `self.char()` 等同于 `self.to(torch.int8)`。 |
| `Tensor.cholesky` | 参见 `torch.cholesky()` |
| `Tensor.cholesky_inverse` | 参见 `torch.cholesky_inverse()` |
| `Tensor.cholesky_solve` | 参见 `torch.cholesky_solve()` |
| `Tensor.chunk` | 参见 `torch.chunk()` |
| `Tensor.clamp` | 参见 `torch.clamp()` |
| `Tensor.clamp_` | `clamp()` 的原地版本 |
| `Tensor.clip` | `clamp()` 的别名 |
| `Tensor.clip_` | `clamp_()` 的别名。 |
| `Tensor.clone` | 查看 `torch.clone()` |
| `Tensor.contiguous` | 返回一个包含与 `self` 张量相同数据的内存连续张量 |
| `Tensor.copy_` | 将 `src` 中的元素复制到 `self` 张量中并返回 `self` |
| `Tensor.conj` | 查看 `torch.conj()` |
| `Tensor.conj_physical` | 查看 `torch.conj_physical()` |
| `Tensor.conj_physical_` | `conj_physical()` 的原地版本 |
| `Tensor.resolve_conj` | 查看 `torch.resolve_conj()` |
| `Tensor.resolve_neg` | 查看 `torch.resolve_neg()` |
| `Tensor.copysign` | 查看 `torch.copysign()` |
| `Tensor.copysign_` | `copysign()` 的原地版本 |
| `Tensor.cos` | 查看 `torch.cos()` |
| `Tensor.cos_` | `cos()` 的原地版本 |
| `Tensor.cosh` | 查看 `torch.cosh()` |
| `Tensor.cosh_` | `cosh()` 的原地版本 |
| `Tensor.corrcoef` | 查看 `torch.corrcoef()` |
| `Tensor.count_nonzero` | 查看 `torch.count_nonzero()` |
| `Tensor.cov` | 查看 `torch.cov()` |
| `Tensor.acosh` | 查看 `torch.acosh()` |
| `Tensor.acosh_` | `acosh()` 的原地版本 |
| `Tensor.arccosh` | acosh() -> Tensor |
| `Tensor.arccosh_` | acosh_() -> Tensor |
| `Tensor.cpu` | 返回此对象在 CPU 内存中的副本 |
| `Tensor.cross` | 参见`torch.cross()` |
| `Tensor.cuda` | 返回此对象在 CUDA 内存中的副本 |
| `Tensor.logcumsumexp` | 参见`torch.logcumsumexp()` |
| `Tensor.cummax` | 参见`torch.cummax()` |
| `Tensor.cummin` | 参见`torch.cummin()` |
| `Tensor.cumprod` | 参见`torch.cumprod()` |
| `Tensor.cumprod_` | `cumprod()`的原位版本 |
| `Tensor.cumsum` | 参见`torch.cumsum()` |
| `Tensor.cumsum_` | `cumsum()`的原位版本 |
| `Tensor.chalf` | `self.chalf()`等同于`self.to(torch.complex32)` |
| `Tensor.cfloat` | `self.cfloat()`等同于`self.to(torch.complex64)` |
| `Tensor.cdouble` | `self.cdouble()`等同于`self.to(torch.complex128)` |
| `Tensor.data_ptr` | 返回`self`张量的第一个元素的地址 |
| `Tensor.deg2rad` | 参见`torch.deg2rad()` |
| `Tensor.dequantize` | 给定一个量化张量，对其进行去量化并返回去量化的浮点张量 |
| `Tensor.det` | 参见`torch.det()` |
| `Tensor.dense_dim` | 返回稀疏张量 `self` 中的密集维度数 |
| `Tensor.detach` | 返回一个从当前图中分离出来的新张量 |
| `Tensor.detach_` | 将张量从创建它的图中分离出来，使其成为叶子节点 |
| `Tensor.diag` | 参见`torch.diag()` |
| `Tensor.diag_embed` | 查看 `torch.diag_embed()` |
| `Tensor.diagflat` | 查看 `torch.diagflat()` |
| `Tensor.diagonal` | 查看 `torch.diagonal()` |
| `Tensor.diagonal_scatter` | 查看 `torch.diagonal_scatter()` |
| `Tensor.fill_diagonal_` | 填充至少为 2 维的张量的主对角线。 |
| `Tensor.fmax` | 查看 `torch.fmax()` |
| `Tensor.fmin` | 查看 `torch.fmin()` |
| `Tensor.diff` | 查看 `torch.diff()` |
| `Tensor.digamma` | 查看 `torch.digamma()` |
| `Tensor.digamma_` | `digamma()` 的原地版本 |
| `Tensor.dim` | 返回 `self` 张量的维度数量。 |
| `Tensor.dim_order` | 返回一个描述 `self` 张量维度顺序或物理布局的整数元组。 |
| `Tensor.dist` | 查看 `torch.dist()` |
| `Tensor.div` | 查看 `torch.div()` |
| `Tensor.div_` | `div()` 的原地版本 |
| `Tensor.divide` | 查看 `torch.divide()` |
| `Tensor.divide_` | `divide()` 的原地版本 |
| `Tensor.dot` | 查看 `torch.dot()` |
| `Tensor.double` | `self.double()` 等同于 `self.to(torch.float64)`。 |
| `Tensor.dsplit` | 查看 `torch.dsplit()` |
| `Tensor.element_size` | 返回单个元素的字节大小。 |
| `Tensor.eq` | 查看 `torch.eq()` |
| `Tensor.eq_` | `eq()` 的原地版本 |
| `Tensor.equal` | 查看 `torch.equal()` |
| `Tensor.erf` | 查看 `torch.erf()` |
| `Tensor.erf_` | `erf()` 的原地版本 |
| `Tensor.erfc` | 查看 `torch.erfc()` |
| `Tensor.erfc_` | `erfc()` 的原地版本 |
| `Tensor.erfinv` | 查看 `torch.erfinv()` |
| `Tensor.erfinv_` | `erfinv()` 的原地版本 |
| `Tensor.exp` | 查看 `torch.exp()` |
| `Tensor.exp_` | `exp()` 的原地版本 |
| `Tensor.expm1` | 查看 `torch.expm1()` |
| `Tensor.expm1_` | `expm1()` 的原地版本 |
| `Tensor.expand` | 返回一个新的视图，将 `self` 张量中的单例维度扩展到更大的大小 |
| `Tensor.expand_as` | 将此张量扩展到与 `other` 相同的大小 |
| `Tensor.exponential_` | 用从概率密度函数中抽取的元素填充 `self` 张量 |
| `Tensor.fix` | 查看 `torch.fix()` |
| `Tensor.fix_` | `fix()` 的原地版本 |
| `Tensor.fill_` | 用指定值填充 `self` 张量 |
| `Tensor.flatten` | 查看 `torch.flatten()` |
| `Tensor.flip` | 查看 `torch.flip()` |
| `Tensor.fliplr` | 查看 `torch.fliplr()` |
| `Tensor.flipud` | 查看 `torch.flipud()` |
| `Tensor.float` | `self.float()` 等同于 `self.to(torch.float32)` |
| `Tensor.float_power` | 查看 `torch.float_power()` |
| `Tensor.float_power_` | `float_power()` 的原地版本 |
| `Tensor.floor` | 查看 `torch.floor()` |
| `Tensor.floor_` | `floor()` 的原地版本 |
| `Tensor.floor_divide` | 查看 `torch.floor_divide()` |
| `Tensor.floor_divide_` | `floor_divide()` 的原地版本 |
| `Tensor.fmod` | 查看 `torch.fmod()` |
| `Tensor.fmod_` | `fmod()` 的原地版本 |
| `Tensor.frac` | 查看 `torch.frac()` |
| `Tensor.frac_` | `frac()` 的原地版本 |
| `Tensor.frexp` | 查看 `torch.frexp()` |
| `Tensor.gather` | 查看 `torch.gather()` |
| `Tensor.gcd` | 查看 `torch.gcd()` |
| `Tensor.gcd_` | `gcd()` 的原地版本 |
| `Tensor.ge` | 查看 `torch.ge()` |
| `Tensor.ge_` | `ge()` 的原地版本 |
| `Tensor.greater_equal` | 查看 `torch.greater_equal()` |
| `Tensor.greater_equal_` | `greater_equal()` 的原地版本 |
| `Tensor.geometric_` | 用几何分布中的元素填充 `self` 张量： |
| `Tensor.geqrf` | 参见 `torch.geqrf()` |
| `Tensor.ger` | 参见 `torch.ger()` |
| `Tensor.get_device` | 对于 CUDA 张量，此函数返回张量所在 GPU 的设备序数。 |
| `Tensor.gt` | 参见 `torch.gt()` |
| `Tensor.gt_` | `gt()` 的原地版本。 |
| `Tensor.greater` | 参见 `torch.greater()` |
| `Tensor.greater_` | `greater()` 的原地版本。 |
| `Tensor.half` | `self.half()` 等同于 `self.to(torch.float16)`。 |
| `Tensor.hardshrink` | 参见 `torch.nn.functional.hardshrink()` |
| `Tensor.heaviside` | 参见 `torch.heaviside()` |
| `Tensor.histc` | 参见 `torch.histc()` |
| `Tensor.histogram` | 参见 `torch.histogram()` |
| `Tensor.hsplit` | 参见 `torch.hsplit()` |
| `Tensor.hypot` | 参见 `torch.hypot()` |
| `Tensor.hypot_` | `hypot()` 的原地版本。 |
| `Tensor.i0` | 参见 `torch.i0()` |
| `Tensor.i0_` | `i0()` 的原地版本。 |
| `Tensor.igamma` | 参见 `torch.igamma()` |
| `Tensor.igamma_` | `igamma()` 的原地版本。 |
| `Tensor.igammac` | 参见 `torch.igammac()` |
| `Tensor.igammac_` | `igammac()`的原地版本。 |
| `Tensor.index_add_` | 通过将`alpha`倍的`source`元素累加到`self`张量中，按照`index`中给定的顺序添加到索引中。 |
| `Tensor.index_add` | `torch.Tensor.index_add_()`的非原地版本。 |
| `Tensor.index_copy_` | 通过按照`index`中给定的顺序选择的索引，将`tensor`的元素复制到`self`张量中。 |
| `Tensor.index_copy` | `torch.Tensor.index_copy_()`的非原地版本。 |
| `Tensor.index_fill_` | 通过按照`index`中给定的顺序选择的索引，用值`value`填充`self`张量的元素。 |
| `Tensor.index_fill` | `torch.Tensor.index_fill_()`的非原地版本。 |
| `Tensor.index_put_` | 使用`indices`中指定的索引（一个张量元组）将张量`values`中的值放入张量`self`中。 |
| `Tensor.index_put` | `index_put_()`的非原地版本。 |
| `Tensor.index_reduce_` | 通过使用`reduce`参数给定的减少方式，按照`index`中给定的顺序将`source`元素累加到`self`张量中。 |
| `Tensor.index_reduce` |  |
| `Tensor.index_select` | 参见`torch.index_select()` |
| `Tensor.indices` | 返回稀疏 COO 张量的索引张量。 |
| `Tensor.inner` | 参见`torch.inner()`。 |
| `Tensor.int` | `self.int()`等同于`self.to(torch.int32)`。 |
| `Tensor.int_repr` | 给定一个量化张量，`self.int_repr()`返回一个 CPU 张量，数据类型为 uint8_t，存储给定张量的底层 uint8_t 值。 |
| `Tensor.inverse` | 参见`torch.inverse()` |
| `Tensor.isclose` | 参见`torch.isclose()` |
| `Tensor.isfinite` | 参见`torch.isfinite()` |
| `Tensor.isinf` | 参见`torch.isinf()` |
| `Tensor.isposinf` | 参见`torch.isposinf()` |
| `Tensor.isneginf` | 参见`torch.isneginf()` |
| `Tensor.isnan` | 参见`torch.isnan()` |
| `Tensor.is_contiguous` | 如果`self`张量在内存中按照内存格式指定的顺序是连续的，则返回 True。 |
| `Tensor.is_complex` | 如果`self`的数据类型是复数数据类型，则返回 True。 |
| `Tensor.is_conj` | 如果`self`的共轭位设置为 true，则返回 True。 |
| `Tensor.is_floating_point` | 如果`self`的数据类型是浮点数据类型，则返回 True。 |
| `Tensor.is_inference` | 参见`torch.is_inference()` |
| `Tensor.is_leaf` | 所有`requires_grad`为`False`的张量按照惯例都将是叶张量。 |
| `Tensor.is_pinned` | 如果此张量驻留在固定内存中，则返回 true。 |
| `Tensor.is_set_to` | 如果两个张量指向完全相同的内存（相同的存储、偏移、大小和步幅），则返回 True。 |
| `Tensor.is_shared` | 检查张量是否在共享内存中。 |
| `Tensor.is_signed` | 如果`self`的数据类型是有符号数据类型，则返回 True。 |
| `Tensor.is_sparse` | 如果张量使用稀疏 COO 存储布局，则为`True`，否则为`False`。 |
| `Tensor.istft` | 参见`torch.istft()` |
| `Tensor.isreal` | 参见`torch.isreal()` |
| `Tensor.item` | 将此张量的值作为标准 Python 数字返回。 |
| `Tensor.kthvalue` | 参见`torch.kthvalue()` |
| `Tensor.lcm` | 查看 `torch.lcm()` |
| `Tensor.lcm_` | `lcm()` 的原地版本。 |
| `Tensor.ldexp` | 查看 `torch.ldexp()` |
| `Tensor.ldexp_` | `ldexp()` 的原地版本。 |
| `Tensor.le` | 查看 `torch.le()`。 |
| `Tensor.le_` | `le()` 的原地版本。 |
| `Tensor.less_equal` | 查看 `torch.less_equal()`。 |
| `Tensor.less_equal_` | `less_equal()` 的原地版本。 |
| `Tensor.lerp` | 查看 `torch.lerp()` |
| `Tensor.lerp_` | `lerp()` 的原地版本。 |
| `Tensor.lgamma` | 查看 `torch.lgamma()` |
| `Tensor.lgamma_` | `lgamma()` 的原地版本。 |
| `Tensor.log` | 查看 `torch.log()` |
| `Tensor.log_` | `log()` 的原地版本。 |
| `Tensor.logdet` | 查看 `torch.logdet()` |
| `Tensor.log10` | 查看 `torch.log10()` |
| `Tensor.log10_` | `log10()` 的原地版本。 |
| `Tensor.log1p` | 查看 `torch.log1p()` |
| `Tensor.log1p_` | `log1p()` 的原地版本。 |
| `Tensor.log2` | 查看 `torch.log2()` |
| `Tensor.log2_` | `log2()` 的原地版本。 |
| `Tensor.log_normal_` | 使用给定的均值 $\mu$ 和标准差 $\sigma$ 参数化的对数正态分布中的样本填充 `self` 张量 |
| `Tensor.logaddexp` | 参见 `torch.logaddexp()` |
| `Tensor.logaddexp2` | 参见 `torch.logaddexp2()` |
| `Tensor.logsumexp` | 参见 `torch.logsumexp()` |
| `Tensor.logical_and` | 参见 `torch.logical_and()` |
| `Tensor.logical_and_` | `logical_and()` 的原地版本 |
| `Tensor.logical_not` | 参见 `torch.logical_not()` |
| `Tensor.logical_not_` | `logical_not()` 的原地版本 |
| `Tensor.logical_or` | 参见 `torch.logical_or()` |
| `Tensor.logical_or_` | `logical_or()` 的原地版本 |
| `Tensor.logical_xor` | 参见 `torch.logical_xor()` |
| `Tensor.logical_xor_` | `logical_xor()` 的原地版本 |
| `Tensor.logit` | 参见 `torch.logit()` |
| `Tensor.logit_` | `logit()` 的原地版本 |
| `Tensor.long` | `self.long()` 等同于 `self.to(torch.int64)` |
| `Tensor.lt` | 参见 `torch.lt()` |
| `Tensor.lt_` | `lt()` 的原地版本 |
| `Tensor.less` | lt(other) -> Tensor |
| `Tensor.less_` | `less()` 的原地版本 |
| `Tensor.lu` | 查看 `torch.lu()` |
| `Tensor.lu_solve` | 查看 `torch.lu_solve()` |
| `Tensor.as_subclass` | 创建一个具有与`self`相同数据指针的`cls`实例。 |
| `Tensor.map_` | 对`self`张量中的每个元素应用`callable`，并将结果存储在`self`张量中。 |
| `Tensor.masked_scatter_` | 将`source`中的元素复制到`self`张量中，其中`mask`为 True。 |
| `Tensor.masked_scatter` | `torch.Tensor.masked_scatter_()`的非就地版本 |
| `Tensor.masked_fill_` | 在`mask`为 True 的位置，用`value`填充`self`张量的元素。 |
| `Tensor.masked_fill` | `torch.Tensor.masked_fill_()`的非就地版本 |
| `Tensor.masked_select` | 查看 `torch.masked_select()` |
| `Tensor.matmul` | 查看 `torch.matmul()` |
| `Tensor.matrix_power` |

注意

`matrix_power()`已弃用，请使用`torch.linalg.matrix_power()`代替。

|

| `Tensor.matrix_exp` | 查看 `torch.matrix_exp()` |
| --- | --- |
| `Tensor.max` | 查看 `torch.max()` |
| `Tensor.maximum` | 查看 `torch.maximum()` |
| `Tensor.mean` | 查看 `torch.mean()` |
| `Tensor.nanmean` | 查看 `torch.nanmean()` |
| `Tensor.median` | 查看 `torch.median()` |
| `Tensor.nanmedian` | 查看 `torch.nanmedian()` |
| `Tensor.min` | 查看 `torch.min()` |
| `Tensor.minimum` | 查看 `torch.minimum()` |
| `Tensor.mm` | 查看 `torch.mm()` |
| `Tensor.smm` | 查看 `torch.smm()` |
| `Tensor.mode` | 查看 `torch.mode()` |
| `Tensor.movedim` | 查看 `torch.movedim()` |
| `Tensor.moveaxis` | 查看 `torch.moveaxis()` |
| `Tensor.msort` | 查看 `torch.msort()` |
| `Tensor.mul` | 查看 `torch.mul()` |
| `Tensor.mul_` | `mul()` 的原地版本 |
| `Tensor.multiply` | 查看 `torch.multiply()` |
| `Tensor.multiply_` | `multiply()` 的原地版本 |
| `Tensor.multinomial` | 查看 `torch.multinomial()` |
| `Tensor.mv` | 查看 `torch.mv()` |
| `Tensor.mvlgamma` | 查看 `torch.mvlgamma()` |
| `Tensor.mvlgamma_` | `mvlgamma()` 的原地版本 |
| `Tensor.nansum` | 查看 `torch.nansum()` |
| `Tensor.narrow` | 查看 `torch.narrow()` |
| `Tensor.narrow_copy` | 查看 `torch.narrow_copy()` |
| `Tensor.ndimension` | `dim()` 的别名 |
| `Tensor.nan_to_num` | 查看 `torch.nan_to_num()` |
| `Tensor.nan_to_num_` | `nan_to_num()` 的原地版本。 |
| `Tensor.ne` | 参见 `torch.ne()` |
| `Tensor.ne_` | `ne()` 的原地版本。 |
| `Tensor.not_equal` | 参见 `torch.not_equal()` |
| `Tensor.not_equal_` | `not_equal()` 的原地版本。 |
| `Tensor.neg` | 参见 `torch.neg()` |
| `Tensor.neg_` | `neg()` 的原地版本。 |
| `Tensor.negative` | 参见 `torch.negative()` |
| `Tensor.negative_` | `negative()` 的原地版本。 |
| `Tensor.nelement` | `numel()` 的别名 |
| `Tensor.nextafter` | 参见 `torch.nextafter()` |
| `Tensor.nextafter_` | `nextafter()` 的原地版本。 |
| `Tensor.nonzero` | 参见 `torch.nonzero()` |
| `Tensor.norm` | 参见 `torch.norm()` |
| `Tensor.normal_` | 使用由 `mean` 和 `std` 参数化的正态分布样本填充 `self` 张量。 |
| `Tensor.numel` | 参见 `torch.numel()` |
| `Tensor.numpy` | 将张量返回为 NumPy `ndarray`。 |
| `Tensor.orgqr` | 参见 `torch.orgqr()` |
| `Tensor.ormqr` | 参见 `torch.ormqr()` |
| `Tensor.outer` | 参见 `torch.outer()` |
| `Tensor.permute` | 查看 `torch.permute()` |
| `Tensor.pin_memory` | 如果尚未固定，将张量复制到固定内存中。 |
| `Tensor.pinverse` | 查看 `torch.pinverse()` |
| `Tensor.polygamma` | 查看 `torch.polygamma()` |
| `Tensor.polygamma_` | `polygamma()` 的原地版本 |
| `Tensor.positive` | 查看 `torch.positive()` |
| `Tensor.pow` | 查看 `torch.pow()` |
| `Tensor.pow_` | `pow()` 的原地版本 |
| `Tensor.prod` | 查看 `torch.prod()` |
| `Tensor.put_` | 将 `source` 中的元素复制到由 `index` 指定的位置。 |
| `Tensor.qr` | 查看 `torch.qr()` |
| `Tensor.qscheme` | 返回给定 QTensor 的量化方案。 |
| `Tensor.quantile` | 查看 `torch.quantile()` |
| `Tensor.nanquantile` | 查看 `torch.nanquantile()` |
| `Tensor.q_scale` | 给定通过线性（仿射）量化的张量，返回底层量化器的比例。 |
| `Tensor.q_zero_point` | 给定通过线性（仿射）量化的张量，返回底层量化器的零点。 |
| `Tensor.q_per_channel_scales` | 给定通过线性（仿射）逐通道量化的张量，返回底层量化器的比例张量。 |
| `Tensor.q_per_channel_zero_points` | 给定通过线性（仿射）逐通道量化的张量，返回底层量化器的零点张量。 |
| `Tensor.q_per_channel_axis` | 给定通过线性（仿射）逐通道量化的张量，返回应用逐通道量化的维度索引。 |
| `Tensor.rad2deg` | 参见`torch.rad2deg()` |
| `Tensor.random_` | 用从离散均匀分布`[from, to - 1]`中抽样的数字填充`self`张量。 |
| `Tensor.ravel` | 参见`torch.ravel()` |
| `Tensor.reciprocal` | 参见`torch.reciprocal()` |
| `Tensor.reciprocal_` | `reciprocal()`的原位版本 |
| `Tensor.record_stream` | 将张量标记为此流程已使用。 |
| `Tensor.register_hook` | 注册一个反向钩子。 |
| `Tensor.register_post_accumulate_grad_hook` | 注册一个在梯度累积后运行的反向钩子。 |
| `Tensor.remainder` | 参见`torch.remainder()` |
| `Tensor.remainder_` | `remainder()`的原位版本 |
| `Tensor.renorm` | 参见`torch.renorm()` |
| `Tensor.renorm_` | `renorm()`的原位版本 |
| `Tensor.repeat` | 沿指定维度重复此张量。 |
| `Tensor.repeat_interleave` | 参见`torch.repeat_interleave()`。 |
| `Tensor.requires_grad` | 如果需要为此张量计算梯度，则为`True`，否则为`False`。 |
| `Tensor.requires_grad_` | 更改是否应记录此张量上的操作的自动微分：就地设置此张量的`requires_grad`属性。 |
| `Tensor.reshape` | 返回一个与`self`具有相同数据和元素数量但具有指定形状的张量。 |
| `Tensor.reshape_as` | 将此张量返回为与`other`相同形状的张量。 |
| `Tensor.resize_` | 将`self`张量调整为指定大小。 |
| `Tensor.resize_as_` | 将 `self` 张量调整为与指定的 `tensor` 相同大小 |
| `Tensor.retain_grad` | 在 `backward()` 过程中使此张量的 `grad` 被填充 |
| `Tensor.retains_grad` | 如果此张量是非叶子节点且其 `grad` 已启用填充，则为 `True`，否则为 `False` |
| `Tensor.roll` | 查看 `torch.roll()` |
| `Tensor.rot90` | 查看 `torch.rot90()` |
| `Tensor.round` | 查看 `torch.round()` |
| `Tensor.round_` | `round()` 的就地版本 |
| `Tensor.rsqrt` | 查看 `torch.rsqrt()` |
| `Tensor.rsqrt_` | `rsqrt()` 的就地版本 |
| `Tensor.scatter` | `torch.Tensor.scatter_()` 的非就地版本 |
| `Tensor.scatter_` | 将张量 `src` 中的所有值写入到指定在 `index` 张量中的 `self` 中 |
| `Tensor.scatter_add_` | 将张量 `src` 中的所有值添加到 `index` 张量中指定的 `self` 中，类似于 `scatter_()` 的方式 |
| `Tensor.scatter_add` | `torch.Tensor.scatter_add_()` 的非就地版本 |
| `Tensor.scatter_reduce_` | 将 `src` 张量中的所有值按照应用的减少方式（`"sum"`、`"prod"`、`"mean"`、`"amax"`、`"amin"`）减少到 `index` 张量中指定的索引中的 `self` 张量中 |
| `Tensor.scatter_reduce` | `torch.Tensor.scatter_reduce_()` 的非就地版本 |
| `Tensor.select` | 查看 `torch.select()` |
| `Tensor.select_scatter` | 查看 `torch.select_scatter()` |
| `Tensor.set_` | 设置底层存储、大小和步幅。 |
| `Tensor.share_memory_` | 将底层存储移动到共享内存。 |
| `Tensor.short` | `self.short()` 等同于 `self.to(torch.int16)`。 |
| `Tensor.sigmoid` | 查看 `torch.sigmoid()` |
| `Tensor.sigmoid_` | `sigmoid()` 的原地版本 |
| `Tensor.sign` | 查看 `torch.sign()` |
| `Tensor.sign_` | `sign()` 的原地版本 |
| `Tensor.signbit` | 查看 `torch.signbit()` |
| `Tensor.sgn` | 查看 `torch.sgn()` |
| `Tensor.sgn_` | `sgn()` 的原地版本 |
| `Tensor.sin` | 查看 `torch.sin()` |
| `Tensor.sin_` | `sin()` 的原地版本 |
| `Tensor.sinc` | 查看 `torch.sinc()` |
| `Tensor.sinc_` | `sinc()` 的原地版本 |
| `Tensor.sinh` | 查看 `torch.sinh()` |
| `Tensor.sinh_` | `sinh()` 的原地版本 |
| `Tensor.asinh` | 查看 `torch.asinh()` |
| `Tensor.asinh_` | `asinh()` 的原地版本 |
| `Tensor.arcsinh` | 查看 `torch.arcsinh()` |
| `Tensor.arcsinh_` | `arcsinh()` 的原地版本 |
| `Tensor.shape` | 返回 `self` 张量的大小。 |
| `Tensor.size` | 返回 `self` 张量的大小。 |
| `Tensor.slogdet` | 查看 `torch.slogdet()` |
| `Tensor.slice_scatter` | 查看 `torch.slice_scatter()` |
| `Tensor.softmax` | `torch.nn.functional.softmax()` 的别名。 |
| `Tensor.sort` | 查看 `torch.sort()` |
| `Tensor.split` | 查看 `torch.split()` |
| `Tensor.sparse_mask` | 使用稀疏张量 `mask` 的索引过滤来自分块张量 `self` 的值，返回一个新的 稀疏张量。 |
| `Tensor.sparse_dim` | 返回 稀疏张量 `self` 中稀疏维度的数量。 |
| `Tensor.sqrt` | 查看 `torch.sqrt()` |
| `Tensor.sqrt_` | `sqrt()` 的原地版本 |
| `Tensor.square` | 查看 `torch.square()` |
| `Tensor.square_` | `square()` 的原地版本 |
| `Tensor.squeeze` | 查看 `torch.squeeze()` |
| `Tensor.squeeze_` | `squeeze()` 的原地版本 |
| `Tensor.std` | 查看 `torch.std()` |
| `Tensor.stft` | 查看 `torch.stft()` |
| `Tensor.storage` | 返回底层的 `TypedStorage`。 |
| `Tensor.untyped_storage` | 返回底层的 `UntypedStorage`。 |
| `Tensor.storage_offset` | 返回 `self` 张量在底层存储中的偏移量，以存储元素的数量表示（不是字节）。 |
| `Tensor.storage_type` | 返回底层存储的类型。 |
| `Tensor.stride` | 返回 `self` 张量的步幅。 |
| `Tensor.sub` | 查看 `torch.sub()` |
| `Tensor.sub_` | `sub()` 的原地版本 |
| `Tensor.subtract` | 查看 `torch.subtract()` |
| `Tensor.subtract_` | `subtract()` 的原地版本 |
| `Tensor.sum` | 查看 `torch.sum()` |
| `Tensor.sum_to_size` | 将 `this` 张量求和到 `size` |
| `Tensor.svd` | 查看 `torch.svd()` |
| `Tensor.swapaxes` | 查看 `torch.swapaxes()` |
| `Tensor.swapdims` | 查看 `torch.swapdims()` |
| `Tensor.t` | 查看 `torch.t()` |
| `Tensor.t_` | `t()` 的原地版本 |
| `Tensor.tensor_split` | 查看 `torch.tensor_split()` |
| `Tensor.tile` | 查看 `torch.tile()` |
| `Tensor.to` | 执行张量的数据类型和/或设备转换 |
| `Tensor.to_mkldnn` | 返回在 `torch.mkldnn` 布局中的张量的副本 |
| `Tensor.take` | 查看 `torch.take()` |
| `Tensor.take_along_dim` | 查看 `torch.take_along_dim()` |
| `Tensor.tan` | 查看 `torch.tan()` |
| `Tensor.tan_` | `tan()` 的原地版本 |
| `Tensor.tanh` | 查看 `torch.tanh()` |
| `Tensor.tanh_` | `tanh()` 的原地版本 |
| `Tensor.atanh` | 查看 `torch.atanh()` |
| `Tensor.atanh_` | `atanh()` 的原地版本 |
| `Tensor.arctanh` | 参见 `torch.arctanh()` |
| `Tensor.arctanh_` | `arctanh()` 的原地版本 |
| `Tensor.tolist` | 将张量返回为（嵌套的）列表。 |
| `Tensor.topk` | 参见 `torch.topk()` |
| `Tensor.to_dense` | 如果 `self` 不是分块张量，则创建 `self` 的分块副本，否则返回 `self`。 |
| `Tensor.to_sparse` | 返回张量的稀疏副本。 |
| `Tensor.to_sparse_csr` | 将张量转换为压缩行存储格式（CSR）。 |
| `Tensor.to_sparse_csc` | 将张量转换为压缩列存储（CSC）格式。 |
| `Tensor.to_sparse_bsr` | 将张量转换为给定块大小的块稀疏行（BSR）存储格式。 |
| `Tensor.to_sparse_bsc` | 将张量转换为给定块大小的块稀疏列（BSC）存储格式。 |
| `Tensor.trace` | 参见 `torch.trace()` |
| `Tensor.transpose` | 参见 `torch.transpose()` |
| `Tensor.transpose_` | `transpose()` 的原地版本 |
| `Tensor.triangular_solve` | 参见 `torch.triangular_solve()` |
| `Tensor.tril` | 参见 `torch.tril()` |
| `Tensor.tril_` | `tril()` 的原地版本 |
| `Tensor.triu` | 参见 `torch.triu()` |
| `Tensor.triu_` | `triu()` 的原地版本 |
| `Tensor.true_divide` | 参见 `torch.true_divide()` |
| `Tensor.true_divide_` | `true_divide_()` 的原位版本 |
| `Tensor.trunc` | 参见 `torch.trunc()` |
| `Tensor.trunc_` | `trunc()` 的原位版本 |
| `Tensor.type` | 如果未提供 dtype，则返回类型，否则将此对象转换为指定类型 |
| `Tensor.type_as` | 返回此张量转换为给定张量类型的结果 |
| `Tensor.unbind` | 参见 `torch.unbind()` |
| `Tensor.unflatten` | 参见 `torch.unflatten()` |
| `Tensor.unfold` | 返回原始张量的视图，其中包含 `self` 张量在维度 `dimension` 中大小为 `size` 的所有切片 |
| `Tensor.uniform_` | 用从连续均匀分布中抽样的数字填充 `self` 张量 |
| `Tensor.unique` | 返回输入张量的唯一元素 |
| `Tensor.unique_consecutive` | 消除每个连续等价元素组的除第一个元素之外的所有元素 |
| `Tensor.unsqueeze` | 参见 `torch.unsqueeze()` |
| `Tensor.unsqueeze_` | `unsqueeze()` 的原位版本 |
| `Tensor.values` | 返回 稀疏 COO 张量 的值张量 |
| `Tensor.var` | 参见 `torch.var()` |
| `Tensor.vdot` | 参见 `torch.vdot()` |
| `Tensor.view` | 返回一个与 `self` 张量具有相同数据但不同 `shape` 的新张量 |
| `Tensor.view_as` | 将此张量视为与 `other` 相同大小 |
| `Tensor.vsplit` | 参见 `torch.vsplit()` |
| `Tensor.where` | `self.where(condition, y)` 等同于 `torch.where(condition, self, y)` |
| `Tensor.xlogy` | 查看 `torch.xlogy()` |
| `Tensor.xlogy_` | `xlogy()` 的原地版本 |
| `Tensor.zero_` | 用零填充 `self` 张量。 |
