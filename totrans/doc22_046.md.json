["```py\ntorch.distributed.is_available()\u00b6\n```", "```py\ntorch.distributed.init_process_group(backend=None, init_method=None, timeout=None, world_size=-1, rank=-1, store=None, group_name='', pg_options=None)\u00b6\n```", "```py\ntorch.distributed.device_mesh.init_device_mesh(device_type, mesh_shape, *, mesh_dim_names=None)\u00b6\n```", "```py\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>>\n>>> mesh_1d = init_device_mesh(\"cuda\", mesh_shape=(8,))\n>>> mesh_2d = init_device_mesh(\"cuda\", mesh_shape=(2, 8), mesh_dim_names=(\"dp\", \"tp\")) \n```", "```py\ntorch.distributed.is_initialized()\u00b6\n```", "```py\ntorch.distributed.is_mpi_available()\u00b6\n```", "```py\ntorch.distributed.is_nccl_available()\u00b6\n```", "```py\ntorch.distributed.is_gloo_available()\u00b6\n```", "```py\ntorch.distributed.is_torchelastic_launched()\u00b6\n```", "```py\nimport torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4) \n```", "```py\nimport torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank) \n```", "```py\nclass torch.distributed.Backend(name)\u00b6\n```", "```py\nclassmethod register_backend(name, func, extended_api=False, devices=None)\u00b6\n```", "```py\ntorch.distributed.get_backend(group=None)\u00b6\n```", "```py\ntorch.distributed.get_rank(group=None)\u00b6\n```", "```py\ntorch.distributed.get_world_size(group=None)\u00b6\n```", "```py\nclass torch.distributed.Store\u00b6\n```", "```py\nclass torch.distributed.TCPStore\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\") \n```", "```py\nclass torch.distributed.HashStore\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> store = dist.HashStore()\n>>> # store can be used from other threads\n>>> # Use any of the store methods after initialization\n>>> store.set(\"first_key\", \"first_value\") \n```", "```py\nclass torch.distributed.FileStore\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\") \n```", "```py\nclass torch.distributed.PrefixStore\u00b6\n```", "```py\ntorch.distributed.Store.set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) \u2192 None\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\") \n```", "```py\ntorch.distributed.Store.get(self: torch._C._distributed_c10d.Store, arg0: str) \u2192 bytes\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\") \n```", "```py\ntorch.distributed.Store.add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) \u2192 int\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\") \n```", "```py\ntorch.distributed.Store.compare_set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str, arg2: str) \u2192 bytes\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"key\", \"first_value\")\n>>> store.compare_set(\"key\", \"first_value\", \"second_value\")\n>>> # Should return \"second_value\"\n>>> store.get(\"key\") \n```", "```py\ntorch.distributed.Store.wait(*args, **kwargs)\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"]) \n```", "```py\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10)) \n```", "```py\ntorch.distributed.Store.num_keys(self: torch._C._distributed_c10d.Store) \u2192 int\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys() \n```", "```py\ntorch.distributed.Store.delete_key(self: torch._C._distributed_c10d.Store, arg0: str) \u2192 bool\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\") \n```", "```py\ntorch.distributed.Store.set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) \u2192 None\u00b6\n```", "```py\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"]) \n```", "```py\ntorch.distributed.new_group(ranks=None, timeout=None, backend=None, pg_options=None, use_local_synchronization=False)\u00b6\n```", "```py\ntorch.distributed.get_group_rank(group, global_rank)\u00b6\n```", "```py\ntorch.distributed.get_global_rank(group, group_rank)\u00b6\n```", "```py\ntorch.distributed.get_process_group_ranks(group)\u00b6\n```", "```py\nclass torch.distributed.device_mesh.DeviceMesh(device_type, mesh, *, mesh_dim_names=None, _init_process_groups=True)\u00b6\n```", "```py\n>>> from torch.distributed.device_mesh import DeviceMesh\n>>>\n>>> # Initialize device mesh as (2, 4) to represent the topology\n>>> # of cross-host(dim 0), and within-host (dim 1).\n>>> mesh = DeviceMesh(device_type=\"cuda\", mesh=[[0, 1, 2, 3],[4, 5, 6, 7]]) \n```", "```py\ntorch.distributed.send(tensor, dst, group=None, tag=0)\u00b6\n```", "```py\ntorch.distributed.recv(tensor, src=None, group=None, tag=0)\u00b6\n```", "```py\ntorch.distributed.isend(tensor, dst, group=None, tag=0)\u00b6\n```", "```py\ntorch.distributed.irecv(tensor, src=None, group=None, tag=0)\u00b6\n```", "```py\ntorch.distributed.batch_isend_irecv(p2p_op_list)\u00b6\n```", "```py\n>>> send_tensor = torch.arange(2, dtype=torch.float32) + 2 * rank\n>>> recv_tensor = torch.randn(2, dtype=torch.float32)\n>>> send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1)%world_size)\n>>> recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank - 1 + world_size)%world_size)\n>>> reqs = batch_isend_irecv([send_op, recv_op])\n>>> for req in reqs:\n>>>     req.wait()\n>>> recv_tensor\ntensor([2, 3])     # Rank 0\ntensor([0, 1])     # Rank 1 \n```", "```py\nclass torch.distributed.P2POp(op, tensor, peer, group=None, tag=0)\u00b6\n```", "```py\n# Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output) \n```", "```py\ntorch.distributed.broadcast(tensor, src, group=None, async_op=False)\u00b6\n```", "```py\ntorch.distributed.broadcast_object_list(object_list, src=0, group=None, device=None)\u00b6\n```", "```py\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> # Assumes backend is not NCCL\n>>> device = torch.device(\"cpu\")\n>>> dist.broadcast_object_list(objects, src=0, device=device)\n>>> objects\n['foo', 12, {1: 2}] \n```", "```py\ntorch.distributed.all_reduce(tensor, op=<RedOpType.SUM: 0>, group=None, async_op=False)\u00b6\n```", "```py\n>>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1 \n```", "```py\n>>> # All tensors below are of torch.cfloat type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4.+4.j, 6.+6.j]) # Rank 0\ntensor([4.+4.j, 6.+6.j]) # Rank 1 \n```", "```py\ntorch.distributed.reduce(tensor, dst, op=<RedOpType.SUM: 0>, group=None, async_op=False)\u00b6\n```", "```py\ntorch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False)\u00b6\n```", "```py\n>>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1 \n```", "```py\n>>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zeros(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1 \n```", "```py\ntorch.distributed.all_gather_into_tensor(output_tensor, input_tensor, group=None, async_op=False)\u00b6\n```", "```py\n>>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n>>> # We have two ranks.\n>>> device = torch.device(f'cuda:{rank}')\n>>> tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n>>> tensor_in\ntensor([1, 2], device='cuda:0') # Rank 0\ntensor([3, 4], device='cuda:1') # Rank 1\n>>> # Output in concatenation form\n>>> tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device)\n>>> dist.all_gather_into_tensor(tensor_out, tensor_in)\n>>> tensor_out\ntensor([1, 2, 3, 4], device='cuda:0') # Rank 0\ntensor([1, 2, 3, 4], device='cuda:1') # Rank 1\n>>> # Output in stack form\n>>> tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device)\n>>> dist.all_gather_into_tensor(tensor_out2, tensor_in)\n>>> tensor_out2\ntensor([[1, 2],\n [3, 4]], device='cuda:0') # Rank 0\ntensor([[1, 2],\n [3, 4]], device='cuda:1') # Rank 1 \n```", "```py\ntorch.distributed.all_gather_object(object_list, obj, group=None)\u00b6\n```", "```py\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}] \n```", "```py\ntorch.distributed.gather(tensor, gather_list=None, dst=0, group=None, async_op=False)\u00b6\n```", "```py\ntorch.distributed.gather_object(obj, object_gather_list=None, dst=0, group=None)\u00b6\n```", "```py\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n...     gather_objects[dist.get_rank()],\n...     output if dist.get_rank() == 0 else None,\n...     dst=0\n... )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}] \n```", "```py\ntorch.distributed.scatter(tensor, scatter_list=None, src=0, group=None, async_op=False)\u00b6\n```", "```py\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> tensor_size = 2\n>>> t_ones = torch.ones(tensor_size)\n>>> t_fives = torch.ones(tensor_size) * 5\n>>> output_tensor = torch.zeros(tensor_size)\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 2.\n>>>     # Only tensors, all of which must be the same size.\n>>>     scatter_list = [t_ones, t_fives]\n>>> else:\n>>>     scatter_list = None\n>>> dist.scatter(output_tensor, scatter_list, src=0)\n>>> # Rank i gets scatter_list[i]. For example, on rank 1:\n>>> output_tensor\ntensor([5., 5.]) \n```", "```py\ntorch.distributed.scatter_object_list(scatter_object_output_list, scatter_object_input_list, src=0, group=None)\u00b6\n```", "```py\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}] \n```", "```py\ntorch.distributed.reduce_scatter(output, input_list, op=<RedOpType.SUM: 0>, group=None, async_op=False)\u00b6\n```", "```py\ntorch.distributed.reduce_scatter_tensor(output, input, op=<RedOpType.SUM: 0>, group=None, async_op=False)\u00b6\n```", "```py\n>>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n>>> # We have two ranks.\n>>> device = torch.device(f'cuda:{rank}')\n>>> tensor_out = torch.zeros(2, dtype=torch.int64, device=device)\n>>> # Input in concatenation form\n>>> tensor_in = torch.arange(world_size * 2, dtype=torch.int64, device=device)\n>>> tensor_in\ntensor([0, 1, 2, 3], device='cuda:0') # Rank 0\ntensor([0, 1, 2, 3], device='cuda:1') # Rank 1\n>>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n>>> tensor_out\ntensor([0, 2], device='cuda:0') # Rank 0\ntensor([4, 6], device='cuda:1') # Rank 1\n>>> # Input in stack form\n>>> tensor_in = torch.reshape(tensor_in, (world_size, 2))\n>>> tensor_in\ntensor([[0, 1],\n [2, 3]], device='cuda:0') # Rank 0\ntensor([[0, 1],\n [2, 3]], device='cuda:1') # Rank 1\n>>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n>>> tensor_out\ntensor([0, 2], device='cuda:0') # Rank 0\ntensor([4, 6], device='cuda:1') # Rank 1 \n```", "```py\ntorch.distributed.all_to_all_single(output, input, output_split_sizes=None, input_split_sizes=None, group=None, async_op=False)\u00b6\n```", "```py\n>>> input = torch.arange(4) + rank * 4\n>>> input\ntensor([0, 1, 2, 3])     # Rank 0\ntensor([4, 5, 6, 7])     # Rank 1\ntensor([8, 9, 10, 11])   # Rank 2\ntensor([12, 13, 14, 15]) # Rank 3\n>>> output = torch.empty([4], dtype=torch.int64)\n>>> dist.all_to_all_single(output, input)\n>>> output\ntensor([0, 4, 8, 12])    # Rank 0\ntensor([1, 5, 9, 13])    # Rank 1\ntensor([2, 6, 10, 14])   # Rank 2\ntensor([3, 7, 11, 15])   # Rank 3 \n```", "```py\n>>> # Essentially, it is similar to following operation:\n>>> scatter_list = list(input.chunk(world_size))\n>>> gather_list  = list(output.chunk(world_size))\n>>> for i in range(world_size):\n>>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i) \n```", "```py\n>>> # Another example with uneven split\n>>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> output = ...\n>>> dist.all_to_all_single(output, input, output_splits, input_splits)\n>>> output\ntensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0\ntensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1\ntensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2\ntensor([ 5, 17, 18, 24, 36])                                     # Rank 3 \n```", "```py\n>>> # Another example with tensors of torch.cfloat type.\n>>> input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)\n>>> input\ntensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0\ntensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1\ntensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2\ntensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3\n>>> output = torch.empty([4], dtype=torch.int64)\n>>> dist.all_to_all_single(output, input)\n>>> output\ntensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0\ntensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1\ntensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2\ntensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3 \n```", "```py\ntorch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False)\u00b6\n```", "```py\n>>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3 \n```", "```py\n>>> # Essentially, it is similar to following operation:\n>>> scatter_list = input\n>>> gather_list  = output\n>>> for i in range(world_size):\n>>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src=i) \n```", "```py\n>>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> input = list(input.split(input_splits))\n>>> input\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n>>> output = ...\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3 \n```", "```py\n>>> # Another example with tensors of torch.cfloat type.\n>>> input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0\n[tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1\n[tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2\n[tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0\n[tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1\n[tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2\n[tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3 \n```", "```py\ntorch.distributed.barrier(group=None, async_op=False, device_ids=None)\u00b6\n```", "```py\ntorch.distributed.monitored_barrier(group=None, timeout=None, wait_all_ranks=False)\u00b6\n```", "```py\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() != 1:\n>>>     dist.monitored_barrier() # Raises exception indicating that\n>>> # rank 1 did not call into monitored_barrier.\n>>> # Example with wait_all_ranks=True\n>>> if dist.get_rank() == 0:\n>>>     dist.monitored_barrier(wait_all_ranks=True) # Raises exception\n>>> # indicating that ranks 1, 2, ... world_size - 1 did not call into\n>>> # monitored_barrier. \n```", "```py\nclass torch.distributed.ReduceOp\u00b6\n```", "```py\nclass torch.distributed.reduce_op\u00b6\n```", "```py\nimport torch\nimport torch.distributed as dist\nwith torch.profiler():\n    tensor = torch.randn(20, 10)\n    dist.all_reduce(tensor) \n```", "```py\npython -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n           arguments of your training script) \n```", "```py\npython -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node-rank=0 --master-addr=\"192.168.1.1\"\n           --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script) \n```", "```py\npython -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node-rank=1 --master-addr=\"192.168.1.1\"\n           --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script) \n```", "```py\npython -m torch.distributed.launch --help \n```", "```py\n>>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local-rank\", type=int)\n>>> args = parser.parse_args() \n```", "```py\n>>> torch.cuda.set_device(args.local_rank)  # before your code runs \n```", "```py\n>>> with torch.cuda.device(args.local_rank):\n>>>    # your code to run\n>>>    ... \n```", "```py\n>>> torch.distributed.init_process_group(backend='YOUR BACKEND',\n>>>                                      init_method='env://') \n```", "```py\n>>> model = torch.nn.parallel.DistributedDataParallel(model,\n>>>                                                   device_ids=[args.local_rank],\n>>>                                                   output_device=args.local_rank) \n```", "```py\nimport os\nfrom datetime import timedelta\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    # monitored barrier requires gloo process group to perform host-side sync.\n    group_gloo = dist.new_group(backend=\"gloo\")\n    if rank not in [1]:\n        dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2))\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    mp.spawn(worker, nprocs=2, args=()) \n```", "```py\nRuntimeError: Rank 1 failed to pass monitoredBarrier in 2000 ms\n Original exception:\n[gloo/transport/tcp/pair.cc:598] Connection closed by peer [2401:db00:eef0:1100:3560:0:1c05:25d]:8594 \n```", "```py\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nclass TwoLinLayerNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.Linear(10, 10, bias=False)\n        self.b = torch.nn.Linear(10, 1, bias=False)\n\n    def forward(self, x):\n        a = self.a(x)\n        b = self.b(x)\n        return (a, b)\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    print(\"init model\")\n    model = TwoLinLayerNet().cuda()\n    print(\"init ddp\")\n    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n\n    inp = torch.randn(10, 10).cuda()\n    print(\"train\")\n\n    for _ in range(20):\n        output = ddp_model(inp)\n        loss = output[0] + output[1]\n        loss.sum().backward()\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\n        \"TORCH_DISTRIBUTED_DEBUG\"\n    ] = \"DETAIL\"  # set to DETAIL for runtime logging.\n    mp.spawn(worker, nprocs=2, args=()) \n```", "```py\nI0607 16:10:35.739390 515217 logger.cpp:173] [Rank 0]: DDP Initialized with:\nbroadcast_buffers: 1\nbucket_cap_bytes: 26214400\nfind_unused_parameters: 0\ngradient_as_bucket_view: 0\nis_multi_device_module: 0\niteration: 0\nnum_parameter_tensors: 2\noutput_device: 0\nrank: 0\ntotal_parameter_size_bytes: 440\nworld_size: 2\nbackend_name: nccl\nbucket_sizes: 440\ncuda_visible_devices: N/A\ndevice_ids: 0\ndtypes: float\nmaster_addr: localhost\nmaster_port: 29501\nmodule_name: TwoLinLayerNet\nnccl_async_error_handling: N/A\nnccl_blocking_wait: N/A\nnccl_debug: WARN\nnccl_ib_timeout: N/A\nnccl_nthreads: N/A\nnccl_socket_ifname: N/A\ntorch_distributed_debug: INFO \n```", "```py\nI0607 16:18:58.085681 544067 logger.cpp:344] [Rank 1 / 2] Training TwoLinLayerNet unused_parameter_size=0\n Avg forward compute time: 40838608\n Avg backward compute time: 5983335\nAvg backward comm. time: 4326421\n Avg backward comm/comp overlap time: 4207652\nI0607 16:18:58.085693 544066 logger.cpp:344] [Rank 0 / 2] Training TwoLinLayerNet unused_parameter_size=0\n Avg forward compute time: 42850427\n Avg backward compute time: 3885553\nAvg backward comm. time: 2357981\n Avg backward comm/comp overlap time: 2234674 \n```", "```py\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing\n the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\nmaking sure all `forward` function outputs participate in calculating loss.\nIf you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return va\nlue of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\nParameters which did not receive grad for rank 0: a.weight\nParameter indices which did not receive grad for rank 0: 0 \n```", "```py\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    tensor = torch.randn(10 if rank == 0 else 20).cuda()\n    dist.all_reduce(tensor)\n    torch.cuda.synchronize(device=rank)\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n    mp.spawn(worker, nprocs=2, args=()) \n```", "```py\nwork = default_pg.allreduce([tensor], opts)\nRuntimeError: Error when verifying shape tensors for collective ALLREDUCE on rank 0. This likely indicates that input shapes into the collective are mismatched across ranks. Got shapes:  10\n[ torch.LongTensor{1} ] \n```", "```py\nclass torch.distributed.DistError\u00b6\n```", "```py\nclass torch.distributed.DistBackendError\u00b6\n```", "```py\nclass torch.distributed.DistNetworkError\u00b6\n```", "```py\nclass torch.distributed.DistStoreError\u00b6\n```", "```py\ntorch.distributed.breakpoint(rank=0)\u00b6\n```"]