["```py\n/* Example TensorImpl constructor */\nTensorImpl(\n  Storage&&  storage,\n  DispatchKeySet  ks,\n  const  caffe2::TypeMeta  data_type);\n\n// To create a TensorImpl on PrivateUse1 backend, pass in the following ks to TensorImpl creation.\nDispatchKeySet  ks  =  c10::DispatchKeySet{c10::DispatchKey::PrivateUse1,  c10::DispatchKey::AutogradPrivateUse1}; \n```", "```py\nTensor  abs(const  Tensor  &  self);  // {\"schema\": \"aten::abs(Tensor self) -> Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor  &  abs_(Tensor  &  self);  // {\"schema\": \"aten::abs_(Tensor(a!) self) -> Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor  &  abs_out(Tensor  &  out,  const  Tensor  &  self);  // {\"schema\": \"aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"False\"}\nTensor  absolute(const  Tensor  &  self);  // {\"schema\": \"aten::absolute(Tensor self) -> Tensor\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor  &  absolute_(Tensor  &  self);  // {\"schema\": \"aten::absolute_(Tensor(a!) self) -> Tensor(a!)\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor  &  absolute_out(Tensor  &  out,  const  Tensor  &  self);  // {\"schema\": \"aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor  angle(const  Tensor  &  self);  // {\"schema\": \"aten::angle(Tensor self) -> Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor  &  angle_out(Tensor  &  out,  const  Tensor  &  self);  // {\"schema\": \"aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"False\"}\nTensor  sgn(const  Tensor  &  self);  // {\"schema\": \"aten::sgn(Tensor self) -> Tensor\", \"dispatch\": \"True\", \"default\": \"True\"} \n```", "```py\nTORCH_LIBRARY_IMPL(aten,  PrivateUse1,  m)  {\n  m.impl(<schema_my_op1>,  &my_op1);\n  m.impl(<schema_my_op2>,  &my_op2);\n  m.impl(<schema_my_op2_backward>,  &my_op2_backward);\n} \n```", "```py\nTensor  my_op1(const  Tensor&  self,  const  Tensor&  other)  {\n  // call your backend-specific APIs to implement my_op so that\n  // it matches PyTorch's native behavior\n}\nTORCH_LIBRARY_IMPL(aten,  PrivateUse1,  m)  {\n  m.impl(<schema_my_op1>,  &my_op);\n} \n```", "```py\nTensor  my_op2_backward(const  Tensor&  self,  const  Tensor&  other)  {\n  // call your backend-specific APIs to implement my_op2_backward so that\n  // it matches PyTorch's native behavior\n}\n\n// Note backward kernel is still registered to PrivateUse1 instead of AutogradPrivateUse1.\n// PyTorch will wrap your backward kernel with proper autograd setup and then link to it in\n// my_op2's AutogradPrivateUse1 kernel.\nTORCH_LIBRARY_IMPL(aten,  PrivateUse1,  m)  {\n  m.impl(<schema_my_op2>,  &my_op2);\n  m.impl(<schema_my_op2_backward>,  &my_op2_backward);\n} \n```", "```py\nclass  MyAddFunction  :  public  torch::autograd::Function<MyAddFunction>  {\n  public:\n  static  Tensor  forward(AutogradContext  *ctx,  torch::Tensor  self,  torch::Tensor  other)  {\n  at::AutoNonVariableTypeMode  g;\n  return  myadd(self,  other);\n  }\n\n  static  tensor_list  backward(AutogradContext  *ctx,  tensor_list  grad_outputs)  {\n  auto  grad_output  =  grad_outputs[0];\n  return  {grad_output,  grad_output};\n  }\n};\n\nTensor  myadd_autograd(const  Tensor&  self,  const  Tensor&  other)  {\n  return  MyAddFunction::apply(self,  other)[0];\n}\n\n// Register the autograd kernel to AutogradPrivateUse1\nTORCH_LIBRARY_IMPL(aten,  AutogradPrivateUse1,  m)  {\n  m.impl(<myadd_schema>,  &myadd_autograd);\n}\n\n// Register the inference kernel to PrivateUse1\nTORCH_LIBRARY_IMPL(aten,  PrivateUse1,  m)  {\n  m.impl(<myadd_schema>,  &myadd);\n} \n```", "```py\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name='torch_xla',\n    ext_modules=[\n        CppExtension(\n            '_XLAC',\n            torch_xla_sources,\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            library_dirs=library_dirs,\n            extra_link_args=extra_link_args + \\\n                [make_relative_rpath('torch_xla/lib')],\n        ),\n    ],\n    cmdclass={\n        'build_ext': Build,  # Build is a derived class of BuildExtension\n    }\n    # more configs...\n) \n```"]