["```py\nimport math\nimport os\nfrom tempfile import TemporaryDirectory\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn, [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\nfrom torch.nn import [TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder \"torch.nn.TransformerEncoder\"), [TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer \"torch.nn.TransformerEncoderLayer\")\nfrom torch.utils.data import dataset\n\nclass TransformerModel([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n\n    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n                 nlayers: int, dropout: float = 0.5):\n        super().__init__()\n        self.model_type = 'Transformer'\n        self.pos_encoder = [PositionalEncoding](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(d_model, dropout)\n        encoder_layers = [TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer \"torch.nn.TransformerEncoderLayer\")(d_model, nhead, d_hid, dropout)\n        self.transformer_encoder = [TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder \"torch.nn.TransformerEncoder\")(encoder_layers, nlayers)\n        self.embedding = [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding \"torch.nn.Embedding\")(ntoken, d_model)\n        self.d_model = d_model\n        self.linear = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(d_model, ntoken)\n\n        self.init_weights()\n\n    def init_weights(self) -> None:\n        initrange = 0.1\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.linear.bias.data.zero_()\n        self.linear.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), src_mask: [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = None) -> [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"):\n  \"\"\"\n Arguments:\n src: Tensor, shape ``[seq_len, batch_size]``\n src_mask: Tensor, shape ``[seq_len, seq_len]``\n\n Returns:\n output Tensor of shape ``[seq_len, batch_size, ntoken]``\n \"\"\"\n        src = self.embedding(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        if src_mask is None:\n  \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n Unmasked positions are filled with float(0.0).\n \"\"\"\n            src_mask = [nn.Transformer.generate_square_subsequent_mask](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer.generate_square_subsequent_mask \"torch.nn.Transformer.generate_square_subsequent_mask\")(len(src)).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n        output = self.transformer_encoder(src, src_mask)\n        output = self.linear(output)\n        return output \n```", "```py\nclass PositionalEncoding([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(p=dropout)\n\n        position = [torch.arange](https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange \"torch.arange\")(max_len).unsqueeze(1)\n        div_term = [torch.exp](https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp \"torch.exp\")([torch.arange](https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange \"torch.arange\")(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros \"torch.zeros\")(max_len, 1, d_model)\n        pe[:, 0, 0::2] = [torch.sin](https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin \"torch.sin\")(position * div_term)\n        pe[:, 0, 1::2] = [torch.cos](https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos \"torch.cos\")(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) -> [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"):\n  \"\"\"\n Arguments:\n x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x) \n```", "```py\n> %%bash\n> pip  install  portalocker\n> pip  install  torchdata \n> ```", "```py\nfrom torchtext.datasets import [WikiText2](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2 \"torchtext.datasets.WikiText2\")\nfrom torchtext.data.utils import [get_tokenizer](https://pytorch.org/text/stable/data_utils.html#torchtext.data.utils.get_tokenizer \"torchtext.data.utils.get_tokenizer\")\nfrom torchtext.vocab import [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")\n\ntrain_iter = [WikiText2](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2 \"torchtext.datasets.WikiText2\")(split='train')\ntokenizer = [get_tokenizer](https://pytorch.org/text/stable/data_utils.html#torchtext.data.utils.get_tokenizer \"torchtext.data.utils.get_tokenizer\")('basic_english')\n[vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\") = [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")(map(tokenizer, train_iter), specials=['<unk>'])\n[vocab.set_default_index](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.set_default_index \"torchtext.vocab.Vocab.set_default_index\")([vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\")['<unk>'])\n\ndef data_process(raw_text_iter: [dataset.IterableDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset \"torch.utils.data.IterableDataset\")) -> [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"):\n  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n    data = [[torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")([vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\")(tokenizer(item)), dtype=[torch.long](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")) for item in raw_text_iter]\n    return [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")(tuple(filter(lambda t: t.numel() > 0, data)))\n\n# ``train_iter`` was \"consumed\" by the process of building the vocab,\n# so we have to create it again\ntrain_iter, val_iter, test_iter = [WikiText2](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2 \"torchtext.datasets.WikiText2\")()\n[train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = data_process(train_iter)\n[val_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = data_process(val_iter)\n[test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = data_process(test_iter)\n\n[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")('cuda' if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available \"torch.cuda.is_available\")() else 'cpu')\n\ndef batchify(data: [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), bsz: int) -> [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"):\n  \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n that wouldn't cleanly fit.\n\n Arguments:\n data: Tensor, shape ``[N]``\n bsz: int, batch size\n\n Returns:\n Tensor of shape ``[N // bsz, bsz]``\n \"\"\"\n    seq_len = data.size(0) // bsz\n    data = data[:seq_len * bsz]\n    data = data.view(bsz, seq_len).t().contiguous()\n    return data.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\nbatch_size = 20\neval_batch_size = 10\n[train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = batchify([train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), batch_size)  # shape ``[seq_len, batch_size]``\n[val_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = batchify([val_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), eval_batch_size)\n[test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = batchify([test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), eval_batch_size) \n```", "```py\nbptt = 35\ndef get_batch(source: [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), i: int) -> Tuple[[Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")]:\n  \"\"\"\n Args:\n source: Tensor, shape ``[full_seq_len, batch_size]``\n i: int\n\n Returns:\n tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n target has shape ``[seq_len * batch_size]``\n \"\"\"\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target \n```", "```py\nntokens = len([vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\"))  # size of vocabulary\nemsize = 200  # embedding dimension\nd_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\nnlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\nnhead = 2  # number of heads in ``nn.MultiheadAttention``\ndropout = 0.2  # dropout probability\nmodel = [TransformerModel](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(ntokens, emsize, nhead, d_hid, nlayers, dropout).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")) \n```", "```py\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning:\n\nenable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance) \n```", "```py\nimport time\n\n[criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\") = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\nlr = 5.0  # learning rate\n[optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\") = [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")(), lr=lr)\n[scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\") = [torch.optim.lr_scheduler.StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\")([optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\"), 1.0, gamma=0.95)\n\ndef train(model: [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")) -> None:\n    [model.train](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train \"torch.nn.Module.train\")()  # turn on train mode\n    total_loss = 0.\n    log_interval = 200\n    start_time = time.time()\n\n    num_batches = len([train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) // bptt\n    for batch, i in enumerate(range(0, [train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").size(0) - 1, bptt)):\n        data, targets = get_batch([train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), i)\n        output = model(data)\n        output_flat = output.view(-1, ntokens)\n        loss = [criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")(output_flat, targets)\n\n        [optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.zero_grad \"torch.optim.SGD.zero_grad\")()\n        loss.backward()\n        [torch.nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_ \"torch.nn.utils.clip_grad_norm_\")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")(), 0.5)\n        [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\").step()\n\n        total_loss += loss.item()\n        if batch % log_interval == 0 and batch > 0:\n            lr = [scheduler.get_last_lr](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.get_last_lr \"torch.optim.lr_scheduler.StepLR.get_last_lr\")()[0]\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            ppl = math.exp(cur_loss)\n            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(model: [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), eval_data: [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) -> float:\n    [model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")()  # turn on evaluation mode\n    total_loss = 0.\n    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n        for i in range(0, eval_data.size(0) - 1, bptt):\n            data, targets = get_batch(eval_data, i)\n            seq_len = data.size(0)\n            output = model(data)\n            output_flat = output.view(-1, ntokens)\n            total_loss += seq_len * [criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")(output_flat, targets).item()\n    return total_loss / (len(eval_data) - 1) \n```", "```py\nbest_val_loss = float('inf')\nepochs = 3\n\nwith TemporaryDirectory() as tempdir:\n    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train(model)\n        val_loss = evaluate(model, [val_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n        val_ppl = math.exp(val_loss)\n        elapsed = time.time() - epoch_start_time\n        print('-' * 89)\n        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n        print('-' * 89)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            [torch.save](https://pytorch.org/docs/stable/generated/torch.save.html#torch.save \"torch.save\")([model.state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict \"torch.nn.Module.state_dict\")(), best_model_params_path)\n\n        [scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\").step()\n    [model.load_state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict \"torch.nn.Module.load_state_dict\")([torch.load](https://pytorch.org/docs/stable/generated/torch.load.html#torch.load \"torch.load\")(best_model_params_path)) # load best model states \n```", "```py\n| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 31.93 | loss  8.19 | ppl  3613.91\n| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 28.57 | loss  6.88 | ppl   970.94\n| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 28.31 | loss  6.43 | ppl   621.40\n| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 28.48 | loss  6.30 | ppl   542.89\n| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 28.46 | loss  6.18 | ppl   484.73\n| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 28.32 | loss  6.15 | ppl   467.52\n| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 28.53 | loss  6.11 | ppl   450.65\n| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 28.45 | loss  6.11 | ppl   450.73\n| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 28.43 | loss  6.02 | ppl   410.39\n| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 28.56 | loss  6.01 | ppl   409.43\n| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 28.47 | loss  5.89 | ppl   361.18\n| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 28.57 | loss  5.97 | ppl   393.23\n| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 28.53 | loss  5.95 | ppl   383.85\n| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 28.59 | loss  5.88 | ppl   357.86\n-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 87.20s | valid loss  5.78 | valid ppl   324.74\n-----------------------------------------------------------------------------------------\n| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 28.74 | loss  5.86 | ppl   349.96\n| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 28.60 | loss  5.85 | ppl   348.22\n| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 28.49 | loss  5.66 | ppl   286.86\n| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 28.39 | loss  5.70 | ppl   297.60\n| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 28.55 | loss  5.64 | ppl   282.01\n| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 28.56 | loss  5.67 | ppl   290.49\n| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 28.58 | loss  5.68 | ppl   292.36\n| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 28.64 | loss  5.70 | ppl   299.93\n| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 28.58 | loss  5.64 | ppl   282.54\n| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 28.50 | loss  5.66 | ppl   288.23\n| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 28.46 | loss  5.54 | ppl   254.44\n| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 28.58 | loss  5.65 | ppl   282.92\n| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 28.64 | loss  5.64 | ppl   282.54\n| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 28.57 | loss  5.58 | ppl   263.76\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time: 86.73s | valid loss  5.65 | valid ppl   282.95\n-----------------------------------------------------------------------------------------\n| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 28.69 | loss  5.60 | ppl   270.97\n| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 28.55 | loss  5.62 | ppl   276.79\n| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 28.65 | loss  5.42 | ppl   226.33\n| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 28.59 | loss  5.48 | ppl   239.30\n| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 28.51 | loss  5.44 | ppl   229.71\n| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 28.55 | loss  5.48 | ppl   238.78\n| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 28.51 | loss  5.50 | ppl   243.54\n| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 28.56 | loss  5.52 | ppl   248.47\n| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 28.44 | loss  5.46 | ppl   235.26\n| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 28.37 | loss  5.48 | ppl   240.24\n| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 28.43 | loss  5.38 | ppl   217.29\n| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 28.44 | loss  5.47 | ppl   236.64\n| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 28.46 | loss  5.47 | ppl   237.76\n| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 28.49 | loss  5.40 | ppl   220.67\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time: 86.51s | valid loss  5.61 | valid ppl   273.90\n----------------------------------------------------------------------------------------- \n```", "```py\ntest_loss = evaluate(model, [test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\ntest_ppl = math.exp(test_loss)\nprint('=' * 89)\nprint(f'| End of training | test loss {test_loss:5.2f} | '\n      f'test ppl {test_ppl:8.2f}')\nprint('=' * 89) \n```", "```py\n=========================================================================================\n| End of training | test loss  5.52 | test ppl   249.27\n========================================================================================= \n```"]