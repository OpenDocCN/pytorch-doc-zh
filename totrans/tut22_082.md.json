["```py\nimport torch\n\ndef foo(x):\n    return torch.neg(x)\n\n@torch.jit.script\ndef example(x):\n    # Call `foo` using parallelism:\n    # First, we \"fork\" off a task. This task will run `foo` with argument `x`\n    future = torch.jit.fork(foo, x)\n\n    # Call `foo` normally\n    x_normal = foo(x)\n\n    # Second, we \"wait\" on the task. Since the task may be running in\n    # parallel, we have to \"wait\" for its result to become available.\n    # Notice that by having lines of code between the \"fork()\" and \"wait()\"\n    # call for a given Future, we can overlap computations so that they\n    # run in parallel.\n    x_parallel = torch.jit.wait(future)\n\n    return x_normal, x_parallel\n\nprint(example(torch.ones(1))) # (-1., -1.) \n```", "```py\nimport torch\nfrom typing import List\n\ndef foo(x):\n    return torch.neg(x)\n\n@torch.jit.script\ndef example(x):\n    futures : List[torch.jit.Future[torch.Tensor]] = []\n    for _ in range(100):\n        futures.append(torch.jit.fork(foo, x))\n\n    results = []\n    for future in futures:\n        results.append(torch.jit.wait(future))\n\n    return torch.sum(torch.stack(results))\n\nprint(example(torch.ones([]))) \n```", "```py\nimport torch, time\n\n# In RNN parlance, the dimensions we care about are:\n# # of time-steps (T)\n# Batch size (B)\n# Hidden size/number of \"channels\" (C)\nT, B, C = 50, 50, 1024\n\n# A module that defines a single \"bidirectional LSTM\". This is simply two\n# LSTMs applied to the same sequence, but one in reverse\nclass BidirectionalRecurrentLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C)\n        self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C)\n\n    def forward(self, x : torch.Tensor) -> torch.Tensor:\n        # Forward layer\n        output_f, _ = self.cell_f(x)\n\n        # Backward layer. Flip input in the time dimension (dim 0), apply the\n        # layer, then flip the outputs in the time dimension\n        x_rev = torch.flip(x, dims=[0])\n        output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n        output_b_rev = torch.flip(output_b, dims=[0])\n\n        return torch.cat((output_f, output_b_rev), dim=2)\n\n# An \"ensemble\" of `BidirectionalRecurrentLSTM` modules. The modules in the\n# ensemble are run one-by-one on the same input then their results are\n# stacked and summed together, returning the combined result.\nclass LSTMEnsemble(torch.nn.Module):\n    def __init__(self, n_models):\n        super().__init__()\n        self.n_models = n_models\n        self.models = torch.nn.ModuleList([\n            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])\n\n    def forward(self, x : torch.Tensor) -> torch.Tensor:\n        results = []\n        for model in self.models:\n            results.append(model(x))\n        return torch.stack(results).sum(dim=0)\n\n# For a head-to-head comparison to what we're going to do with fork/wait, let's\n# instantiate the model and compile it with TorchScript\nens = torch.jit.script(LSTMEnsemble(n_models=4))\n\n# Normally you would pull this input out of an embedding table, but for the\n# purpose of this demo let's just use random data.\nx = torch.rand(T, B, C)\n\n# Let's run the model once to warm up things like the memory allocator\nens(x)\n\nx = torch.rand(T, B, C)\n\n# Let's see how fast it runs!\ns = time.time()\nens(x)\nprint('Inference took', time.time() - s, ' seconds') \n```", "```py\ndef forward(self, x : torch.Tensor) -> torch.Tensor:\n    # Forward layer - fork() so this can run in parallel to the backward\n    # layer\n    future_f = torch.jit.fork(self.cell_f, x)\n\n    # Backward layer. Flip input in the time dimension (dim 0), apply the\n    # layer, then flip the outputs in the time dimension\n    x_rev = torch.flip(x, dims=[0])\n    output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n    output_b_rev = torch.flip(output_b, dims=[0])\n\n    # Retrieve the output from the forward layer. Note this needs to happen\n    # *after* the stuff we want to parallelize with\n    output_f, _ = torch.jit.wait(future_f)\n\n    return torch.cat((output_f, output_b_rev), dim=2) \n```", "```py\nwith torch.autograd.profiler.profile() as prof:\n    ens(x)\nprof.export_chrome_trace('parallel.json') \n```", "```py\ndef forward(self, x : torch.Tensor) -> torch.Tensor:\n    # Launch tasks for each model\n    futures : List[torch.jit.Future[torch.Tensor]] = []\n    for model in self.models:\n        futures.append(torch.jit.fork(model, x))\n\n    # Collect the results from the launched tasks\n    results : List[torch.Tensor] = []\n    for future in futures:\n        results.append(torch.jit.wait(future))\n\n    return torch.stack(results).sum(dim=0) \n```", "```py\ndef forward(self, x : torch.Tensor) -> torch.Tensor:\n    futures = [torch.jit.fork(model, x) for model in self.models]\n    results = [torch.jit.wait(fut) for fut in futures]\n    return torch.stack(results).sum(dim=0) \n```"]