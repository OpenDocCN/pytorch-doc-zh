["```py\npip3  install  --pre  torch  torchvision  torchaudio  -f  https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html \n```", "```py\nimport os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, GPT2TokenizerFast\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport functools\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers.models.t5.modeling_t5 import T5Block\n\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n checkpoint_wrapper,\n CheckpointImpl,\n apply_activation_checkpointing_wrapper)\n\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    BackwardPrefetch,\n    ShardingStrategy,\n    FullStateDictConfig,\n    StateDictType,\n)\nfrom torch.distributed.fsdp.wrap import (\n    transformer_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom summarization_dataset import *\nfrom transformers.models.t5.modeling_t5 import T5Block\nfrom typing import Type\nimport time\nimport tqdm\nfrom datetime import datetime \n```", "```py\ndef setup():\n    # initialize the process group\n    dist.init_process_group(\"nccl\")\n\ndef cleanup():\n    dist.destroy_process_group() \n```", "```py\ndef setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer \n```", "```py\ndef get_date_of_run():\n  \"\"\"create date and time for file save uniqueness\n example: 2022-05-07-08:31:12_PM'\n \"\"\"\n    date_of_run = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n    print(f\"--> current date and time of run = {date_of_run}\")\n    return date_of_run\n\ndef format_metrics_to_gb(item):\n  \"\"\"quick function to format numbers to gigabyte and round to 4 digit precision\"\"\"\n    metric_num = item / g_gigabyte\n    metric_num = round(metric_num, ndigits=4)\n    return metric_num \n```", "```py\ndef train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    local_rank = int(os.environ['LOCAL_RANK'])\n    fsdp_loss = torch.zeros(2).to(local_rank)\n\n    if sampler:\n        sampler.set_epoch(epoch)\n    if rank==0:\n        inner_pbar = tqdm.tqdm(\n            range(len(train_loader)), colour=\"blue\", desc=\"r0 Training Epoch\"\n        )\n    for batch in train_loader:\n        for key in batch.keys():\n            batch[key] = batch[key].to(local_rank)\n        optimizer.zero_grad()\n        output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"] )\n        loss = output[\"loss\"]\n        loss.backward()\n        optimizer.step()\n        fsdp_loss[0] += loss.item()\n        fsdp_loss[1] += len(batch)\n        if rank==0:\n            inner_pbar.update(1)\n\n    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n    train_accuracy = fsdp_loss[0] / fsdp_loss[1]\n\n    if rank == 0:\n        inner_pbar.close()\n        print(\n                f\"Train Epoch: \\t{epoch}, Loss: \\t{train_accuracy:.4f}\"\n            )\n    return train_accuracy \n```", "```py\ndef validation(model, rank, world_size, val_loader):\n    model.eval()\n    correct = 0\n    local_rank = int(os.environ['LOCAL_RANK'])\n    fsdp_loss = torch.zeros(3).to(local_rank)\n    if rank == 0:\n        inner_pbar = tqdm.tqdm(\n            range(len(val_loader)), colour=\"green\", desc=\"Validation Epoch\"\n        )\n    with torch.no_grad():\n        for batch in val_loader:\n            for key in batch.keys():\n                batch[key] = batch[key].to(local_rank)\n            output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"])\n            fsdp_loss[0] += output[\"loss\"].item()  # sum up batch loss\n            fsdp_loss[1] += len(batch)\n\n            if rank==0:\n                inner_pbar.update(1)\n\n    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n    val_loss = fsdp_loss[0] / fsdp_loss[1]\n    if rank == 0:\n        inner_pbar.close()\n        print(f\"Validation Loss: {val_loss:.4f}\")\n    return val_loss \n```", "```py\ndef fsdp_main(args):\n\n    model, tokenizer = setup_model(\"t5-base\")\n\n    local_rank = int(os.environ['LOCAL_RANK'])\n    rank = int(os.environ['RANK'])\n    world_size = int(os.environ['WORLD_SIZE'])\n\n    dataset = load_dataset('wikihow', 'all', data_dir='data/')\n    print(dataset.keys())\n    print(\"Size of train dataset: \", dataset['train'].shape)\n    print(\"Size of Validation dataset: \", dataset['validation'].shape)\n\n    #wikihow(tokenizer, type_path, num_samples, input_length, output_length, print_text=False)\n    train_dataset = wikihow(tokenizer, 'train', 1500, 512, 150, False)\n    val_dataset = wikihow(tokenizer, 'validation', 300, 512, 150, False)\n\n    sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True)\n    sampler2 = DistributedSampler(val_dataset, rank=rank, num_replicas=world_size)\n\n    setup()\n\n    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n    cuda_kwargs = {'num_workers': 2,\n                    'pin_memory': True,\n                    'shuffle': False}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n    val_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)\n\n    t5_auto_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls={\n            T5Block,\n        },\n    )\n    sharding_strategy: ShardingStrategy = ShardingStrategy.SHARD_GRAD_OP #for Zero2 and FULL_SHARD for Zero3\n    torch.cuda.set_device(local_rank)\n\n    #init_start_event = torch.cuda.Event(enable_timing=True)\n    #init_end_event = torch.cuda.Event(enable_timing=True)\n\n    #init_start_event.record()\n\n    bf16_ready = (\n    torch.version.cuda\n    and torch.cuda.is_bf16_supported()\n    and LooseVersion(torch.version.cuda) >= \"11.0\"\n    and dist.is_nccl_available()\n    and nccl.version() >= (2, 10)\n    )\n\n    if bf16_ready:\n        mp_policy = bfSixteen\n    else:\n        mp_policy = None # defaults to fp32\n\n    # model is on CPU before input to FSDP\n    model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=mp_policy,\n        #sharding_strategy=sharding_strategy,\n        device_id=torch.cuda.current_device())\n\n    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    best_val_loss = float(\"inf\")\n    curr_val_loss = float(\"inf\")\n    file_save_name = \"T5-model-\"\n\n    if rank == 0:\n        time_of_run = get_date_of_run()\n        dur = []\n        train_acc_tracking = []\n        val_acc_tracking = []\n        training_start_time = time.time()\n\n    if rank == 0 and args.track_memory:\n        mem_alloc_tracker = []\n        mem_reserved_tracker = []\n\n    for epoch in range(1, args.epochs + 1):\n        t0 = time.time()\n        train_accuracy = train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n        if args.run_validation:\n            curr_val_loss = validation(model, rank, world_size, val_loader)\n        scheduler.step()\n\n        if rank == 0:\n\n            print(f\"--> epoch {epoch} completed...entering save and stats zone\")\n\n            dur.append(time.time() - t0)\n            train_acc_tracking.append(train_accuracy.item())\n\n            if args.run_validation:\n                val_acc_tracking.append(curr_val_loss.item())\n\n            if args.track_memory:\n                mem_alloc_tracker.append(\n                    format_metrics_to_gb(torch.cuda.memory_allocated())\n                )\n                mem_reserved_tracker.append(\n                    format_metrics_to_gb(torch.cuda.memory_reserved())\n                )\n            print(f\"completed save and stats zone...\")\n\n        if args.save_model and curr_val_loss < best_val_loss:\n\n            # save\n            if rank == 0:\n                print(f\"--> entering save model state\")\n\n            save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n            with FSDP.state_dict_type(\n                model, StateDictType.FULL_STATE_DICT, save_policy\n            ):\n                cpu_state = model.state_dict()\n            #print(f\"saving process: rank {rank}  done w state_dict\")\n\n            if rank == 0:\n                print(f\"--> saving model ...\")\n                currEpoch = (\n                    \"-\" + str(epoch) + \"-\" + str(round(curr_val_loss.item(), 4)) + \".pt\"\n                )\n                print(f\"--> attempting to save model prefix {currEpoch}\")\n                save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n                print(f\"--> saving as model name {save_name}\")\n\n                torch.save(cpu_state, save_name)\n\n        if curr_val_loss < best_val_loss:\n\n            best_val_loss = curr_val_loss\n            if rank==0:\n                print(f\"-->>>> New Val Loss Record: {best_val_loss}\")\n\n    dist.barrier()\n    cleanup() \n```", "```py\nif __name__ == '__main__':\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch T5 FSDP Example')\n    parser.add_argument('--batch-size', type=int, default=4, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=4, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n                        help='number of epochs to train (default: 3)')\n    parser.add_argument('--lr', type=float, default=.002, metavar='LR',\n                        help='learning rate (default: .002)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--track_memory', action='store_false', default=True,\n                        help='track the gpu memory')\n    parser.add_argument('--run_validation', action='store_false', default=True,\n                        help='running the validation')\n    parser.add_argument('--save-model', action='store_false', default=True,\n                        help='For Saving the current Model')\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    fsdp_main(args) \n```", "```py\ntorchrun  --nnodes  1  --nproc_per_node  4  T5_training.py \n```", "```py\nt5_auto_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls={\n            T5Block,\n        },\n    )\ntorch.cuda.set_device(local_rank)\n\nmodel = FSDP(model,\n    fsdp_auto_wrap_policy=t5_auto_wrap_policy) \n```", "```py\nbf16_ready = (\n    torch.version.cuda\n    and torch.cuda.is_bf16_supported()\n    and LooseVersion(torch.version.cuda) >= \"11.0\"\n    and dist.is_nccl_available()\n    and nccl.version() >= (2, 10)\n) \n```", "```py\nfpSixteen = MixedPrecision(\n    param_dtype=torch.float16,\n    # Gradient communication precision.\n    reduce_dtype=torch.float16,\n    # Buffer precision.\n    buffer_dtype=torch.float16,\n)\n\nbfSixteen = MixedPrecision(\n    param_dtype=torch.bfloat16,\n    # Gradient communication precision.\n    reduce_dtype=torch.bfloat16,\n    # Buffer precision.\n    buffer_dtype=torch.bfloat16,\n)\n\nfp32_policy = MixedPrecision(\n    param_dtype=torch.float32,\n    # Gradient communication precision.\n    reduce_dtype=torch.float32,\n    # Buffer precision.\n    buffer_dtype=torch.float32,\n) \n```", "```py\ngrad_bf16  =  MixedPrecision(reduce_dtype=torch.bfloat16) \n```", "```py\nmodel = FSDP(model,\n       auto_wrap_policy=t5_auto_wrap_policy,\n       mixed_precision=bfSixteen) \n```", "```py\ntorch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device()) \n```", "```py\ntorch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device(),\n        sharding_strategy=ShardingStrategy.SHARD_GRAD_OP # ZERO2) \n```", "```py\ntorch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device(),\n        backward_prefetch = BackwardPrefetch.BACKWARD_PRE) \n```", "```py\nsave_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\nwith FSDP.state_dict_type(\n            model, StateDictType.FULL_STATE_DICT, save_policy\n        ):\n            cpu_state = model.state_dict()\nif rank == 0:\n save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n torch.save(cpu_state, save_name) \n```"]