["```py\nimport torch\n# Simple module for demonstration\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\nmodule = MyModule()\n\nfrom torch.fx import symbolic_trace\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\n# High-level intermediate representation (IR) - Graph representation\nprint(symbolic_traced.graph)\n\"\"\"\ngraph():\n %x : [num_users=1] = placeholder[target=x]\n %param : [num_users=1] = get_attr[target=param]\n %add : [num_users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n %linear : [num_users=1] = call_module[target=linear](args = (%add,), kwargs = {})\n %clamp : [num_users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})\n return clamp\n\"\"\"\n\n# Code generation - valid Python code\nprint(symbolic_traced.code)\n\"\"\"\ndef forward(self, x):\n param = self.param\n add = x + param;  x = param = None\n linear = self.linear(add);  add = None\n clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n return clamp\n\"\"\" \n```", "```py\nimport torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph) \n```", "```py\nimport torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module:\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm \n```", "```py\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular() \n```", "```py\nimport torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph) \n```", "```py\n# Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node) \n```", "```py\n# Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n  \"\"\"\n Decompose `model` into smaller constituent operations.\n Currently,this only supports decomposing ReLU into its\n mathematical definition: (x > 0) * x\n \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    tracer = torch.fx.proxy.GraphAppendingTracer(new_graph)\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph) \n```", "```py\nimport torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n  \"\"\"\n Shape propagation. This class takes a `GraphModule`.\n Then, its `propagate` method executes the `GraphModule`\n node-by-node with the given arguments. As each operation\n executes, the ShapeProp class stores away the shape and\n element type for the output values of each operation on\n the `shape` and `dtype` attributes of the operation's\n `Node`.\n \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result) \n```", "```py\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\" \n```", "```py\nassert torch.allclose(resnet18(input_image), transformed_resnet18(input_image)) \n```", "```py\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value) \n```", "```py\n# Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n x = self.x\n add_1 = x + y;  x = y = None\n return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM() \n```", "```py\nm = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar() \n```", "```py\n# Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n add = x + y;  x = y = None\n return add\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph():\n %x : [num_users=1] = placeholder[target=x]\n %y : [num_users=1] = placeholder[target=y]\n %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})\n return add\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args    kwargs\n-------------  ------  -----------------------  ------  --------\nplaceholder    x       x                        ()      {}\nplaceholder    y       y                        ()      {}\ncall_function  add     <built-in function add>  (x, y)  {}\noutput         output  output                   (add,)  {}\n\"\"\" \n```", "```py\n# Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n  \"\"\"\n Transformations on `g` go here\n \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed) \n```", "```py\ndef func_to_trace(x):\n    if x.sum() > 0:\n        return torch.relu(x)\n    else:\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n <...>\n File \"dyn.py\", line 6, in func_to_trace\n if x.sum() > 0:\n File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n return self.tracer.to_bool(self)\n File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\" \n```", "```py\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n linear_1 = self.linear(x);  x = None\n return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n linear_1 = self.linear(x);  x = None\n relu_1 = torch.relu(linear_1);  linear_1 = None\n return relu_1\n\"\"\" \n```", "```py\ndef f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\nfx.symbolic_trace(f, concrete_args={'flag': True}) \n```", "```py\nimport torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n  \"\"\"\n Normalize `x` by the size of the batch dimension\n \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n <...>\n File \"sqrt.py\", line 9, in normalize\n return x / sqrt(len(x))\n File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\" \n```", "```py\ntorch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n len_1 = len(x)\n sqrt_1 = math.sqrt(len_1);  len_1 = None\n truediv = x / sqrt_1;  x = sqrt_1 = None\n return truediv\n\"\"\" \n```", "```py\nclass MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph) \n```", "```py\nclass MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n linear_1 = self.linear(x);  x = None\n neg_1 = torch.neg(linear_1);  linear_1 = None\n return neg_1\n\"\"\" \n```", "```py\n    > @torch.fx.wrap\n    > def torch_randn(x, shape):\n    >     return torch.randn(shape)\n    > \n    > def f(x):\n    >     return x + torch_randn(x, 5)\n    > fx.symbolic_trace(f) \n    > ```", "```py\n    > import torch\n    > import torch.fx\n    > \n    > class DropoutRepro(torch.nn.Module):\n    >   def forward(self, x):\n    >     return torch.nn.functional.dropout(x, training=self.training)\n    > \n    > traced = torch.fx.symbolic_trace(DropoutRepro())\n    > print(traced.code)\n    > \"\"\"\n    > def forward(self, x):\n    >  dropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False);  x = None\n    >  return dropout\n    > \"\"\"\n    > \n    > traced.eval()\n    > \n    > x = torch.randn(5, 3)\n    > torch.testing.assert_close(traced(x), x)\n    > \"\"\"\n    > AssertionError: Tensor-likes are not close!\n    > \n    > Mismatched elements: 15 / 15 (100.0%)\n    > Greatest absolute difference: 1.6207983493804932 at index (0, 2) (up to 1e-05 allowed)\n    > Greatest relative difference: 1.0 at index (0, 0) (up to 0.0001 allowed)\n    > \"\"\" \n    > ```", "```py\n    > class DropoutRepro2(torch.nn.Module):\n    >   def __init__(self):\n    >     super().__init__()\n    >     self.drop = torch.nn.Dropout()\n    > \n    >   def forward(self, x):\n    >     return self.drop(x)\n    > \n    > traced = torch.fx.symbolic_trace(DropoutRepro2())\n    > print(traced.code)\n    > \"\"\"\n    > def forward(self, x):\n    >  drop = self.drop(x);  x = None\n    >  return drop\n    > \"\"\"\n    > \n    > traced.eval()\n    > \n    > x = torch.randn(5, 3)\n    > torch.testing.assert_close(traced(x), x) \n    > ```", "```py\ntorch.fx.symbolic_trace(root, concrete_args=None)\u00b6\n```", "```py\ndef f(a, b):\n    if b == True:\n        return a\n    else:\n        return a*2 \n```", "```py\nf = fx.symbolic_trace(f, concrete_args={'b': False})\nassert f(3, False)  == 6 \n```", "```py\ndef f(x):\n    out = 0\n    for v in x.values():\n        out += v\n    return out\nf = fx.symbolic_trace(f, concrete_args={'x': {'a': fx.PH, 'b': fx.PH, 'c': fx.PH}})\nassert f({'a': 1, 'b': 2, 'c': 4}) == 7 \n```", "```py\ntorch.fx.wrap(fn_or_name)\u00b6\n```", "```py\n# foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\ntorch.fx.wrap('my_custom_function')\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y) \n```", "```py\n# foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y \n```", "```py\nclass torch.fx.GraphModule(*args, **kwargs)\u00b6\n```", "```py\n__init__(root, graph, class_name='GraphModule')\u00b6\n```", "```py\nadd_submodule(target, m)\u00b6\n```", "```py\nproperty code: str\u00b6\n```", "```py\ndelete_all_unused_submodules()\u00b6\n```", "```py\ndelete_submodule(target)\u00b6\n```", "```py\nproperty graph: Graph\u00b6\n```", "```py\nprint_readable(print_output=True)\u00b6\n```", "```py\nrecompile()\u00b6\n```", "```py\nto_folder(folder, module_name='FxModule')\u00b6\n```", "```py\nclass torch.fx.Graph(owning_module=None, tracer_cls=None, tracer_extras=None)\u00b6\n```", "```py\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m) \n```", "```py\nprint(gm.graph) \n```", "```py\ngraph(x):\n    %linear_weight : [num_users=1] = self.linear.weight\n    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [num_users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [num_users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [num_users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [num_users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1 \n```", "```py\n__init__(owning_module=None, tracer_cls=None, tracer_extras=None)\u00b6\n```", "```py\ncall_function(the_function, args=None, kwargs=None, type_expr=None)\u00b6\n```", "```py\ncall_method(method_name, args=None, kwargs=None, type_expr=None)\u00b6\n```", "```py\ncall_module(module_name, args=None, kwargs=None, type_expr=None)\u00b6\n```", "```py\ncreate_node(op, target, args=None, kwargs=None, name=None, type_expr=None)\u00b6\n```", "```py\neliminate_dead_code()\u00b6\n```", "```py\ndef forward(self, x):\n    a = x + 1\n    return x + self.attr_1 \n```", "```py\ndef forward(self, x):\n    return x + self.attr_1 \n```", "```py\nerase_node(to_erase)\u00b6\n```", "```py\nget_attr(qualified_name, type_expr=None)\u00b6\n```", "```py\ngraph_copy(g, val_map, return_output_node=False)\u00b6\n```", "```py\ninserting_after(n=None)\u00b6\n```", "```py\nwith g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently \n```", "```py\ninserting_before(n=None)\u00b6\n```", "```py\nwith g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently \n```", "```py\nlint()\u00b6\n```", "```py\nnode_copy(node, arg_transform=<function Graph.<lambda>>)\u00b6\n```", "```py\n# Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n]) \n```", "```py\nproperty nodes: _node_list\u00b6\n```", "```py\non_generate_code(make_transformer)\u00b6\n```", "```py\n> gm: fx.GraphModule = ...\n> \n> # This is a code transformer we want to register. This code\n> # transformer prepends a pdb import and trace statement at the very\n> # beginning of the generated torch.fx code to allow for manual\n> # debugging with the PDB library.\n> def insert_pdb(body):\n>     return [\"import pdb; pdb.set_trace()\\n\", *body]\n> \n> # Registers `insert_pdb`, and overwrites the current registered\n> # code transformer (given by `_` to the lambda):\n> gm.graph.on_generate_code(\n>     lambda _: insert_pdb\n> )\n> \n> # Or alternatively, registers a code transformer which first\n> # runs `body` through existing registered transformer, then\n> # through `insert_pdb`:\n> gm.graph.on_generate_code(\n>     lambda current_trans: (\n>         lambda body: insert_pdb(\n>             current_trans(body) if current_trans\n>             else body\n>         )\n>     )\n> )\n> \n> gm.recompile()\n> gm(*inputs)  # drops into pdb \n> ```", "```py\n> # ... continue from previous example\n> \n> with gm.graph.on_generate_code(lambda _: insert_pdb):\n>     # do more stuff with `gm`...\n>     gm.recompile()\n>     gm(*inputs)  # drops into pdb\n> \n> # now previous code transformer is restored (but `gm`'s code with pdb\n> # remains - that means you can run `gm` with pdb here too, until you\n> # run next `recompile()`). \n> ```", "```py\noutput(result, type_expr=None)\u00b6\n```", "```py\nplaceholder(name, type_expr=None, default_value)\u00b6\n```", "```py\nprint_tabular()\u00b6\n```", "```py\nprocess_inputs(*args)\u00b6\n```", "```py\nprocess_outputs(out)\u00b6\n```", "```py\npython_code(root_module, *, verbose=False)\u00b6\n```", "```py\nset_codegen(codegen)\u00b6\n```", "```py\nclass torch.fx.Node(graph, name, op, target, args, kwargs, return_type=None)\u00b6\n```", "```py\nproperty all_input_nodes: List[Node]\u00b6\n```", "```py\nappend(x)\u00b6\n```", "```py\nproperty args: Tuple[Optional[Union[Tuple[Any, ...], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]], ...]\u00b6\n```", "```py\nformat_node(placeholder_names=None, maybe_return_typename=None)\u00b6\n```", "```py\ninsert_arg(idx, arg)\u00b6\n```", "```py\nis_impure()\u00b6\n```", "```py\nproperty kwargs: Dict[str, Optional[Union[Tuple[Any, ...], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]]\u00b6\n```", "```py\nproperty next: Node\u00b6\n```", "```py\nnormalized_arguments(root, arg_types=None, kwarg_types=None, normalize_to_only_use_kwargs=False)\u00b6\n```", "```py\nprepend(x)\u00b6\n```", "```py\nBefore: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax \n```", "```py\nproperty prev: Node\u00b6\n```", "```py\nreplace_all_uses_with(replace_with, delete_user_cb=<function Node.<lambda>>, *, propagate_meta=False)\u00b6\n```", "```py\nreplace_input_with(old_input, new_input)\u00b6\n```", "```py\nproperty stack_trace: Optional[str]\u00b6\n```", "```py\nupdate_arg(idx, arg)\u00b6\n```", "```py\nupdate_kwarg(key, arg)\u00b6\n```", "```py\nclass torch.fx.Tracer(autowrap_modules=(math,), autowrap_functions=())\u00b6\n```", "```py\ncall_module(m, forward, args, kwargs)\u00b6\n```", "```py\ncreate_arg(a)\u00b6\n```", "```py\ncreate_args_for_root(root_fn, is_module, concrete_args=None)\u00b6\n```", "```py\ncreate_node(kind, target, args, kwargs, name=None, type_expr=None)\u00b6\n```", "```py\ncreate_proxy(kind, target, args, kwargs, name=None, type_expr=None, proxy_factory_fn=None)\u00b6\n```", "```py\ngetattr(attr, attr_val, parameter_proxy_cache)\u00b6\n```", "```py\nis_leaf_module(m, module_qualified_name)\u00b6\n```", "```py\niter(obj)\u00b6\n```", "```py\nkeys(obj)\u00b6\n```", "```py\npath_of_module(mod)\u00b6\n```", "```py\nproxy(node)\u00b6\n```", "```py\nto_bool(obj)\u00b6\n```", "```py\ntrace(root, concrete_args=None)\u00b6\n```", "```py\nclass torch.fx.Proxy(node, tracer=None)\u00b6\n```", "```py\nfor i in range(self.some_hyperparameter):\n    indexed_item = proxied_value[i] \n```", "```py\nclass torch.fx.Interpreter(module, garbage_collect_values=True)\u00b6\n```", "```py\nrun()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output() \n```", "```py\nclass NegSigmSwapInterpreter(Interpreter):\n    def call_function(self, target : Target,\n                      args : Tuple, kwargs : Dict) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : Target,\n                    args : Tuple, kwargs : Dict) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_close(result, torch.neg(input).sigmoid()) \n```", "```py\nboxed_run(args_list)\u00b6\n```", "```py\ncall_function(target, args, kwargs)\u00b6\n```", "```py\ncall_method(target, args, kwargs)\u00b6\n```", "```py\ncall_module(target, args, kwargs)\u00b6\n```", "```py\nfetch_args_kwargs_from_env(n)\u00b6\n```", "```py\nfetch_attr(target)\u00b6\n```", "```py\nget_attr(target, args, kwargs)\u00b6\n```", "```py\nmap_nodes_to_values(args, n)\u00b6\n```", "```py\noutput(target, args, kwargs)\u00b6\n```", "```py\nplaceholder(target, args, kwargs)\u00b6\n```", "```py\nrun(*args, initial_env=None, enable_io_processing=True)\u00b6\n```", "```py\nrun_node(n)\u00b6\n```", "```py\nclass torch.fx.Transformer(module)\u00b6\n```", "```py\nclass NegSigmSwapXformer(Transformer):\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_close(transformed(input), torch.neg(input).sigmoid()) \n```", "```py\ncall_function(target, args, kwargs)\u00b6\n```", "```py\ncall_module(target, args, kwargs)\u00b6\n```", "```py\nget_attr(target, args, kwargs)\u00b6\n```", "```py\nplaceholder(target, args, kwargs)\u00b6\n```", "```py\ntransform()\u00b6\n```", "```py\ntorch.fx.replace_pattern(gm, pattern, replacement)\u00b6\n```", "```py\nclass Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node] \n```", "```py\nimport torch\nfrom torch.fx import symbolic_trace, subgraph_rewriter\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, w1, w2):\n        m1 = torch.cat([w1, w2]).sum()\n        m2 = torch.cat([w1, w2]).sum()\n        return x + torch.max(m1) + torch.max(m2)\n\ndef pattern(w1, w2):\n    return torch.cat([w1, w2]).sum()\n\ndef replacement(w1, w2):\n    return torch.stack([w1, w2])\n\ntraced_module = symbolic_trace(M())\n\nsubgraph_rewriter.replace_pattern(traced_module, pattern, replacement) \n```", "```py\ndef pattern(x, y):\n    return torch.neg(x) + torch.relu(y) \n```", "```py\ndef replacement(x, y):\n    return torch.relu(x) \n```", "```py\ndef forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2 \n```"]