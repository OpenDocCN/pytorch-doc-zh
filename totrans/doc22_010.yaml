- en: Broadcasting semantics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播语义
- en: 原文：[https://pytorch.org/docs/stable/notes/broadcasting.html](https://pytorch.org/docs/stable/notes/broadcasting.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/notes/broadcasting.html](https://pytorch.org/docs/stable/notes/broadcasting.html)
- en: Many PyTorch operations support NumPy’s broadcasting semantics. See [https://numpy.org/doc/stable/user/basics.broadcasting.html](https://numpy.org/doc/stable/user/basics.broadcasting.html)
    for details.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 许多PyTorch操作支持NumPy的广播语义。有关详细信息，请参阅[https://numpy.org/doc/stable/user/basics.broadcasting.html](https://numpy.org/doc/stable/user/basics.broadcasting.html)。
- en: In short, if a PyTorch operation supports broadcast, then its Tensor arguments
    can be automatically expanded to be of equal sizes (without making copies of the
    data).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，如果PyTorch操作支持广播，则其张量参数可以自动扩展为相等大小（而不会复制数据）。
- en: General semantics
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般语义
- en: 'Two tensors are “broadcastable” if the following rules hold:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下规则，则两个张量是“可广播的”：
- en: Each tensor has at least one dimension.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个张量至少有一个维度。
- en: When iterating over the dimension sizes, starting at the trailing dimension,
    the dimension sizes must either be equal, one of them is 1, or one of them does
    not exist.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在迭代维度大小时，从尾部维度开始，维度大小必须要么相等，要么其中一个为1，要么其中一个不存在。
- en: 'For Example:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If two tensors `x`, `y` are “broadcastable”, the resulting tensor size is calculated
    as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个张量`x`，`y`是“可广播的”，则结果张量大小计算如下：
- en: If the number of dimensions of `x` and `y` are not equal, prepend 1 to the dimensions
    of the tensor with fewer dimensions to make them equal length.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`x`和`y`的维度数不相等，则在较少维度的张量的维度前添加1，使它们长度相等。
- en: Then, for each dimension size, the resulting dimension size is the max of the
    sizes of `x` and `y` along that dimension.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，对于每个维度大小，结果维度大小是沿该维度的`x`和`y`的大小的最大值。
- en: 'For Example:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In-place semantics
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 就地语义
- en: One complication is that in-place operations do not allow the in-place tensor
    to change shape as a result of the broadcast.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个复杂之处在于就地操作不允许就地张量由于广播而改变形状。
- en: 'For Example:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Backwards compatibility
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向后兼容性
- en: Prior versions of PyTorch allowed certain pointwise functions to execute on
    tensors with different shapes, as long as the number of elements in each tensor
    was equal. The pointwise operation would then be carried out by viewing each tensor
    as 1-dimensional. PyTorch now supports broadcasting and the “1-dimensional” pointwise
    behavior is considered deprecated and will generate a Python warning in cases
    where tensors are not broadcastable, but have the same number of elements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的先前版本允许某些逐点函数在具有不同形状的张量上执行，只要每个张量中的元素数量相等即可。然后，逐点操作将通过将每个张量视为1维来执行。PyTorch现在支持广播，而“1维”逐点行为被视为已弃用，并且在张量不可广播但具有相同数量的元素的情况下会生成Python警告。
- en: 'Note that the introduction of broadcasting can cause backwards incompatible
    changes in the case where two tensors do not have the same shape, but are broadcastable
    and have the same number of elements. For Example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，广播的引入可能会导致向后不兼容的更改，即两个张量的形状不相同，但可以广播并且具有相同数量的元素的情况。例如：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'would previously produce a Tensor with size: torch.Size([4,1]), but now produces
    a Tensor with size: torch.Size([4,4]). In order to help identify cases in your
    code where backwards incompatibilities introduced by broadcasting may exist, you
    may set torch.utils.backcompat.broadcast_warning.enabled to True, which will generate
    a python warning in such cases.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以前会产生一个大小为torch.Size([4,1])的张量，但现在会产生一个大小为torch.Size([4,4])的张量。为了帮助识别代码中可能存在的由广播引入的向后不兼容性，您可以将torch.utils.backcompat.broadcast_warning.enabled设置为True，在这种情况下会生成一个Python警告。
- en: 'For Example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
