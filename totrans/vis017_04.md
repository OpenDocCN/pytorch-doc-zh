# 模型和预训练权重

> 原文：[https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)

`torchvision.models`子包含有用于解决不同任务的模型的定义，包括：图像分类、像素级语义分割、目标检测、实例分割、人体关键点检测、视频分类和光流。

## 有关预训练权重的一般信息[](#general-information-on-pre-trained-weights "此标题的永久链接")

TorchVision为每个提供的架构提供了预训练权重，使用PyTorch [`torch.hub`](https://pytorch.org/docs/stable/hub.html#module-torch.hub "(在PyTorch v2.2中)")。实例化预训练模型将下载其权重到缓存目录。可以使用TORCH_HOME环境变量设置此目录。有关详细信息，请参阅[`torch.hub.load_state_dict_from_url()`](https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url "(在PyTorch v2.2中)")。

注意

此库中提供的预训练模型可能具有根据用于训练的数据集派生的自己的许可证或条款。您有责任确定是否有权限将这些模型用于您的用例。

注意

对于将序列化的`state_dict`加载到使用旧版本PyTorch创建的模型，向后兼容性是有保证的。相反，加载整个保存的模型或序列化的`ScriptModules`（使用旧版本PyTorch序列化）可能不会保留历史行为。请参考以下[文档](https://pytorch.org/docs/stable/notes/serialization.html#id6)

### 初始化预训练模型[](#initializing-pre-trained-models "此标题的永久链接")

从v0.13开始，TorchVision提供了一个新的[多权重支持API](https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/)，用于将不同权重加载到现有的模型构建器方法中：

```py
from torchvision.models import resnet50, ResNet50_Weights

# Old weights with accuracy 76.130%
resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)

# New weights with accuracy 80.858%
resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)

# Best available weights (currently alias for IMAGENET1K_V2)
# Note that these weights may change across versions
resnet50(weights=ResNet50_Weights.DEFAULT)

# Strings are also supported
resnet50(weights="IMAGENET1K_V2")

# No weights - random initialization
resnet50(weights=None) 
```

迁移到新API非常简单。在这两个API之间的以下方法调用是等效的：

```py
from torchvision.models import resnet50, ResNet50_Weights

# Using pretrained weights:
resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
resnet50(weights="IMAGENET1K_V1")
resnet50(pretrained=True)  # deprecated
resnet50(True)  # deprecated

# Using no weights:
resnet50(weights=None)
resnet50()
resnet50(pretrained=False)  # deprecated
resnet50(False)  # deprecated 
```

请注意，`pretrained`参数现在已弃用，使用它将发出警告，并将在v0.15中删除。

### 使用预训练模型[](#using-the-pre-trained-models "此标题的永久链接")

在使用预训练模型之前，必须对图像进行预处理（调整大小以获得正确的分辨率/插值，应用推理变换，重新缩放值等）。没有标准的方法可以做到这一点，因为它取决于给定模型的训练方式。它可能会因模型系列、变体甚至权重版本而有所不同。使用正确的预处理方法至关重要，否则可能导致准确性降低或输出不正确。

每个预训练模型的推理变换的所有必要信息都在其权重文档中提供。为了简化推理，TorchVision将必要的预处理变换捆绑到每个模型权重中。这些可以通过`weight.transforms`属性访问：

```py
# Initialize the Weight Transforms
weights = ResNet50_Weights.DEFAULT
preprocess = weights.transforms()

# Apply it to the input image
img_transformed = preprocess(img) 
```

有些模型使用具有不同训练和评估行为的模块，例如批量归一化。要在这些模式之间切换，请适当使用`model.train()`或`model.eval()`。有关详细信息，请参阅[`train()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train "(在PyTorch v2.2中)")或[`eval()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval "(在PyTorch v2.2中)")。

```py
# Initialize model
weights = ResNet50_Weights.DEFAULT
model = resnet50(weights=weights)

# Set model to eval mode
model.eval() 
```

### 列出和检索可用模型[](#listing-and-retrieving-available-models "此标题的永久链接")

从v0.14开始，TorchVision提供了一种新机制，允许按名称列出和检索模型和权重。以下是如何使用它们的几个示例：

```py
# List available models
all_models = list_models()
classification_models = list_models(module=torchvision.models)

# Initialize models
m1 = get_model("mobilenet_v3_large", weights=None)
m2 = get_model("quantized_mobilenet_v3_large", weights="DEFAULT")

# Fetch weights
weights = get_weight("MobileNet_V3_Large_QuantizedWeights.DEFAULT")
assert weights == MobileNet_V3_Large_QuantizedWeights.DEFAULT

weights_enum = get_model_weights("quantized_mobilenet_v3_large")
assert weights_enum == MobileNet_V3_Large_QuantizedWeights

weights_enum2 = get_model_weights(torchvision.models.quantization.mobilenet_v3_large)
assert weights_enum == weights_enum2 
```

以下是可用的公共函数，用于检索模型及其对应的权重：

| [`get_model`](generated/torchvision.models.get_model.html#torchvision.models.get_model "torchvision.models.get_model")(name, **config) | 获取模型名称和配置，并返回一个实例化的模型。 |
| --- | --- |
| [`get_model_weights`](generated/torchvision.models.get_model_weights.html#torchvision.models.get_model_weights "torchvision.models.get_model_weights")(name) | 返回与给定模型关联的权重枚举类。 |
| [`get_weight`](generated/torchvision.models.get_weight.html#torchvision.models.get_weight "torchvision.models.get_weight")(name) | 通过完整名称获取权重枚举值。 |
| [`list_models`](generated/torchvision.models.list_models.html#torchvision.models.list_models "torchvision.models.list_models")([module, include, exclude]) | 返回已注册模型名称的列表。 |

### 使用 Hub 中的模型[](#using-models-from-hub "跳转到此标题")

大多数预训练模型可以直接通过 PyTorch Hub 访问，无需安装 TorchVision：

```py
import torch

# Option 1: passing weights param as string
model = torch.hub.load("pytorch/vision", "resnet50", weights="IMAGENET1K_V2")

# Option 2: passing weights param as enum
weights = torch.hub.load("pytorch/vision", "get_weight", weights="ResNet50_Weights.IMAGENET1K_V2")
model = torch.hub.load("pytorch/vision", "resnet50", weights=weights) 
```

您还可以通过 PyTorch Hub 检索特定模型的所有可用权重：

```py
import torch

weight_enum = torch.hub.load("pytorch/vision", "get_model_weights", name="resnet50")
print([weight for weight in weight_enum]) 
```

上述唯一的例外是包含在 `torchvision.models.detection` 中的检测模型。这些模型需要安装 TorchVision，因为它们依赖于自定义的 C++ 运算符。

## 分类[](#classification "跳转到此标题")

以下分类模型可用，带有或不带有预训练权重：

+   [AlexNet](models/alexnet.html)

+   [ConvNeXt](models/convnext.html)

+   [DenseNet](models/densenet.html)

+   [EfficientNet](models/efficientnet.html)

+   [EfficientNetV2](models/efficientnetv2.html)

+   [GoogLeNet](models/googlenet.html)

+   [Inception V3](models/inception.html)

+   [MaxVit](models/maxvit.html)

+   [MNASNet](models/mnasnet.html)

+   [MobileNet V2](models/mobilenetv2.html)

+   [MobileNet V3](models/mobilenetv3.html)

+   [RegNet](models/regnet.html)

+   [ResNet](models/resnet.html)

+   [ResNeXt](models/resnext.html)

+   [ShuffleNet V2](models/shufflenetv2.html)

+   [SqueezeNet](models/squeezenet.html)

+   [SwinTransformer](models/swin_transformer.html)

+   [VGG](models/vgg.html)

+   [VisionTransformer](models/vision_transformer.html)

+   [Wide ResNet](models/wide_resnet.html)

以下是如何使用预训练图像分类模型的示例：

```py
from torchvision.io import read_image
from torchvision.models import resnet50, ResNet50_Weights

img = read_image("test/assets/encode_jpeg/grace_hopper_517x606.jpg")

# Step 1: Initialize model with the best available weights
weights = ResNet50_Weights.DEFAULT
model = resnet50(weights=weights)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = preprocess(img).unsqueeze(0)

# Step 4: Use the model and print the predicted category
prediction = model(batch).squeeze(0).softmax(0)
class_id = prediction.argmax().item()
score = prediction[class_id].item()
category_name = weights.meta["categories"][class_id]
print(f"{category_name}: {100  *  score:.1f}%") 
```

预训练模型输出的类别可以在 `weights.meta["categories"]` 中找到。

### 所有可用分类权重的表格[](#table-of-all-available-classification-weights "跳转到此标题")

在 ImageNet-1K 上使用单个裁剪报告准确性：

| **权重** | **Acc@1** | **Acc@5** | **参数** | **GFLOPS** | **Recipe** |
| --- | --- | --- | --- | --- | --- |
| [`AlexNet_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.alexnet.html#torchvision.models.AlexNet_Weights "torchvision.models.AlexNet_Weights") | 56.522 | 79.066 | 61.1M | 0.71 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| [`ConvNeXt_Base_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.convnext_base.html#torchvision.models.ConvNeXt_Base_Weights "torchvision.models.ConvNeXt_Base_Weights") | 84.062 | 96.87 | 88.6M | 15.36 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#convnext) |
| [`ConvNeXt_Large_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.convnext_large.html#torchvision.models.ConvNeXt_Large_Weights "torchvision.models.ConvNeXt_Large_Weights") | 84.414 | 96.976 | 197.8M | 34.36 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#convnext) |
| [`ConvNeXt_Small_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.convnext_small.html#torchvision.models.ConvNeXt_Small_Weights "torchvision.models.ConvNeXt_Small_Weights") | 83.616 | 96.65 | 50.2M | 8.68 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#convnext) |
| [`ConvNeXt_Tiny_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.convnext_tiny.html#torchvision.models.ConvNeXt_Tiny_Weights "torchvision.models.ConvNeXt_Tiny_Weights") | 82.52 | 96.146 | 28.6M | 4.46 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#convnext) |
| [`DenseNet121_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.densenet121.html#torchvision.models.DenseNet121_Weights "torchvision.models.DenseNet121_Weights") | 74.434 | 91.972 | 8.0M | 2.83 | [链接](https://github.com/pytorch/vision/pull/116) |
| [`DenseNet161_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.densenet161.html#torchvision.models.DenseNet161_Weights "torchvision.models.DenseNet161_Weights") | 77.138 | 93.56 | 28.7M | 7.73 | [链接](https://github.com/pytorch/vision/pull/116) |
| [`DenseNet169_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.densenet169.html#torchvision.models.DenseNet169_Weights "torchvision.models.DenseNet169_Weights") | 75.6 | 92.806 | 14.1M | 3.36 | [链接](https://github.com/pytorch/vision/pull/116) |
| [`DenseNet201_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.densenet201.html#torchvision.models.DenseNet201_Weights "torchvision.models.DenseNet201_Weights") | 76.896 | 93.37 | 20.0M | 4.29 | [链接](https://github.com/pytorch/vision/pull/116) |
| [`EfficientNet_B0_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.EfficientNet_B0_Weights "torchvision.models.EfficientNet_B0_Weights") | 77.692 | 93.532 | 5.3M | 0.39 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| [`EfficientNet_B1_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b1.html#torchvision.models.EfficientNet_B1_Weights "torchvision.models.EfficientNet_B1_Weights") | 78.642 | 94.186 | 7.8M | 0.69 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| [`EfficientNet_B1_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.efficientnet_b1.html#torchvision.models.EfficientNet_B1_Weights "torchvision.models.EfficientNet_B1_Weights") | 79.838 | 94.934 | 7.8M | 0.69 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe-with-lr-wd-crop-tuning) |
| [`EfficientNet_B2_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights "torchvision.models.EfficientNet_B2_Weights") | 80.608 | 95.31 | 9.1M | 1.09 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| [`EfficientNet_B3_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b3.html#torchvision.models.EfficientNet_B3_Weights "torchvision.models.EfficientNet_B3_Weights") | 82.008 | 96.054 | 12.2M | 1.83 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| [`EfficientNet_B4_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b4.html#torchvision.models.EfficientNet_B4_Weights "torchvision.models.EfficientNet_B4_Weights") | 83.384 | 96.594 | 19.3M | 4.39 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| [`EfficientNet_B5_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b5.html#torchvision.models.EfficientNet_B5_Weights "torchvision.models.EfficientNet_B5_Weights") | 83.444 | 96.628 | 30.4M | 10.27 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| [`EfficientNet_B6_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b6.html#torchvision.models.EfficientNet_B6_Weights "torchvision.models.EfficientNet_B6_Weights") | 84.008 | 96.916 | 43.0M | 19.07 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| [`EfficientNet_B7_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b7.html#torchvision.models.EfficientNet_B7_Weights "torchvision.models.EfficientNet_B7_Weights") | 84.122 | 96.908 | 66.3M | 37.75 | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| [`EfficientNet_V2_L_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_v2_l.html#torchvision.models.EfficientNet_V2_L_Weights "torchvision.models.EfficientNet_V2_L_Weights") | 85.808 | 97.788 | 118.5M | 56.08 | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2) |
| [`EfficientNet_V2_M_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_v2_m.html#torchvision.models.EfficientNet_V2_M_Weights "torchvision.models.EfficientNet_V2_M_Weights") | 85.112 | 97.156 | 54.1M | 24.58 | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2) |
| [`EfficientNet_V2_S_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_v2_s.html#torchvision.models.EfficientNet_V2_S_Weights "torchvision.models.EfficientNet_V2_S_Weights") | 84.228 | 96.878 | 21.5M | 8.37 | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2) |
| [`GoogLeNet_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.googlenet.html#torchvision.models.GoogLeNet_Weights "torchvision.models.GoogLeNet_Weights") | 69.778 | 89.53 | 6.6M | 1.5 | [link](https://github.com/pytorch/vision/tree/main/references/classification#googlenet) |
| [`Inception_V3_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.inception_v3.html#torchvision.models.Inception_V3_Weights "torchvision.models.Inception_V3_Weights") | 77.294 | 93.45 | 27.2M | 5.71 | [link](https://github.com/pytorch/vision/tree/main/references/classification#inception-v3) |
| [`MNASNet0_5_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mnasnet0_5.html#torchvision.models.MNASNet0_5_Weights "torchvision.models.MNASNet0_5_Weights") | 67.734 | 87.49 | 2.2M | 0.1 | [link](https://github.com/1e100/mnasnet_trainer) |
| [`MNASNet0_75_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mnasnet0_75.html#torchvision.models.MNASNet0_75_Weights "torchvision.models.MNASNet0_75_Weights") | 71.18 | 90.496 | 3.2M | 0.21 | [link](https://github.com/pytorch/vision/pull/6019) |
| [`MNASNet1_0_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mnasnet1_0.html#torchvision.models.MNASNet1_0_Weights "torchvision.models.MNASNet1_0_Weights") | 73.456 | 91.51 | 4.4M | 0.31 | [link](https://github.com/1e100/mnasnet_trainer) |
| [`MNASNet1_3_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mnasnet1_3.html#torchvision.models.MNASNet1_3_Weights "torchvision.models.MNASNet1_3_Weights") | 76.506 | 93.522 | 6.3M | 0.53 | [link](https://github.com/pytorch/vision/pull/6019) |
| [`MaxVit_T_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.maxvit_t.html#torchvision.models.MaxVit_T_Weights "torchvision.models.MaxVit_T_Weights") | 83.7 | 96.722 | 30.9M | 5.56 | [link](https://github.com/pytorch/vision/tree/main/references/classification#maxvit) |
| [`MobileNet_V2_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.MobileNet_V2_Weights "torchvision.models.MobileNet_V2_Weights") | 71.878 | 90.286 | 3.5M | 0.3 | [link](https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv2) |
| [`MobileNet_V2_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.MobileNet_V2_Weights "torchvision.models.MobileNet_V2_Weights") | 72.154 | 90.822 | 3.5M | 0.3 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning) |
| [`MobileNet_V3_Large_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.MobileNet_V3_Large_Weights "torchvision.models.MobileNet_V3_Large_Weights") | 74.042 | 91.34 | 5.5M | 0.22 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv3-large--small) |
| [`MobileNet_V3_Large_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.MobileNet_V3_Large_Weights "torchvision.models.MobileNet_V3_Large_Weights") | 75.274 | 92.566 | 5.5M | 0.22 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning) |
| [`MobileNet_V3_Small_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mobilenet_v3_small.html#torchvision.models.MobileNet_V3_Small_Weights "torchvision.models.MobileNet_V3_Small_Weights") | 67.668 | 87.402 | 2.5M | 0.06 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv3-large--small) |
| [`RegNet_X_16GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_16gf.html#torchvision.models.RegNet_X_16GF_Weights "torchvision.models.RegNet_X_16GF_Weights") | 80.058 | 94.944 | 54.3M | 15.94 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#medium-models) |
| [`RegNet_X_16GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_16gf.html#torchvision.models.RegNet_X_16GF_Weights "torchvision.models.RegNet_X_16GF_Weights") | 82.716 | 96.196 | 54.3M | 15.94 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`RegNet_X_1_6GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_1_6gf.html#torchvision.models.RegNet_X_1_6GF_Weights "torchvision.models.RegNet_X_1_6GF_Weights") | 77.04 | 93.44 | 9.2M | 1.6 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| [`RegNet_X_1_6GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_1_6gf.html#torchvision.models.RegNet_X_1_6GF_Weights "torchvision.models.RegNet_X_1_6GF_Weights") | 79.668 | 94.922 | 9.2M | 1.6 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |
| [`RegNet_X_32GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_32gf.html#torchvision.models.RegNet_X_32GF_Weights "torchvision.models.RegNet_X_32GF_Weights") | 80.622 | 95.248 | 107.8M | 31.74 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#large-models) |
| [`RegNet_X_32GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_32gf.html#torchvision.models.RegNet_X_32GF_Weights "torchvision.models.RegNet_X_32GF_Weights") | 83.014 | 96.288 | 107.8M | 31.74 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`RegNet_X_3_2GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_3_2gf.html#torchvision.models.RegNet_X_3_2GF_Weights "torchvision.models.RegNet_X_3_2GF_Weights") | 78.364 | 93.992 | 15.3M | 3.18 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#medium-models) |
| [`RegNet_X_3_2GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_3_2gf.html#torchvision.models.RegNet_X_3_2GF_Weights "torchvision.models.RegNet_X_3_2GF_Weights") | 81.196 | 95.43 | 15.3M | 3.18 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`RegNet_X_400MF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_400mf.html#torchvision.models.RegNet_X_400MF_Weights "torchvision.models.RegNet_X_400MF_Weights") | 72.834 | 90.95 | 5.5M | 0.41 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| [`RegNet_X_400MF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_400mf.html#torchvision.models.RegNet_X_400MF_Weights "torchvision.models.RegNet_X_400MF_Weights") | 74.864 | 92.322 | 5.5M | 0.41 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |
| [`RegNet_X_800MF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_800mf.html#torchvision.models.RegNet_X_800MF_Weights "torchvision.models.RegNet_X_800MF_Weights") | 75.212 | 92.348 | 7.3M | 0.8 | [link](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| [`RegNet_X_800MF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_800mf.html#torchvision.models.RegNet_X_800MF_Weights "torchvision.models.RegNet_X_800MF_Weights") | 77.522 | 93.826 | 7.3M | 0.8 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |
| [`RegNet_X_8GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_8gf.html#torchvision.models.RegNet_X_8GF_Weights "torchvision.models.RegNet_X_8GF_Weights") | 79.344 | 94.686 | 39.6M | 8 | [link](https://github.com/pytorch/vision/tree/main/references/classification#medium-models) |
| [`RegNet_X_8GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_8gf.html#torchvision.models.RegNet_X_8GF_Weights "torchvision.models.RegNet_X_8GF_Weights") | 81.682 | 95.678 | 39.6M | 8 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.regnet_y_128gf.html#torchvision.models.RegNet_Y_128GF_Weights "torchvision.models.RegNet_Y_128GF_Weights") | 88.228 | 98.682 | 644.8M | 374.57 | [link](https://github.com/facebookresearch/SWAG) |
| [`RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.regnet_y_128gf.html#torchvision.models.RegNet_Y_128GF_Weights "torchvision.models.RegNet_Y_128GF_Weights") | 86.068 | 97.844 | 644.8M | 127.52 | [link](https://github.com/pytorch/vision/pull/5793) |
| [`RegNet_Y_16GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights "torchvision.models.RegNet_Y_16GF_Weights") | 80.424 | 95.24 | 83.6M | 15.91 | [link](https://github.com/pytorch/vision/tree/main/references/classification#large-models) |
| [`RegNet_Y_16GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights "torchvision.models.RegNet_Y_16GF_Weights") | 82.886 | 96.328 | 83.6M | 15.91 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights "torchvision.models.RegNet_Y_16GF_Weights") | 86.012 | 98.054 | 83.6M | 46.73 | [link](https://github.com/facebookresearch/SWAG) |
| [`RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights "torchvision.models.RegNet_Y_16GF_Weights") | 83.976 | 97.244 | 83.6M | 15.91 | [link](https://github.com/pytorch/vision/pull/5793) |
| [`RegNet_Y_1_6GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_1_6gf.html#torchvision.models.RegNet_Y_1_6GF_Weights "torchvision.models.RegNet_Y_1_6GF_Weights") | 77.95 | 93.966 | 11.2M | 1.61 | [link](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| [`RegNet_Y_1_6GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_1_6gf.html#torchvision.models.RegNet_Y_1_6GF_Weights "torchvision.models.RegNet_Y_1_6GF_Weights") | 80.876 | 95.444 | 11.2M | 1.61 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`RegNet_Y_32GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights "torchvision.models.RegNet_Y_32GF_Weights") | 80.878 | 95.34 | 145.0M | 32.28 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#large-models) |
| [`RegNet_Y_32GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights "torchvision.models.RegNet_Y_32GF_Weights") | 83.368 | 96.498 | 145.0M | 32.28 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights "torchvision.models.RegNet_Y_32GF_Weights") | 86.838 | 98.362 | 145.0M | 94.83 | [链接](https://github.com/facebookresearch/SWAG) |
| [`RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights "torchvision.models.RegNet_Y_32GF_Weights") | 84.622 | 97.48 | 145.0M | 32.28 | [链接](https://github.com/pytorch/vision/pull/5793) |
| [`RegNet_Y_3_2GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_3_2gf.html#torchvision.models.RegNet_Y_3_2GF_Weights "torchvision.models.RegNet_Y_3_2GF_Weights") | 78.948 | 94.576 | 19.4M | 3.18 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#medium-models) |
| [`RegNet_Y_3_2GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_3_2gf.html#torchvision.models.RegNet_Y_3_2GF_Weights "torchvision.models.RegNet_Y_3_2GF_Weights") | 81.982 | 95.972 | 19.4M | 3.18 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`RegNet_Y_400MF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_400mf.html#torchvision.models.RegNet_Y_400MF_Weights "torchvision.models.RegNet_Y_400MF_Weights") | 74.046 | 91.716 | 4.3M | 0.4 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| [`RegNet_Y_400MF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_400mf.html#torchvision.models.RegNet_Y_400MF_Weights "torchvision.models.RegNet_Y_400MF_Weights") | 75.804 | 92.742 | 4.3M | 0.4 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`RegNet_Y_800MF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_800mf.html#torchvision.models.RegNet_Y_800MF_Weights "torchvision.models.RegNet_Y_800MF_Weights") | 76.42 | 93.136 | 6.4M | 0.83 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| [`RegNet_Y_800MF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_800mf.html#torchvision.models.RegNet_Y_800MF_Weights "torchvision.models.RegNet_Y_800MF_Weights") | 78.828 | 94.502 | 6.4M | 0.83 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`RegNet_Y_8GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_8gf.html#torchvision.models.RegNet_Y_8GF_Weights "torchvision.models.RegNet_Y_8GF_Weights") | 80.032 | 95.048 | 39.4M | 8.47 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#medium-models) |
| [`RegNet_Y_8GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_8gf.html#torchvision.models.RegNet_Y_8GF_Weights "torchvision.models.RegNet_Y_8GF_Weights") | 82.828 | 96.33 | 39.4M | 8.47 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`ResNeXt101_32X8D_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnext101_32x8d.html#torchvision.models.ResNeXt101_32X8D_Weights "torchvision.models.ResNeXt101_32X8D_Weights") | 79.312 | 94.526 | 88.8M | 16.41 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnext) |
| [`ResNeXt101_32X8D_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.resnext101_32x8d.html#torchvision.models.ResNeXt101_32X8D_Weights "torchvision.models.ResNeXt101_32X8D_Weights") | 82.834 | 96.228 | 88.8M | 16.41 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |
| [`ResNeXt101_64X4D_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnext101_64x4d.html#torchvision.models.ResNeXt101_64X4D_Weights "torchvision.models.ResNeXt101_64X4D_Weights") | 83.246 | 96.454 | 83.5M | 15.46 | [链接](https://github.com/pytorch/vision/pull/5935) |
| [`ResNeXt50_32X4D_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnext50_32x4d.html#torchvision.models.ResNeXt50_32X4D_Weights "torchvision.models.ResNeXt50_32X4D_Weights") | 77.618 | 93.698 | 25.0M | 4.23 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnext) |
| [`ResNeXt50_32X4D_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.resnext50_32x4d.html#torchvision.models.ResNeXt50_32X4D_Weights "torchvision.models.ResNeXt50_32X4D_Weights") | 81.198 | 95.34 | 25.0M | 4.23 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`ResNet101_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnet101.html#torchvision.models.ResNet101_Weights "torchvision.models.ResNet101_Weights") | 77.374 | 93.546 | 44.5M | 7.8 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnet) |
| [`ResNet101_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.resnet101.html#torchvision.models.ResNet101_Weights "torchvision.models.ResNet101_Weights") | 81.886 | 95.78 | 44.5M | 7.8 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`ResNet152_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnet152.html#torchvision.models.ResNet152_Weights "torchvision.models.ResNet152_Weights") | 78.312 | 94.046 | 60.2M | 11.51 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnet) |
| [`ResNet152_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.resnet152.html#torchvision.models.ResNet152_Weights "torchvision.models.ResNet152_Weights") | 82.284 | 96.002 | 60.2M | 11.51 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`ResNet18_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnet18.html#torchvision.models.ResNet18_Weights "torchvision.models.ResNet18_Weights") | 69.758 | 89.078 | 11.7M | 1.81 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnet) |
| [`ResNet34_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnet34.html#torchvision.models.ResNet34_Weights "torchvision.models.ResNet34_Weights") | 73.314 | 91.42 | 21.8M | 3.66 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnet) |
| [`ResNet50_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights "torchvision.models.ResNet50_Weights") | 76.13 | 92.862 | 25.6M | 4.09 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnet) |
| [`ResNet50_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights "torchvision.models.ResNet50_Weights") | 80.858 | 95.434 | 25.6M | 4.09 | [链接](https://github.com/pytorch/vision/issues/3995#issuecomment-1013906621) |
| [`ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.shufflenet_v2_x0_5.html#torchvision.models.ShuffleNet_V2_X0_5_Weights "torchvision.models.ShuffleNet_V2_X0_5_Weights") | 60.552 | 81.746 | 1.4M | 0.04 | [链接](https://github.com/ericsun99/Shufflenet-v2-Pytorch) |
| [`ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.shufflenet_v2_x1_0.html#torchvision.models.ShuffleNet_V2_X1_0_Weights "torchvision.models.ShuffleNet_V2_X1_0_Weights") | 69.362 | 88.316 | 2.3M | 0.14 | [链接](https://github.com/ericsun99/Shufflenet-v2-Pytorch) |
| [`ShuffleNet_V2_X1_5_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.shufflenet_v2_x1_5.html#torchvision.models.ShuffleNet_V2_X1_5_Weights "torchvision.models.ShuffleNet_V2_X1_5_Weights") | 72.996 | 91.086 | 3.5M | 0.3 | [链接](https://github.com/pytorch/vision/pull/5906) |
| [`ShuffleNet_V2_X2_0_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.shufflenet_v2_x2_0.html#torchvision.models.ShuffleNet_V2_X2_0_Weights "torchvision.models.ShuffleNet_V2_X2_0_Weights") | 76.23 | 93.006 | 7.4M | 0.58 | [链接](https://github.com/pytorch/vision/pull/5906) |
| [`SqueezeNet1_0_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.squeezenet1_0.html#torchvision.models.SqueezeNet1_0_Weights "torchvision.models.SqueezeNet1_0_Weights") | 58.092 | 80.42 | 1.2M | 0.82 | [链接](https://github.com/pytorch/vision/pull/49#issuecomment-277560717) |
| [`SqueezeNet1_1_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.squeezenet1_1.html#torchvision.models.SqueezeNet1_1_Weights "torchvision.models.SqueezeNet1_1_Weights") | 58.178 | 80.624 | 1.2M | 0.35 | [链接](https://github.com/pytorch/vision/pull/49#issuecomment-277560717) |
| [`Swin_B_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_b.html#torchvision.models.Swin_B_Weights "torchvision.models.Swin_B_Weights") | 83.582 | 96.64 | 87.8M | 15.43 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer) |
| [`Swin_S_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_s.html#torchvision.models.Swin_S_Weights "torchvision.models.Swin_S_Weights") | 83.196 | 96.36 | 49.6M | 8.74 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer) |
| [`Swin_T_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_t.html#torchvision.models.Swin_T_Weights "torchvision.models.Swin_T_Weights") | 81.474 | 95.776 | 28.3M | 4.49 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer) |
| [`Swin_V2_B_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_v2_b.html#torchvision.models.Swin_V2_B_Weights "torchvision.models.Swin_V2_B_Weights") | 84.112 | 96.864 | 87.9M | 20.32 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2) |
| [`Swin_V2_S_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_v2_s.html#torchvision.models.Swin_V2_S_Weights "torchvision.models.Swin_V2_S_Weights") | 83.712 | 96.816 | 49.7M | 11.55 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2) |
| [`Swin_V2_T_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_v2_t.html#torchvision.models.Swin_V2_T_Weights "torchvision.models.Swin_V2_T_Weights") | 82.072 | 96.132 | 28.4M | 5.94 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2) |
| [`VGG11_BN_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg11_bn.html#torchvision.models.VGG11_BN_Weights "torchvision.models.VGG11_BN_Weights") | 70.37 | 89.81 | 132.9M | 7.61 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| [`VGG11_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg11.html#torchvision.models.VGG11_Weights "torchvision.models.VGG11_Weights") | 69.02 | 88.628 | 132.9M | 7.61 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| [`VGG13_BN_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg13_bn.html#torchvision.models.VGG13_BN_Weights "torchvision.models.VGG13_BN_Weights") | 71.586 | 90.374 | 133.1M | 11.31 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| [`VGG13_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg13.html#torchvision.models.VGG13_Weights "torchvision.models.VGG13_Weights") | 69.928 | 89.246 | 133.0M | 11.31 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| [`VGG16_BN_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg16_bn.html#torchvision.models.VGG16_BN_Weights "torchvision.models.VGG16_BN_Weights") | 73.36 | 91.516 | 138.4M | 15.47 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| [`VGG16_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights "torchvision.models.VGG16_Weights") | 71.592 | 90.382 | 138.4M | 15.47 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| [`VGG16_Weights.IMAGENET1K_FEATURES`](models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights "torchvision.models.VGG16_Weights") | nan | nan | 138.4M | 15.47 | [链接](https://github.com/amdegroot/ssd.pytorch#training-ssd) |
| [`VGG19_BN_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg19_bn.html#torchvision.models.VGG19_BN_Weights "torchvision.models.VGG19_BN_Weights") | 74.218 | 91.842 | 143.7M | 19.63 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| [`VGG19_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg19.html#torchvision.models.VGG19_Weights "torchvision.models.VGG19_Weights") | 72.376 | 90.876 | 143.7M | 19.63 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| [`ViT_B_16_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights "torchvision.models.ViT_B_16_Weights") | 81.072 | 95.318 | 86.6M | 17.56 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#vit_b_16) |
| [`ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights "torchvision.models.ViT_B_16_Weights") | 85.304 | 97.65 | 86.9M | 55.48 | [链接](https://github.com/facebookresearch/SWAG) |
| [`ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights "torchvision.models.ViT_B_16_Weights") | 81.886 | 96.18 | 86.6M | 17.56 | [链接](https://github.com/pytorch/vision/pull/5793) |
| [`ViT_B_32_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vit_b_32.html#torchvision.models.ViT_B_32_Weights "torchvision.models.ViT_B_32_Weights") | 75.912 | 92.466 | 88.2M | 4.41 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#vit_b_32) |
| [`ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.vit_h_14.html#torchvision.models.ViT_H_14_Weights "torchvision.models.ViT_H_14_Weights") | 88.552 | 98.694 | 633.5M | 1016.72 | [链接](https://github.com/facebookresearch/SWAG) |
| [`ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.vit_h_14.html#torchvision.models.ViT_H_14_Weights "torchvision.models.ViT_H_14_Weights") | 85.708 | 97.73 | 632.0M | 167.29 | [链接](https://github.com/pytorch/vision/pull/5793) |
| [`ViT_L_16_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights "torchvision.models.ViT_L_16_Weights") | 79.662 | 94.638 | 304.3M | 61.55 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#vit_l_16) |
| [`ViT_L_16_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights "torchvision.models.ViT_L_16_Weights") | 88.064 | 98.512 | 305.2M | 361.99 | [链接](https://github.com/facebookresearch/SWAG) |
| [`ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights "torchvision.models.ViT_L_16_Weights") | 85.146 | 97.422 | 304.3M | 61.55 | [链接](https://github.com/pytorch/vision/pull/5793) |
| [`ViT_L_32_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vit_l_32.html#torchvision.models.ViT_L_32_Weights "torchvision.models.ViT_L_32_Weights") | 76.972 | 93.07 | 306.5M | 15.38 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#vit_l_32) |
| [`Wide_ResNet101_2_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.wide_resnet101_2.html#torchvision.models.Wide_ResNet101_2_Weights "torchvision.models.Wide_ResNet101_2_Weights") | 78.848 | 94.284 | 126.9M | 22.75 | [链接](https://github.com/pytorch/vision/pull/912#issue-445437439) |
| [`Wide_ResNet101_2_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.wide_resnet101_2.html#torchvision.models.Wide_ResNet101_2_Weights "torchvision.models.Wide_ResNet101_2_Weights") | 82.51 | 96.02 | 126.9M | 22.75 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| [`Wide_ResNet50_2_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.wide_resnet50_2.html#torchvision.models.Wide_ResNet50_2_Weights "torchvision.models.Wide_ResNet50_2_Weights") | 78.468 | 94.086 | 68.9M | 11.4 | [链接](https://github.com/pytorch/vision/pull/912#issue-445437439) |
| [`Wide_ResNet50_2_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.wide_resnet50_2.html#torchvision.models.Wide_ResNet50_2_Weights "torchvision.models.Wide_ResNet50_2_Weights") | 81.602 | 95.758 | 68.9M | 11.4 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |

### 量化模型[](#quantized-models "跳转到此标题")

以下架构支持带有或不带预训练权重的INT8量化模型：

+   [量化的GoogLeNet](models/googlenet_quant.html)

+   [量化的InceptionV3](models/inception_quant.html)

+   [量化的MobileNet V2](models/mobilenetv2_quant.html)

+   [量化的MobileNet V3](models/mobilenetv3_quant.html)

+   [量化的ResNet](models/resnet_quant.html)

+   [量化的ResNeXt](models/resnext_quant.html)

+   [量化的ShuffleNet V2](models/shufflenetv2_quant.html)

以下是如何使用预训练的量化图像分类模型的示例：

```py
from torchvision.io import read_image
from torchvision.models.quantization import resnet50, ResNet50_QuantizedWeights

img = read_image("test/assets/encode_jpeg/grace_hopper_517x606.jpg")

# Step 1: Initialize model with the best available weights
weights = ResNet50_QuantizedWeights.DEFAULT
model = resnet50(weights=weights, quantize=True)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = preprocess(img).unsqueeze(0)

# Step 4: Use the model and print the predicted category
prediction = model(batch).squeeze(0).softmax(0)
class_id = prediction.argmax().item()
score = prediction[class_id].item()
category_name = weights.meta["categories"][class_id]
print(f"{category_name}: {100  *  score}%") 
```

预训练模型输出的类别可以在`weights.meta["categories"]`中找到。

#### 所有可用的量化分类权重表[](#table-of-all-available-quantized-classification-weights "跳转到此标题")

准确率是在ImageNet-1K上使用单个裁剪报告的：

| **权重** | **准确率@1** | **准确率@5** | **参数** | **GIPS** | **配方** |
| --- | --- | --- | --- | --- | --- |
| [`GoogLeNet_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.googlenet.html#torchvision.models.quantization.GoogLeNet_QuantizedWeights "torchvision.models.quantization.GoogLeNet_QuantizedWeights") | 69.826 | 89.404 | 6.6M | 1.5 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| [`Inception_V3_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.inception_v3.html#torchvision.models.quantization.Inception_V3_QuantizedWeights "torchvision.models.quantization.Inception_V3_QuantizedWeights") | 77.176 | 93.354 | 27.2M | 5.71 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| [`MobileNet_V2_QuantizedWeights.IMAGENET1K_QNNPACK_V1`](models/generated/torchvision.models.quantization.mobilenet_v2.html#torchvision.models.quantization.MobileNet_V2_QuantizedWeights "torchvision.models.quantization.MobileNet_V2_QuantizedWeights") | 71.658 | 90.15 | 3.5M | 0.3 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#qat-mobilenetv2) |
| [`MobileNet_V3_Large_QuantizedWeights.IMAGENET1K_QNNPACK_V1`](models/generated/torchvision.models.quantization.mobilenet_v3_large.html#torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights "torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights") | 73.004 | 90.858 | 5.5M | 0.22 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#qat-mobilenetv3) |
| [`ResNeXt101_32X8D_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.resnext101_32x8d.html#torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights "torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights") | 78.986 | 94.48 | 88.8M | 16.41 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| [`ResNeXt101_32X8D_QuantizedWeights.IMAGENET1K_FBGEMM_V2`](models/generated/torchvision.models.quantization.resnext101_32x8d.html#torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights "torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights") | 82.574 | 96.132 | 88.8M | 16.41 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| [`ResNeXt101_64X4D_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.resnext101_64x4d.html#torchvision.models.quantization.ResNeXt101_64X4D_QuantizedWeights "torchvision.models.quantization.ResNeXt101_64X4D_QuantizedWeights") | 82.898 | 96.326 | 83.5M | 15.46 | [链接](https://github.com/pytorch/vision/pull/5935) |
| [`ResNet18_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.resnet18.html#torchvision.models.quantization.ResNet18_QuantizedWeights "torchvision.models.quantization.ResNet18_QuantizedWeights") | 69.494 | 88.882 | 11.7M | 1.81 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| [`ResNet50_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.resnet50.html#torchvision.models.quantization.ResNet50_QuantizedWeights "torchvision.models.quantization.ResNet50_QuantizedWeights") | 75.92 | 92.814 | 25.6M | 4.09 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| [`ResNet50_QuantizedWeights.IMAGENET1K_FBGEMM_V2`](models/generated/torchvision.models.quantization.resnet50.html#torchvision.models.quantization.ResNet50_QuantizedWeights "torchvision.models.quantization.ResNet50_QuantizedWeights") | 80.282 | 94.976 | 25.6M | 4.09 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| [`ShuffleNet_V2_X0_5_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.shufflenet_v2_x0_5.html#torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights "torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights") | 57.972 | 79.78 | 1.4M | 0.04 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| [`ShuffleNet_V2_X1_0_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.shufflenet_v2_x1_0.html#torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights "torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights") | 68.36 | 87.582 | 2.3M | 0.14 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| [`ShuffleNet_V2_X1_5_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.shufflenet_v2_x1_5.html#torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights "torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights") | 72.052 | 90.7 | 3.5M | 0.3 | [链接](https://github.com/pytorch/vision/pull/5906) |
| [`ShuffleNet_V2_X2_0_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.shufflenet_v2_x2_0.html#torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights "torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights") | 75.354 | 92.488 | 7.4M | 0.58 | [链接](https://github.com/pytorch/vision/pull/5906) |

## 语义分割[](#semantic-segmentation "跳转到此标题")

警告

分割模块处于Beta阶段，不保证向后兼容性。

以下语义分割模型可用，带或不带预训练权重：

+   [DeepLabV3](models/deeplabv3.html)

+   [FCN](models/fcn.html)

+   [LRASPP](models/lraspp.html)

以下是如何使用预训练语义分割模型的示例：

```py
from torchvision.io.image import read_image
from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights
from torchvision.transforms.functional import to_pil_image

img = read_image("gallery/assets/dog1.jpg")

# Step 1: Initialize model with the best available weights
weights = FCN_ResNet50_Weights.DEFAULT
model = fcn_resnet50(weights=weights)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = preprocess(img).unsqueeze(0)

# Step 4: Use the model and visualize the prediction
prediction = model(batch)["out"]
normalized_masks = prediction.softmax(dim=1)
class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta["categories"])}
mask = normalized_masks[0, class_to_idx["dog"]]
to_pil_image(mask).show() 
```

预训练模型输出的类别可以在 `weights.meta["categories"]` 中找到。模型的输出格式在[语义分割模型](auto_examples/others/plot_visualization_utils.html#semantic-seg-output)中有说明。

### 所有可用语义分割权重的表格[](#table-of-all-available-semantic-segmentation-weights "跳转到此标题")

所有模型都是在COCO val2017的子集上评估的，涵盖了Pascal VOC数据集中存在的20个类别：

| **权重** | **平均IoU** | **像素准确率** | **参数** | **GFLOPS** | **配置** |
| --- | --- | --- | --- | --- | --- |
| [`DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.deeplabv3_mobilenet_v3_large.html#torchvision.models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights "torchvision.models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights") | 60.3 | 91.2 | 11.0M | 10.45 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_mobilenet_v3_large) |
| [`DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.deeplabv3_resnet101.html#torchvision.models.segmentation.DeepLabV3_ResNet101_Weights "torchvision.models.segmentation.DeepLabV3_ResNet101_Weights") | 67.4 | 92.4 | 61.0M | 258.74 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#fcn_resnet101) |
| [`DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.deeplabv3_resnet50.html#torchvision.models.segmentation.DeepLabV3_ResNet50_Weights "torchvision.models.segmentation.DeepLabV3_ResNet50_Weights") | 66.4 | 92.4 | 42.0M | 178.72 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_resnet50) |
| [`FCN_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.fcn_resnet101.html#torchvision.models.segmentation.FCN_ResNet101_Weights "torchvision.models.segmentation.FCN_ResNet101_Weights") | 63.7 | 91.9 | 54.3M | 232.74 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_resnet101) |
| [`FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.fcn_resnet50.html#torchvision.models.segmentation.FCN_ResNet50_Weights "torchvision.models.segmentation.FCN_ResNet50_Weights") | 60.5 | 91.4 | 35.3M | 152.72 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#fcn_resnet50) |
| [`LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.lraspp_mobilenet_v3_large.html#torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights "torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights") | 57.9 | 91.2 | 3.2M | 2.09 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#lraspp_mobilenet_v3_large) |

## 目标检测、实例分割和人体关键点检测[](#object-detection-instance-segmentation-and-person-keypoint-detection "跳转到此标题")

检测、实例分割和关键点检测的预训练模型是使用torchvision中的分类模型初始化的。这些模型期望一个`Tensor[C, H, W]`列表。查看模型的构造函数以获取更多信息。

警告

检测模块处于Beta阶段，不保证向后兼容性。

### 目标检测[](#object-detection "跳转到此标题")

以下目标检测模型可用，有或没有预训练权重：

+   [Faster R-CNN](models/faster_rcnn.html)

+   [FCOS](models/fcos.html)

+   [RetinaNet](models/retinanet.html)

+   [SSD](models/ssd.html)

+   [SSDlite](models/ssdlite.html)

以下是如何使用预训练目标检测模型的示例：

```py
from torchvision.io.image import read_image
from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image

img = read_image("test/assets/encode_jpeg/grace_hopper_517x606.jpg")

# Step 1: Initialize model with the best available weights
weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = [preprocess(img)]

# Step 4: Use the model and visualize the prediction
prediction = model(batch)[0]
labels = [weights.meta["categories"][i] for i in prediction["labels"]]
box = draw_bounding_boxes(img, boxes=prediction["boxes"],
                          labels=labels,
                          colors="red",
                          width=4, font_size=30)
im = to_pil_image(box.detach())
im.show() 
```

预训练模型输出的类别可以在`weights.meta["categories"]`中找到。有关如何绘制模型边界框的详细信息，您可以参考[实例分割模型](auto_examples/others/plot_visualization_utils.html#instance-seg-output)。

#### 所有可用的目标检测权重表[](#table-of-all-available-object-detection-weights "跳转到此标题")

在COCO val2017上报告了Box MAPs：

| **权重** | **Box MAP** | **参数** | **GFLOPS** | **Recipe** |
| --- | --- | --- | --- | --- |
| [`FCOS_ResNet50_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.fcos_resnet50_fpn.html#torchvision.models.detection.FCOS_ResNet50_FPN_Weights "torchvision.models.detection.FCOS_ResNet50_FPN_Weights") | 39.2 | 32.3M | 128.21 | [链接](https://github.com/pytorch/vision/tree/main/references/detection#fcos-resnet-50-fpn) |
| [`FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn.html#torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights "torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights") | 22.8 | 19.4M | 0.72 | [链接](https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-mobilenetv3-large-320-fpn) |
| [`FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn.html#torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights "torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights") | 32.8 | 19.4M | 4.49 | [链接](https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-mobilenetv3-large-fpn) |
| [`FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1`](models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn_v2.html#torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights "torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights") | 46.7 | 43.7M | 280.37 | [链接](https://github.com/pytorch/vision/pull/5763) |
| [`FasterRCNN_ResNet50_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights "torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights") | 37 | 41.8M | 134.38 | [链接](https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-resnet-50-fpn) |
| [`RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1`](models/generated/torchvision.models.detection.retinanet_resnet50_fpn_v2.html#torchvision.models.detection.RetinaNet_ResNet50_FPN_V2_Weights "torchvision.models.detection.RetinaNet_ResNet50_FPN_V2_Weights") | 41.5 | 38.2M | 152.24 | [link](https://github.com/pytorch/vision/pull/5756) |
| [`RetinaNet_ResNet50_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.retinanet_resnet50_fpn.html#torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights "torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights") | 36.4 | 34.0M | 151.54 | [link](https://github.com/pytorch/vision/tree/main/references/detection#retinanet) |
| [`SSD300_VGG16_Weights.COCO_V1`](models/generated/torchvision.models.detection.ssd300_vgg16.html#torchvision.models.detection.SSD300_VGG16_Weights "torchvision.models.detection.SSD300_VGG16_Weights") | 25.1 | 35.6M | 34.86 | [link](https://github.com/pytorch/vision/tree/main/references/detection#ssd300-vgg16) |
| [`SSDLite320_MobileNet_V3_Large_Weights.COCO_V1`](models/generated/torchvision.models.detection.ssdlite320_mobilenet_v3_large.html#torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights "torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights") | 21.3 | 3.4M | 0.58 | [link](https://github.com/pytorch/vision/tree/main/references/detection#ssdlite320-mobilenetv3-large) |

### 实例分割[](#instance-segmentation "跳转到此标题")

以下是可用的实例分割模型，带或不带预训练权重：

+   [Mask R-CNN](models/mask_rcnn.html)

有关如何绘制模型的蒙版的详细信息，您可以参考[实例分割模型](auto_examples/others/plot_visualization_utils.html#instance-seg-output)。

#### 所有可用实例分割权重的表格[](#table-of-all-available-instance-segmentation-weights "跳转到此标题")

在COCO val2017上报告了框和蒙版MAPs：

| **权重** | **框MAP** | **蒙版MAP** | **参数** | **GFLOPS** | **链接** |
| --- | --- | --- | --- | --- | --- |
| [`MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1`](models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn_v2.html#torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights "torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights") | 47.4 | 41.8 | 46.4M | 333.58 | [link](https://github.com/pytorch/vision/pull/5773) |
| [`MaskRCNN_ResNet50_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html#torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights "torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights") | 37.9 | 34.6 | 44.4M | 134.38 | [link](https://github.com/pytorch/vision/tree/main/references/detection#mask-r-cnn) |

### 关键点检测[](#keypoint-detection "跳转到此标题")

以下是可用的人体关键点检测模型，带或不带预训练权重：

+   [Keypoint R-CNN](models/keypoint_rcnn.html)

预训练模型输出的类别可以在`weights.meta["keypoint_names"]`中找到。有关如何绘制模型的边界框的详细信息，您可以参考[可视化关键点](auto_examples/others/plot_visualization_utils.html#keypoint-output)。

#### 所有可用关键点检测权重的表格[](#table-of-all-available-keypoint-detection-weights "跳转到此标题")

在COCO val2017上报告了框和关键点MAPs：

| **权重** | **框MAP** | **关键点MAP** | **参数** | **GFLOPS** | **链接** |
| --- | --- | --- | --- | --- | --- |
| [`KeypointRCNN_ResNet50_FPN_Weights.COCO_LEGACY`](models/generated/torchvision.models.detection.keypointrcnn_resnet50_fpn.html#torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights "torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights") | 50.6 | 61.1 | 59.1M | 133.92 | [link](https://github.com/pytorch/vision/issues/1606) |
| [`KeypointRCNN_ResNet50_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.keypointrcnn_resnet50_fpn.html#torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights "torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights") | 54.6 | 65 | 59.1M | 137.42 | [链接](https://github.com/pytorch/vision/tree/main/references/detection#keypoint-r-cnn) |

## 视频分类[](#video-classification "跳转到此标题的永久链接")

警告

视频模块处于Beta阶段，不保证向后兼容性。

以下视频分类模型可用，带有或不带有预训练权重：

+   [视频MViT](models/video_mvit.html)

+   [视频ResNet](models/video_resnet.html)

+   [视频S3D](models/video_s3d.html)

+   [视频SwinTransformer](models/video_swin_transformer.html)

以下是如何使用预训练视频分类模型的示例：

```py
from torchvision.io.video import read_video
from torchvision.models.video import r3d_18, R3D_18_Weights

vid, _, _ = read_video("test/assets/videos/v_SoccerJuggling_g23_c01.avi", output_format="TCHW")
vid = vid[:32]  # optionally shorten duration

# Step 1: Initialize model with the best available weights
weights = R3D_18_Weights.DEFAULT
model = r3d_18(weights=weights)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = preprocess(vid).unsqueeze(0)

# Step 4: Use the model and print the predicted category
prediction = model(batch).squeeze(0).softmax(0)
label = prediction.argmax().item()
score = prediction[label].item()
category_name = weights.meta["categories"][label]
print(f"{category_name}: {100  *  score}%") 
```

预训练模型输出的类别可以在`weights.meta["categories"]`中找到。

### 所有可用视频分类权重的表格[](#table-of-all-available-video-classification-weights "跳转到此标题的永久链接")

准确率是使用单个裁剪在剪辑长度为16的Kinetics-400上报告的：

| **权重** | **准确率@1** | **准确率@5** | **参数** | **GFLOPS** | **配置** |
| --- | --- | --- | --- | --- | --- |
| [`MC3_18_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.mc3_18.html#torchvision.models.video.MC3_18_Weights "torchvision.models.video.MC3_18_Weights") | 63.96 | 84.13 | 11.7M | 43.34 | [链接](https://github.com/pytorch/vision/tree/main/references/video_classification) |
| [`MViT_V1_B_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.mvit_v1_b.html#torchvision.models.video.MViT_V1_B_Weights "torchvision.models.video.MViT_V1_B_Weights") | 78.477 | 93.582 | 36.6M | 70.6 | [链接](https://github.com/facebookresearch/pytorchvideo/blob/main/docs/source/model_zoo.md) |
| [`MViT_V2_S_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.mvit_v2_s.html#torchvision.models.video.MViT_V2_S_Weights "torchvision.models.video.MViT_V2_S_Weights") | 80.757 | 94.665 | 34.5M | 64.22 | [链接](https://github.com/facebookresearch/SlowFast/blob/main/MODEL_ZOO.md) |
| [`R2Plus1D_18_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.r2plus1d_18.html#torchvision.models.video.R2Plus1D_18_Weights "torchvision.models.video.R2Plus1D_18_Weights") | 67.463 | 86.175 | 31.5M | 40.52 | [链接](https://github.com/pytorch/vision/tree/main/references/video_classification) |
| [`R3D_18_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.r3d_18.html#torchvision.models.video.R3D_18_Weights "torchvision.models.video.R3D_18_Weights") | 63.2 | 83.479 | 33.4M | 40.7 | [链接](https://github.com/pytorch/vision/tree/main/references/video_classification) |
| [`S3D_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.s3d.html#torchvision.models.video.S3D_Weights "torchvision.models.video.S3D_Weights") | 68.368 | 88.05 | 8.3M | 17.98 | [链接](https://github.com/pytorch/vision/tree/main/references/video_classification#s3d) |
| [`Swin3D_B_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.swin3d_b.html#torchvision.models.video.Swin3D_B_Weights "torchvision.models.video.Swin3D_B_Weights") | 79.427 | 94.386 | 88.0M | 140.67 | [链接](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400) |
| [`Swin3D_B_Weights.KINETICS400_IMAGENET22K_V1`](models/generated/torchvision.models.video.swin3d_b.html#torchvision.models.video.Swin3D_B_Weights "torchvision.models.video.Swin3D_B_Weights") | 81.643 | 95.574 | 88.0M | 140.67 | [链接](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400) |
| [`Swin3D_S_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.swin3d_s.html#torchvision.models.video.Swin3D_S_Weights "torchvision.models.video.Swin3D_S_Weights") | 79.521 | 94.158 | 49.8M | 82.84 | [链接](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400) |
| [`Swin3D_T_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.swin3d_t.html#torchvision.models.video.Swin3D_T_Weights "torchvision.models.video.Swin3D_T_Weights") | 77.715 | 93.519 | 28.2M | 43.88 | [链接](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400) |

## 光流

以下光流模型可用，带有或不带有预训练

+   [RAFT](models/raft.html)
