["```py\nconda  install  pytorch  torchvision  captum  flask-compress  matplotlib=3.3.4  -c  pytorch \n```", "```py\npip  install  torch  torchvision  captum  matplotlib==3.3.4  Flask-Compress \n```", "```py\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport captum\nfrom captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\nfrom captum.attr import visualization as viz\n\nimport os, sys\nimport json\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap \n```", "```py\nmodel = [models.resnet18](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18 \"torchvision.models.resnet18\")(weights='IMAGENET1K_V1')\nmodel = model.eval() \n```", "```py\ntest_img = Image.open('img/cat.jpg')\ntest_img_data = np.asarray(test_img)\nplt.imshow(test_img_data)\nplt.show() \n```", "```py\n# model expects 224x224 3-color image\ntransform = [transforms.Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\")([\n [transforms.Resize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html#torchvision.transforms.Resize \"torchvision.transforms.Resize\")(224),\n [transforms.CenterCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.CenterCrop.html#torchvision.transforms.CenterCrop \"torchvision.transforms.CenterCrop\")(224),\n [transforms.ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor \"torchvision.transforms.ToTensor\")()\n])\n\n# standard ImageNet normalization\ntransform_normalize = [transforms.Normalize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize \"torchvision.transforms.Normalize\")(\n     mean=[0.485, 0.456, 0.406],\n     std=[0.229, 0.224, 0.225]\n )\n\ntransformed_img = transform(test_img)\ninput_img = transform_normalize(transformed_img)\ninput_img = input_img.unsqueeze(0) # the model requires a dummy batch dimension\n\nlabels_path = 'img/imagenet_class_index.json'\nwith open(labels_path) as json_data:\n    idx_to_labels = json.load(json_data) \n```", "```py\noutput = model(input_img)\noutput = [F.softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax \"torch.nn.functional.softmax\")(output, dim=1)\nprediction_score, pred_label_idx = [torch.topk](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk \"torch.topk\")(output, 1)\npred_label_idx.squeeze_()\npredicted_label = idx_to_labels[str(pred_label_idx.item())][1]\nprint('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')') \n```", "```py\n# Initialize the attribution algorithm with the model\nintegrated_gradients = IntegratedGradients(model)\n\n# Ask the algorithm to attribute our output target to\nattributions_ig = integrated_gradients.attribute(input_img, target=pred_label_idx, n_steps=200)\n\n# Show the original image for comparison\n_ = viz.visualize_image_attr(None, np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                      method=\"original_image\", title=\"Original Image\")\n\ndefault_cmap = LinearSegmentedColormap.from_list('custom blue',\n                                                 [(0, '#ffffff'),\n                                                  (0.25, '#0000ff'),\n                                                  (1, '#0000ff')], N=256)\n\n_ = viz.visualize_image_attr(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n                             np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                             method='heat_map',\n                             cmap=default_cmap,\n                             show_colorbar=True,\n                             sign='positive',\n                             title='Integrated Gradients') \n```", "```py\nocclusion = Occlusion(model)\n\nattributions_occ = occlusion.attribute(input_img,\n                                       target=pred_label_idx,\n                                       strides=(3, 8, 8),\n                                       sliding_window_shapes=(3,15, 15),\n                                       baselines=0)\n\n_ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      [\"original_image\", \"heat_map\", \"heat_map\", \"masked_image\"],\n                                      [\"all\", \"positive\", \"negative\", \"positive\"],\n                                      show_colorbar=True,\n                                      titles=[\"Original\", \"Positive Attribution\", \"Negative Attribution\", \"Masked\"],\n                                      fig_size=(18, 6)\n                                     ) \n```", "```py\nlayer_gradcam = LayerGradCam(model, model.layer3[1].conv2)\nattributions_lgc = layer_gradcam.attribute(input_img, target=pred_label_idx)\n\n_ = viz.visualize_image_attr(attributions_lgc[0].cpu().permute(1,2,0).detach().numpy(),\n                             sign=\"all\",\n                             title=\"Layer 3 Block 1 Conv 2\") \n```", "```py\nupsamp_attr_lgc = LayerAttribution.interpolate(attributions_lgc, input_img.shape[2:])\n\nprint(attributions_lgc.shape)\nprint(upsamp_attr_lgc.shape)\nprint(input_img.shape)\n\n_ = viz.visualize_image_attr_multiple(upsamp_attr_lgc[0].cpu().permute(1,2,0).detach().numpy(),\n                                      transformed_img.permute(1,2,0).numpy(),\n                                      [\"original_image\",\"blended_heat_map\",\"masked_image\"],\n                                      [\"all\",\"positive\",\"positive\"],\n                                      show_colorbar=True,\n                                      titles=[\"Original\", \"Positive Attribution\", \"Masked\"],\n                                      fig_size=(18, 6)) \n```", "```py\nimgs = ['img/cat.jpg', 'img/teapot.jpg', 'img/trilobite.jpg']\n\nfor img in imgs:\n    img = Image.open(img)\n    transformed_img = transform(img)\n    input_img = transform_normalize(transformed_img)\n    input_img = input_img.unsqueeze(0) # the model requires a dummy batch dimension\n\n    output = model(input_img)\n    output = [F.softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax \"torch.nn.functional.softmax\")(output, dim=1)\n    prediction_score, pred_label_idx = [torch.topk](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk \"torch.topk\")(output, 1)\n    pred_label_idx.squeeze_()\n    predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n    print('Predicted:', predicted_label, '/', pred_label_idx.item(), ' (', prediction_score.squeeze().item(), ')') \n```", "```py\nfrom captum.insights import AttributionVisualizer, Batch\nfrom captum.insights.attr_vis.features import ImageFeature\n\n# Baseline is all-zeros input - this may differ depending on your data\ndef baseline_func(input):\n    return input * 0\n\n# merging our image transforms from above\ndef full_img_transform(input):\n    i = Image.open(input)\n    i = transform(i)\n    i = transform_normalize(i)\n    i = i.unsqueeze(0)\n    return i\n\ninput_imgs = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")(list(map(lambda i: full_img_transform(i), imgs)), 0)\n\nvisualizer = AttributionVisualizer(\n    models=[model],\n    score_func=lambda o: [torch.nn.functional.softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax \"torch.nn.functional.softmax\")(o, 1),\n    classes=list(map(lambda k: idx_to_labels[k][1], idx_to_labels.keys())),\n    features=[\n        ImageFeature(\n            \"Photo\",\n            baseline_transforms=[baseline_func],\n            input_transforms=[],\n        )\n    ],\n    dataset=[Batch(input_imgs, labels=[282,849,69])]\n) \n```", "```py\nvisualizer.render() \n```"]