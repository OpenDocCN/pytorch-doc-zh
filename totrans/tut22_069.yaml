- en: Profiling your PyTorch Module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/beginner/profiler.html](https://pytorch.org/tutorials/beginner/profiler.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-beginner-profiler-py) to download the full example
    code
  prefs: []
  type: TYPE_NORMAL
- en: '**Author:** [Suraj Subramanian](https://github.com/suraj813)'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch includes a profiler API that is useful to identify the time and memory
    costs of various PyTorch operations in your code. Profiler can be easily integrated
    in your code, and the results can be printed as a table or returned in a JSON
    trace file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Profiler supports multithreaded models. Profiler runs in the same thread as
    the operation but it will also profile child operators that might run in another
    thread. Concurrently-running profilers will be scoped to their own thread to prevent
    mixing of results.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch 1.8 introduces the new API that will replace the older profiler API
    in the future releases. Check the new API at [this page](https://pytorch.org/docs/master/profiler.html).
  prefs: []
  type: TYPE_NORMAL
- en: Head on over to [this recipe](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)
    for a quicker walkthrough of Profiler API usage.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Performance debugging using Profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Profiler can be useful to identify performance bottlenecks in your models.
    In this example, we build a custom module that performs two sub-tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: a linear transformation on the input, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use the transformation result to get indices on a mask tensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We wrap the code for each sub-task in separate labelled context managers using
    `profiler.record_function("label")`. In the profiler output, the aggregate performance
    metrics of all operations in the sub-task will show up under its corresponding
    label.
  prefs: []
  type: TYPE_NORMAL
- en: Note that using Profiler incurs some overhead, and is best used only for investigating
    code. Remember to remove it if you are benchmarking runtimes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Profile the forward pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We initialize random input and mask tensors, and the model.
  prefs: []
  type: TYPE_NORMAL
- en: Before we run the profiler, we warm-up CUDA to ensure accurate performance benchmarking.
    We wrap the forward pass of our module in the `profiler.profile` context manager.
    The `with_stack=True` parameter appends the file and line number of the operation
    in the trace.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`with_stack=True` incurs an additional overhead, and is better suited for investigating
    code. Remember to remove it if you are benchmarking performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Print profiler results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we print the profiler results. `profiler.key_averages` aggregates the
    results by operator name, and optionally by input shapes and/or stack trace events.
    Grouping by input shapes is useful to identify which tensor shapes are utilized
    by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use `group_by_stack_n=5` which aggregates runtimes by the operation
    and its traceback (truncated to the most recent 5 events), and display the events
    in the order they are registered. The table can also be sorted by passing a `sort_by`
    argument (refer to the [docs](https://pytorch.org/docs/stable/autograd.html#profiler)
    for valid sorting keys).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When running profiler in a notebook, you might see entries like `<ipython-input-18-193a910735e8>(13):
    forward` instead of filenames in the stacktrace. These correspond to `<notebook-cell>(line
    number): calling-function`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Improve memory performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note that the most expensive operations - in terms of memory and time - are
    at `forward (10)` representing the operations within MASK INDICES. Let’s try to
    tackle the memory consumption first. We can see that the `.to()` operation at
    line 12 consumes 953.67 Mb. This operation copies `mask` to the CPU. `mask` is
    initialized with a `torch.double` datatype. Can we reduce the memory footprint
    by casting it to `torch.float` instead?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The CPU memory footprint for this operation has halved.
  prefs: []
  type: TYPE_NORMAL
- en: Improve time performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the time consumed has also reduced a bit, it’s still too high. Turns out
    copying a matrix from CUDA to CPU is pretty expensive! The `aten::copy_` operator
    in `forward (12)` copies `mask` to CPU so that it can use the NumPy `argwhere`
    function. `aten::copy_` at `forward(13)` copies the array back to CUDA as a tensor.
    We could eliminate both of these if we use a `torch` function `nonzero()` here
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have seen how Profiler can be used to investigate time and memory bottlenecks
    in PyTorch models. Read more about Profiler here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Profiler Usage Recipe](https://pytorch.org/tutorials/recipes/recipes/profiler.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Profiling RPC-Based Workloads](https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Profiler API Docs](https://pytorch.org/docs/stable/autograd.html?highlight=profiler#profiler)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 0 minutes 0.000 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: profiler.py`](../_downloads/1df539a85371bf035ce170fb872b4f7f/profiler.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: profiler.ipynb`](../_downloads/9fc6c90b1bbbfd4201d66c498708f33f/profiler.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
