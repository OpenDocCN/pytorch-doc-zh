["```py\nimport threading\n\nimport torch\nimport torch.nn as nn\n\nfrom torchvision.models.resnet import Bottleneck\n\nnum_classes = 1000\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\nclass ResNetBase(nn.Module):\n    def __init__(self, block, inplanes, num_classes=1000,\n                groups=1, width_per_group=64, norm_layer=None):\n        super(ResNetBase, self).__init__()\n\n        self._lock = threading.Lock()\n        self._block = block\n        self._norm_layer = nn.BatchNorm2d\n        self.inplanes = inplanes\n        self.dilation = 1\n        self.groups = groups\n        self.base_width = width_per_group\n\n    def _make_layer(self, planes, blocks, stride=1):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if stride != 1 or self.inplanes != planes * self._block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * self._block.expansion, stride),\n                norm_layer(planes * self._block.expansion),\n            )\n\n        layers = []\n        layers.append(self._block(self.inplanes, planes, stride, downsample, self.groups,\n                                self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * self._block.expansion\n        for _ in range(1, blocks):\n            layers.append(self._block(self.inplanes, planes, groups=self.groups,\n                                    base_width=self.base_width, dilation=self.dilation,\n                                    norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def parameter_rrefs(self):\n        return [RRef(p) for p in self.parameters()] \n```", "```py\nclass ResNetShard1(ResNetBase):\n    def __init__(self, device, *args, **kwargs):\n        super(ResNetShard1, self).__init__(\n            Bottleneck, 64, num_classes=num_classes, *args, **kwargs)\n\n        self.device = device\n        self.seq = nn.Sequential(\n            nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False),\n            self._norm_layer(self.inplanes),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            self._make_layer(64, 3),\n            self._make_layer(128, 4, stride=2)\n        ).to(self.device)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x_rref):\n        x = x_rref.to_here().to(self.device)\n        with self._lock:\n            out =  self.seq(x)\n        return out.cpu()\n\nclass ResNetShard2(ResNetBase):\n    def __init__(self, device, *args, **kwargs):\n        super(ResNetShard2, self).__init__(\n            Bottleneck, 512, num_classes=num_classes, *args, **kwargs)\n\n        self.device = device\n        self.seq = nn.Sequential(\n            self._make_layer(256, 6, stride=2),\n            self._make_layer(512, 3, stride=2),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        ).to(self.device)\n\n        self.fc =  nn.Linear(512 * self._block.expansion, num_classes).to(self.device)\n\n    def forward(self, x_rref):\n        x = x_rref.to_here().to(self.device)\n        with self._lock:\n            out = self.fc(torch.flatten(self.seq(x), 1))\n        return out.cpu() \n```", "```py\nclass DistResNet50(nn.Module):\n    def __init__(self, num_split, workers, *args, **kwargs):\n        super(DistResNet50, self).__init__()\n\n        self.num_split = num_split\n\n        # Put the first part of the ResNet50 on workers[0]\n        self.p1_rref = rpc.remote(\n            workers[0],\n            ResNetShard1,\n            args = (\"cuda:0\",) + args,\n            kwargs = kwargs\n        )\n\n        # Put the second part of the ResNet50 on workers[1]\n        self.p2_rref = rpc.remote(\n            workers[1],\n            ResNetShard2,\n            args = (\"cuda:1\",) + args,\n            kwargs = kwargs\n        )\n\n    def forward(self, xs):\n        out_futures = []\n        for x in iter(xs.split(self.num_split, dim=0)):\n            x_rref = RRef(x)\n            y_rref = self.p1_rref.remote().forward(x_rref)\n            z_fut = self.p2_rref.rpc_async().forward(y_rref)\n            out_futures.append(z_fut)\n\n        return torch.cat(torch.futures.wait_all(out_futures))\n\n    def parameter_rrefs(self):\n        remote_params = []\n        remote_params.extend(self.p1_rref.remote().parameter_rrefs().to_here())\n        remote_params.extend(self.p2_rref.remote().parameter_rrefs().to_here())\n        return remote_params \n```", "```py\nimport torch.distributed.autograd as dist_autograd\nimport torch.optim as optim\nfrom torch.distributed.optim import DistributedOptimizer\n\nnum_batches = 3\nbatch_size = 120\nimage_w = 128\nimage_h = 128\n\ndef run_master(num_split):\n    # put the two model parts on worker1 and worker2 respectively\n    model = DistResNet50(num_split, [\"worker1\", \"worker2\"])\n    loss_fn = nn.MSELoss()\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model.parameter_rrefs(),\n        lr=0.05,\n    )\n\n    one_hot_indices = torch.LongTensor(batch_size) \\\n                        .random_(0, num_classes) \\\n                        .view(batch_size, 1)\n\n    for i in range(num_batches):\n        print(f\"Processing batch {i}\")\n        # generate random inputs and labels\n        inputs = torch.randn(batch_size, 3, image_w, image_h)\n        labels = torch.zeros(batch_size, num_classes) \\\n                    .scatter_(1, one_hot_indices, 1)\n\n        with dist_autograd.context() as context_id:\n            outputs = model(inputs)\n            dist_autograd.backward(context_id, [loss_fn(outputs, labels)])\n            opt.step(context_id) \n```", "```py\nimport os\nimport time\n\nimport torch.multiprocessing as mp\n\ndef run_worker(rank, world_size, num_split):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=128)\n\n    if rank == 0:\n        rpc.init_rpc(\n            \"master\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=options\n        )\n        run_master(num_split)\n    else:\n        rpc.init_rpc(\n            f\"worker{rank}\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=options\n        )\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\nif __name__==\"__main__\":\n    world_size = 3\n    for num_split in [1, 2, 4, 8]:\n        tik = time.time()\n        mp.spawn(run_worker, args=(world_size, num_split), nprocs=world_size, join=True)\n        tok = time.time()\n        print(f\"number of splits = {num_split}, execution time = {tok  -  tik}\") \n```"]