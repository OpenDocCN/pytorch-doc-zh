["```py\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef example(rank, world_size):\n    # create default process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n    # create local model\n    model = nn.Linear(10, 10).to(rank)\n    # construct DDP model\n    ddp_model = DDP(model, device_ids=[rank])\n    # define loss function and optimizer\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    # forward pass\n    outputs = ddp_model(torch.randn(20, 10).to(rank))\n    labels = torch.randn(20, 10).to(rank)\n    # backward pass\n    loss_fn(outputs, labels).backward()\n    # update parameters\n    optimizer.step()\n\ndef main():\n    world_size = 2\n    mp.spawn(example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True)\n\nif __name__==\"__main__\":\n    # Environment variables which need to be\n    # set when using c10d's default \"env\"\n    # initialization mode.\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    main() \n```", "```py\nddp_model = DDP(model, device_ids=[rank])\nddp_model = torch.compile(ddp_model) \n```"]