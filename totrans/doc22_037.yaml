- en: Automatic Mixed Precision package - torch.amp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/amp.html](https://pytorch.org/docs/stable/amp.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[`torch.amp`](#module-torch.amp "torch.amp") provides convenience methods for
    mixed precision, where some operations use the `torch.float32` (`float`) datatype
    and other operations use lower precision floating point datatype (`lower_precision_fp`):
    `torch.float16` (`half`) or `torch.bfloat16`. Some ops, like linear layers and
    convolutions, are much faster in `lower_precision_fp`. Other ops, like reductions,
    often require the dynamic range of `float32`. Mixed precision tries to match each
    op to its appropriate datatype.'
  prefs: []
  type: TYPE_NORMAL
- en: Ordinarily, “automatic mixed precision training” with datatype of `torch.float16`
    uses [`torch.autocast`](#torch.autocast "torch.autocast") and [`torch.cuda.amp.GradScaler`](#torch.cuda.amp.GradScaler
    "torch.cuda.amp.GradScaler") together, as shown in the [CUDA Automatic Mixed Precision
    examples](notes/amp_examples.html#amp-examples) and [CUDA Automatic Mixed Precision
    recipe](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html). However,
    [`torch.autocast`](#torch.autocast "torch.autocast") and [`torch.cuda.amp.GradScaler`](#torch.cuda.amp.GradScaler
    "torch.cuda.amp.GradScaler") are modular, and may be used separately if desired.
    As shown in the CPU example section of [`torch.autocast`](#torch.autocast "torch.autocast"),
    “automatic mixed precision training/inference” on CPU with datatype of `torch.bfloat16`
    only uses [`torch.autocast`](#torch.autocast "torch.autocast").
  prefs: []
  type: TYPE_NORMAL
- en: 'For CUDA and CPU, APIs are also provided separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.autocast("cuda", args...)` is equivalent to `torch.cuda.amp.autocast(args...)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.autocast("cpu", args...)` is equivalent to `torch.cpu.amp.autocast(args...)`.
    For CPU, only lower precision floating point datatype of `torch.bfloat16` is supported
    for now.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.autocast`](#torch.autocast "torch.autocast") and [`torch.cpu.amp.autocast`](#torch.cpu.amp.autocast
    "torch.cpu.amp.autocast") are new in version 1.10.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Autocasting](#autocasting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gradient Scaling](#gradient-scaling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Autocast Op Reference](#autocast-op-reference)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Op Eligibility](#op-eligibility)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CUDA Op-Specific Behavior](#cuda-op-specific-behavior)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CUDA Ops that can autocast to `float16`](#cuda-ops-that-can-autocast-to-float16)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CUDA Ops that can autocast to `float32`](#cuda-ops-that-can-autocast-to-float32)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CUDA Ops that promote to the widest input type](#cuda-ops-that-promote-to-the-widest-input-type)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Prefer `binary_cross_entropy_with_logits` over `binary_cross_entropy`](#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CPU Op-Specific Behavior](#cpu-op-specific-behavior)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CPU Ops that can autocast to `bfloat16`](#cpu-ops-that-can-autocast-to-bfloat16)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CPU Ops that can autocast to `float32`](#cpu-ops-that-can-autocast-to-float32)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CPU Ops that promote to the widest input type](#cpu-ops-that-promote-to-the-widest-input-type)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '## [Autocasting](#id4)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Instances of [`autocast`](#torch.autocast "torch.autocast") serve as context
    managers or decorators that allow regions of your script to run in mixed precision.
  prefs: []
  type: TYPE_NORMAL
- en: In these regions, ops run in an op-specific dtype chosen by autocast to improve
    performance while maintaining accuracy. See the [Autocast Op Reference](#autocast-op-reference)
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: When entering an autocast-enabled region, Tensors may be any type. You should
    not call `half()` or `bfloat16()` on your model(s) or inputs when using autocasting.
  prefs: []
  type: TYPE_NORMAL
- en: '[`autocast`](#torch.autocast "torch.autocast") should wrap only the forward
    pass(es) of your network, including the loss computation(s). Backward passes under
    autocast are not recommended. Backward ops run in the same type that autocast
    used for corresponding forward ops.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example for CUDA Devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: See the [CUDA Automatic Mixed Precision examples](notes/amp_examples.html#amp-examples)
    for usage (along with gradient scaling) in more complex scenarios (e.g., gradient
    penalty, multiple models/losses, custom autograd functions).
  prefs: []
  type: TYPE_NORMAL
- en: '[`autocast`](#torch.autocast "torch.autocast") can also be used as a decorator,
    e.g., on the `forward` method of your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Floating-point Tensors produced in an autocast-enabled region may be `float16`.
    After returning to an autocast-disabled region, using them with floating-point
    Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s)
    produced in the autocast region back to `float32` (or other dtype if desired).
    If a Tensor from the autocast region is already `float32`, the cast is a no-op,
    and incurs no additional overhead. CUDA Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'CPU Training Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'CPU Inference Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'CPU Inference Example with Jit Trace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Type mismatch errors *in* an autocast-enabled region are a bug; if this is what
    you observe, please file an issue.
  prefs: []
  type: TYPE_NORMAL
- en: '`autocast(enabled=False)` subregions can be nested in autocast-enabled regions.
    Locally disabling autocast can be useful, for example, if you want to force a
    subregion to run in a particular `dtype`. Disabling autocast gives you explicit
    control over the execution type. In the subregion, inputs from the surrounding
    region should be cast to `dtype` before use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The autocast state is thread-local. If you want it enabled in a new thread,
    the context manager or decorator must be invoked in that thread. This affects
    [`torch.nn.DataParallel`](generated/torch.nn.DataParallel.html#torch.nn.DataParallel
    "torch.nn.DataParallel") and [`torch.nn.parallel.DistributedDataParallel`](generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") when used with more than one GPU
    per process (see [Working with Multiple GPUs](notes/amp_examples.html#amp-multigpu)).
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**device_type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *required*) – Device type to use. Possible values are:
    ‘cuda’, ‘cpu’, ‘xpu’ and ‘hpu’. The type is the same as the type attribute of
    a [`torch.device`](tensor_attributes.html#torch.device "torch.device"). Thus,
    you may obtain the device type of a tensor using Tensor.device.type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**enabled** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Whether autocasting should be enabled in
    the region. Default: `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dtype** (*torch_dtype**,* *optional*) – Whether to use torch.float16 or torch.bfloat16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cache_enabled** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Whether the weight cache inside autocast
    should be enabled. Default: `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: See [`torch.autocast`](#torch.autocast "torch.autocast").
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.cuda.amp.autocast(args...)` is equivalent to `torch.autocast("cuda",
    args...)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Create a helper decorator for `forward` methods of custom autograd functions.
  prefs: []
  type: TYPE_NORMAL
- en: Autograd functions are subclasses of [`torch.autograd.Function`](autograd.html#torch.autograd.Function
    "torch.autograd.Function"). See the [example page](notes/amp_examples.html#amp-custom-examples)
    for more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**cast_inputs** ([`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype")
    or None, optional, default=None) – If not `None`, when `forward` runs in an autocast-enabled
    region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point
    Tensors are not affected), then executes `forward` with autocast disabled. If
    `None`, `forward`’s internal ops execute with the current autocast state.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the decorated `forward` is called outside an autocast-enabled region, [`custom_fwd`](#torch.cuda.amp.custom_fwd
    "torch.cuda.amp.custom_fwd") is a no-op and `cast_inputs` has no effect.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Create a helper decorator for backward methods of custom autograd functions.
  prefs: []
  type: TYPE_NORMAL
- en: Autograd functions are subclasses of [`torch.autograd.Function`](autograd.html#torch.autograd.Function
    "torch.autograd.Function"). Ensures that `backward` executes with the same autocast
    state as `forward`. See the [example page](notes/amp_examples.html#amp-custom-examples)
    for more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'See [`torch.autocast`](#torch.autocast "torch.autocast"). `torch.cpu.amp.autocast(args...)`
    is equivalent to `torch.autocast("cpu", args...)`  ## [Gradient Scaling](#id5)[](#gradient-scaling
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: If the forward pass for a particular op has `float16` inputs, the backward pass
    for that op will produce `float16` gradients. Gradient values with small magnitudes
    may not be representable in `float16`. These values will flush to zero (“underflow”),
    so the update for the corresponding parameters will be lost.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent underflow, “gradient scaling” multiplies the network’s loss(es) by
    a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing
    backward through the network are then scaled by the same factor. In other words,
    gradient values have a larger magnitude, so they don’t flush to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Each parameter’s gradient (`.grad` attribute) should be unscaled before the
    optimizer updates the parameters, so the scale factor does not interfere with
    the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: AMP/fp16 may not work for every model! For example, most bf16-pretrained models
    cannot operate in the fp16 numerical range of max 65504 and will cause gradients
    to overflow instead of underflow. In this case, the scale factor may decrease
    under 1 as an attempt to bring gradients to a number representable in the fp16
    dynamic range. While one may expect the scale to always be above 1, our GradScaler
    does NOT make this guarantee to maintain performance. If you encounter NaNs in
    your loss or gradients when running with AMP/fp16, verify your model is compatible.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: An instance `scaler` of [`GradScaler`](#torch.cuda.amp.GradScaler "torch.cuda.amp.GradScaler").
  prefs: []
  type: TYPE_NORMAL
- en: Helps perform the steps of gradient scaling conveniently.
  prefs: []
  type: TYPE_NORMAL
- en: '`scaler.scale(loss)` multiplies a given loss by `scaler`’s current scale factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scaler.step(optimizer)` safely unscales gradients and calls `optimizer.step()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scaler.update()` updates `scaler`’s scale factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: See the [Automatic Mixed Precision examples](notes/amp_examples.html#amp-examples)
    for usage (along with autocasting) in more complex cases like gradient clipping,
    gradient accumulation, gradient penalty, and multiple losses/optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: '`scaler` dynamically estimates the scale factor each iteration. To minimize
    gradient underflow, a large scale factor should be used. However, `float16` values
    can “overflow” (become inf or NaN) if the scale factor is too large. Therefore,
    the optimal scale factor is the largest factor that can be used without incurring
    inf or NaN gradient values. `scaler` approximates the optimal scale factor over
    time by checking the gradients for infs and NaNs during every `scaler.step(optimizer)`
    (or optional separate `scaler.unscale_(optimizer)`, see [`unscale_()`](#torch.cuda.amp.GradScaler.unscale_
    "torch.cuda.amp.GradScaler.unscale_")).'
  prefs: []
  type: TYPE_NORMAL
- en: If infs/NaNs are found, `scaler.step(optimizer)` skips the underlying `optimizer.step()`
    (so the params themselves remain uncorrupted) and `update()` multiplies the scale
    by `backoff_factor`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no infs/NaNs are found, `scaler.step(optimizer)` runs the underlying `optimizer.step()`
    as usual. If `growth_interval` unskipped iterations occur consecutively, `update()`
    multiplies the scale by `growth_factor`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scale factor often causes infs/NaNs to appear in gradients for the first
    few iterations as its value calibrates. `scaler.step` will skip the underlying
    `optimizer.step()` for these iterations. After that, step skipping should occur
    rarely (once every few hundred or thousand iterations).
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**init_scale** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional**,* *default=2.**16*) – Initial scale factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**growth_factor** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional**,* *default=2.0*) – Factor by which the scale
    is multiplied during [`update()`](#torch.cuda.amp.GradScaler.update "torch.cuda.amp.GradScaler.update")
    if no inf/NaN gradients occur for `growth_interval` consecutive iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**backoff_factor** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")*,* *optional**,* *default=0.5*) – Factor by which the scale
    is multiplied during [`update()`](#torch.cuda.amp.GradScaler.update "torch.cuda.amp.GradScaler.update")
    if inf/NaN gradients occur in an iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**growth_interval** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* *optional**,* *default=2000*) – Number of consecutive
    iterations without inf/NaN gradients that must occur for the scale to be multiplied
    by `growth_factor`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**enabled** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If `False`, disables gradient scaling. [`step()`](#torch.cuda.amp.GradScaler.step
    "torch.cuda.amp.GradScaler.step") simply invokes the underlying `optimizer.step()`,
    and other methods become no-ops. Default: `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Return a Python float containing the scale backoff factor.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[float](https://docs.python.org/3/library/functions.html#float "(in Python
    v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Return a Python float containing the scale growth factor.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[float](https://docs.python.org/3/library/functions.html#float "(in Python
    v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Return a Python int containing the growth interval.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[int](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Return a Python float containing the current scale, or 1.0 if scaling is disabled.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '[`get_scale()`](#torch.cuda.amp.GradScaler.get_scale "torch.cuda.amp.GradScaler.get_scale")
    incurs a CPU-GPU sync.'
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[float](https://docs.python.org/3/library/functions.html#float "(in Python
    v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Return a bool indicating whether this instance is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Load the scaler state.
  prefs: []
  type: TYPE_NORMAL
- en: If this instance is disabled, [`load_state_dict()`](#torch.cuda.amp.GradScaler.load_state_dict
    "torch.cuda.amp.GradScaler.load_state_dict") is a no-op.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**state_dict** ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)")) – scaler state. Should be an object returned from a call
    to [`state_dict()`](#torch.cuda.amp.GradScaler.state_dict "torch.cuda.amp.GradScaler.state_dict").'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Multiplies (‘scales’) a tensor or list of tensors by the scale factor.
  prefs: []
  type: TYPE_NORMAL
- en: Returns scaled outputs. If this instance of [`GradScaler`](#torch.cuda.amp.GradScaler
    "torch.cuda.amp.GradScaler") is not enabled, outputs are returned unmodified.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**outputs** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor") *or* *iterable*
    *of* *Tensors*) – Outputs to scale.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Set a new scale backoff factor.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**new_scale** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")) – Value to use as the new scale backoff factor.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Set a new scale growth factor.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**new_scale** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")) – Value to use as the new scale growth factor.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Set a new growth interval.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**new_interval** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – Value to use as the new growth interval.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Return the state of the scaler as a [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)").
  prefs: []
  type: TYPE_NORMAL
- en: 'It contains five entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"scale"` - a Python float containing the current scale'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"growth_factor"` - a Python float containing the current growth factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"backoff_factor"` - a Python float containing the current backoff factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"growth_interval"` - a Python int containing the current growth interval'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"_growth_tracker"` - a Python int containing the number of recent consecutive
    unskipped steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If this instance is not enabled, returns an empty dict.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you wish to checkpoint the scaler’s state after a particular iteration, [`state_dict()`](#torch.cuda.amp.GradScaler.state_dict
    "torch.cuda.amp.GradScaler.state_dict") should be called after [`update()`](#torch.cuda.amp.GradScaler.update
    "torch.cuda.amp.GradScaler.update").
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python
    v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)"), [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in
    Python v3.12)")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Invoke `unscale_(optimizer)` followed by parameter update, if gradients are
    not infs/NaN.
  prefs: []
  type: TYPE_NORMAL
- en: '[`step()`](#torch.cuda.amp.GradScaler.step "torch.cuda.amp.GradScaler.step")
    carries out the following two operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Internally invokes `unscale_(optimizer)` (unless [`unscale_()`](#torch.cuda.amp.GradScaler.unscale_
    "torch.cuda.amp.GradScaler.unscale_") was explicitly called for `optimizer` earlier
    in the iteration). As part of the [`unscale_()`](#torch.cuda.amp.GradScaler.unscale_
    "torch.cuda.amp.GradScaler.unscale_"), gradients are checked for infs/NaNs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If no inf/NaN gradients are found, invokes `optimizer.step()` using the unscaled
    gradients. Otherwise, `optimizer.step()` is skipped to avoid corrupting the params.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`*args` and `**kwargs` are forwarded to `optimizer.step()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Returns the return value of `optimizer.step(*args, **kwargs)`.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – Optimizer that applies the gradients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**args** ([*Any*](https://docs.python.org/3/library/typing.html#typing.Any
    "(in Python v3.12)")) – Any arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** ([*Any*](https://docs.python.org/3/library/typing.html#typing.Any
    "(in Python v3.12)")) – Any keyword arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")[[float](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")]'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Closure use is not currently supported.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Divides (“unscales”) the optimizer’s gradient tensors by the scale factor.
  prefs: []
  type: TYPE_NORMAL
- en: '[`unscale_()`](#torch.cuda.amp.GradScaler.unscale_ "torch.cuda.amp.GradScaler.unscale_")
    is optional, serving cases where you need to [modify or inspect gradients](notes/amp_examples.html#working-with-unscaled-gradients)
    between the backward pass(es) and [`step()`](#torch.cuda.amp.GradScaler.step "torch.cuda.amp.GradScaler.step").
    If [`unscale_()`](#torch.cuda.amp.GradScaler.unscale_ "torch.cuda.amp.GradScaler.unscale_")
    is not called explicitly, gradients will be unscaled automatically during [`step()`](#torch.cuda.amp.GradScaler.step
    "torch.cuda.amp.GradScaler.step").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple example, using [`unscale_()`](#torch.cuda.amp.GradScaler.unscale_ "torch.cuda.amp.GradScaler.unscale_")
    to enable clipping of unscaled gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – Optimizer that owns the gradients to be unscaled.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '[`unscale_()`](#torch.cuda.amp.GradScaler.unscale_ "torch.cuda.amp.GradScaler.unscale_")
    does not incur a CPU-GPU sync.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '[`unscale_()`](#torch.cuda.amp.GradScaler.unscale_ "torch.cuda.amp.GradScaler.unscale_")
    should only be called once per optimizer per [`step()`](#torch.cuda.amp.GradScaler.step
    "torch.cuda.amp.GradScaler.step") call, and only after all gradients for that
    optimizer’s assigned parameters have been accumulated. Calling [`unscale_()`](#torch.cuda.amp.GradScaler.unscale_
    "torch.cuda.amp.GradScaler.unscale_") twice for a given optimizer between each
    [`step()`](#torch.cuda.amp.GradScaler.step "torch.cuda.amp.GradScaler.step") triggers
    a RuntimeError.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '[`unscale_()`](#torch.cuda.amp.GradScaler.unscale_ "torch.cuda.amp.GradScaler.unscale_")
    may unscale sparse gradients out of place, replacing the `.grad` attribute.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Update the scale factor.
  prefs: []
  type: TYPE_NORMAL
- en: If any optimizer steps were skipped the scale is multiplied by `backoff_factor`
    to reduce it. If `growth_interval` unskipped iterations occurred consecutively,
    the scale is multiplied by `growth_factor` to increase it.
  prefs: []
  type: TYPE_NORMAL
- en: Passing `new_scale` sets the new scale value manually. (`new_scale` is not used
    directly, it’s used to fill GradScaler’s internal scale tensor. So if `new_scale`
    was a tensor, later in-place changes to that tensor will not further affect the
    scale GradScaler uses internally.)
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**new_scale** (float or `torch.cuda.FloatTensor`, optional, default=None) –
    New scale factor.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '[`update()`](#torch.cuda.amp.GradScaler.update "torch.cuda.amp.GradScaler.update")
    should only be called at the end of the iteration, after `scaler.step(optimizer)`
    has been invoked for all optimizers used this iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'For performance reasons, we do not check the scale factor value to avoid synchronizations,
    so the scale factor is not guaranteed to be above 1\. If the scale falls below
    1 and/or you are seeing NaNs in your gradients or loss, something is likely wrong.
    For example, bf16-pretrained models are often incompatible with AMP/fp16 due to
    differing dynamic ranges.  ## [Autocast Op Reference](#id6)[](#autocast-op-reference
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '### [Op Eligibility](#id7)'
  prefs: []
  type: TYPE_NORMAL
- en: Ops that run in `float64` or non-floating-point dtypes are not eligible, and
    will run in these types whether or not autocast is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Only out-of-place ops and Tensor methods are eligible. In-place variants and
    calls that explicitly supply an `out=...` Tensor are allowed in autocast-enabled
    regions, but won’t go through autocasting. For example, in an autocast-enabled
    region `a.addmm(b, c)` can autocast, but `a.addmm_(b, c)` and `a.addmm(b, c, out=d)`
    cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ops called with an explicit `dtype=...` argument are not eligible, and will
    produce output that respects the `dtype` argument.  ### [CUDA Op-Specific Behavior](#id8)[](#cuda-op-specific-behavior
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: The following lists describe the behavior of eligible ops in autocast-enabled
    regions. These ops always go through autocasting whether they are invoked as part
    of a [`torch.nn.Module`](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module"),
    as a function, or as a [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor")
    method. If functions are exposed in multiple namespaces, they go through autocasting
    regardless of the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Ops not listed below do not go through autocasting. They run in the type defined
    by their inputs. However, autocasting may still change the type in which unlisted
    ops run if they’re downstream from autocasted ops.
  prefs: []
  type: TYPE_NORMAL
- en: If an op is unlisted, we assume it’s numerically stable in `float16`. If you
    believe an unlisted op is numerically unstable in `float16`, please file an issue.
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA Ops that can autocast to `float16`](#id9)[](#cuda-ops-that-can-autocast-to-float16
    "Permalink to this heading")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`__matmul__`, `addbmm`, `addmm`, `addmv`, `addr`, `baddbmm`, `bmm`, `chain_matmul`,
    `multi_dot`, `conv1d`, `conv2d`, `conv3d`, `conv_transpose1d`, `conv_transpose2d`,
    `conv_transpose3d`, `GRUCell`, `linear`, `LSTMCell`, `matmul`, `mm`, `mv`, `prelu`,
    `RNNCell`'
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA Ops that can autocast to `float32`](#id10)[](#cuda-ops-that-can-autocast-to-float32
    "Permalink to this heading")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`__pow__`, `__rdiv__`, `__rpow__`, `__rtruediv__`, `acos`, `asin`, `binary_cross_entropy_with_logits`,
    `cosh`, `cosine_embedding_loss`, `cdist`, `cosine_similarity`, `cross_entropy`,
    `cumprod`, `cumsum`, `dist`, `erfinv`, `exp`, `expm1`, `group_norm`, `hinge_embedding_loss`,
    `kl_div`, `l1_loss`, `layer_norm`, `log`, `log_softmax`, `log10`, `log1p`, `log2`,
    `margin_ranking_loss`, `mse_loss`, `multilabel_margin_loss`, `multi_margin_loss`,
    `nll_loss`, `norm`, `normalize`, `pdist`, `poisson_nll_loss`, `pow`, `prod`, `reciprocal`,
    `rsqrt`, `sinh`, `smooth_l1_loss`, `soft_margin_loss`, `softmax`, `softmin`, `softplus`,
    `sum`, `renorm`, `tan`, `triplet_margin_loss`'
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA Ops that promote to the widest input type](#id11)[](#cuda-ops-that-promote-to-the-widest-input-type
    "Permalink to this heading")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These ops don’t require a particular dtype for stability, but take multiple
    inputs and require that the inputs’ dtypes match. If all of the inputs are `float16`,
    the op runs in `float16`. If any of the inputs is `float32`, autocast casts all
    inputs to `float32` and runs the op in `float32`.
  prefs: []
  type: TYPE_NORMAL
- en: '`addcdiv`, `addcmul`, `atan2`, `bilinear`, `cross`, `dot`, `grid_sample`, `index_put`,
    `scatter_add`, `tensordot`'
  prefs: []
  type: TYPE_NORMAL
- en: Some ops not listed here (e.g., binary ops like `add`) natively promote inputs
    without autocasting’s intervention. If inputs are a mixture of `float16` and `float32`,
    these ops run in `float32` and produce `float32` output, regardless of whether
    autocast is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '[Prefer `binary_cross_entropy_with_logits` over `binary_cross_entropy`](#id12)[](#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy
    "Permalink to this heading")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The backward passes of [`torch.nn.functional.binary_cross_entropy()`](generated/torch.nn.functional.binary_cross_entropy.html#torch.nn.functional.binary_cross_entropy
    "torch.nn.functional.binary_cross_entropy") (and [`torch.nn.BCELoss`](generated/torch.nn.BCELoss.html#torch.nn.BCELoss
    "torch.nn.BCELoss"), which wraps it) can produce gradients that aren’t representable
    in `float16`. In autocast-enabled regions, the forward input may be `float16`,
    which means the backward gradient must be representable in `float16` (autocasting
    `float16` forward inputs to `float32` doesn’t help, because that cast must be
    reversed in backward). Therefore, `binary_cross_entropy` and `BCELoss` raise an
    error in autocast-enabled regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many models use a sigmoid layer right before the binary cross entropy layer.
    In this case, combine the two layers using [`torch.nn.functional.binary_cross_entropy_with_logits()`](generated/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits
    "torch.nn.functional.binary_cross_entropy_with_logits") or [`torch.nn.BCEWithLogitsLoss`](generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss
    "torch.nn.BCEWithLogitsLoss"). `binary_cross_entropy_with_logits` and `BCEWithLogits`
    are safe to autocast.  ### [CPU Op-Specific Behavior](#id13)[](#cpu-op-specific-behavior
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: The following lists describe the behavior of eligible ops in autocast-enabled
    regions. These ops always go through autocasting whether they are invoked as part
    of a [`torch.nn.Module`](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module"),
    as a function, or as a [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor")
    method. If functions are exposed in multiple namespaces, they go through autocasting
    regardless of the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Ops not listed below do not go through autocasting. They run in the type defined
    by their inputs. However, autocasting may still change the type in which unlisted
    ops run if they’re downstream from autocasted ops.
  prefs: []
  type: TYPE_NORMAL
- en: If an op is unlisted, we assume it’s numerically stable in `bfloat16`. If you
    believe an unlisted op is numerically unstable in `bfloat16`, please file an issue.
  prefs: []
  type: TYPE_NORMAL
- en: '[CPU Ops that can autocast to `bfloat16`](#id14)[](#cpu-ops-that-can-autocast-to-bfloat16
    "Permalink to this heading")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`conv1d`, `conv2d`, `conv3d`, `bmm`, `mm`, `baddbmm`, `addmm`, `addbmm`, `linear`,
    `matmul`, `_convolution`'
  prefs: []
  type: TYPE_NORMAL
- en: '[CPU Ops that can autocast to `float32`](#id15)[](#cpu-ops-that-can-autocast-to-float32
    "Permalink to this heading")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`conv_transpose1d`, `conv_transpose2d`, `conv_transpose3d`, `avg_pool3d`, `binary_cross_entropy`,
    `grid_sampler`, `grid_sampler_2d`, `_grid_sampler_2d_cpu_fallback`, `grid_sampler_3d`,
    `polar`, `prod`, `quantile`, `nanquantile`, `stft`, `cdist`, `trace`, `view_as_complex`,
    `cholesky`, `cholesky_inverse`, `cholesky_solve`, `inverse`, `lu_solve`, `orgqr`,
    `inverse`, `ormqr`, `pinverse`, `max_pool3d`, `max_unpool2d`, `max_unpool3d`,
    `adaptive_avg_pool3d`, `reflection_pad1d`, `reflection_pad2d`, `replication_pad1d`,
    `replication_pad2d`, `replication_pad3d`, `mse_loss`, `ctc_loss`, `kl_div`, `multilabel_margin_loss`,
    `fft_fft`, `fft_ifft`, `fft_fft2`, `fft_ifft2`, `fft_fftn`, `fft_ifftn`, `fft_rfft`,
    `fft_irfft`, `fft_rfft2`, `fft_irfft2`, `fft_rfftn`, `fft_irfftn`, `fft_hfft`,
    `fft_ihfft`, `linalg_matrix_norm`, `linalg_cond`, `linalg_matrix_rank`, `linalg_solve`,
    `linalg_cholesky`, `linalg_svdvals`, `linalg_eigvals`, `linalg_eigvalsh`, `linalg_inv`,
    `linalg_householder_product`, `linalg_tensorinv`, `linalg_tensorsolve`, `fake_quantize_per_tensor_affine`,
    `eig`, `geqrf`, `lstsq`, `_lu_with_info`, `qr`, `solve`, `svd`, `symeig`, `triangular_solve`,
    `fractional_max_pool2d`, `fractional_max_pool3d`, `adaptive_max_pool3d`, `multilabel_margin_loss_forward`,
    `linalg_qr`, `linalg_cholesky_ex`, `linalg_svd`, `linalg_eig`, `linalg_eigh`,
    `linalg_lstsq`, `linalg_inv_ex`'
  prefs: []
  type: TYPE_NORMAL
- en: '[CPU Ops that promote to the widest input type](#id16)[](#cpu-ops-that-promote-to-the-widest-input-type
    "Permalink to this heading")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These ops don’t require a particular dtype for stability, but take multiple
    inputs and require that the inputs’ dtypes match. If all of the inputs are `bfloat16`,
    the op runs in `bfloat16`. If any of the inputs is `float32`, autocast casts all
    inputs to `float32` and runs the op in `float32`.
  prefs: []
  type: TYPE_NORMAL
- en: '`cat`, `stack`, `index_copy`'
  prefs: []
  type: TYPE_NORMAL
- en: Some ops not listed here (e.g., binary ops like `add`) natively promote inputs
    without autocasting’s intervention. If inputs are a mixture of `bfloat16` and
    `float32`, these ops run in `float32` and produce `float32` output, regardless
    of whether autocast is enabled.
  prefs: []
  type: TYPE_NORMAL
