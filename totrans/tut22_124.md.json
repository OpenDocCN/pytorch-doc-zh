["```py\ndef run_worker(rank, world_size):\n  r\"\"\"\n A wrapper function that initializes RPC, calls the function, and shuts down\n RPC.\n \"\"\"\n\n    # We need to use different port numbers in TCP init_method for init_rpc and\n    # init_process_group to avoid port conflicts.\n    rpc_backend_options = TensorPipeRpcBackendOptions()\n    rpc_backend_options.init_method = \"tcp://localhost:29501\"\n\n    # Rank 2 is master, 3 is ps and 0 and 1 are trainers.\n    if rank == 2:\n        rpc.init_rpc(\n            \"master\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n\n        remote_emb_module = RemoteModule(\n            \"ps\",\n            torch.nn.EmbeddingBag,\n            args=(NUM_EMBEDDINGS, EMBEDDING_DIM),\n            kwargs={\"mode\": \"sum\"},\n        )\n\n        # Run the training loop on trainers.\n        futs = []\n        for trainer_rank in [0, 1]:\n            trainer_name = \"trainer{}\".format(trainer_rank)\n            fut = rpc.rpc_async(\n                trainer_name, _run_trainer, args=(remote_emb_module, trainer_rank)\n            )\n            futs.append(fut)\n\n        # Wait for all training to finish.\n        for fut in futs:\n            fut.wait()\n    elif rank <= 1:\n        # Initialize process group for Distributed DataParallel on trainers.\n        dist.init_process_group(\n            backend=\"gloo\", rank=rank, world_size=2, init_method=\"tcp://localhost:29500\"\n        )\n\n        # Initialize RPC.\n        trainer_name = \"trainer{}\".format(rank)\n        rpc.init_rpc(\n            trainer_name,\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n\n        # Trainer just waits for RPCs from master.\n    else:\n        rpc.init_rpc(\n            \"ps\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n        # parameter server do nothing\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\nif __name__ == \"__main__\":\n    # 2 trainers, 1 parameter server, 1 master.\n    world_size = 4\n    mp.spawn(run_worker, args=(world_size,), nprocs=world_size, join=True) \n```", "```py\nclass HybridModel(torch.nn.Module):\n  r\"\"\"\n The model consists of a sparse part and a dense part.\n 1) The dense part is an nn.Linear module that is replicated across all trainers using DistributedDataParallel.\n 2) The sparse part is a Remote Module that holds an nn.EmbeddingBag on the parameter server.\n This remote model can get a Remote Reference to the embedding table on the parameter server.\n \"\"\"\n\n    def __init__(self, remote_emb_module, device):\n        super(HybridModel, self).__init__()\n        self.remote_emb_module = remote_emb_module\n        self.fc = DDP(torch.nn.Linear(16, 8).cuda(device), device_ids=[device])\n        self.device = device\n\n    def forward(self, indices, offsets):\n        emb_lookup = self.remote_emb_module.forward(indices, offsets)\n        return self.fc(emb_lookup.cuda(self.device)) \n```", "```py\ndef _run_trainer(remote_emb_module, rank):\n  r\"\"\"\n Each trainer runs a forward pass which involves an embedding lookup on the\n parameter server and running nn.Linear locally. During the backward pass,\n DDP is responsible for aggregating the gradients for the dense part\n (nn.Linear) and distributed autograd ensures gradients updates are\n propagated to the parameter server.\n \"\"\"\n\n    # Setup the model.\n    model = HybridModel(remote_emb_module, rank)\n\n    # Retrieve all model parameters as rrefs for DistributedOptimizer.\n\n    # Retrieve parameters for embedding table.\n    model_parameter_rrefs = model.remote_emb_module.remote_parameters()\n\n    # model.fc.parameters() only includes local parameters.\n    # NOTE: Cannot call model.parameters() here,\n    # because this will call remote_emb_module.parameters(),\n    # which supports remote_parameters() but not parameters().\n    for param in model.fc.parameters():\n        model_parameter_rrefs.append(RRef(param))\n\n    # Setup distributed optimizer\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model_parameter_rrefs,\n        lr=0.05,\n    )\n\n    criterion = torch.nn.CrossEntropyLoss() \n```", "```py\n def get_next_batch(rank):\n        for _ in range(10):\n            num_indices = random.randint(20, 50)\n            indices = torch.LongTensor(num_indices).random_(0, NUM_EMBEDDINGS)\n\n            # Generate offsets.\n            offsets = []\n            start = 0\n            batch_size = 0\n            while start < num_indices:\n                offsets.append(start)\n                start += random.randint(1, 10)\n                batch_size += 1\n\n            offsets_tensor = torch.LongTensor(offsets)\n            target = torch.LongTensor(batch_size).random_(8).cuda(rank)\n            yield indices, offsets_tensor, target\n\n    # Train for 100 epochs\n    for epoch in range(100):\n        # create distributed autograd context\n        for indices, offsets, target in get_next_batch(rank):\n            with dist_autograd.context() as context_id:\n                output = model(indices, offsets)\n                loss = criterion(output, target)\n\n                # Run distributed backward pass\n                dist_autograd.backward(context_id, [loss])\n\n                # Tun distributed optimizer\n                opt.step(context_id)\n\n                # Not necessary to zero grads as each iteration creates a different\n                # distributed autograd context which hosts different grads\n        print(\"Training done for epoch {}\".format(epoch)) \n```"]