- en: Tensor Parallelism - torch.distributed.tensor.parallel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量并行 - torch.distributed.tensor.parallel
- en: 原文：[https://pytorch.org/docs/stable/distributed.tensor.parallel.html](https://pytorch.org/docs/stable/distributed.tensor.parallel.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/distributed.tensor.parallel.html](https://pytorch.org/docs/stable/distributed.tensor.parallel.html)
- en: 'Tensor Parallelism(TP) is built on top of the PyTorch DistributedTensor ([DTensor](https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md))
    and provides different parallelism styles: Colwise and Rowwise Parallelism.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行（TP）建立在PyTorch分布式张量（[DTensor](https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md)）之上，并提供不同的并行化样式：列并行和行并行。
- en: Warning
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Tensor Parallelism APIs are experimental and subject to change.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行API是实验性的，可能会发生变化。
- en: 'The entrypoint to parallelize your `nn.Module` using Tensor Parallelism is:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用张量并行并行化您的`nn.Module`的入口点是：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Apply Tensor Parallelism in PyTorch by parallelizing modules or sub-modules
    based on a user-specified plan.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通过根据用户指定的计划并行化模块或子模块来应用PyTorch中的张量并行。
- en: We parallelize module or sub_modules based on a parallelize_plan. The parallelize_plan
    contains `ParallelStyle`, which indicates how user wants the module or sub_module
    to be parallelized.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据并行化计划并行化模块或子模块。并行化计划包含`ParallelStyle`，指示用户希望如何并行化模块或子模块。
- en: User can also specify different parallel style per module fully qualified name
    (FQN).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 用户还可以根据模块的完全限定名称（FQN）指定不同的并行样式。
- en: Note that `parallelize_module` only accepts a 1-D `DeviceMesh`, if you have
    a 2-D or N-D `DeviceMesh`, slice the DeviceMesh to a 1-D sub DeviceMesh first
    then pass to this API(i.e. `device_mesh["tp"]`)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`parallelize_module`仅接受1-D的`DeviceMesh`，如果您有2-D或N-D的`DeviceMesh`，请先将DeviceMesh切片为1-D子DeviceMesh，然后将其传递给此API（即`device_mesh["tp"]`）
- en: Parameters
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**module** (`nn.Module`) – Module to be parallelized.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module**（`nn.Module`）- 要并行化的模块。'
- en: '**device_mesh** (`DeviceMesh`) – Object which describes the mesh topology of
    devices for the DTensor.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device_mesh**（`DeviceMesh`）- 描述DTensor设备网格拓扑的对象。'
- en: '**parallelize_plan** (Union[`ParallelStyle`, Dict[str, `ParallelStyle`]]) –
    The plan used to parallelize the module. It can be either a `ParallelStyle` object
    which contains how we prepare input/output for Tensor Parallelism or it can be
    a dict of module FQN and its corresponding `ParallelStyle` object.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**parallelize_plan**（Union[`ParallelStyle`, Dict[str, `ParallelStyle`]]) –
    用于并行化模块的计划。可以是一个包含我们如何为张量并行准备输入/输出的`ParallelStyle`对象，也可以是模块FQN及其对应的`ParallelStyle`对象的字典。'
- en: '**tp_mesh_dim** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* *deprecated*) – The dimension of `device_mesh` where we
    perform Tensor Parallelism on, this field is deprecated and will be removed in
    future. If you have a 2-D or N-D `DeviceMesh`, consider passing in device_mesh[“tp”]'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tp_mesh_dim**（[*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* *已弃用*）- 在其中执行张量并行的`device_mesh`的维度，此字段已弃用，并将在将来删除。如果您有一个2-D或N-D的`DeviceMesh`，请考虑传递device_mesh[“tp”]'
- en: Returns
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A `nn.Module` object parallelized.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`nn.Module`对象并行化。
- en: Return type
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Module*](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.modules.module.Module")'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Module*](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.modules.module.Module")'
- en: 'Example::'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '示例::'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For complex module architecture like Attention, MLP layers, we recommend composing
    different ParallelStyles together (i.e. `ColwiseParallel` and `RowwiseParallel`)
    and pass as a parallelize_plan, to achieves the desired sharding computation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像Attention、MLP层这样的复杂模块架构，我们建议将不同的ParallelStyles组合在一起（即`ColwiseParallel`和`RowwiseParallel`），并将其作为并行化计划传递，以实现所需的分片计算。
- en: 'Tensor Parallelism supports the following parallel styles:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行支持以下并行样式：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Partition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear
    and nn.Embedding. Users can compose it together with RowwiseParallel to achieve
    the sharding of more complicated modules. (i.e. MLP, Attention)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以行方式对兼容的nn.Module进行分区。目前支持nn.Linear和nn.Embedding。用户可以将其与RowwiseParallel组合在一起，以实现更复杂模块的分片（即MLP、Attention）
- en: Keyword Arguments
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 关键字参数
- en: '**input_layouts** (*Placement**,* *optional*) – The DTensor layout of input
    tensor for the nn.Module, this is used to annotate the input tensor to become
    a DTensor. If not specified, we assume the input tensor to be replicated.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_layouts**（*Placement**,* *可选*）- 用于nn.Module的输入张量的DTensor布局，用于注释输入张量以成为DTensor。如果未指定，则我们假定输入张量是复制的。'
- en: '**output_layouts** (*Placement**,* *optional*) – The DTensor layout of the
    output for the nn.Module, this is used to ensure the output of the nn.Module with
    the user desired layout. If not specified, the output tensor is sharded on the
    last dimension.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_layouts**（*Placement**,* *可选*）- 用于nn.Module输出的DTensor布局，用于确保nn.Module的输出具有用户期望的布局。如果未指定，则输出张量在最后一个维度上进行分片。'
- en: '**use_local_output** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Whether to use local [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor") instead of `DTensor` for the module output, default: True.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**use_local_output**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *可选*）- 是否使用本地[`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor")而不是`DTensor`作为模块输出，默认值为True。'
- en: Returns
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A `ParallelStyle` object that represents Colwise sharding of the nn.Module.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表示nn.Module的Colwise分片的`ParallelStyle`对象。
- en: 'Example::'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '示例::'
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: By default `ColwiseParallel` output is sharded on the last dimension if the
    `output_layouts` not specified, if there’re operators that require specific tensor
    shape (i.e. before the paired `RowwiseParallel`), keep in mind that if the output
    is sharded the operator might need to be adjusted to the sharded size.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，如果未指定`output_layouts`，则`ColwiseParallel`输出在最后一个维度上进行分片，如果有需要特定张量形状的运算符（即在配对的`RowwiseParallel`之前），请记住，如果输出被分片，运算符可能需要调整为分片大小。
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Partition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear
    only. Users can compose it with ColwiseParallel to achieve the sharding of more
    complicated modules. (i.e. MLP, Attention)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将兼容的nn.Module按行划分。目前仅支持nn.Linear。用户可以将其与ColwiseParallel组合，以实现更复杂模块的分片（即MLP，Attention）
- en: Keyword Arguments
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 关键参数
- en: '**input_layouts** (*Placement**,* *optional*) – The DTensor layout of input
    tensor for the nn.Module, this is used to annotate the input tensor to become
    a DTensor. If not specified, we assume the input tensor to be sharded on the last
    dimension.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_layouts** (*Placement**,* *optional*) – nn.Module的输入张量的DTensor布局，用于注释输入张量以成为DTensor。如果未指定，我们假定输入张量在最后一个维度上被分片。'
- en: '**output_layouts** (*Placement**,* *optional*) – The DTensor layout of the
    output for the nn.Module, this is used to ensure the output of the nn.Module with
    the user desired layout. If not specified, the output tensor is replicated.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_layouts** (*Placement**,* *optional*) – nn.Module输出的DTensor布局，用于确保nn.Module的输出具有用户期望的布局。如果未指定，则输出张量将被复制。'
- en: '**use_local_output** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Whether to use local [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor") instead of `DTensor` for the module output, default: True.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**use_local_output** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – 是否使用本地[`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor")而不是`DTensor`作为模块输出，默认值为True。'
- en: Returns
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A `ParallelStyle` object that represents Rowwise sharding of the nn.Module.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 代表nn.Module的Rowwise分片的`ParallelStyle`对象。
- en: 'Example::'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '示例::'
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To simply configure the nn.Module’s inputs and outputs with DTensor layouts
    and perform necessary layout redistributions, without distribute the module parameters
    to DTensors, the following classes can be used in the `parallelize_plan` of `parallelize_module`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要简单配置nn.Module的输入和输出以及执行必要的布局重分配，而不将模块参数分发到DTensors，可以在`parallelize_module`的`parallelize_plan`中使用以下类：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Configure the nn.Module’s inputs to convert the input tensors of the nn.Module
    to DTensors at runtime according to `input_layouts`, and perform layout redistribution
    according to the `desired_input_layouts`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`input_layouts`配置nn.Module的输入，根据`desired_input_layouts`执行布局重分配，将nn.Module的输入张量转换为DTensors。
- en: Keyword Arguments
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 关键参数
- en: '**input_layouts** (*Union**[**Placement**,* *Tuple**[**Placement**]**]*) –
    The DTensor layouts of input tensors for the nn.Module, this is used to convert
    the input tensors to DTensors. If some inputs are not torch.Tensor or no need
    to convert to DTensors, `None` need to be specified as a placeholder.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_layouts** (*Union**[**Placement**,* *Tuple**[**Placement**]**]*) –
    nn.Module的输入张量的DTensor布局，用于将输入张量转换为DTensors。如果某些输入不是torch.Tensor或不需要转换为DTensors，则需要指定`None`作为占位符。'
- en: '**desired_input_layouts** (*Union**[**Placement**,* *Tuple**[**Placement**]**]*)
    – The desired DTensor layout of input tensors for the nn.Module, this is used
    to ensure the inputs of the nn.Module have the desired DTensor layouts. This argument
    needs to have the same length with `input_layouts`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**desired_input_layouts** (*Union**[**Placement**,* *Tuple**[**Placement**]**]*)
    – nn.Module输入张量的期望DTensor布局，用于确保nn.Module的输入具有期望的DTensor布局。此参数需要与`input_layouts`具有相同的长度。'
- en: '**use_local_output** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Whether to use local [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor") instead of `DTensor` for the module inputs, default: False.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**use_local_output** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – 是否使用本地[`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor")而不是`DTensor`作为模块输入，默认值为False。'
- en: Returns
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A `ParallelStyle` object that prepares the sharding layouts of the nn.Module’s
    inputs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 准备nn.Module输入的分片布局的`ParallelStyle`对象。
- en: 'Example::'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '示例::'
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Configure the nn.Module’s outputs to convert the output tensors of the nn.Module
    to DTensors at runtime according to `output_layouts`, and perform layout redistribution
    according to the `desired_output_layouts`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`output_layouts`配置nn.Module的输出，根据`desired_output_layouts`执行布局重分配，将nn.Module的输出张量转换为DTensors。
- en: Keyword Arguments
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 关键参数
- en: '**output_layouts** (*Union**[**Placement**,* *Tuple**[**Placement**]**]*) –
    The DTensor layouts of output tensors for the nn.Module, this is used to convert
    the output tensors to DTensors if they are [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor"). If some outputs are not torch.Tensor or no need to convert to
    DTensors, `None` need to be specified as a placeholder.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_layouts** (*Union**[**Placement**,* *Tuple**[**Placement**]**]*) –
    nn.Module输出张量的DTensor布局，用于将输出张量转换为DTensors（如果它们是[`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor")）。如果某些输出不是torch.Tensor或不需要转换为DTensors，则需要指定`None`作为占位符。'
- en: '**desired_output_layouts** (*Union**[**Placement**,* *Tuple**[**Placement**]**]*)
    – The desired DTensor layouts of output tensors for the nn.Module, this is used
    to ensure the outputs of the nn.Module have the desired DTensor layouts.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**desired_output_layouts** (*Union**[**Placement**,* *Tuple**[**Placement**]**]*)
    – nn.Module输出张量的期望DTensor布局，用于确保nn.Module的输出具有期望的DTensor布局。'
- en: '**use_local_output** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Whether to use local [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor") instead of `DTensor` for the module outputs, default: False.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**use_local_output** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – 是否使用本地[`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor")而不是`DTensor`作为模块输出，默认值为False。'
- en: Returns
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A ParallelStyle object that prepares the sharding layouts of the nn.Module’s
    outputs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 准备nn.Module输出的分片布局的`ParallelStyle`对象。
- en: 'Example::'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '示例::'
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For models like Transformer, we recommend users to use `ColwiseParallel` and
    `RowwiseParallel` together in the parallelize_plan for achieve the desired sharding
    for the entire model (i.e. Attention and MLP).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Transformer等模型，我们建议用户在`parallelize_plan`中同时使用`ColwiseParallel`和`RowwiseParallel`来实现整个模型的期望分片（即Attention和MLP）。
