- en: FullyShardedDataParallel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FullyShardedDataParallel
- en: 原文：[https://pytorch.org/docs/stable/fsdp.html](https://pytorch.org/docs/stable/fsdp.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/fsdp.html](https://pytorch.org/docs/stable/fsdp.html)
- en: '[PRE0]'
  id: totrans-2
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A wrapper for sharding module parameters across data parallel workers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在数据并行工作者之间分片模块参数的包装器。
- en: This is inspired by [Xu et al.](https://arxiv.org/abs/2004.13336) as well as
    the ZeRO Stage 3 from [DeepSpeed](https://www.deepspeed.ai/). FullyShardedDataParallel
    is commonly shortened to FSDP.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这受到[Xu等人](https://arxiv.org/abs/2004.13336)以及[DeepSpeed](https://www.deepspeed.ai/)的ZeRO阶段3的启发。FullyShardedDataParallel通常缩写为FSDP。
- en: 'Example:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Warning
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: The optimizer must be initialized *after* the module has been wrapped with FSDP
    since FSDP will shard and transform the module’s parameters in a way that may
    not preserve the original parameter variables. Thus, the previously initialized
    optimizer may have stale references to the parameters.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器必须在模块被FSDP包装之后初始化，因为FSDP将以一种可能不保留原始参数变量的方式对模块的参数进行分片和转换。因此，先前初始化的优化器可能会对参数有过时的引用。
- en: Warning
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: If the destination CUDA device has ID `dev_id`, either (1) `module` should already
    be placed on that device, (2) the device should be set using `torch.cuda.set_device(dev_id)`,
    or (3) `dev_id` should be passed into the `device_id` constructor argument. This
    FSDP instance’s compute device will be that destination device. For (1) and (3),
    the FSDP initialization always occurs on GPU. For (2), the FSDP initialization
    happens on `module` ‘s current device, which may be CPU.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标CUDA设备的ID为`dev_id`，则（1）`module`应该已经放置在该设备上，（2）可以使用`torch.cuda.set_device(dev_id)`设置设备，或者（3）应该将`dev_id`传递给`device_id`构造函数参数。此FSDP实例的计算设备将是该目标设备。对于（1）和（3），FSDP初始化始终在GPU上进行。对于（2），FSDP初始化发生在`module`当前的设备上，可能是CPU。
- en: Warning
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: FSDP currently does not support gradient accumulation outside `no_sync()` when
    using CPU offloading. Trying to do so yields incorrect results since FSDP will
    use the newly-reduced gradient instead of accumulating with any existing gradient.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用CPU卸载时，FSDP当前不支持在`no_sync()`之外支持梯度累积。尝试这样做会产生不正确的结果，因为FSDP将使用新减少的梯度而不是与任何现有梯度累积。
- en: Warning
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Changing the original parameter variable names after construction will lead
    to undefined behavior.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造之后更改原始参数变量名称将导致未定义的行为。
- en: Warning
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Passing in the `sync_module_states=True` flag requires `module` to be on GPU
    or to use the `device_id` argument to specify a CUDA device that FSDP will move
    `module` to in the FSDP constructor. This is because `sync_module_states=True`
    requires GPU communication.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 传递`sync_module_states=True`标志需要`module`在GPU上或使用`device_id`参数来指定FSDP将`module`移动到的CUDA设备。这是因为`sync_module_states=True`需要GPU通信。
- en: Warning
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: As of PyTorch 1.12, FSDP only offers limited support for shared parameters (for
    example, setting one `Linear` layer’s weight to another’s). In particular, modules
    that share parameters must be wrapped as part of the same FSDP unit. If enhanced
    shared parameter support is needed for your use case, please ping [https://github.com/pytorch/pytorch/issues/77724](https://github.com/pytorch/pytorch/issues/77724)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 截至PyTorch 1.12，FSDP仅对共享参数提供有限支持（例如，将一个`Linear`层的权重设置为另一个的）。特别是，共享参数的模块必须作为同一FSDP单元的一部分进行包装。如果您的用例需要增强的共享参数支持，请访问[https://github.com/pytorch/pytorch/issues/77724](https://github.com/pytorch/pytorch/issues/77724)
- en: Warning
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: FSDP has some constraints on freezing parameters (i.e. setting `param.requires_grad=False`).
    For `use_orig_params=False`, each FSDP instance must manage parameters that are
    all frozen or all non-frozen. For `use_orig_params=True`, FSDP supports mixing
    frozen and non-frozen, but we recommend not doing so since then the gradient memory
    usage will be higher than expected (namely, equivalent to not freezing those parameters).
    This means that ideally, frozen parameters should be isolated into their own `nn.Module`
    s and wrapped separately with FSDP.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: FSDP对冻结参数（即设置`param.requires_grad=False`）有一些约束。对于`use_orig_params=False`，每个FSDP实例必须管理所有冻结或所有非冻结的参数。对于`use_orig_params=True`，FSDP支持混合冻结和非冻结，但我们建议不要这样做，因为梯度内存使用量将高于预期（即等同于不冻结这些参数）。这意味着理想情况下，冻结参数应该被隔离到自己的`nn.Module`中，并分别用FSDP包装。
- en: Note
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Attempting to run the forward pass of a submodule that is contained in an FSDP
    instance is not supported and will result in errors. This is because the submodule’s
    parameters will be sharded, but it itself is not an FSDP instance, so its forward
    pass will not all-gather the full parameters appropriately. This could potentially
    happen when attempting to run only the encoder of a encoder-decoder model, and
    the encoder is not wrapped in its own FSDP instance. To resolve this, please wrap
    the submodule in its own FSDP unit.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行包含在FSDP实例中的子模块的前向传递不受支持，将导致错误。这是因为子模块的参数将被分片，但它本身不是FSDP实例，因此其前向传递将不会适当地聚集所有参数。当尝试仅运行编码器-解码器模型的编码器时，可能会发生这种情况，并且编码器未包装在自己的FSDP实例中。要解决此问题，请将子模块包装在自己的FSDP单元中。
- en: Note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: FSDP moves input tensors to the `forward` method to the GPU compute device,
    so the user does not need to manually move them from CPU.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: FSDP将输入张量移动到GPU计算设备的`forward`方法中，因此用户不需要手动将它们从CPU移动。
- en: Warning
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: The user should not modify the parameters between forward and backward without
    using the [`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params") context
    since the modifications may not persist. Moreover, for `use_orig_params=False`,
    accessing the original parameters between forward and backward may raise an illegal
    memory access.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 用户不应在前向和后向之间修改参数，而不使用[`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params")上下文，因为修改可能不会持久。此外，对于`use_orig_params=False`，在前向和后向之间访问原始参数可能会引发非法内存访问。
- en: Warning
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: For `use_orig_params=True`, `ShardingStrategy.SHARD_GRAD_OP` exposes the unsharded
    parameters, not the sharded parameters, after forward since it does not free the
    unsharded ones, unlike `ShardingStrategy.FULL_SHARD`. One caveat is that, since
    gradients are always sharded or `None`, `ShardingStrategy.SHARD_GRAD_OP` will
    not expose the sharded gradients with the unsharded parameters after forward.
    If you want to inspect the gradients, try [`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params") with `with_grads=True`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`use_orig_params=True`，`ShardingStrategy.SHARD_GRAD_OP`在前向传播后暴露未分片的参数，而不是分片的参数，因为它不释放未分片的参数，不像`ShardingStrategy.FULL_SHARD`。一个注意事项是，由于梯度总是被分片或为`None`，`ShardingStrategy.SHARD_GRAD_OP`在前向传播后不会暴露带有未分片参数的分片梯度。如果要检查梯度，请尝试使用`with_grads=True`调用[`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params")。
- en: Warning
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: FSDP replaces managed modules’ parameters with `torch.Tensor` views during forward
    and backward computation for autograd-related reasons. If your module’s forward
    relies on saved references to the parameters instead of reacquiring the references
    each iteration, then it will not see FSDP’s newly created views, and autograd
    will not work correctly.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: FSDP在前向和后向计算期间用`torch.Tensor`视图替换托管模块的参数，出于自动求导相关原因。如果您的模块的前向依赖于对参数的保存引用，而不是在每次迭代中重新获取引用，则它将看不到FSDP新创建的视图，并且自动求导将无法正常工作。
- en: Note
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: With `limit_all_gathers=True`, you may see a gap in the FSDP pre-forward where
    the CPU thread is not issuing any kernels. This is intentional and shows the rate
    limiter in effect. Synchronizing the CPU thread in that way prevents over-allocating
    memory for subsequent all-gathers, and it should not actually delay GPU kernel
    execution.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`limit_all_gathers=True`，您可能会看到FSDP在前向传播中存在一个CPU线程不发出任何内核的间隙。这是有意的，并显示了速率限制器的效果。以这种方式同步CPU线程可以防止为后续全聚合过度分配内存，并且实际上不应延迟GPU内核执行。
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When using `sharding_strategy=ShardingStrategy.HYBRID_SHARD` with the sharding
    process group being intra-node and the replication process group being inter-node,
    setting `NCCL_CROSS_NIC=1` can help improve the all-reduce times over the replication
    process group for some cluster setups.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`sharding_strategy=ShardingStrategy.HYBRID_SHARD`，分片进程组为节点内，复制进程组为节点间时，设置`NCCL_CROSS_NIC=1`可以帮助改善某些集群设置下复制进程组的全聚合时间。
- en: Parameters
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**module** ([*nn.Module*](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module"))
    – This is the module to be wrapped with FSDP.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module** ([*nn.Module*](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module"))
    – 这是要用FSDP包装的模块。'
- en: '**process_group** (*Optional**[**Union**[**ProcessGroup**,* *Tuple**[**ProcessGroup**,*
    *ProcessGroup**]**]**]*) – This is the process group over which the model is sharded
    and thus the one used for FSDP’s all-gather and reduce-scatter collective communications.
    If `None`, then FSDP uses the default process group. For hybrid sharding strategies
    such as `ShardingStrategy.HYBRID_SHARD`, users can pass in a tuple of process
    groups, representing the groups over which to shard and replicate, respectively.
    If `None`, then FSDP constructs process groups for the user to shard intra-node
    and replicate inter-node. (Default: `None`)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**process_group** (*可选**[**Union**[**ProcessGroup**,* *Tuple**[**ProcessGroup**,*
    *ProcessGroup**]**]**]*) – 这是模型被分片的进程组，因此也是FSDP的全聚合和减少散播集体通信所使用的进程组。如果为`None`，则FSDP使用默认进程组。对于混合分片策略，如`ShardingStrategy.HYBRID_SHARD`，用户可以传入一个进程组的元组，分别表示分片和复制的组。如果为`None`，则FSDP为用户构建进程组，以便在节点内进行分片和在节点间进行复制。（默认值：`None`）'
- en: '**sharding_strategy** (*Optional**[*[*ShardingStrategy*](#torch.distributed.fsdp.ShardingStrategy
    "torch.distributed.fsdp.ShardingStrategy")*]*) – This configures the sharding
    strategy, which may trade off memory saving and communication overhead. See [`ShardingStrategy`](#torch.distributed.fsdp.ShardingStrategy
    "torch.distributed.fsdp.ShardingStrategy") for details. (Default: `FULL_SHARD`)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sharding_strategy** (*可选**[*[*ShardingStrategy*](#torch.distributed.fsdp.ShardingStrategy
    "torch.distributed.fsdp.ShardingStrategy")*]*) – 这配置了分片策略，可能会权衡内存节省和通信开销。详细信息请参见[`ShardingStrategy`](#torch.distributed.fsdp.ShardingStrategy
    "torch.distributed.fsdp.ShardingStrategy")。（默认值：`FULL_SHARD`）'
- en: '**cpu_offload** (*Optional**[*[*CPUOffload*](#torch.distributed.fsdp.CPUOffload
    "torch.distributed.fsdp.CPUOffload")*]*) – This configures CPU offloading. If
    this is set to `None`, then no CPU offloading happens. See [`CPUOffload`](#torch.distributed.fsdp.CPUOffload
    "torch.distributed.fsdp.CPUOffload") for details. (Default: `None`)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cpu_offload** (*可选**[*[*CPUOffload*](#torch.distributed.fsdp.CPUOffload "torch.distributed.fsdp.CPUOffload")*]*)
    – 这配置了CPU卸载。如果设置为`None`，则不会发生CPU卸载。详细信息请参见[`CPUOffload`](#torch.distributed.fsdp.CPUOffload
    "torch.distributed.fsdp.CPUOffload")。（默认值：`None`）'
- en: '**auto_wrap_policy** (*Optional**[**Union**[**Callable**[**[*[*nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*,* [*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*]**,* [*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*]**,* *ModuleWrapPolicy**,* *CustomPolicy**]**]*) –'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**auto_wrap_policy** (*可选**[**Union**[**Callable**[**[*[*nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*,* [*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*]**,* [*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*]**,* *ModuleWrapPolicy**,* *CustomPolicy**]**]*) –'
- en: 'This specifies a policy to apply FSDP to submodules of `module`, which is needed
    for communication and computation overlap and thus affects performance. If `None`,
    then FSDP only applies to `module`, and users should manually apply FSDP to parent
    modules themselves (proceeding bottom-up). For convenience, this accepts `ModuleWrapPolicy`
    directly, which allows users to specify the module classes to wrap (e.g. the transformer
    block). Otherwise, this should be a callable that takes in three arguments `module:
    nn.Module`, `recurse: bool`, and `nonwrapped_numel: int` and should return a `bool`
    specifying whether the passed-in `module` should have FSDP applied if `recurse=False`
    or if the traversal should continue into the module’s subtree if `recurse=True`.
    Users may add additional arguments to the callable. The `size_based_auto_wrap_policy`
    in `torch.distributed.fsdp.wrap.py` gives an example callable that applies FSDP
    to a module if the parameters in its subtree exceed 100M numel. We recommend printing
    the model after applying FSDP and adjusting as needed.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '这指定了一个策略，将FSDP应用于`module`的子模块，这对通信和计算重叠至关重要，从而影响性能。如果为`None`，则FSDP仅应用于`module`，用户应手动将FSDP应用于父模块（自下而上进行）。为方便起见，这直接接受`ModuleWrapPolicy`，允许用户指定要包装的模块类（例如变换器块）。否则，这应该是一个可调用对象，接受三个参数`module:
    nn.Module`、`recurse: bool`和`nonwrapped_numel: int`，并应返回一个`bool`，指定是否应在`recurse=False`时应用FSDP到传入的`module`，或者如果`recurse=True`，遍历应继续到模块的子树。用户可以向可调用对象添加其他参数。`torch.distributed.fsdp.wrap.py`中的`size_based_auto_wrap_policy`提供了一个示例可调用对象，如果其子树中的参数超过100M个元素，则将FSDP应用于模块。我们建议在应用FSDP后打印模型，并根据需要进行调整。'
- en: 'Example:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**backward_prefetch** (*Optional**[*[*BackwardPrefetch*](#torch.distributed.fsdp.BackwardPrefetch
    "torch.distributed.fsdp.BackwardPrefetch")*]*) – This configures explicit backward
    prefetching of all-gathers. If `None`, then FSDP does not backward prefetch, and
    there is no communication and computation overlap in the backward pass. See [`BackwardPrefetch`](#torch.distributed.fsdp.BackwardPrefetch
    "torch.distributed.fsdp.BackwardPrefetch") for details. (Default: `BACKWARD_PRE`)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**backward_prefetch** (*可选**[*[*BackwardPrefetch*](#torch.distributed.fsdp.BackwardPrefetch
    "torch.distributed.fsdp.BackwardPrefetch")*]*) – 这配置了所有gather的显式向后预取。如果为`None`，则FSDP不进行向后预取，在向后传递中没有通信和计算重叠。详细信息请参见[`BackwardPrefetch`](#torch.distributed.fsdp.BackwardPrefetch
    "torch.distributed.fsdp.BackwardPrefetch")。（默认值：`BACKWARD_PRE`）'
- en: '**mixed_precision** (*Optional**[*[*MixedPrecision*](#torch.distributed.fsdp.MixedPrecision
    "torch.distributed.fsdp.MixedPrecision")*]*) – This configures native mixed precision
    for FSDP. If this is set to `None`, then no mixed precision is used. Otherwise,
    parameter, buffer, and gradient reduction dtypes can be set. See [`MixedPrecision`](#torch.distributed.fsdp.MixedPrecision
    "torch.distributed.fsdp.MixedPrecision") for details. (Default: `None`)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mixed_precision** (*可选**[*[*MixedPrecision*](#torch.distributed.fsdp.MixedPrecision
    "torch.distributed.fsdp.MixedPrecision")*]*) – 这配置了FSDP的本机混合精度。如果设置为`None`，则不使用混合精度。否则，可以设置参数、缓冲区和梯度减少的数据类型。详细信息请参见[`MixedPrecision`](#torch.distributed.fsdp.MixedPrecision
    "torch.distributed.fsdp.MixedPrecision")。（默认值：`None`）'
- en: '**ignored_modules** (*Optional**[**Iterable**[*[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*]**]*) – Modules whose own parameters and child modules’ parameters
    and buffers are ignored by this instance. None of the modules directly in `ignored_modules`
    should be [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instances, and any child modules
    that are already-constructed [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instances will not be ignored
    if they are nested under this instance. This argument may be used to avoid sharding
    specific parameters at module granularity when using an `auto_wrap_policy` or
    if parameters’ sharding is not managed by FSDP. (Default: `None`)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ignored_modules** (*可选**[**可迭代**[*[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*]**]*) – 忽略此实例的参数和子模块的参数和缓冲区的模块。`ignored_modules`中直接的模块都不应该是[`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")实例，如果已构建的子模块是[`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")实例，并且它们嵌套在此实例下，则不会被忽略。当使用`auto_wrap_policy`时，或者如果参数的分片不是由FSDP管理时，可以使用此参数避免以模块粒度分片特定参数。（默认值：`None`）'
- en: '**param_init_fn** (*Optional**[**Callable**[**[*[*nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*]**,* *None**]**]*) –'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**param_init_fn** (*可选**[**可调用**[**[*[*nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*]**,* *None**]**]*) –'
- en: A `Callable[torch.nn.Module] -> None` that specifies how modules that are currently
    on the meta device should be initialized onto an actual device. As of v1.12, FSDP
    detects modules with parameters or buffers on meta device via `is_meta` and either
    applies `param_init_fn` if specified or calls `nn.Module.reset_parameters()` otherwise.
    For both cases, the implementation should *only* initialize the parameters/buffers
    of the module, not those of its submodules. This is to avoid re-initialization.
    In addition, FSDP also supports deferred initialization via torchdistX’s ([https://github.com/pytorch/torchdistX](https://github.com/pytorch/torchdistX))
    `deferred_init()` API, where the deferred modules are initialized by calling `param_init_fn`
    if specified or torchdistX’s default `materialize_module()` otherwise. If `param_init_fn`
    is specified, then it is applied to all meta-device modules, meaning that it should
    probably case on the module type. FSDP calls the initialization function before
    parameter flattening and sharding.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个 `Callable[torch.nn.Module] -> None`，指定了当前在元设备上的模块应该如何初始化到实际设备上。从 v1.12 开始，FSDP
    通过 `is_meta` 检测在元设备上具有参数或缓冲区的模块，如果指定了 `param_init_fn`，则应用它，否则调用 `nn.Module.reset_parameters()`。对于这两种情况，实现应该
    *仅* 初始化模块的参数/缓冲区，而不是其子模块的参数/缓冲区。这是为了避免重新初始化。此外，FSDP 还支持通过 torchdistX 的 `deferred_init()`
    API 延迟初始化，其中延迟模块通过调用 `param_init_fn`（如果指定）或 torchdistX 的默认 `materialize_module()`
    进行初始化。如果指定了 `param_init_fn`，则它将应用于所有元设备模块，这意味着它可能会根据模块类型进行分类。FSDP 在参数展平和分片之前调用初始化函数。
- en: 'Example:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**device_id** (*Optional**[**Union**[*[*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* [*torch.device*](tensor_attributes.html#torch.device "torch.device")*]**]*)
    – An `int` or `torch.device` giving the CUDA device on which FSDP initialization
    takes place, including the module initialization if needed and the parameter sharding.
    This should be specified to improve initialization speed if `module` is on CPU.
    If the default CUDA device was set (e.g. via `torch.cuda.set_device`), then the
    user may pass `torch.cuda.current_device` to this. (Default: `None`)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device_id**（*Optional**[**Union**[*[*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* [*torch.device*](tensor_attributes.html#torch.device "torch.device")*]***)
    – 一个 `int` 或 `torch.device`，指定 FSDP 初始化所在的 CUDA 设备，包括如果需要的话模块初始化和参数分片。如果模块在 CPU
    上，则应指定此项以提高初始化速度。如果设置了默认的 CUDA 设备（例如通过 `torch.cuda.set_device`），则用户可以将 `torch.cuda.current_device`
    传递给此项。（默认值：`None`）'
- en: '**sync_module_states** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then each FSDP module will broadcast module
    parameters and buffers from rank 0 to ensure that they are replicated across ranks
    (adding communication overhead to this constructor). This can help load `state_dict`
    checkpoints via `load_state_dict` in a memory efficient way. See [`FullStateDictConfig`](#torch.distributed.fsdp.FullStateDictConfig
    "torch.distributed.fsdp.FullStateDictConfig") for an example of this. (Default:
    `False`)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sync_module_states**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")）- 如果为 `True`，则每个 FSDP 模块将从排名 0 广播模块参数和缓冲区，以确保它们在各个排名之间复制（为此构造函数增加通信开销）。这可以帮助以内存高效的方式通过
    `load_state_dict` 加载 `state_dict` 检查点。请参阅 [`FullStateDictConfig`](#torch.distributed.fsdp.FullStateDictConfig
    "torch.distributed.fsdp.FullStateDictConfig") 以获取此示例。（默认值：`False`）'
- en: '**forward_prefetch** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP *explicitly* prefetches the next
    forward-pass all-gather before the current forward computation. This is only useful
    for CPU-bound workloads, in which case issuing the next all-gather earlier may
    improve overlap. This should only be used for static-graph models since the prefetching
    follows the first iteration’s execution order. (Default: `False`)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**forward_prefetch**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")）- 如果为 `True`，则 FSDP *明确* 在当前前向计算之前预取下一个前向传递的所有聚集。这仅对 CPU
    绑定的工作负载有用，在这种情况下，提前发出下一个所有聚集可能会提高重叠。这仅适用于静态图模型，因为预取遵循第一次迭代的执行顺序。（默认值：`False`）'
- en: '**limit_all_gathers** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP explicitly synchronizes the CPU thread
    to ensure GPU memory usage from only *two* consecutive FSDP instances (the current
    instance running computation and the next instance whose all-gather is prefetched).
    If `False`, then FSDP allows the CPU thread to issue all-gathers without any extra
    synchronization. (Default: `True`) We often refer to this feature as the “rate
    limiter”. This flag should only be set to `False` for specific CPU-bound workloads
    with low memory pressure in which case the CPU thread can aggressively issue all
    kernels without concern for the GPU memory usage.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**limit_all_gathers**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")）- 如果为 `True`，则 FSDP 明确同步 CPU 线程，以确保 GPU 内存使用仅来自 *两个* 连续的
    FSDP 实例（当前实例运行计算和下一个实例，其所有聚集都是预取的）。如果为 `False`，则 FSDP 允许 CPU 线程发出所有聚集而无需任何额外的同步。（默认值：`True`）我们通常将此功能称为“速率限制器”。此标志应仅针对具有低内存压力的特定
    CPU 绑定工作负载设置为 `False`，在这种情况下，CPU 线程可以积极发出所有内核，而不必担心 GPU 内存使用。'
- en: '**use_orig_params** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Setting this to `True` has FSDP use `module` ‘s original
    parameters. FSDP exposes those original parameters to the user via `nn.Module.named_parameters()`
    instead of FSDP’s internal `FlatParameter` s. This means that the optimizer step
    runs on the original parameters, enabling per-original-parameter hyperparameters.
    FSDP preserves the original parameter variables and manipulates their data between
    unsharded and sharded forms, where they are always views into the underlying unsharded
    or sharded `FlatParameter`, respectively. With the current algorithm, the sharded
    form is always 1D, losing the original tensor structure. An original parameter
    may have all, some, or none of its data present for a given rank. In the none
    case, its data will be like a size-0 empty tensor. Users should not author programs
    relying on what data is present for a given original parameter in its sharded
    form. `True` is required to use `torch.compile()`. Setting this to `False` exposes
    FSDP’s internal `FlatParameter` s to the user via `nn.Module.named_parameters()`.
    (Default: `False`)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**use_orig_params**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")）- 将此设置为`True`会使FSDP使用`module`的原始参数。FSDP通过`nn.Module.named_parameters()`向用户公开这些原始参数，而不是通过FSDP的内部`FlatParameter`。这意味着优化器步骤在原始参数上运行，从而实现每个原始参数的超参数。FSDP保留原始参数变量，并在未分片和分片形式之间操作它们的数据，其中它们始终是底层未分片或分片`FlatParameter`的视图。根据当前算法，分片形式始终是1D，丢失了原始张量结构。对于给定等级，原始参数可能具有全部、部分或没有数据。在没有数据的情况下，其数据将类似于大小为0的空张量。用户不应编写依赖于给定原始参数在其分片形式中存在哪些数据的程序。`True`是使用`torch.compile()`所必需的。将其设置为`False`通过`nn.Module.named_parameters()`向用户公开FSDP的内部`FlatParameter`。
    （默认值：`False`）'
- en: '**ignored_states** (*Optional**[**Iterable**[**torch.nn.Parameter**]**]**,*
    *Optional**[**Iterable**[*[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*]**]*) – Ignored parameters or modules that will not be managed
    by this FSDP instance, meaning that the parameters are not sharded and their gradients
    are not reduced across ranks. This argument unifies with the existing `ignored_modules`
    argument, and we may deprecate `ignored_modules` soon. For backward compatibility,
    we keep both `ignored_states` and ignored_modules`, but FSDP only allows one of
    them to be specified as not `None`.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ignored_states**（*可选**[**Iterable**[**torch.nn.Parameter**]**]**,* *可选**[**Iterable**[*[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*]**]*)- 不受此FSDP实例管理的被忽略的参数或模块，这意味着参数未分片，它们的梯度未在等级之间减少。此参数与现有的`ignored_modules`参数统一，我们可能很快会弃用`ignored_modules`。为了向后兼容，我们保留`ignored_states`和`ignored_modules`，但是FSDP只允许其中一个被指定为非`None`。'
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Apply `fn` recursively to every submodule (as returned by `.children()`) as
    well as self.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 递归地将`fn`应用于每个子模块（由`.children()`返回）以及自身。
- en: Typical use includes initializing the parameters of a model (see also [torch.nn.init](nn.init.html#nn-init-doc)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 典型用法包括初始化模型的参数（另请参阅[torch.nn.init](nn.init.html#nn-init-doc)）。
- en: Compared to `torch.nn.Module.apply`, this version additionally gathers the full
    parameters before applying `fn`. It should not be called from within another `summon_full_params`
    context.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与`torch.nn.Module.apply`相比，此版本在应用`fn`之前还会收集完整的参数。不应在另一个`summon_full_params`上下文中调用它。
- en: Parameters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**fn** (`Module` -> None) – function to be applied to each submodule'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**fn**（`Module` -> None）- 要应用于每个子模块的函数'
- en: Returns
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: self
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: self
- en: Return type
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[Module](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[Module](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")'
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Check if this instance is a root FSDP module.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 检查此实例是否为根FSDP模块。
- en: Return type
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Clip the gradient norm of all parameters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪所有参数的梯度规范。
- en: The norm is computed over all parameters’ gradients as viewed as a single vector,
    and the gradients are modified in-place.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 规范是计算所有参数的梯度作为单个向量的规范，并且梯度会就地修改。
- en: Parameters
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**max_norm** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)") *or* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – max norm of the gradients'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_norm**（[*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)") *或* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")）- 梯度的最大规范'
- en: '**norm_type** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)") *or* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – type of the used p-norm. Can be `''inf''` for infinity
    norm.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**norm_type**（[*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)") *或* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")）- 使用的p-范数类型。可以是`''inf''`表示无穷范数。'
- en: Returns
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: Total norm of the parameters (viewed as a single vector).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的总规范（视为单个向量）。
- en: Return type
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
- en: Note
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If every FSDP instance uses `NO_SHARD`, meaning that no gradients are sharded
    across ranks, then you may directly use [`torch.nn.utils.clip_grad_norm_()`](generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_
    "torch.nn.utils.clip_grad_norm_").
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个FSDP实例都使用`NO_SHARD`，即没有梯度在等级之间分片，则可以直接使用[`torch.nn.utils.clip_grad_norm_()`](generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_
    "torch.nn.utils.clip_grad_norm_")。
- en: Note
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If at least some FSDP instance uses a sharded strategy (i.e. one other than
    `NO_SHARD`), then you should use this method instead of [`torch.nn.utils.clip_grad_norm_()`](generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_
    "torch.nn.utils.clip_grad_norm_") since this method handles the fact that gradients
    are sharded across ranks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果至少有一些FSDP实例使用分片策略（即`NO_SHARD`之外的策略），则应使用此方法而不是[`torch.nn.utils.clip_grad_norm_()`](generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_
    "torch.nn.utils.clip_grad_norm_")，因为此方法处理了梯度在等级之间分片的事实。
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The total norm returned will have the “largest” dtype across all parameters/gradients
    as defined by PyTorch’s type promotion semantics. For example, if *all* parameters/gradients
    use a low precision dtype, then the returned norm’s dtype will be that low precision
    dtype, but if there exists at least one parameter/ gradient using FP32, then the
    returned norm’s dtype will be FP32.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的总规范将具有PyTorch类型提升语义定义的所有参数/梯度中的“最大”dtype。例如，如果*所有*参数/梯度使用低精度dtype，则返回的规范的dtype将是该低精度dtype，但如果至少存在一个使用FP32的参数/梯度，则返回的规范的dtype将是FP32。
- en: Warning
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This needs to be called on all ranks since it uses collective communications.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用集体通信，因此需要在所有秩上调用此函数。
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Flatten a sharded optimizer state-dict.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 展平分片的优化器状态字典。
- en: The API is similar to [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict").
    The only difference is that the input `sharded_optim_state_dict` should be returned
    from [`sharded_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict"). Therefore,
    there will be all-gather calls on each rank to gather `ShardedTensor` s.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: API类似于[`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict")。唯一的区别是输入的`sharded_optim_state_dict`应该从[`sharded_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict")返回。因此，每个秩上都会有全聚合调用以收集`ShardedTensor`
    s。
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**sharded_optim_state_dict** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*) – Optimizer state dict corresponding to the
    unflattened parameters and holding the sharded optimizer state.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sharded_optim_state_dict**（*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*） - 对应于未展平参数并保存分片优化器状态的优化器状态字典。'
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Refer to [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict").'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model**（[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")） - 参考[`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict")。'
- en: '**optim** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – Optimizer for `model` ‘s parameters.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim**（[*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer")）
    - 用于`model`的参数的优化器。'
- en: Returns
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: Refer to [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict").
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 参考[`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict")。
- en: Return type
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python
    v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)"), [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in
    Python v3.12)")]'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python
    v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)"), [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in
    Python v3.12)")]'
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Run the forward pass for the wrapped module, inserting FSDP-specific pre- and
    post-forward sharding logic.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 运行包装模块的前向传递，插入FSDP特定的前向和后向分片逻辑。
- en: Return type
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in Python
    v3.12)")'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in Python
    v3.12)")'
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Return all nested FSDP instances.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 返回所有嵌套的FSDP实例。
- en: This possibly includes `module` itself and only includes FSDP root modules if
    `root_only=True`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能包括`module`本身，仅在`root_only=True`时包括FSDP根模块。
- en: Parameters
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**module** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module, which may or may not be an `FSDP` module.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module**（[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")） - 根模块，可能是或可能不是`FSDP`模块。'
- en: '**root_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Whether to return only FSDP root modules. (Default: `False`)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**root_only**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")） - 是否仅返回FSDP根模块。 （默认值：`False`）'
- en: Returns
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: FSDP modules that are nested in the input `module`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套在输入`module`中的FSDP模块。
- en: Return type
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: List[[FullyShardedDataParallel](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")]
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: List[[FullyShardedDataParallel](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")]
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Return the full optimizer state-dict.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 返回完整的优化器状态字典。
- en: Consolidates the full optimizer state on rank 0 and returns it as a [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)") following the convention of [`torch.optim.Optimizer.state_dict()`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict"), i.e. with keys `"state"` and `"param_groups"`.
    The flattened parameters in `FSDP` modules contained in `model` are mapped back
    to their unflattened parameters.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在秩0上合并完整的优化器状态，并将其作为[`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)")返回，遵循[`torch.optim.Optimizer.state_dict()`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict")的约定，即具有键“state”和“param_groups”。`model`中包含的`FSDP`模块中的展平参数将映射回其未展平参数。
- en: Warning
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This needs to be called on all ranks since it uses collective communications.
    However, if `rank0_only=True`, then the state dict is only populated on rank 0,
    and all other ranks return an empty [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)").
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用集体通信，因此需要在所有秩上调用此函数。但是，如果`rank0_only=True`，则状态字典仅在秩为0时填充，并且所有其他秩返回一个空的[`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)")。
- en: Warning
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Unlike `torch.optim.Optimizer.state_dict()`, this method uses full parameter
    names as keys instead of parameter IDs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与`torch.optim.Optimizer.state_dict()`不同，此方法使用完整的参数名称作为键，而不是参数ID。
- en: Note
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Like in [`torch.optim.Optimizer.state_dict()`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict"), the tensors contained in the optimizer state
    dict are not cloned, so there may be aliasing surprises. For best practices, consider
    saving the returned optimizer state dict immediately, e.g. using `torch.save()`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与[`torch.optim.Optimizer.state_dict()`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict")中一样，优化器状态字典中包含的张量不会被克隆，因此可能会出现别名意外。为了最佳实践，考虑立即保存返回的优化器状态字典，例如使用`torch.save()`。
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module (which may or may not be a [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instance) whose parameters
    were passed into the optimizer `optim`.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**（[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module"））-
    根模块（可能是或不是[`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")实例），其参数被传递给优化器`optim`。'
- en: '**optim** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – Optimizer for `model` ‘s parameters.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim**（[*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer")）-
    用于`model`的参数的优化器。'
- en: '**optim_input** (*Optional**[**Union**[**List**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]**,* *Iterable**[**torch.nn.Parameter**]**]**]*)
    – Input passed into the optimizer `optim` representing either a [`list`](https://docs.python.org/3/library/stdtypes.html#list
    "(in Python v3.12)") of parameter groups or an iterable of parameters; if `None`,
    then this method assumes the input was `model.parameters()`. This argument is
    deprecated, and there is no need to pass it in anymore. (Default: `None`)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim_input**（*Optional**[**Union**[**List**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]**,** *Iterable**[**torch.nn.Parameter**]**]**]*)
    - 传递给优化器`optim`的输入，表示参数组的[`list`](https://docs.python.org/3/library/stdtypes.html#list
    "(in Python v3.12)")或参数的可迭代对象；如果为`None`，则此方法假定输入为`model.parameters()`。此参数已被弃用，不再需要传递它。
    （默认值：`None`）'
- en: '**rank0_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, saves the populated [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)") only on rank 0; if `False`, saves it on all ranks. (Default:
    `True`)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rank0_only**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")）- 如果为`True`，则仅在rank 0上保存填充的[`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)")；如果为`False`，则在所有rank上保存。 （默认值：`True`）'
- en: '**group** (*dist.ProcessGroup*) – Model’s process group or `None` if using
    the default process group. (Default: `None`)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**group**（*dist.ProcessGroup*）- 模型的进程组或如果使用默认进程组则为`None`。 （默认值：`None`）'
- en: Returns
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A [`dict`](https://docs.python.org/3/library/stdtypes.html#dict "(in Python
    v3.12)") containing the optimizer state for `model` ‘s original unflattened parameters
    and including keys “state” and “param_groups” following the convention of [`torch.optim.Optimizer.state_dict()`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict"). If `rank0_only=True`, then nonzero ranks
    return an empty [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)").
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 包含`model`原始未扁平化参数的优化器状态和包括“state”和“param_groups”键的[`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)")。如果`rank0_only=True`，则非零rank返回一个空的[`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)")。
- en: Return type
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Get the state_dict_type and the corresponding configurations for the FSDP modules
    rooted at `module`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 获取根据`module`根模块的FSDP模块的state_dict_type和相应的配置。
- en: The target module does not have to be an FSDP module.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 目标模块不必是FSDP模块。
- en: Returns
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A `StateDictSettings` containing the state_dict_type and state_dict / optim_state_dict
    configs that are currently set.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 包含当前设置的state_dict_type和state_dict / optim_state_dict配置的`StateDictSettings`。
- en: Raises
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 引发
- en: '**AssertionError` if the StateDictSettings for differen** –'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如果StateDictSettings不同，则会引发AssertionError** -'
- en: '**FSDP submodules differ.** –'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FSDP子模块不同。** -'
- en: Return type
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*StateDictSettings*](#torch.distributed.fsdp.StateDictSettings "torch.distributed.fsdp.api.StateDictSettings")'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[*StateDictSettings*](#torch.distributed.fsdp.StateDictSettings "torch.distributed.fsdp.api.StateDictSettings")'
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Return the wrapped module.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 返回包装的模块。
- en: '[PRE13]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Return an iterator over module buffers, yielding both the name of the buffer
    and the buffer itself.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个模块缓冲区的迭代器，同时产生缓冲区的名称和缓冲区本身。
- en: Intercepts buffer names and removes all occurrences of the FSDP-specific flattened
    buffer prefix when inside the [`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params") context
    manager.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在[`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params")上下文管理器中，拦截缓冲区名称并删除所有FSDP特定的扁平化缓冲区前缀的出现。
- en: Return type
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Iterator*](https://docs.python.org/3/library/typing.html#typing.Iterator
    "(in Python v3.12)")[[*Tuple*](https://docs.python.org/3/library/typing.html#typing.Tuple
    "(in Python v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [*Tensor*](tensors.html#torch.Tensor "torch.Tensor")]]'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Iterator*](https://docs.python.org/3/library/typing.html#typing.Iterator
    "(in Python v3.12)")[[*Tuple*](https://docs.python.org/3/library/typing.html#typing.Tuple
    "(in Python v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [*Tensor*](tensors.html#torch.Tensor "torch.Tensor")]]'
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Return an iterator over module parameters, yielding both the name of the parameter
    and the parameter itself.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个模块参数的迭代器，同时产生参数的名称和参数本身。
- en: Intercepts parameter names and removes all occurrences of the FSDP-specific
    flattened parameter prefix when inside the [`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params") context
    manager.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 拦截参数名称，并在[`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params")上下文管理器内部删除所有FSDP特定的扁平化参数前缀的出现。
- en: Return type
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Iterator*](https://docs.python.org/3/library/typing.html#typing.Iterator
    "(in Python v3.12)")[[*Tuple*](https://docs.python.org/3/library/typing.html#typing.Tuple
    "(in Python v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [*Parameter*](generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter
    "torch.nn.parameter.Parameter")]]'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Iterator*](https://docs.python.org/3/library/typing.html#typing.Iterator
    "(在Python v3.12中)")[[*Tuple*](https://docs.python.org/3/library/typing.html#typing.Tuple
    "(在Python v3.12中)")[[str](https://docs.python.org/3/library/stdtypes.html#str
    "(在Python v3.12中)"), [*Parameter*](generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter
    "torch.nn.parameter.Parameter")]]'
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Disable gradient synchronizations across FSDP instances.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用FSDP实例之间的梯度同步。
- en: Within this context, gradients will be accumulated in module variables, which
    will later be synchronized in the first forward-backward pass after exiting the
    context. This should only be used on the root FSDP instance and will recursively
    apply to all children FSDP instances.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在此上下文中，梯度将在模块变量中累积，稍后在退出上下文后的第一个前向-后向传递中进行同步。这应仅用于根FSDP实例，并将递归应用于所有子FSDP实例。
- en: Note
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This likely results in higher memory usage because FSDP will accumulate the
    full model gradients (instead of gradient shards) until the eventual sync.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能导致更高的内存使用，因为FSDP将累积完整模型梯度（而不是梯度片段），直到最终同步。
- en: Note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When used with CPU offloading, the gradients will not be offloaded to CPU when
    inside the context manager. Instead, they will only be offloaded right after the
    eventual sync.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当与CPU卸载一起使用时，在上下文管理器内部梯度不会被卸载到CPU。相反，它们只会在最终同步之后被卸载。
- en: Return type
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Generator*](https://docs.python.org/3/library/typing.html#typing.Generator
    "(in Python v3.12)")'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Generator*](https://docs.python.org/3/library/typing.html#typing.Generator
    "(在Python v3.12中)")'
- en: '[PRE16]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Transform the state-dict of an optimizer corresponding to a sharded model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 转换与分片模型对应的优化器的状态字典。
- en: 'The given state-dict can be transformed to one of three types: 1) full optimizer
    state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的状态字典可以转换为三种类型之一：1）完整的优化器状态字典，2）分片的优化器状态字典，3）本地的优化器状态字典。
- en: For full optimizer state_dict, all states are unflattened and not sharded. Rank0
    only and CPU only can be specified via [`state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type") to avoid OOM.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完整的优化器状态字典，所有状态都是未扁平化且未分片的。可以通过[`state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type")指定仅Rank0和仅CPU，以避免OOM。
- en: For sharded optimizer state_dict, all states are unflattened but sharded. CPU
    only can be specified via [`state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type") to further
    save memory.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分片的优化器状态字典，所有状态都是未扁平化但是分片的。可以通过[`state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type")指定仅CPU，以进一步节省内存。
- en: For local state_dict, no transformation will be performed. But a state will
    be converted from nn.Tensor to ShardedTensor to represent its sharding nature
    (this is not supported yet).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本地state_dict，不会执行任何转换。但是，状态将从nn.Tensor转换为ShardedTensor以表示其分片性质（目前不支持）。
- en: 'Example:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module (which may or may not be a [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instance) whose parameters
    were passed into the optimizer `optim`.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – 根模块（可能是或可能不是[`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")实例）其参数被传递给优化器`optim`。'
- en: '**optim** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – Optimizer for `model` ‘s parameters.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – 用于`model`参数的优化器。'
- en: '**optim_state_dict** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*) – the target optimizer state_dict to transform.
    If the value is None, optim.state_dict() will be used. ( Default: `None`)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim_state_dict** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(在Python v3.12中)")*,* *Any**]*) – 要转换的目标优化器状态字典。如果值为None，则将使用optim.state_dict()。（默认值：`None`）'
- en: '**group** (*dist.ProcessGroup*) – Model’s process group across which parameters
    are sharded or `None` if using the default process group. ( Default: `None`)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**group** (*dist.ProcessGroup*) – 模型的进程组，参数被分片到该组中，如果使用默认进程组则为`None`。（默认值：`None`）'
- en: Returns
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: A [`dict`](https://docs.python.org/3/library/stdtypes.html#dict "(in Python
    v3.12)") containing the optimizer state for `model`. The sharding of the optimizer
    state is based on `state_dict_type`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 包含`model`的优化器状态的[`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(在Python v3.12)")。优化器状态的分片基于`state_dict_type`。
- en: Return type
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(在Python v3.12中)"),
    Any]
- en: '[PRE18]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Convert an optimizer state-dict so that it can be loaded into the optimizer
    associated with the FSDP model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 将优化器状态字典转换为可以加载到与FSDP模型关联的优化器中的格式。
- en: Given a `optim_state_dict` that is transformed through [`optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict"), it gets converted
    to the flattened optimizer state_dict that can be loaded to `optim` which is the
    optimizer for `model`. `model` must be sharded by FullyShardedDataParallel.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 给定通过[`optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict")转换的`optim_state_dict`，它被转换为可以加载到`optim`的扁平化优化器state_dict，该`optim`是`model`的优化器。`model`必须由FullyShardedDataParallel进行分片。
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module (which may or may not be a [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instance) whose parameters
    were passed into the optimizer `optim`.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – 根模块（可能是或不是[`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")实例），其参数传递给了优化器`optim`。'
- en: '**optim** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – Optimizer for `model` ‘s parameters.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – `model`的参数的优化器。'
- en: '**optim_state_dict** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*) – The optimizer states to be loaded.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim_state_dict** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*) – 要加载的优化器状态。'
- en: '**is_named_optimizer** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Is this optimizer a NamedOptimizer or KeyedOptimizer.
    Only set to True if `optim` is TorchRec’s KeyedOptimizer or torch.distributed’s
    NamedOptimizer.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**is_named_optimizer** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – 这个优化器是NamedOptimizer还是KeyedOptimizer。只有在`optim`是TorchRec的KeyedOptimizer或torch.distributed的NamedOptimizer时才设置为True。'
- en: '**load_directly** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If this is set to True, this API will also call optim.load_state_dict(result)
    before returning the result. Otherwise, users are responsible to call `optim.load_state_dict()`
    (Default: `False`)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**load_directly** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – 如果设置为True，则此API在返回结果之前还将调用optim.load_state_dict(result)。否则，用户需要调用`optim.load_state_dict()`（默认值：`False`）'
- en: '**group** (*dist.ProcessGroup*) – Model’s process group across which parameters
    are sharded or `None` if using the default process group. ( Default: `None`)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**group** (*dist.ProcessGroup*) – 模型的进程组，参数在其中进行分片，如果使用默认进程组，则为`None`。（默认值：`None`）'
- en: Return type
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python
    v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)"), [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in
    Python v3.12)")]'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python
    v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)"), [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in
    Python v3.12)")]'
- en: '[PRE20]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Register a communication hook.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注册通信钩子。
- en: This is an enhancement that provides a flexible hook to users where they can
    specify how FSDP aggregates gradients across multiple workers. This hook can be
    used to implement several algorithms like [GossipGrad](https://arxiv.org/abs/1803.05880)
    and gradient compression which involve different communication strategies for
    parameter syncs while training with [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel").
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个增强功能，为用户提供了一个灵活的钩子，他们可以在其中指定FSDP如何在多个工作进程之间聚合梯度。这个钩子可以用于实现几种算法，如[GossipGrad](https://arxiv.org/abs/1803.05880)和涉及不同通信策略的梯度压缩，这些策略用于参数同步，同时使用[`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")进行训练。
- en: Warning
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: FSDP communication hook should be registered before running an initial forward
    pass and only once.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行初始前向传递之前，应注册FSDP通信钩子，并且只注册一次。
- en: Parameters
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**state** ([*object*](https://docs.python.org/3/library/functions.html#object
    "(in Python v3.12)")) –'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state** ([*object*](https://docs.python.org/3/library/functions.html#object
    "(in Python v3.12)")) –'
- en: Passed to the hook to maintain any state information during the training process.
    Examples include error feedback in gradient compression, peers to communicate
    with next in [GossipGrad](https://arxiv.org/abs/1803.05880), etc. It is locally
    stored by each worker and shared by all the gradient tensors on the worker.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传递给钩子以在训练过程中维护任何状态信息。示例包括梯度压缩中的错误反馈，与[GossipGrad](https://arxiv.org/abs/1803.05880)中下一个通信的对等体等。它由每个工作进程本地存储，并由工作进程上的所有梯度张量共享。
- en: '**hook** (*Callable*) – Callable, which has one of the following signatures:
    1) `hook: Callable[torch.Tensor] -> None`: This function takes in a Python tensor,
    which represents the full, flattened, unsharded gradient with respect to all variables
    corresponding to the model this FSDP unit is wrapping (that are not wrapped by
    other FSDP sub-units). It then performs all necessary processing and returns `None`;
    2) `hook: Callable[torch.Tensor, torch.Tensor] -> None`: This function takes in
    two Python tensors, the first one represents the full, flattened, unsharded gradient
    with respect to all variables corresponding to the model this FSDP unit is wrapping
    (that are not wrapped by other FSDP sub-units). The latter represents a pre-sized
    tensor to store a chunk of a sharded gradient after reduction. In both cases,
    callable performs all necessary processing and returns `None`. Callables with
    signature 1 are expected to handle gradient communication for a NO_SHARD case.
    Callables with signature 2 are expected to handle gradient communication for sharded
    cases.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**hook** (*Callable*) – 可调用函数，具有以下签名之一：1) `hook: Callable[torch.Tensor] ->
    None`：此函数接受一个Python张量，表示与此FSDP单元包装的模型对应的所有变量的全面、扁平化、未分片梯度（未被其他FSDP子单元包装）。然后执行所有必要的处理并返回`None`；2)
    `hook: Callable[torch.Tensor, torch.Tensor] -> None`：此函数接受两个Python张量，第一个表示与此FSDP单元包装的模型对应的所有变量的全面、扁平化、未分片梯度（未被其他FSDP子单元包装）。后者表示一个预先大小的张量，用于存储分片梯度的一部分。在这两种情况下，可调用函数执行所有必要的处理并返回`None`。具有签名1的可调用函数应处理NO_SHARD情况的梯度通信。具有签名2的可调用函数应处理分片情况的梯度通信。'
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Re-keys the optimizer state dict `optim_state_dict` to use the key type `optim_state_key_type`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 重新调整优化器状态字典`optim_state_dict`以使用键类型`optim_state_key_type`。
- en: This can be used to achieve compatibility between optimizer state dicts from
    models with FSDP instances and ones without.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用于实现具有FSDP实例和没有FSDP实例的模型的优化器状态字典之间的兼容性。
- en: 'To re-key an FSDP full optimizer state dict (i.e. from [`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict")) to use
    parameter IDs and be loadable to a non-wrapped model:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 重新调整FSDP全优化器状态字典（即从[`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict")）以使用参数ID，并且可以加载到非包装模型中：
- en: '[PRE22]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To re-key a normal optimizer state dict from a non-wrapped model to be loadable
    to a wrapped model:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 重新调整来自非包装模型的普通优化器状态字典，以便加载到包装模型中：
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Returns
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: The optimizer state dict re-keyed using the parameter keys specified by `optim_state_key_type`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用由`optim_state_key_type`指定的参数键重新调整的优化器状态字典。
- en: Return type
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 字典[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
- en: '[PRE24]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Scatter the full optimizer state dict from rank 0 to all other ranks.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 将来自排名0的完整优化器状态字典分散到所有其他排名。
- en: Returns the sharded optimizer state dict on each rank. The return value is the
    same as [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict"),
    and on rank 0, the first argument should be the return value of [`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict").
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个排名上返回分片的优化器状态字典。返回值与[`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict")相同，在排名0上，第一个参数应该是[`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict")的返回值。
- en: 'Example:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE25]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Both [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict")
    and [`scatter_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict")
    may be used to get the sharded optimizer state dict to load. Assuming that the
    full optimizer state dict resides in CPU memory, the former requires each rank
    to have the full dict in CPU memory, where each rank individually shards the dict
    without any communication, while the latter requires only rank 0 to have the full
    dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and
    communicates it to ranks appropriately. Hence, the former has higher aggregate
    CPU memory cost, while the latter has higher communication cost.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict")和[`scatter_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict")都可以用于获取分片优化器状态字典以进行加载。假设完整的优化器状态字典驻留在CPU内存中，前者要求每个排名在CPU内存中具有完整字典，其中每个排名单独对字典进行分片而无需任何通信，而后者只要求排名0在CPU内存中具有完整字典，其中排名0将每个分片移动到GPU内存（用于NCCL）并适当地将其通信给排名。因此，前者具有更高的总体CPU内存成本，而后者具有更高的通信成本。'
- en: Parameters
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**full_optim_state_dict** (*Optional**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]*) – Optimizer state dict corresponding to the
    unflattened parameters and holding the full non-sharded optimizer state if on
    rank 0; the argument is ignored on nonzero ranks.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**full_optim_state_dict** (*Optional**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]*) – 与未扁平化参数对应并保存完整非分片优化器状态的优化器状态字典（如果在排名0上）；在非零排名上忽略该参数。'
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module (which may or may not be a [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instance) whose parameters
    correspond to the optimizer state in `full_optim_state_dict`.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model**（[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")）- 根模块（可能是或可能不是[`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")实例），其参数对应于`full_optim_state_dict`中的优化器状态。'
- en: '**optim_input** (*Optional**[**Union**[**List**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]**,* *Iterable**[**torch.nn.Parameter**]**]**]*)
    – Input passed into the optimizer representing either a [`list`](https://docs.python.org/3/library/stdtypes.html#list
    "(in Python v3.12)") of parameter groups or an iterable of parameters; if `None`,
    then this method assumes the input was `model.parameters()`. This argument is
    deprecated, and there is no need to pass it in anymore. (Default: `None`)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim_input**（*可选**[**Union**[**List**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]**,* *Iterable**[**torch.nn.Parameter**]**]**]*)
    - 传递给优化器的输入，表示参数组的[`list`](https://docs.python.org/3/library/stdtypes.html#list
    "(in Python v3.12)")或参数的可迭代对象；如果为`None`，则此方法假定输入为`model.parameters()`。此参数已被弃用，不再需要传递它。（默认值：`None`）'
- en: '**optim** (*Optional**[*[*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer
    "torch.optim.Optimizer")*]*) – Optimizer that will load the state dict returned
    by this method. This is the preferred argument to use over `optim_input`. (Default:
    `None`)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim**（*可选**[*[*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer
    "torch.optim.Optimizer")*]*）- 由此方法返回的状态字典将加载的优化器。这是优选的参数，优于`optim_input`。（默认值：`None`）'
- en: '**group** (*dist.ProcessGroup*) – Model’s process group or `None` if using
    the default process group. (Default: `None`)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**group**（*dist.ProcessGroup*）- 模型的进程组或如果使用默认进程组则为`None`。（默认值：`None`）'
- en: Returns
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: The full optimizer state dict now remapped to flattened parameters instead of
    unflattened parameters and restricted to only include this rank’s part of the
    optimizer state.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在完整的优化器状态字典已经重新映射到扁平化的参数，而不是未扁平化的参数，并且仅限于包括此排名部分的优化器状态。
- en: Return type
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
- en: '[PRE26]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Set the `state_dict_type` of all the descendant FSDP modules of the target module.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 设置目标模块的所有后代FSDP模块的`state_dict_type`。
- en: Also takes (optional) configuration for the model’s and optimizer’s state dict.
    The target module does not have to be a FSDP module. If the target module is a
    FSDP module, its `state_dict_type` will also be changed.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 还接受（可选）模型和优化器状态字典的配置。目标模块不必是一个FSDP模块。如果目标模块是一个FSDP模块，它的`state_dict_type`也将被更改。
- en: Note
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This API should be called for only the top-level (root) module.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 此API应仅用于顶层（根）模块。
- en: Note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This API enables users to transparently use the conventional `state_dict` API
    to take model checkpoints in cases where the root FSDP module is wrapped by another
    `nn.Module`. For example, the following will ensure `state_dict` is called on
    all non-FSDP instances, while dispatching into sharded_state_dict implementation
    for FSDP:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 此API使用户能够透明地使用传统的`state_dict` API，在根FSDP模块被另一个`nn.Module`包装的情况下进行模型检查点。例如，以下代码将确保在所有非FSDP实例上调用`state_dict`，同时将分派到FSDP的sharded_state_dict实现：
- en: 'Example:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE27]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**module** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module**（[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")）- 根模块。'
- en: '**state_dict_type** (*StateDictType*) – the desired `state_dict_type` to set.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict_type**（*StateDictType*）- 要设置的期望的`state_dict_type`。'
- en: '**state_dict_config** (*Optional**[*[*StateDictConfig*](#torch.distributed.fsdp.StateDictConfig
    "torch.distributed.fsdp.StateDictConfig")*]*) – the configuration for the target
    `state_dict_type`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict_config**（*可选**[*[*StateDictConfig*](#torch.distributed.fsdp.StateDictConfig
    "torch.distributed.fsdp.StateDictConfig")*]*）- 目标`state_dict_type`的配置。'
- en: '**optim_state_dict_config** (*Optional**[*[*OptimStateDictConfig*](#torch.distributed.fsdp.OptimStateDictConfig
    "torch.distributed.fsdp.OptimStateDictConfig")*]*) – the configuration for the
    optimizer state dict.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim_state_dict_config**（*可选**[*[*OptimStateDictConfig*](#torch.distributed.fsdp.OptimStateDictConfig
    "torch.distributed.fsdp.OptimStateDictConfig")*]*）- 优化器状态字典的配置。'
- en: Returns
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: A StateDictSettings that include the previous state_dict type and configuration
    for the module.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 包括模块的先前state_dict类型和配置的StateDictSettings。
- en: Return type
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*StateDictSettings*](#torch.distributed.fsdp.StateDictSettings "torch.distributed.fsdp.api.StateDictSettings")'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[*StateDictSettings*](#torch.distributed.fsdp.StateDictSettings "torch.distributed.fsdp.api.StateDictSettings")'
- en: '[PRE28]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Shard a full optimizer state-dict.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 分片完整的优化器状态字典。
- en: Remaps the state in `full_optim_state_dict` to flattened parameters instead
    of unflattened parameters and restricts to only this rank’s part of the optimizer
    state. The first argument should be the return value of [`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict").
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 将`full_optim_state_dict`中的状态重新映射为扁平化的参数，而不是未扁平化的参数，并且限制为仅此排名部分的优化器状态。第一个参数应该是[`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict")的返回值。
- en: 'Example:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE29]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Both [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict")
    and [`scatter_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict")
    may be used to get the sharded optimizer state dict to load. Assuming that the
    full optimizer state dict resides in CPU memory, the former requires each rank
    to have the full dict in CPU memory, where each rank individually shards the dict
    without any communication, while the latter requires only rank 0 to have the full
    dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and
    communicates it to ranks appropriately. Hence, the former has higher aggregate
    CPU memory cost, while the latter has higher communication cost.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict")和[`scatter_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict")都可用于获取分片优化器状态字典以加载。假设完整的优化器状态字典驻留在CPU内存中，前者要求每个排名在CPU内存中具有完整的字典，其中每个排名单独对字典进行分片而无需任何通信，而后者仅要求排名0在CPU内存中具有完整的字典，其中排名0将每个分片移动到GPU内存（用于NCCL）并适当地将其通信给排名。因此，前者具有更高的总体CPU内存成本，而后者具有更高的通信成本。'
- en: Parameters
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**full_optim_state_dict** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*) – Optimizer state dict corresponding to the
    unflattened parameters and holding the full non-sharded optimizer state.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**full_optim_state_dict**（*字典**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*) - 与未扁平化参数对应的优化器状态字典，保存完整的非分片优化器状态。'
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module (which may or may not be a [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instance) whose parameters
    correspond to the optimizer state in `full_optim_state_dict`.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**（[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")）-
    根模块（可能是或不是[`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")实例），其参数对应于`full_optim_state_dict`中的优化器状态。'
- en: '**optim_input** (*Optional**[**Union**[**List**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]**,* *Iterable**[**torch.nn.Parameter**]**]**]*)
    – Input passed into the optimizer representing either a [`list`](https://docs.python.org/3/library/stdtypes.html#list
    "(in Python v3.12)") of parameter groups or an iterable of parameters; if `None`,
    then this method assumes the input was `model.parameters()`. This argument is
    deprecated, and there is no need to pass it in anymore. (Default: `None`)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim_input**（*可选**[**Union**[**List**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]**,* *Iterable**[**torch.nn.Parameter**]**]**]*)
    - 传递给优化器的输入，表示参数组的[`list`](https://docs.python.org/3/library/stdtypes.html#list
    "(in Python v3.12)")或参数的可迭代对象；如果为`None`，则此方法假定输入为`model.parameters()`。此参数已被弃用，不再需要传递它。（默认值：`None`）'
- en: '**optim** (*Optional**[*[*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer
    "torch.optim.Optimizer")*]*) – Optimizer that will load the state dict returned
    by this method. This is the preferred argument to use over `optim_input`. (Default:
    `None`)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim**（*可选**[*[*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer
    "torch.optim.Optimizer")*]*) - 将由此方法返回的状态字典加载的优化器。这是优选的参数，用于覆盖`optim_input`。（默认值：`None`）'
- en: Returns
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: The full optimizer state dict now remapped to flattened parameters instead of
    unflattened parameters and restricted to only include this rank’s part of the
    optimizer state.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在完整的优化器状态字典已重新映射为扁平化参数，而不是未扁平化参数，并且仅限于包括此排名部分的优化器状态。
- en: Return type
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 字典[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
- en: '[PRE30]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Return the optimizer state-dict in its sharded form.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 以其分片形式返回优化器状态字典。
- en: The API is similar to [`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict") but this
    API chunks all non-zero-dimension states to `ShardedTensor` to save memory. This
    API should only be used when the model `state_dict` is derived with the context
    manager `with state_dict_type(SHARDED_STATE_DICT):`.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: API类似于[`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict")，但此API将所有非零维状态分块为`ShardedTensor`以节省内存。当使用上下文管理器`with
    state_dict_type(SHARDED_STATE_DICT):`派生模型`state_dict`时，应仅使用此API。
- en: For the detailed usage, refer to [`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict").
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细用法，请参考[`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict")。
- en: Warning
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: The returned state dict contains `ShardedTensor` and cannot be directly used
    by the regular `optim.load_state_dict`.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的状态字典包含`ShardedTensor`，不能直接被常规的`optim.load_state_dict`使用。
- en: Return type
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python
    v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)"), [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in
    Python v3.12)")]'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[*字典*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python
    v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)"), [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in
    Python v3.12)")]'
- en: '[PRE31]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Set the `state_dict_type` of all the descendant FSDP modules of the target module.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 设置目标模块的所有后代FSDP模块的`state_dict_type`。
- en: This context manager has the same functions as [`set_state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type"). Read the
    document of [`set_state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type") for the
    detail.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 此上下文管理器具有与[`set_state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type")相同的功能。阅读[`set_state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type")的文档以获取详细信息。
- en: 'Example:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE32]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**module** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**module**（[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module"））- 根模块。'
- en: '**state_dict_type** (*StateDictType*) – the desired `state_dict_type` to set.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict_type**（*StateDictType*） - 要设置的期望`state_dict_type`。'
- en: '**state_dict_config** (*Optional**[*[*StateDictConfig*](#torch.distributed.fsdp.StateDictConfig
    "torch.distributed.fsdp.StateDictConfig")*]*) – the model `state_dict` configuration
    for the target `state_dict_type`.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**state_dict_config**（*可选**[*[*StateDictConfig*](#torch.distributed.fsdp.StateDictConfig
    "torch.distributed.fsdp.StateDictConfig")*]*) - 目标`state_dict_type`的模型`state_dict`配置。'
- en: '**optim_state_dict_config** (*Optional**[*[*OptimStateDictConfig*](#torch.distributed.fsdp.OptimStateDictConfig
    "torch.distributed.fsdp.OptimStateDictConfig")*]*) – the optimizer `state_dict`
    configuration for the target `state_dict_type`.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optim_state_dict_config**（*可选**[*[*OptimStateDictConfig*](#torch.distributed.fsdp.OptimStateDictConfig
    "torch.distributed.fsdp.OptimStateDictConfig")*]*) - 目标`state_dict_type`的优化器`state_dict`配置。'
- en: Return type
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Generator*](https://docs.python.org/3/library/typing.html#typing.Generator
    "(in Python v3.12)")'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[*生成器*](https://docs.python.org/3/library/typing.html#typing.Generator "(在Python
    v3.12中)")'
- en: '[PRE33]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Expose full params for FSDP instances with this context manager.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此上下文管理器为FSDP实例公开完整参数。
- en: Can be useful *after* forward/backward for a model to get the params for additional
    processing or checking. It can take a non-FSDP module and will summon full params
    for all contained FSDP modules as well as their children, depending on the `recurse`
    argument.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型进行前向/反向传播之后，可以用于获取参数以进行额外处理或检查。它可以接受一个非FSDP模块，并将召唤所有包含的FSDP模块以及它们的子模块的完整参数，取决于`recurse`参数。
- en: Note
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This can be used on inner FSDPs.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以在内部FSDP上使用。
- en: Note
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This can *not* be used within a forward or backward pass. Nor can forward and
    backward be started from within this context.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这不能在前向或反向传递中使用。也不能在此上下文中启动前向和反向传递。
- en: Note
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Parameters will revert to their local shards after the context manager exits,
    storage behavior is the same as forward.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 参数在上下文管理器退出后将恢复为其本地分片，存储行为与前向传播相同。
- en: Note
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The full parameters can be modified, but only the portion corresponding to the
    local param shard will persist after the context manager exits (unless `writeback=False`,
    in which case changes will be discarded). In the case where FSDP does not shard
    the parameters, currently only when `world_size == 1`, or `NO_SHARD` config, the
    modification is persisted regardless of `writeback`.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 完整参数可以被修改，但只有对应于本地参数分片的部分将在上下文管理器退出后保留（除非`writeback=False`，在这种情况下更改将被丢弃）。在FSDP不对参数进行分片的情况下，目前仅当`world_size
    == 1`或`NO_SHARD`配置时，修改将被持久化，无论`writeback`如何。
- en: Note
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This method works on modules which are not FSDP themselves but may contain multiple
    independent FSDP units. In that case, the given arguments will apply to all contained
    FSDP units.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法适用于本身不是FSDP但可能包含多个独立FSDP单元的模块。在这种情况下，给定的参数将适用于所有包含的FSDP单元。
- en: Warning
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Note that `rank0_only=True` in conjunction with `writeback=True` is not currently
    supported and will raise an error. This is because model parameter shapes would
    be different across ranks within the context, and writing to them can lead to
    inconsistency across ranks when the context is exited.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`rank0_only=True`与`writeback=True`结合使用目前不受支持，将引发错误。这是因为模型参数形状在上下文中的不同排名之间会有所不同，对其进行写入可能会导致在退出上下文时跨排名之间的不一致性。
- en: Warning
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Note that `offload_to_cpu` and `rank0_only=False` will result in full parameters
    being redundantly copied to CPU memory for GPUs that reside on the same machine,
    which may incur the risk of CPU OOM. It is recommended to use `offload_to_cpu`
    with `rank0_only=True`.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`offload_to_cpu`和`rank0_only=False`会导致完整参数被冗余地复制到CPU内存中，对于与GPU位于同一台机器上的情况，这可能会带来CPU内存溢出的风险。建议使用`offload_to_cpu`与`rank0_only=True`。
- en: Parameters
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**recurse** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *Optional*) – recursively summon all params for nested
    FSDP instances (default: True).'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**recurse**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在Python v3.12中)")*,* *可选*) - 递归召唤所有嵌套FSDP实例的参数（默认值：True）。'
- en: '**writeback** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *Optional*) – if `False`, modifications to params are
    discarded after the context manager exits; disabling this can be slightly more
    efficient (default: True)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**writeback**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在Python v3.12中)")*,* *可选*) - 如果为`False`，则在上下文管理器退出后对参数的修改将被丢弃；禁用此选项可能会稍微更有效（默认值：True）'
- en: '**rank0_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *Optional*) – if `True`, full parameters are materialized
    on only global rank 0\. This means that within the context, only rank 0 will have
    full parameters and the other ranks will have sharded parameters. Note that setting
    `rank0_only=True` with `writeback=True` is not supported, as model parameter shapes
    will be different across ranks within the context, and writing to them can lead
    to inconsistency across ranks when the context is exited.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rank0_only**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在Python v3.12中)")*,* *可选*) - 如果为`True`，则只有全局排名0上的完整参数会被实现。这意味着在上下文中，只有排名0会有完整参数，其他排名将具有分片参数。请注意，在`rank0_only=True`与`writeback=True`一起使用时不受支持，因为模型参数形状在上下文中的不同排名之间会有所不同，对其进行写入可能会导致在退出上下文时跨排名之间的不一致性。'
- en: '**offload_to_cpu** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *Optional*) – If `True`, full parameters are offloaded
    to CPU. Note that this offloading currently only occurs if the parameter is sharded
    (which is only not the case for world_size = 1 or `NO_SHARD` config). It is recommended
    to use `offload_to_cpu` with `rank0_only=True` to avoid redundant copies of model
    parameters being offloaded to the same CPU memory.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**offload_to_cpu**（[*布尔*](https://docs.python.org/3/library/functions.html#bool
    "(在Python v3.12版本)")*,* *可选*) – 如果为`True`，完整的参数会被卸载到CPU。请注意，只有在参数被分片时（对于world_size
    = 1或`NO_SHARD`配置除外），才会发生此卸载。建议使用`offload_to_cpu`与`rank0_only=True`一起使用，以避免将模型参数的冗余副本卸载到相同的CPU内存中。'
- en: '**with_grads** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *Optional*) – If `True`, gradients are also unsharded
    with the parameters. Currently, this is only supported when passing `use_orig_params=True`
    to the FSDP constructor and `offload_to_cpu=False` to this method. (Default: `False`)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**with_grads**（[*布尔*](https://docs.python.org/3/library/functions.html#bool
    "(在Python v3.12版本)")*,* *可选*) – 如果为`True`，梯度也会与参数一起取消分片。目前，只有在将`use_orig_params=True`传递给FSDP构造函数并将`offload_to_cpu=False`传递给此方法时才支持此功能。（默认值：`False`）'
- en: Return type
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Generator*](https://docs.python.org/3/library/typing.html#typing.Generator
    "(in Python v3.12)")'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[*生成器*](https://docs.python.org/3/library/typing.html#typing.Generator "(在Python
    v3.12版本)")'
- en: '[PRE34]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This configures explicit backward prefetching, which improves throughput by
    enabling communication and computation overlap in the backward pass at the cost
    of slightly increased memory usage.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这配置了显式的向后预取，通过在向后传递中启用通信和计算重叠来提高吞吐量，但会略微增加内存使用量。
- en: '`BACKWARD_PRE`: This enables the most overlap but increases memory usage the
    most. This prefetches the next set of parameters *before* the current set of parameters’
    gradient computation. This overlaps the *next all-gather* and the *current gradient
    computation*, and at the peak, it holds the current set of parameters, next set
    of parameters, and current set of gradients in memory.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BACKWARD_PRE`：这会增加最多的重叠，但也会增加最多的内存使用量。这会在当前一组参数的梯度计算*之前*预取下一组参数。这会重叠*下一个全局聚集*和*当前梯度计算*，在峰值时，它会在内存中保存当前一组参数、下一组参数和当前一组梯度。'
- en: '`BACKWARD_POST`: This enables less overlap but requires less memory usage.
    This prefetches the next set of parameters *after* the current set of parameters’
    gradient computation. This overlaps the *current reduce-scatter* and the *next
    gradient computation*, and it frees the current set of parameters before allocating
    memory for the next set of parameters, only holding the next set of parameters
    and current set of gradients in memory at the peak.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BACKWARD_POST`：这会减少重叠，但需要更少的内存使用量。这会在当前一组参数的梯度计算*之后*预取下一组参数。这会重叠*当前的reduce-scatter*和*下一个梯度计算*，并在为下一组参数分配内存之前释放当前一组参数，仅在内存中保留下一组参数和当前一组梯度。'
- en: FSDP’s `backward_prefetch` argument accepts `None`, which disables the backward
    prefetching altogether. This has no overlap and does not increase memory usage.
    In general, we do not recommend this setting since it may degrade throughput significantly.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FSDP的`backward_prefetch`参数接受`None`，这会完全禁用向后预取。这不会重叠，也不会增加内存使用量。总的来说，我们不建议使用这个设置，因为它可能会显著降低吞吐量。
- en: 'For more technical context: For a single process group using NCCL backend,
    any collectives, even if issued from different streams, contend for the same per-device
    NCCL stream, which implies that the relative order in which the collectives are
    issued matters for overlapping. The two backward prefetching values correspond
    to different issue orders.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 更多技术背景：对于使用NCCL后端的单个进程组，任何集合，即使从不同流发出，也会争夺相同的每个设备NCCL流，这意味着发出集合的相对顺序对于重叠很重要。两个向后预取值对应不同的发出顺序。
- en: '[PRE35]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This specifies the sharding strategy to be used for distributed training by
    [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel").
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这指定了由[`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")用于分布式训练的分片策略。
- en: '`FULL_SHARD`: Parameters, gradients, and optimizer states are sharded. For
    the parameters, this strategy unshards (via all-gather) before the forward, reshards
    after the forward, unshards before the backward computation, and reshards after
    the backward computation. For gradients, it synchronizes and shards them (via
    reduce-scatter) after the backward computation. The sharded optimizer states are
    updated locally per rank.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FULL_SHARD`：参数、梯度和优化器状态被分片。对于参数，此策略在前向传递之前取消分片（通过全局聚集），在前向传递后重新分片，在向后计算之前取消分片，并在向后计算后重新分片。对于梯度，它在向后计算后同步和分片它们（通过reduce-scatter）。分片的优化器状态在每个秩上本地更新。'
- en: '`SHARD_GRAD_OP`: Gradients and optimizer states are sharded during computation,
    and additionally, parameters are sharded outside computation. For the parameters,
    this strategy unshards before the forward, does not reshard them after the forward,
    and only reshards them after the backward computation. The sharded optimizer states
    are updated locally per rank. Inside `no_sync()`, the parameters are not resharded
    after the backward computation.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SHARD_GRAD_OP`：梯度和优化器状态在计算过程中被分片，此外，参数在计算之外被分片。对于参数，此策略在前向传递之前取消分片，在前向传递后不再分片，仅在向后计算后重新分片。分片的优化器状态在每个秩上本地更新。在`no_sync()`中，参数在向后计算后不再分片。'
- en: '`NO_SHARD`: Parameters, gradients, and optimizer states are not sharded but
    instead replicated across ranks similar to PyTorch’s `DistributedDataParallel`
    API. For gradients, this strategy synchronizes them (via all-reduce) after the
    backward computation. The unsharded optimizer states are updated locally per rank.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NO_SHARD`：参数、梯度和优化器状态不分片，而是在各个秩之间复制，类似于PyTorch的`DistributedDataParallel`API。对于梯度，此策略在向后计算后同步它们（通过全局归约）。未分片的优化器状态在每个秩上本地更新。'
- en: '`HYBRID_SHARD`: Apply `FULL_SHARD` within a node, and replicate parameters
    across nodes. This results in reduced communication volume as expensive all-gathers
    and reduce-scatters are only done within a node, which can be more performant
    for medium -sized models.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HYBRID_SHARD`：在节点内应用`FULL_SHARD`，并在节点之间复制参数。这会减少通信量，因为昂贵的所有收集和减少散射仅在节点内完成，对于中等大小的模型可能更高效。'
- en: '`_HYBRID_SHARD_ZERO2`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters
    across nodes. This is like `HYBRID_SHARD`, except this may provide even higher
    throughput since the unsharded parameters are not freed after the forward pass,
    saving the all-gathers in the pre-backward.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_HYBRID_SHARD_ZERO2`：在节点内应用`SHARD_GRAD_OP`，并在节点之间复制参数。这类似于`HYBRID_SHARD`，不同之处在于在前向传递后不释放未分片参数，从而节省了前向传递中的所有收集操作。'
- en: '[PRE36]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This configures FSDP-native mixed precision training.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这配置了FSDP本机混合精度训练。
- en: Variables
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 变量
- en: '**param_dtype** (*Optional**[*[*torch.dtype*](tensor_attributes.html#torch.dtype
    "torch.dtype")*]*) – This specifies the dtype for model parameters during forward
    and backward and thus the dtype for forward and backward computation. Outside
    forward and backward, the *sharded* parameters are kept in full precision (e.g.
    for the optimizer step), and for model checkpointing, the parameters are always
    saved in full precision. (Default: `None`)'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**param_dtype** (*Optional**[*[*torch.dtype*](tensor_attributes.html#torch.dtype
    "torch.dtype")*]*) – 这指定了模型参数在前向和反向期间的数据类型，因此也是前向和反向计算的数据类型。在前向和反向之外，*分片*参数保持全精度（例如，用于优化器步骤），并且对于模型检查点，参数始终以全精度保存。（默认值：`None`）'
- en: '**reduce_dtype** (*Optional**[*[*torch.dtype*](tensor_attributes.html#torch.dtype
    "torch.dtype")*]*) – This specifies the dtype for gradient reduction (i.e. reduce-scatter
    or all-reduce). If this is `None` but `param_dtype` is not `None`, then this takes
    on the `param_dtype` value, still running gradient reduction in low precision.
    This is permitted to differ from `param_dtype`, e.g. to force gradient reduction
    to run in full precision. (Default: `None`)'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**reduce_dtype** (*Optional**[*[*torch.dtype*](tensor_attributes.html#torch.dtype
    "torch.dtype")*]*) – 这指定了梯度减少的数据类型（即reduce-scatter或all-reduce）。如果这是`None`，但`param_dtype`不是`None`，那么它将采用`param_dtype`的值，仍然以低精度运行梯度减少。这允许与`param_dtype`不同，例如强制梯度减少以全精度运行。（默认值：`None`）'
- en: '**buffer_dtype** (*Optional**[*[*torch.dtype*](tensor_attributes.html#torch.dtype
    "torch.dtype")*]*) – This specifies the dtype for buffers. FSDP does not shard
    buffers. Rather, FSDP casts them to `buffer_dtype` in the first forward pass and
    keeps them in that dtype thereafter. For model checkpointing, the buffers are
    saved in full precision except for `LOCAL_STATE_DICT`. (Default: `None`)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**buffer_dtype** (*Optional**[*[*torch.dtype*](tensor_attributes.html#torch.dtype
    "torch.dtype")*]*) – 这指定了缓冲区的数据类型。FSDP不对缓冲区进行分片。相反，FSDP在第一次前向传递中将它们转换为`buffer_dtype`，并在此后保持该数据类型。对于模型检查点，缓冲区以全精度保存，除了`LOCAL_STATE_DICT`。（默认值：`None`）'
- en: '**keep_low_precision_grads** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `False`, then FSDP upcasts gradients to full precision
    after the backward pass in preparation for the optimizer step. If `True`, then
    FSDP keeps the gradients in the dtype used for gradient reduction, which can save
    memory if using a custom optimizer that supports running in low precision. (Default:
    `False`)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keep_low_precision_grads** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – 如果为`False`，则FSDP在反向传递后将梯度向上转换为全精度，以准备进行优化器步骤。如果为`True`，则FSDP保持梯度在用于梯度减少的数据类型中，这可以节省内存，如果使用支持低精度运行的自定义优化器。（默认值：`False`）'
- en: '**cast_forward_inputs** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then this FSDP module casts its forward args
    and kwargs to `param_dtype`. This is to ensure that parameter and input dtypes
    match for forward computation, as required by many ops. This may need to be set
    to `True` when only applying mixed precision to some but not all FSDP modules,
    in which case a mixed-precision FSDP submodule needs to recast its inputs. (Default:
    `False`)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cast_forward_inputs** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – 如果为`True`，则此FSDP模块将其前向参数和kwargs转换为`param_dtype`。这是为了确保前向计算所需的参数和输入数据类型匹配，许多操作都需要。当仅对一些而不是所有FSDP模块应用混合精度时，可能需要将其设置为`True`，在这种情况下，混合精度FSDP子模块需要重新转换其输入。（默认值：`False`）'
- en: '**cast_root_forward_inputs** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then the root FSDP module casts its forward
    args and kwargs to `param_dtype`, overriding the value of `cast_forward_inputs`.
    For non-root FSDP modules, this does not do anything. (Default: `True`)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cast_root_forward_inputs** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – 如果为`True`，则根FSDP模块将其前向参数和kwargs转换为`param_dtype`，覆盖`cast_forward_inputs`的值。对于非根FSDP模块，这不会做任何事情。（默认值：`True`）'
- en: '**_module_classes_to_ignore** (*Sequence**[**Type**[*[*torch.nn.modules.module.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.modules.module.Module")*]**]*) – (Sequence[Type[nn.Module]]): This specifies
    module classes to ignore for mixed precision when using an `auto_wrap_policy`:
    Modules of these classes will have FSDP applied to them separately with mixed
    precision disabled (meaning that the final FSDP construction would deviate from
    the specified policy). If `auto_wrap_policy` is not specified, then this does
    not do anything. This API is experimental and subject to change. (Default: `(_BatchNorm,)`)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**_module_classes_to_ignore** (*Sequence**[**Type**[*[*torch.nn.modules.module.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.modules.module.Module")*]**]*) – （Sequence[Type[nn.Module]]）：这指定了在使用`auto_wrap_policy`时要忽略的模块类别：这些类别的模块将单独应用FSDP，关闭混合精度（意味着最终的FSDP构造将偏离指定的策略）。如果未指定`auto_wrap_policy`，则这不会做任何事情。此API是实验性的，可能会更改。（默认值：`(_BatchNorm,)`）'
- en: Note
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This API is experimental and subject to change.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 此API是实验性的，可能会更改。
- en: Note
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Only floating point tensors are cast to their specified dtypes.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 只有浮点张量会被转换为指定的数据类型。
- en: Note
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In `summon_full_params`, parameters are forced to full precision, but buffers
    are not.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `summon_full_params` 中，参数被强制转换为全精度，但缓冲区不会。
- en: Note
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Layer norm and batch norm accumulate in `float32` even when their inputs are
    in a low precision like `float16` or `bfloat16`. Disabling FSDP’s mixed precision
    for those norm modules only means that the affine parameters are kept in `float32`.
    However, this incurs separate all-gathers and reduce-scatters for those norm modules,
    which may be inefficient, so if the workload permits, the user should prefer to
    still apply mixed precision to those modules.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化和批量归一化即使在其输入为低精度（如 `float16` 或 `bfloat16`）时也会累积为 `float32`。对于这些归一化模块禁用 FSDP
    的混合精度只意味着仿射参数保留为 `float32`。然而，这会为这些归一化模块产生单独的全局收集和减少散播，这可能是低效的，因此如果工作负载允许，用户应该仍然将混合精度应用于这些模块。
- en: Note
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: By default, if the user passes a model with any `_BatchNorm` modules and specifies
    an `auto_wrap_policy`, then the batch norm modules will have FSDP applied to them
    separately with mixed precision disabled. See the `_module_classes_to_ignore`
    argument.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，如果用户传递了一个带有任何 `_BatchNorm` 模块并指定了 `auto_wrap_policy` 的模型，那么批量归一化模块将单独应用
    FSDP，但混合精度被禁用。请参阅 `_module_classes_to_ignore` 参数。
- en: Note
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`MixedPrecision` has `cast_root_forward_inputs=True` and `cast_forward_inputs=False`
    by default. For the root FSDP instance, its `cast_root_forward_inputs` takes precedence
    over its `cast_forward_inputs`. For non-root FSDP instances, their `cast_root_forward_inputs`
    values are ignored. The default setting is sufficient for the typical case where
    each FSDP instance has the same `MixedPrecision` configuration and only needs
    to cast inputs to the `param_dtype` at the beginning of the model’s forward pass.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '`MixedPrecision` 默认情况下具有 `cast_root_forward_inputs=True` 和 `cast_forward_inputs=False`。对于根
    FSDP 实例，其 `cast_root_forward_inputs` 优先于其 `cast_forward_inputs`。对于非根 FSDP 实例，它们的
    `cast_root_forward_inputs` 值将被忽略。默认设置对于典型情况足够，其中每个 FSDP 实例具有相同的 `MixedPrecision`
    配置，并且只需要在模型前向传递的开始时将输入转换为 `param_dtype`。'
- en: Note
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For nested FSDP instances with different `MixedPrecision` configurations, we
    recommend setting individual `cast_forward_inputs` values to configure casting
    inputs or not before each instance’s forward. In such a case, since the casts
    happen before each FSDP instance’s forward, a parent FSDP instance should have
    its non-FSDP submodules run before its FSDP submodules to avoid the activation
    dtype being changed due to a different `MixedPrecision` configuration.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有不同 `MixedPrecision` 配置的嵌套 FSDP 实例，我们建议设置单独的 `cast_forward_inputs` 值来配置每个实例的前向传递之前是否转换输入。在这种情况下，由于转换发生在每个
    FSDP 实例的前向传递之前，父 FSDP 实例应该在其 FSDP 子模块之前运行其非 FSDP 子模块，以避免由于不同的 `MixedPrecision`
    配置而导致激活数据类型发生变化。
- en: 'Example:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE37]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The above shows a working example. On the other hand, if `model[1]` were replaced
    with `model[0]`, meaning that the submodule using different `MixedPrecision` ran
    its forward first, then `model[1]` would incorrectly see `float16` activations
    instead of `bfloat16` ones.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 上面展示了一个工作示例。另一方面，如果 `model[1]` 被替换为 `model[0]`，意味着使用不同 `MixedPrecision` 的子模块先运行其前向传播，那么
    `model[1]` 将错误地看到 `float16` 激活而不是 `bfloat16` 的激活。
- en: '[PRE38]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This configures CPU offloading.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这配置了 CPU 卸载。
- en: Variables
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 变量
- en: '**offload_params** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – This specifies whether to offload parameters to CPU when
    not involved in computation. If `True`, then this offloads gradients to CPU as
    well, meaning that the optimizer step runs on CPU.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**offload_params** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – 这指定是否在计算中未涉及时将参数卸载到 CPU。如果为 `True`，则梯度也会被卸载到 CPU，这意味着优化器步骤在
    CPU 上运行。'
- en: '[PRE39]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`StateDictConfig` is the base class for all `state_dict` configuration classes.
    Users should instantiate a child class (e.g. `FullStateDictConfig`) in order to
    configure settings for the corresponding `state_dict` type supported by FSDP.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '`StateDictConfig` 是所有 `state_dict` 配置类的基类。用户应该实例化一个子类（例如 `FullStateDictConfig`）以配置由
    FSDP 支持的相应 `state_dict` 类型的设置。'
- en: Variables
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 变量
- en: '**offload_to_cpu** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP offloads the state dict values to
    CPU, and if `False`, then FSDP keeps them on GPU. (Default: `False`)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '**offload_to_cpu** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – 如果为 `True`，则 FSDP 将状态字典值卸载到 CPU，如果为 `False`，则 FSDP 保留在
    GPU 上。（默认值：`False`）'
- en: '[PRE40]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`FullStateDictConfig` is a config class meant to be used with `StateDictType.FULL_STATE_DICT`.
    We recommend enabling both `offload_to_cpu=True` and `rank0_only=True` when saving
    full state dicts to save GPU memory and CPU memory, respectively. This config
    class is meant to be used via the `state_dict_type()` context manager as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`FullStateDictConfig` 是一个配置类，用于与 `StateDictType.FULL_STATE_DICT` 一起使用。我们建议在保存完整状态字典时分别启用
    `offload_to_cpu=True` 和 `rank0_only=True`，以节省 GPU 内存和 CPU 内存。这个配置类应该通过 `state_dict_type()`
    上下文管理器来使用，如下所示：'
- en: '[PRE41]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Variables
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 变量
- en: '**rank0_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then only rank 0 saves the full state dict,
    and nonzero ranks save an empty dict. If `False`, then all ranks save the full
    state dict. (Default: `False`)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**rank0_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – 如果为 `True`，则只有排名为 0 的进程保存完整状态字典，非零排名的进程保存一个空字典。如果为 `False`，则所有排名都保存完整状态字典。（默认值：`False`）'
- en: '[PRE42]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`ShardedStateDictConfig` is a config class meant to be used with `StateDictType.SHARDED_STATE_DICT`.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '`ShardedStateDictConfig` 是一个配置类，用于与 `StateDictType.SHARDED_STATE_DICT` 一起使用。'
- en: Variables
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 变量
- en: '**_use_dtensor** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP saves the state dict values as `DTensor`,
    and if `False`, then FSDP saves them as `ShardedTensor`. (Default: `False`)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '**_use_dtensor** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – 如果为 `True`，则 FSDP 将状态字典值保存为 `DTensor`，如果为 `False`，则 FSDP
    将它们保存为 `ShardedTensor`。（默认值：`False`）'
- en: Warning
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: '`_use_dtensor` is a private field of [`ShardedStateDictConfig`](#torch.distributed.fsdp.ShardedStateDictConfig
    "torch.distributed.fsdp.ShardedStateDictConfig") and it is used by FSDP to determine
    the type of state dict values. Users should not manually modify `_use_dtensor`.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '`_use_dtensor`是[`ShardedStateDictConfig`](#torch.distributed.fsdp.ShardedStateDictConfig
    "torch.distributed.fsdp.ShardedStateDictConfig")的私有字段，由FSDP用于确定状态字典值的类型。用户不应手动修改`_use_dtensor`。'
- en: '[PRE43]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`OptimStateDictConfig` is the base class for all `optim_state_dict` configuration
    classes. Users should instantiate a child class (e.g. `FullOptimStateDictConfig`)
    in order to configure settings for the corresponding `optim_state_dict` type supported
    by FSDP.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '`OptimStateDictConfig`是所有`optim_state_dict`配置类的基类。用户应该实例化一个子类（例如`FullOptimStateDictConfig`）以配置由FSDP支持的相应`optim_state_dict`类型的设置。'
- en: Variables
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 变量
- en: '**offload_to_cpu** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP offloads the state dict’s tensor
    values to CPU, and if `False`, then FSDP keeps them on the original device (which
    is GPU unless parameter CPU offloading is enabled). (Default: `True`)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**offload_to_cpu**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在Python v3.12中)")） - 如果为`True`，则FSDP将状态字典的张量值转移到CPU，如果为`False`，则FSDP将其保留在原始设备上（即GPU，除非启用了CPU卸载参数）。
    （默认值：`True`）'
- en: '[PRE45]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Variables
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 变量
- en: '**rank0_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then only rank 0 saves the full state dict,
    and nonzero ranks save an empty dict. If `False`, then all ranks save the full
    state dict. (Default: `False`)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '**rank0_only**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在Python v3.12中)")） - 如果为`True`，则只有排名为0的进程保存完整的状态字典，非零排名的进程保存空字典。如果为`False`，则所有进程都保存完整的状态字典。（默认值：`False`）'
- en: '[PRE46]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '`ShardedOptimStateDictConfig` is a config class meant to be used with `StateDictType.SHARDED_STATE_DICT`.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '`ShardedOptimStateDictConfig`是一个配置类，用于与`StateDictType.SHARDED_STATE_DICT`一起使用。'
- en: Variables
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 变量
- en: '**_use_dtensor** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP saves the state dict values as `DTensor`,
    and if `False`, then FSDP saves them as `ShardedTensor`. (Default: `False`)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '**_use_dtensor**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在Python v3.12中)")） - 如果为`True`，则FSDP将状态字典的值保存为`DTensor`，如果为`False`，则FSDP将其保存为`ShardedTensor`。（默认值：`False`）'
- en: Warning
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: '`_use_dtensor` is a private field of [`ShardedOptimStateDictConfig`](#torch.distributed.fsdp.ShardedOptimStateDictConfig
    "torch.distributed.fsdp.ShardedOptimStateDictConfig") and it is used by FSDP to
    determine the type of state dict values. Users should not manually modify `_use_dtensor`.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '`_use_dtensor`是[`ShardedOptimStateDictConfig`](#torch.distributed.fsdp.ShardedOptimStateDictConfig
    "torch.distributed.fsdp.ShardedOptimStateDictConfig")的私有字段，由FSDP用于确定状态字典值的类型。用户不应手动修改`_use_dtensor`。'
- en: '[PRE47]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
