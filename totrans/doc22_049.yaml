- en: FullyShardedDataParallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/fsdp.html](https://pytorch.org/docs/stable/fsdp.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A wrapper for sharding module parameters across data parallel workers.
  prefs: []
  type: TYPE_NORMAL
- en: This is inspired by [Xu et al.](https://arxiv.org/abs/2004.13336) as well as
    the ZeRO Stage 3 from [DeepSpeed](https://www.deepspeed.ai/). FullyShardedDataParallel
    is commonly shortened to FSDP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer must be initialized *after* the module has been wrapped with FSDP
    since FSDP will shard and transform the module’s parameters in a way that may
    not preserve the original parameter variables. Thus, the previously initialized
    optimizer may have stale references to the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: If the destination CUDA device has ID `dev_id`, either (1) `module` should already
    be placed on that device, (2) the device should be set using `torch.cuda.set_device(dev_id)`,
    or (3) `dev_id` should be passed into the `device_id` constructor argument. This
    FSDP instance’s compute device will be that destination device. For (1) and (3),
    the FSDP initialization always occurs on GPU. For (2), the FSDP initialization
    happens on `module` ‘s current device, which may be CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: FSDP currently does not support gradient accumulation outside `no_sync()` when
    using CPU offloading. Trying to do so yields incorrect results since FSDP will
    use the newly-reduced gradient instead of accumulating with any existing gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Changing the original parameter variable names after construction will lead
    to undefined behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Passing in the `sync_module_states=True` flag requires `module` to be on GPU
    or to use the `device_id` argument to specify a CUDA device that FSDP will move
    `module` to in the FSDP constructor. This is because `sync_module_states=True`
    requires GPU communication.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: As of PyTorch 1.12, FSDP only offers limited support for shared parameters (for
    example, setting one `Linear` layer’s weight to another’s). In particular, modules
    that share parameters must be wrapped as part of the same FSDP unit. If enhanced
    shared parameter support is needed for your use case, please ping [https://github.com/pytorch/pytorch/issues/77724](https://github.com/pytorch/pytorch/issues/77724)
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: FSDP has some constraints on freezing parameters (i.e. setting `param.requires_grad=False`).
    For `use_orig_params=False`, each FSDP instance must manage parameters that are
    all frozen or all non-frozen. For `use_orig_params=True`, FSDP supports mixing
    frozen and non-frozen, but we recommend not doing so since then the gradient memory
    usage will be higher than expected (namely, equivalent to not freezing those parameters).
    This means that ideally, frozen parameters should be isolated into their own `nn.Module`
    s and wrapped separately with FSDP.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Attempting to run the forward pass of a submodule that is contained in an FSDP
    instance is not supported and will result in errors. This is because the submodule’s
    parameters will be sharded, but it itself is not an FSDP instance, so its forward
    pass will not all-gather the full parameters appropriately. This could potentially
    happen when attempting to run only the encoder of a encoder-decoder model, and
    the encoder is not wrapped in its own FSDP instance. To resolve this, please wrap
    the submodule in its own FSDP unit.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: FSDP moves input tensors to the `forward` method to the GPU compute device,
    so the user does not need to manually move them from CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The user should not modify the parameters between forward and backward without
    using the [`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params") context
    since the modifications may not persist. Moreover, for `use_orig_params=False`,
    accessing the original parameters between forward and backward may raise an illegal
    memory access.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: For `use_orig_params=True`, `ShardingStrategy.SHARD_GRAD_OP` exposes the unsharded
    parameters, not the sharded parameters, after forward since it does not free the
    unsharded ones, unlike `ShardingStrategy.FULL_SHARD`. One caveat is that, since
    gradients are always sharded or `None`, `ShardingStrategy.SHARD_GRAD_OP` will
    not expose the sharded gradients with the unsharded parameters after forward.
    If you want to inspect the gradients, try [`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params") with `with_grads=True`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: FSDP replaces managed modules’ parameters with `torch.Tensor` views during forward
    and backward computation for autograd-related reasons. If your module’s forward
    relies on saved references to the parameters instead of reacquiring the references
    each iteration, then it will not see FSDP’s newly created views, and autograd
    will not work correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With `limit_all_gathers=True`, you may see a gap in the FSDP pre-forward where
    the CPU thread is not issuing any kernels. This is intentional and shows the rate
    limiter in effect. Synchronizing the CPU thread in that way prevents over-allocating
    memory for subsequent all-gathers, and it should not actually delay GPU kernel
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When using `sharding_strategy=ShardingStrategy.HYBRID_SHARD` with the sharding
    process group being intra-node and the replication process group being inter-node,
    setting `NCCL_CROSS_NIC=1` can help improve the all-reduce times over the replication
    process group for some cluster setups.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**module** ([*nn.Module*](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module"))
    – This is the module to be wrapped with FSDP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**process_group** (*Optional**[**Union**[**ProcessGroup**,* *Tuple**[**ProcessGroup**,*
    *ProcessGroup**]**]**]*) – This is the process group over which the model is sharded
    and thus the one used for FSDP’s all-gather and reduce-scatter collective communications.
    If `None`, then FSDP uses the default process group. For hybrid sharding strategies
    such as `ShardingStrategy.HYBRID_SHARD`, users can pass in a tuple of process
    groups, representing the groups over which to shard and replicate, respectively.
    If `None`, then FSDP constructs process groups for the user to shard intra-node
    and replicate inter-node. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sharding_strategy** (*Optional**[*[*ShardingStrategy*](#torch.distributed.fsdp.ShardingStrategy
    "torch.distributed.fsdp.ShardingStrategy")*]*) – This configures the sharding
    strategy, which may trade off memory saving and communication overhead. See [`ShardingStrategy`](#torch.distributed.fsdp.ShardingStrategy
    "torch.distributed.fsdp.ShardingStrategy") for details. (Default: `FULL_SHARD`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cpu_offload** (*Optional**[*[*CPUOffload*](#torch.distributed.fsdp.CPUOffload
    "torch.distributed.fsdp.CPUOffload")*]*) – This configures CPU offloading. If
    this is set to `None`, then no CPU offloading happens. See [`CPUOffload`](#torch.distributed.fsdp.CPUOffload
    "torch.distributed.fsdp.CPUOffload") for details. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**auto_wrap_policy** (*Optional**[**Union**[**Callable**[**[*[*nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*,* [*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*]**,* [*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*]**,* *ModuleWrapPolicy**,* *CustomPolicy**]**]*) –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This specifies a policy to apply FSDP to submodules of `module`, which is needed
    for communication and computation overlap and thus affects performance. If `None`,
    then FSDP only applies to `module`, and users should manually apply FSDP to parent
    modules themselves (proceeding bottom-up). For convenience, this accepts `ModuleWrapPolicy`
    directly, which allows users to specify the module classes to wrap (e.g. the transformer
    block). Otherwise, this should be a callable that takes in three arguments `module:
    nn.Module`, `recurse: bool`, and `nonwrapped_numel: int` and should return a `bool`
    specifying whether the passed-in `module` should have FSDP applied if `recurse=False`
    or if the traversal should continue into the module’s subtree if `recurse=True`.
    Users may add additional arguments to the callable. The `size_based_auto_wrap_policy`
    in `torch.distributed.fsdp.wrap.py` gives an example callable that applies FSDP
    to a module if the parameters in its subtree exceed 100M numel. We recommend printing
    the model after applying FSDP and adjusting as needed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**backward_prefetch** (*Optional**[*[*BackwardPrefetch*](#torch.distributed.fsdp.BackwardPrefetch
    "torch.distributed.fsdp.BackwardPrefetch")*]*) – This configures explicit backward
    prefetching of all-gathers. If `None`, then FSDP does not backward prefetch, and
    there is no communication and computation overlap in the backward pass. See [`BackwardPrefetch`](#torch.distributed.fsdp.BackwardPrefetch
    "torch.distributed.fsdp.BackwardPrefetch") for details. (Default: `BACKWARD_PRE`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mixed_precision** (*Optional**[*[*MixedPrecision*](#torch.distributed.fsdp.MixedPrecision
    "torch.distributed.fsdp.MixedPrecision")*]*) – This configures native mixed precision
    for FSDP. If this is set to `None`, then no mixed precision is used. Otherwise,
    parameter, buffer, and gradient reduction dtypes can be set. See [`MixedPrecision`](#torch.distributed.fsdp.MixedPrecision
    "torch.distributed.fsdp.MixedPrecision") for details. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ignored_modules** (*Optional**[**Iterable**[*[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*]**]*) – Modules whose own parameters and child modules’ parameters
    and buffers are ignored by this instance. None of the modules directly in `ignored_modules`
    should be [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instances, and any child modules
    that are already-constructed [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instances will not be ignored
    if they are nested under this instance. This argument may be used to avoid sharding
    specific parameters at module granularity when using an `auto_wrap_policy` or
    if parameters’ sharding is not managed by FSDP. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**param_init_fn** (*Optional**[**Callable**[**[*[*nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*]**,* *None**]**]*) –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Callable[torch.nn.Module] -> None` that specifies how modules that are currently
    on the meta device should be initialized onto an actual device. As of v1.12, FSDP
    detects modules with parameters or buffers on meta device via `is_meta` and either
    applies `param_init_fn` if specified or calls `nn.Module.reset_parameters()` otherwise.
    For both cases, the implementation should *only* initialize the parameters/buffers
    of the module, not those of its submodules. This is to avoid re-initialization.
    In addition, FSDP also supports deferred initialization via torchdistX’s ([https://github.com/pytorch/torchdistX](https://github.com/pytorch/torchdistX))
    `deferred_init()` API, where the deferred modules are initialized by calling `param_init_fn`
    if specified or torchdistX’s default `materialize_module()` otherwise. If `param_init_fn`
    is specified, then it is applied to all meta-device modules, meaning that it should
    probably case on the module type. FSDP calls the initialization function before
    parameter flattening and sharding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**device_id** (*Optional**[**Union**[*[*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* [*torch.device*](tensor_attributes.html#torch.device "torch.device")*]**]*)
    – An `int` or `torch.device` giving the CUDA device on which FSDP initialization
    takes place, including the module initialization if needed and the parameter sharding.
    This should be specified to improve initialization speed if `module` is on CPU.
    If the default CUDA device was set (e.g. via `torch.cuda.set_device`), then the
    user may pass `torch.cuda.current_device` to this. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sync_module_states** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then each FSDP module will broadcast module
    parameters and buffers from rank 0 to ensure that they are replicated across ranks
    (adding communication overhead to this constructor). This can help load `state_dict`
    checkpoints via `load_state_dict` in a memory efficient way. See [`FullStateDictConfig`](#torch.distributed.fsdp.FullStateDictConfig
    "torch.distributed.fsdp.FullStateDictConfig") for an example of this. (Default:
    `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**forward_prefetch** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP *explicitly* prefetches the next
    forward-pass all-gather before the current forward computation. This is only useful
    for CPU-bound workloads, in which case issuing the next all-gather earlier may
    improve overlap. This should only be used for static-graph models since the prefetching
    follows the first iteration’s execution order. (Default: `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**limit_all_gathers** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP explicitly synchronizes the CPU thread
    to ensure GPU memory usage from only *two* consecutive FSDP instances (the current
    instance running computation and the next instance whose all-gather is prefetched).
    If `False`, then FSDP allows the CPU thread to issue all-gathers without any extra
    synchronization. (Default: `True`) We often refer to this feature as the “rate
    limiter”. This flag should only be set to `False` for specific CPU-bound workloads
    with low memory pressure in which case the CPU thread can aggressively issue all
    kernels without concern for the GPU memory usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_orig_params** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Setting this to `True` has FSDP use `module` ‘s original
    parameters. FSDP exposes those original parameters to the user via `nn.Module.named_parameters()`
    instead of FSDP’s internal `FlatParameter` s. This means that the optimizer step
    runs on the original parameters, enabling per-original-parameter hyperparameters.
    FSDP preserves the original parameter variables and manipulates their data between
    unsharded and sharded forms, where they are always views into the underlying unsharded
    or sharded `FlatParameter`, respectively. With the current algorithm, the sharded
    form is always 1D, losing the original tensor structure. An original parameter
    may have all, some, or none of its data present for a given rank. In the none
    case, its data will be like a size-0 empty tensor. Users should not author programs
    relying on what data is present for a given original parameter in its sharded
    form. `True` is required to use `torch.compile()`. Setting this to `False` exposes
    FSDP’s internal `FlatParameter` s to the user via `nn.Module.named_parameters()`.
    (Default: `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ignored_states** (*Optional**[**Iterable**[**torch.nn.Parameter**]**]**,*
    *Optional**[**Iterable**[*[*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")*]**]*) – Ignored parameters or modules that will not be managed
    by this FSDP instance, meaning that the parameters are not sharded and their gradients
    are not reduced across ranks. This argument unifies with the existing `ignored_modules`
    argument, and we may deprecate `ignored_modules` soon. For backward compatibility,
    we keep both `ignored_states` and ignored_modules`, but FSDP only allows one of
    them to be specified as not `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Apply `fn` recursively to every submodule (as returned by `.children()`) as
    well as self.
  prefs: []
  type: TYPE_NORMAL
- en: Typical use includes initializing the parameters of a model (see also [torch.nn.init](nn.init.html#nn-init-doc)).
  prefs: []
  type: TYPE_NORMAL
- en: Compared to `torch.nn.Module.apply`, this version additionally gathers the full
    parameters before applying `fn`. It should not be called from within another `summon_full_params`
    context.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**fn** (`Module` -> None) – function to be applied to each submodule'
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: self
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[Module](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Check if this instance is a root FSDP module.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Clip the gradient norm of all parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The norm is computed over all parameters’ gradients as viewed as a single vector,
    and the gradients are modified in-place.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**max_norm** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)") *or* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – max norm of the gradients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**norm_type** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)") *or* [*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – type of the used p-norm. Can be `''inf''` for infinity
    norm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Total norm of the parameters (viewed as a single vector).
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If every FSDP instance uses `NO_SHARD`, meaning that no gradients are sharded
    across ranks, then you may directly use [`torch.nn.utils.clip_grad_norm_()`](generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_
    "torch.nn.utils.clip_grad_norm_").
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If at least some FSDP instance uses a sharded strategy (i.e. one other than
    `NO_SHARD`), then you should use this method instead of [`torch.nn.utils.clip_grad_norm_()`](generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_
    "torch.nn.utils.clip_grad_norm_") since this method handles the fact that gradients
    are sharded across ranks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The total norm returned will have the “largest” dtype across all parameters/gradients
    as defined by PyTorch’s type promotion semantics. For example, if *all* parameters/gradients
    use a low precision dtype, then the returned norm’s dtype will be that low precision
    dtype, but if there exists at least one parameter/ gradient using FP32, then the
    returned norm’s dtype will be FP32.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This needs to be called on all ranks since it uses collective communications.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Flatten a sharded optimizer state-dict.
  prefs: []
  type: TYPE_NORMAL
- en: The API is similar to [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict").
    The only difference is that the input `sharded_optim_state_dict` should be returned
    from [`sharded_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict"). Therefore,
    there will be all-gather calls on each rank to gather `ShardedTensor` s.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**sharded_optim_state_dict** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*) – Optimizer state dict corresponding to the
    unflattened parameters and holding the sharded optimizer state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Refer to [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict").'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – Optimizer for `model` ‘s parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Refer to [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python
    v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)"), [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in
    Python v3.12)")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Run the forward pass for the wrapped module, inserting FSDP-specific pre- and
    post-forward sharding logic.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in Python
    v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Return all nested FSDP instances.
  prefs: []
  type: TYPE_NORMAL
- en: This possibly includes `module` itself and only includes FSDP root modules if
    `root_only=True`.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**module** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module, which may or may not be an `FSDP` module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**root_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Whether to return only FSDP root modules. (Default: `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: FSDP modules that are nested in the input `module`.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: List[[FullyShardedDataParallel](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel")]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Return the full optimizer state-dict.
  prefs: []
  type: TYPE_NORMAL
- en: Consolidates the full optimizer state on rank 0 and returns it as a [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)") following the convention of [`torch.optim.Optimizer.state_dict()`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict"), i.e. with keys `"state"` and `"param_groups"`.
    The flattened parameters in `FSDP` modules contained in `model` are mapped back
    to their unflattened parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This needs to be called on all ranks since it uses collective communications.
    However, if `rank0_only=True`, then the state dict is only populated on rank 0,
    and all other ranks return an empty [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)").
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Unlike `torch.optim.Optimizer.state_dict()`, this method uses full parameter
    names as keys instead of parameter IDs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Like in [`torch.optim.Optimizer.state_dict()`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict"), the tensors contained in the optimizer state
    dict are not cloned, so there may be aliasing surprises. For best practices, consider
    saving the returned optimizer state dict immediately, e.g. using `torch.save()`.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module (which may or may not be a [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instance) whose parameters
    were passed into the optimizer `optim`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – Optimizer for `model` ‘s parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim_input** (*Optional**[**Union**[**List**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]**,* *Iterable**[**torch.nn.Parameter**]**]**]*)
    – Input passed into the optimizer `optim` representing either a [`list`](https://docs.python.org/3/library/stdtypes.html#list
    "(in Python v3.12)") of parameter groups or an iterable of parameters; if `None`,
    then this method assumes the input was `model.parameters()`. This argument is
    deprecated, and there is no need to pass it in anymore. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rank0_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, saves the populated [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)") only on rank 0; if `False`, saves it on all ranks. (Default:
    `True`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*dist.ProcessGroup*) – Model’s process group or `None` if using
    the default process group. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A [`dict`](https://docs.python.org/3/library/stdtypes.html#dict "(in Python
    v3.12)") containing the optimizer state for `model` ‘s original unflattened parameters
    and including keys “state” and “param_groups” following the convention of [`torch.optim.Optimizer.state_dict()`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict"). If `rank0_only=True`, then nonzero ranks
    return an empty [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)").
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Get the state_dict_type and the corresponding configurations for the FSDP modules
    rooted at `module`.
  prefs: []
  type: TYPE_NORMAL
- en: The target module does not have to be an FSDP module.
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A `StateDictSettings` containing the state_dict_type and state_dict / optim_state_dict
    configs that are currently set.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '**AssertionError` if the StateDictSettings for differen** –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FSDP submodules differ.** –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*StateDictSettings*](#torch.distributed.fsdp.StateDictSettings "torch.distributed.fsdp.api.StateDictSettings")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Return the wrapped module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module buffers, yielding both the name of the buffer
    and the buffer itself.
  prefs: []
  type: TYPE_NORMAL
- en: Intercepts buffer names and removes all occurrences of the FSDP-specific flattened
    buffer prefix when inside the [`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params") context
    manager.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Iterator*](https://docs.python.org/3/library/typing.html#typing.Iterator
    "(in Python v3.12)")[[*Tuple*](https://docs.python.org/3/library/typing.html#typing.Tuple
    "(in Python v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [*Tensor*](tensors.html#torch.Tensor "torch.Tensor")]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Return an iterator over module parameters, yielding both the name of the parameter
    and the parameter itself.
  prefs: []
  type: TYPE_NORMAL
- en: Intercepts parameter names and removes all occurrences of the FSDP-specific
    flattened parameter prefix when inside the [`summon_full_params()`](#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params
    "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params") context
    manager.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Iterator*](https://docs.python.org/3/library/typing.html#typing.Iterator
    "(in Python v3.12)")[[*Tuple*](https://docs.python.org/3/library/typing.html#typing.Tuple
    "(in Python v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [*Parameter*](generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter
    "torch.nn.parameter.Parameter")]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Disable gradient synchronizations across FSDP instances.
  prefs: []
  type: TYPE_NORMAL
- en: Within this context, gradients will be accumulated in module variables, which
    will later be synchronized in the first forward-backward pass after exiting the
    context. This should only be used on the root FSDP instance and will recursively
    apply to all children FSDP instances.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This likely results in higher memory usage because FSDP will accumulate the
    full model gradients (instead of gradient shards) until the eventual sync.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When used with CPU offloading, the gradients will not be offloaded to CPU when
    inside the context manager. Instead, they will only be offloaded right after the
    eventual sync.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Generator*](https://docs.python.org/3/library/typing.html#typing.Generator
    "(in Python v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Transform the state-dict of an optimizer corresponding to a sharded model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The given state-dict can be transformed to one of three types: 1) full optimizer
    state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.'
  prefs: []
  type: TYPE_NORMAL
- en: For full optimizer state_dict, all states are unflattened and not sharded. Rank0
    only and CPU only can be specified via [`state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type") to avoid OOM.
  prefs: []
  type: TYPE_NORMAL
- en: For sharded optimizer state_dict, all states are unflattened but sharded. CPU
    only can be specified via [`state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type") to further
    save memory.
  prefs: []
  type: TYPE_NORMAL
- en: For local state_dict, no transformation will be performed. But a state will
    be converted from nn.Tensor to ShardedTensor to represent its sharding nature
    (this is not supported yet).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module (which may or may not be a [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instance) whose parameters
    were passed into the optimizer `optim`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – Optimizer for `model` ‘s parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim_state_dict** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*) – the target optimizer state_dict to transform.
    If the value is None, optim.state_dict() will be used. ( Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*dist.ProcessGroup*) – Model’s process group across which parameters
    are sharded or `None` if using the default process group. ( Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A [`dict`](https://docs.python.org/3/library/stdtypes.html#dict "(in Python
    v3.12)") containing the optimizer state for `model`. The sharding of the optimizer
    state is based on `state_dict_type`.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Convert an optimizer state-dict so that it can be loaded into the optimizer
    associated with the FSDP model.
  prefs: []
  type: TYPE_NORMAL
- en: Given a `optim_state_dict` that is transformed through [`optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict"), it gets converted
    to the flattened optimizer state_dict that can be loaded to `optim` which is the
    optimizer for `model`. `model` must be sharded by FullyShardedDataParallel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module (which may or may not be a [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instance) whose parameters
    were passed into the optimizer `optim`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim** ([*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer "torch.optim.Optimizer"))
    – Optimizer for `model` ‘s parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim_state_dict** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*) – The optimizer states to be loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**is_named_optimizer** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Is this optimizer a NamedOptimizer or KeyedOptimizer.
    Only set to True if `optim` is TorchRec’s KeyedOptimizer or torch.distributed’s
    NamedOptimizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**load_directly** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If this is set to True, this API will also call optim.load_state_dict(result)
    before returning the result. Otherwise, users are responsible to call `optim.load_state_dict()`
    (Default: `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*dist.ProcessGroup*) – Model’s process group across which parameters
    are sharded or `None` if using the default process group. ( Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python
    v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)"), [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in
    Python v3.12)")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Register a communication hook.
  prefs: []
  type: TYPE_NORMAL
- en: This is an enhancement that provides a flexible hook to users where they can
    specify how FSDP aggregates gradients across multiple workers. This hook can be
    used to implement several algorithms like [GossipGrad](https://arxiv.org/abs/1803.05880)
    and gradient compression which involve different communication strategies for
    parameter syncs while training with [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel").
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: FSDP communication hook should be registered before running an initial forward
    pass and only once.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**state** ([*object*](https://docs.python.org/3/library/functions.html#object
    "(in Python v3.12)")) –'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passed to the hook to maintain any state information during the training process.
    Examples include error feedback in gradient compression, peers to communicate
    with next in [GossipGrad](https://arxiv.org/abs/1803.05880), etc. It is locally
    stored by each worker and shared by all the gradient tensors on the worker.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**hook** (*Callable*) – Callable, which has one of the following signatures:
    1) `hook: Callable[torch.Tensor] -> None`: This function takes in a Python tensor,
    which represents the full, flattened, unsharded gradient with respect to all variables
    corresponding to the model this FSDP unit is wrapping (that are not wrapped by
    other FSDP sub-units). It then performs all necessary processing and returns `None`;
    2) `hook: Callable[torch.Tensor, torch.Tensor] -> None`: This function takes in
    two Python tensors, the first one represents the full, flattened, unsharded gradient
    with respect to all variables corresponding to the model this FSDP unit is wrapping
    (that are not wrapped by other FSDP sub-units). The latter represents a pre-sized
    tensor to store a chunk of a sharded gradient after reduction. In both cases,
    callable performs all necessary processing and returns `None`. Callables with
    signature 1 are expected to handle gradient communication for a NO_SHARD case.
    Callables with signature 2 are expected to handle gradient communication for sharded
    cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Re-keys the optimizer state dict `optim_state_dict` to use the key type `optim_state_key_type`.
  prefs: []
  type: TYPE_NORMAL
- en: This can be used to achieve compatibility between optimizer state dicts from
    models with FSDP instances and ones without.
  prefs: []
  type: TYPE_NORMAL
- en: 'To re-key an FSDP full optimizer state dict (i.e. from [`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict")) to use
    parameter IDs and be loadable to a non-wrapped model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To re-key a normal optimizer state dict from a non-wrapped model to be loadable
    to a wrapped model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer state dict re-keyed using the parameter keys specified by `optim_state_key_type`.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Scatter the full optimizer state dict from rank 0 to all other ranks.
  prefs: []
  type: TYPE_NORMAL
- en: Returns the sharded optimizer state dict on each rank. The return value is the
    same as [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict"),
    and on rank 0, the first argument should be the return value of [`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Both [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict")
    and [`scatter_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict")
    may be used to get the sharded optimizer state dict to load. Assuming that the
    full optimizer state dict resides in CPU memory, the former requires each rank
    to have the full dict in CPU memory, where each rank individually shards the dict
    without any communication, while the latter requires only rank 0 to have the full
    dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and
    communicates it to ranks appropriately. Hence, the former has higher aggregate
    CPU memory cost, while the latter has higher communication cost.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**full_optim_state_dict** (*Optional**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]*) – Optimizer state dict corresponding to the
    unflattened parameters and holding the full non-sharded optimizer state if on
    rank 0; the argument is ignored on nonzero ranks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module (which may or may not be a [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instance) whose parameters
    correspond to the optimizer state in `full_optim_state_dict`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim_input** (*Optional**[**Union**[**List**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]**,* *Iterable**[**torch.nn.Parameter**]**]**]*)
    – Input passed into the optimizer representing either a [`list`](https://docs.python.org/3/library/stdtypes.html#list
    "(in Python v3.12)") of parameter groups or an iterable of parameters; if `None`,
    then this method assumes the input was `model.parameters()`. This argument is
    deprecated, and there is no need to pass it in anymore. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim** (*Optional**[*[*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer
    "torch.optim.Optimizer")*]*) – Optimizer that will load the state dict returned
    by this method. This is the preferred argument to use over `optim_input`. (Default:
    `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**group** (*dist.ProcessGroup*) – Model’s process group or `None` if using
    the default process group. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: The full optimizer state dict now remapped to flattened parameters instead of
    unflattened parameters and restricted to only include this rank’s part of the
    optimizer state.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Set the `state_dict_type` of all the descendant FSDP modules of the target module.
  prefs: []
  type: TYPE_NORMAL
- en: Also takes (optional) configuration for the model’s and optimizer’s state dict.
    The target module does not have to be a FSDP module. If the target module is a
    FSDP module, its `state_dict_type` will also be changed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This API should be called for only the top-level (root) module.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This API enables users to transparently use the conventional `state_dict` API
    to take model checkpoints in cases where the root FSDP module is wrapped by another
    `nn.Module`. For example, the following will ensure `state_dict` is called on
    all non-FSDP instances, while dispatching into sharded_state_dict implementation
    for FSDP:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**module** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**state_dict_type** (*StateDictType*) – the desired `state_dict_type` to set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**state_dict_config** (*Optional**[*[*StateDictConfig*](#torch.distributed.fsdp.StateDictConfig
    "torch.distributed.fsdp.StateDictConfig")*]*) – the configuration for the target
    `state_dict_type`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim_state_dict_config** (*Optional**[*[*OptimStateDictConfig*](#torch.distributed.fsdp.OptimStateDictConfig
    "torch.distributed.fsdp.OptimStateDictConfig")*]*) – the configuration for the
    optimizer state dict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A StateDictSettings that include the previous state_dict type and configuration
    for the module.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*StateDictSettings*](#torch.distributed.fsdp.StateDictSettings "torch.distributed.fsdp.api.StateDictSettings")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Shard a full optimizer state-dict.
  prefs: []
  type: TYPE_NORMAL
- en: Remaps the state in `full_optim_state_dict` to flattened parameters instead
    of unflattened parameters and restricts to only this rank’s part of the optimizer
    state. The first argument should be the return value of [`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Both [`shard_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict")
    and [`scatter_full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict")
    may be used to get the sharded optimizer state dict to load. Assuming that the
    full optimizer state dict resides in CPU memory, the former requires each rank
    to have the full dict in CPU memory, where each rank individually shards the dict
    without any communication, while the latter requires only rank 0 to have the full
    dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and
    communicates it to ranks appropriately. Hence, the former has higher aggregate
    CPU memory cost, while the latter has higher communication cost.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**full_optim_state_dict** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]*) – Optimizer state dict corresponding to the
    unflattened parameters and holding the full non-sharded optimizer state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module (which may or may not be a [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel") instance) whose parameters
    correspond to the optimizer state in `full_optim_state_dict`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim_input** (*Optional**[**Union**[**List**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *Any**]**]**,* *Iterable**[**torch.nn.Parameter**]**]**]*)
    – Input passed into the optimizer representing either a [`list`](https://docs.python.org/3/library/stdtypes.html#list
    "(in Python v3.12)") of parameter groups or an iterable of parameters; if `None`,
    then this method assumes the input was `model.parameters()`. This argument is
    deprecated, and there is no need to pass it in anymore. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim** (*Optional**[*[*torch.optim.Optimizer*](optim.html#torch.optim.Optimizer
    "torch.optim.Optimizer")*]*) – Optimizer that will load the state dict returned
    by this method. This is the preferred argument to use over `optim_input`. (Default:
    `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: The full optimizer state dict now remapped to flattened parameters instead of
    unflattened parameters and restricted to only include this rank’s part of the
    optimizer state.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: Dict[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"),
    Any]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Return the optimizer state-dict in its sharded form.
  prefs: []
  type: TYPE_NORMAL
- en: The API is similar to [`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict") but this
    API chunks all non-zero-dimension states to `ShardedTensor` to save memory. This
    API should only be used when the model `state_dict` is derived with the context
    manager `with state_dict_type(SHARDED_STATE_DICT):`.
  prefs: []
  type: TYPE_NORMAL
- en: For the detailed usage, refer to [`full_optim_state_dict()`](#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict
    "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict").
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The returned state dict contains `ShardedTensor` and cannot be directly used
    by the regular `optim.load_state_dict`.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python
    v3.12)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)"), [*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in
    Python v3.12)")]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Set the `state_dict_type` of all the descendant FSDP modules of the target module.
  prefs: []
  type: TYPE_NORMAL
- en: This context manager has the same functions as [`set_state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type"). Read the
    document of [`set_state_dict_type()`](#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type
    "torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type") for the
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**module** ([*torch.nn.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")) – Root module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**state_dict_type** (*StateDictType*) – the desired `state_dict_type` to set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**state_dict_config** (*Optional**[*[*StateDictConfig*](#torch.distributed.fsdp.StateDictConfig
    "torch.distributed.fsdp.StateDictConfig")*]*) – the model `state_dict` configuration
    for the target `state_dict_type`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optim_state_dict_config** (*Optional**[*[*OptimStateDictConfig*](#torch.distributed.fsdp.OptimStateDictConfig
    "torch.distributed.fsdp.OptimStateDictConfig")*]*) – the optimizer `state_dict`
    configuration for the target `state_dict_type`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Generator*](https://docs.python.org/3/library/typing.html#typing.Generator
    "(in Python v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Expose full params for FSDP instances with this context manager.
  prefs: []
  type: TYPE_NORMAL
- en: Can be useful *after* forward/backward for a model to get the params for additional
    processing or checking. It can take a non-FSDP module and will summon full params
    for all contained FSDP modules as well as their children, depending on the `recurse`
    argument.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This can be used on inner FSDPs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This can *not* be used within a forward or backward pass. Nor can forward and
    backward be started from within this context.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Parameters will revert to their local shards after the context manager exits,
    storage behavior is the same as forward.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The full parameters can be modified, but only the portion corresponding to the
    local param shard will persist after the context manager exits (unless `writeback=False`,
    in which case changes will be discarded). In the case where FSDP does not shard
    the parameters, currently only when `world_size == 1`, or `NO_SHARD` config, the
    modification is persisted regardless of `writeback`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This method works on modules which are not FSDP themselves but may contain multiple
    independent FSDP units. In that case, the given arguments will apply to all contained
    FSDP units.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Note that `rank0_only=True` in conjunction with `writeback=True` is not currently
    supported and will raise an error. This is because model parameter shapes would
    be different across ranks within the context, and writing to them can lead to
    inconsistency across ranks when the context is exited.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Note that `offload_to_cpu` and `rank0_only=False` will result in full parameters
    being redundantly copied to CPU memory for GPUs that reside on the same machine,
    which may incur the risk of CPU OOM. It is recommended to use `offload_to_cpu`
    with `rank0_only=True`.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**recurse** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *Optional*) – recursively summon all params for nested
    FSDP instances (default: True).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**writeback** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *Optional*) – if `False`, modifications to params are
    discarded after the context manager exits; disabling this can be slightly more
    efficient (default: True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rank0_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *Optional*) – if `True`, full parameters are materialized
    on only global rank 0\. This means that within the context, only rank 0 will have
    full parameters and the other ranks will have sharded parameters. Note that setting
    `rank0_only=True` with `writeback=True` is not supported, as model parameter shapes
    will be different across ranks within the context, and writing to them can lead
    to inconsistency across ranks when the context is exited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**offload_to_cpu** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *Optional*) – If `True`, full parameters are offloaded
    to CPU. Note that this offloading currently only occurs if the parameter is sharded
    (which is only not the case for world_size = 1 or `NO_SHARD` config). It is recommended
    to use `offload_to_cpu` with `rank0_only=True` to avoid redundant copies of model
    parameters being offloaded to the same CPU memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**with_grads** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *Optional*) – If `True`, gradients are also unsharded
    with the parameters. Currently, this is only supported when passing `use_orig_params=True`
    to the FSDP constructor and `offload_to_cpu=False` to this method. (Default: `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Generator*](https://docs.python.org/3/library/typing.html#typing.Generator
    "(in Python v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This configures explicit backward prefetching, which improves throughput by
    enabling communication and computation overlap in the backward pass at the cost
    of slightly increased memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: '`BACKWARD_PRE`: This enables the most overlap but increases memory usage the
    most. This prefetches the next set of parameters *before* the current set of parameters’
    gradient computation. This overlaps the *next all-gather* and the *current gradient
    computation*, and at the peak, it holds the current set of parameters, next set
    of parameters, and current set of gradients in memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BACKWARD_POST`: This enables less overlap but requires less memory usage.
    This prefetches the next set of parameters *after* the current set of parameters’
    gradient computation. This overlaps the *current reduce-scatter* and the *next
    gradient computation*, and it frees the current set of parameters before allocating
    memory for the next set of parameters, only holding the next set of parameters
    and current set of gradients in memory at the peak.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FSDP’s `backward_prefetch` argument accepts `None`, which disables the backward
    prefetching altogether. This has no overlap and does not increase memory usage.
    In general, we do not recommend this setting since it may degrade throughput significantly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more technical context: For a single process group using NCCL backend,
    any collectives, even if issued from different streams, contend for the same per-device
    NCCL stream, which implies that the relative order in which the collectives are
    issued matters for overlapping. The two backward prefetching values correspond
    to different issue orders.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This specifies the sharding strategy to be used for distributed training by
    [`FullyShardedDataParallel`](#torch.distributed.fsdp.FullyShardedDataParallel
    "torch.distributed.fsdp.FullyShardedDataParallel").
  prefs: []
  type: TYPE_NORMAL
- en: '`FULL_SHARD`: Parameters, gradients, and optimizer states are sharded. For
    the parameters, this strategy unshards (via all-gather) before the forward, reshards
    after the forward, unshards before the backward computation, and reshards after
    the backward computation. For gradients, it synchronizes and shards them (via
    reduce-scatter) after the backward computation. The sharded optimizer states are
    updated locally per rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SHARD_GRAD_OP`: Gradients and optimizer states are sharded during computation,
    and additionally, parameters are sharded outside computation. For the parameters,
    this strategy unshards before the forward, does not reshard them after the forward,
    and only reshards them after the backward computation. The sharded optimizer states
    are updated locally per rank. Inside `no_sync()`, the parameters are not resharded
    after the backward computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NO_SHARD`: Parameters, gradients, and optimizer states are not sharded but
    instead replicated across ranks similar to PyTorch’s `DistributedDataParallel`
    API. For gradients, this strategy synchronizes them (via all-reduce) after the
    backward computation. The unsharded optimizer states are updated locally per rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HYBRID_SHARD`: Apply `FULL_SHARD` within a node, and replicate parameters
    across nodes. This results in reduced communication volume as expensive all-gathers
    and reduce-scatters are only done within a node, which can be more performant
    for medium -sized models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_HYBRID_SHARD_ZERO2`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters
    across nodes. This is like `HYBRID_SHARD`, except this may provide even higher
    throughput since the unsharded parameters are not freed after the forward pass,
    saving the all-gathers in the pre-backward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This configures FSDP-native mixed precision training.
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs: []
  type: TYPE_NORMAL
- en: '**param_dtype** (*Optional**[*[*torch.dtype*](tensor_attributes.html#torch.dtype
    "torch.dtype")*]*) – This specifies the dtype for model parameters during forward
    and backward and thus the dtype for forward and backward computation. Outside
    forward and backward, the *sharded* parameters are kept in full precision (e.g.
    for the optimizer step), and for model checkpointing, the parameters are always
    saved in full precision. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**reduce_dtype** (*Optional**[*[*torch.dtype*](tensor_attributes.html#torch.dtype
    "torch.dtype")*]*) – This specifies the dtype for gradient reduction (i.e. reduce-scatter
    or all-reduce). If this is `None` but `param_dtype` is not `None`, then this takes
    on the `param_dtype` value, still running gradient reduction in low precision.
    This is permitted to differ from `param_dtype`, e.g. to force gradient reduction
    to run in full precision. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**buffer_dtype** (*Optional**[*[*torch.dtype*](tensor_attributes.html#torch.dtype
    "torch.dtype")*]*) – This specifies the dtype for buffers. FSDP does not shard
    buffers. Rather, FSDP casts them to `buffer_dtype` in the first forward pass and
    keeps them in that dtype thereafter. For model checkpointing, the buffers are
    saved in full precision except for `LOCAL_STATE_DICT`. (Default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keep_low_precision_grads** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `False`, then FSDP upcasts gradients to full precision
    after the backward pass in preparation for the optimizer step. If `True`, then
    FSDP keeps the gradients in the dtype used for gradient reduction, which can save
    memory if using a custom optimizer that supports running in low precision. (Default:
    `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cast_forward_inputs** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then this FSDP module casts its forward args
    and kwargs to `param_dtype`. This is to ensure that parameter and input dtypes
    match for forward computation, as required by many ops. This may need to be set
    to `True` when only applying mixed precision to some but not all FSDP modules,
    in which case a mixed-precision FSDP submodule needs to recast its inputs. (Default:
    `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cast_root_forward_inputs** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then the root FSDP module casts its forward
    args and kwargs to `param_dtype`, overriding the value of `cast_forward_inputs`.
    For non-root FSDP modules, this does not do anything. (Default: `True`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**_module_classes_to_ignore** (*Sequence**[**Type**[*[*torch.nn.modules.module.Module*](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.modules.module.Module")*]**]*) – (Sequence[Type[nn.Module]]): This specifies
    module classes to ignore for mixed precision when using an `auto_wrap_policy`:
    Modules of these classes will have FSDP applied to them separately with mixed
    precision disabled (meaning that the final FSDP construction would deviate from
    the specified policy). If `auto_wrap_policy` is not specified, then this does
    not do anything. This API is experimental and subject to change. (Default: `(_BatchNorm,)`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This API is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Only floating point tensors are cast to their specified dtypes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In `summon_full_params`, parameters are forced to full precision, but buffers
    are not.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Layer norm and batch norm accumulate in `float32` even when their inputs are
    in a low precision like `float16` or `bfloat16`. Disabling FSDP’s mixed precision
    for those norm modules only means that the affine parameters are kept in `float32`.
    However, this incurs separate all-gathers and reduce-scatters for those norm modules,
    which may be inefficient, so if the workload permits, the user should prefer to
    still apply mixed precision to those modules.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By default, if the user passes a model with any `_BatchNorm` modules and specifies
    an `auto_wrap_policy`, then the batch norm modules will have FSDP applied to them
    separately with mixed precision disabled. See the `_module_classes_to_ignore`
    argument.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`MixedPrecision` has `cast_root_forward_inputs=True` and `cast_forward_inputs=False`
    by default. For the root FSDP instance, its `cast_root_forward_inputs` takes precedence
    over its `cast_forward_inputs`. For non-root FSDP instances, their `cast_root_forward_inputs`
    values are ignored. The default setting is sufficient for the typical case where
    each FSDP instance has the same `MixedPrecision` configuration and only needs
    to cast inputs to the `param_dtype` at the beginning of the model’s forward pass.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For nested FSDP instances with different `MixedPrecision` configurations, we
    recommend setting individual `cast_forward_inputs` values to configure casting
    inputs or not before each instance’s forward. In such a case, since the casts
    happen before each FSDP instance’s forward, a parent FSDP instance should have
    its non-FSDP submodules run before its FSDP submodules to avoid the activation
    dtype being changed due to a different `MixedPrecision` configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The above shows a working example. On the other hand, if `model[1]` were replaced
    with `model[0]`, meaning that the submodule using different `MixedPrecision` ran
    its forward first, then `model[1]` would incorrectly see `float16` activations
    instead of `bfloat16` ones.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This configures CPU offloading.
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs: []
  type: TYPE_NORMAL
- en: '**offload_params** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – This specifies whether to offload parameters to CPU when
    not involved in computation. If `True`, then this offloads gradients to CPU as
    well, meaning that the optimizer step runs on CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`StateDictConfig` is the base class for all `state_dict` configuration classes.
    Users should instantiate a child class (e.g. `FullStateDictConfig`) in order to
    configure settings for the corresponding `state_dict` type supported by FSDP.'
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs: []
  type: TYPE_NORMAL
- en: '**offload_to_cpu** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP offloads the state dict values to
    CPU, and if `False`, then FSDP keeps them on GPU. (Default: `False`)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`FullStateDictConfig` is a config class meant to be used with `StateDictType.FULL_STATE_DICT`.
    We recommend enabling both `offload_to_cpu=True` and `rank0_only=True` when saving
    full state dicts to save GPU memory and CPU memory, respectively. This config
    class is meant to be used via the `state_dict_type()` context manager as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Variables
  prefs: []
  type: TYPE_NORMAL
- en: '**rank0_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then only rank 0 saves the full state dict,
    and nonzero ranks save an empty dict. If `False`, then all ranks save the full
    state dict. (Default: `False`)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '`ShardedStateDictConfig` is a config class meant to be used with `StateDictType.SHARDED_STATE_DICT`.'
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs: []
  type: TYPE_NORMAL
- en: '**_use_dtensor** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP saves the state dict values as `DTensor`,
    and if `False`, then FSDP saves them as `ShardedTensor`. (Default: `False`)'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`_use_dtensor` is a private field of [`ShardedStateDictConfig`](#torch.distributed.fsdp.ShardedStateDictConfig
    "torch.distributed.fsdp.ShardedStateDictConfig") and it is used by FSDP to determine
    the type of state dict values. Users should not manually modify `_use_dtensor`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`OptimStateDictConfig` is the base class for all `optim_state_dict` configuration
    classes. Users should instantiate a child class (e.g. `FullOptimStateDictConfig`)
    in order to configure settings for the corresponding `optim_state_dict` type supported
    by FSDP.'
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs: []
  type: TYPE_NORMAL
- en: '**offload_to_cpu** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP offloads the state dict’s tensor
    values to CPU, and if `False`, then FSDP keeps them on the original device (which
    is GPU unless parameter CPU offloading is enabled). (Default: `True`)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Variables
  prefs: []
  type: TYPE_NORMAL
- en: '**rank0_only** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then only rank 0 saves the full state dict,
    and nonzero ranks save an empty dict. If `False`, then all ranks save the full
    state dict. (Default: `False`)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '`ShardedOptimStateDictConfig` is a config class meant to be used with `StateDictType.SHARDED_STATE_DICT`.'
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs: []
  type: TYPE_NORMAL
- en: '**_use_dtensor** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, then FSDP saves the state dict values as `DTensor`,
    and if `False`, then FSDP saves them as `ShardedTensor`. (Default: `False`)'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`_use_dtensor` is a private field of [`ShardedOptimStateDictConfig`](#torch.distributed.fsdp.ShardedOptimStateDictConfig
    "torch.distributed.fsdp.ShardedOptimStateDictConfig") and it is used by FSDP to
    determine the type of state dict values. Users should not manually modify `_use_dtensor`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
