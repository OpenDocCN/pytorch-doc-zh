# torch.special

> 原文：[`pytorch.org/docs/stable/special.html`](https://pytorch.org/docs/stable/special.html)

torch.special 模块，模仿 SciPy 的[special](https://docs.scipy.org/doc/scipy/reference/special.html)模块。

## 函数

```py
torch.special.airy_ai(input, *, out=None) → Tensor
```

Airy 函数$\text{Ai}\left(\text{input}\right)$。

参数

- 输入张量。

关键参数

- 输出张量（可选）- 输出张量。

```py
torch.special.bessel_j0(input, *, out=None) → Tensor
```

第一类贝塞尔函数的阶数为$0$。

参数

- 输入张量。

关键参数

- 输出张量（可选）- 输出张量。

```py
torch.special.bessel_j1(input, *, out=None) → Tensor
```

第一类贝塞尔函数的阶数为$1$。

参数

- 输入张量。

关键参数

- 输出张量（可选）- 输出张量。

```py
torch.special.digamma(input, *, out=None) → Tensor
```

计算输入上的伽玛函数的对数导数。

$\digamma(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}$

参数

- 输入张量。在其上计算 digamma 函数的张量

关键参数

- 输出张量（可选）- 输出张量。

注意

这个函数类似于 SciPy 的 scipy.special.digamma。

注意

从 PyTorch 1.8 开始，digamma 函数对于 0 返回-Inf。之前对于 0 返回 NaN。

示例：

```py
>>> a = torch.tensor([1, 0.5])
>>> torch.special.digamma(a)
tensor([-0.5772, -1.9635]) 
```

```py
torch.special.entr(input, *, out=None) → Tensor
```

计算输入上的熵（如下所定义），逐元素。

$\begin{align} \text{entr(x)} = \begin{cases} -x * \ln(x) & x > 0 \\ 0 & x = 0.0 \\ -\infty & x < 0 \end{cases} \end{align}$

参数

- 输入张量。

关键参数

- 输出张量（可选）- 输出张量。

示例：

```py
>>> a = torch.arange(-0.5, 1, 0.5)
>>> a
tensor([-0.5000,  0.0000,  0.5000])
>>> torch.special.entr(a)
tensor([  -inf, 0.0000, 0.3466]) 
```

```py
torch.special.erf(input, *, out=None) → Tensor
```

计算输入的误差函数。误差函数定义如下：

$\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t²} dt$

参数

- 输入张量。

关键参数

- 输出张量（可选）- 输出张量。

示例：

```py
>>> torch.special.erf(torch.tensor([0, -1., 10.]))
tensor([ 0.0000, -0.8427,  1.0000]) 
```

```py
torch.special.erfc(input, *, out=None) → Tensor
```

计算输入的互补误差函数。互补误差函数定义如下：

$\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t²} dt$

参数

- 输入张量。

关键参数

- 输出张量（可选）- 输出张量。

示例：

```py
>>> torch.special.erfc(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 1.8427,  0.0000]) 
```

```py
torch.special.erfcx(input, *, out=None) → Tensor
```

计算输入的每个元素的缩放互补误差函数。缩放互补误差函数定义如下：

$\mathrm{erfcx}(x) = e^{x²} \mathrm{erfc}(x)$

参数

- 输入张量。

关键参数

- 输出张量（可选）- 输出张量。

示例：

```py
>>> torch.special.erfcx(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 5.0090, 0.0561]) 
```

```py
torch.special.erfinv(input, *, out=None) → Tensor
```

计算输入的逆误差函数。逆误差函数在范围$(-1, 1)$内定义如下：

$\mathrm{erfinv}(\mathrm{erf}(x)) = x$

参数

- 输入张量。

关键参数

- 输出张量（可选）- 输出张量。

示例：

```py
>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))
tensor([ 0.0000,  0.4769,    -inf]) 
```

```py
torch.special.exp2(input, *, out=None) → Tensor
```

计算输入的以 2 为底的指数函数。

$y_{i} = 2^{x_{i}}$

参数

- 输入张量。

关键参数

**out**（*张量**，* *可选*）- 输出张量。

示例：

```py
>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))
tensor([ 1.,  2.,  8., 16.]) 
```

```py
torch.special.expit(input, *, out=None) → Tensor
```

计算`input`元素的 expit（也称为逻辑 Sigmoid 函数）。

$\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}$ outi​=1+e−inputi​1​

参数

**输入**（*张量*）- 输入张量。

关键参数

**out**（*张量**，* *可选*）- 输出张量。

示例：

```py
>>> t = torch.randn(4)
>>> t
tensor([ 0.9213,  1.0887, -0.8858, -1.7683])
>>> torch.special.expit(t)
tensor([ 0.7153,  0.7481,  0.2920,  0.1458]) 
```

```py
torch.special.expm1(input, *, out=None) → Tensor
```

计算`input`元素减 1 的指数。

$y_{i} = e^{x_{i}} - 1$ yi​=exi​−1

注意

对于较小的 x 值，此函数提供比 exp(x) - 1 更高的精度。

参数

**输入**（*张量*）- 输入张量。

关键参数

**out**（*张量**，* *可选*）- 输出张量。

示例：

```py
>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))
tensor([ 0.,  1.]) 
```

```py
torch.special.gammainc(input, other, *, out=None) → Tensor
```

计算正则化的下不完全伽玛函数：

$\text{out}_{i} = \frac{1}{\Gamma(\text{input}_i)} \int_0^{\text{other}_i} t^{\text{input}_i-1} e^{-t} dt$

其中$\text{input}_i$​和$\text{other}_i$​均为弱正且至少一个严格为正。如果两者都为零或其中一个为负，则$\text{out}_i=\text{nan}$。上述方程中的$\Gamma(\cdot)$是伽玛函数，

$\Gamma(\text{input}_i) = \int_0^\infty t^{(\text{input}_i-1)} e^{-t} dt.$ Γ(inputi​)=∫0∞​t(inputi​−1)e−tdt.

有关相关函数，请参见`torch.special.gammaincc()`和`torch.special.gammaln()`。

支持广播到公共形状和浮点输入。

注意

目前不支持对`input`的反向传播。请在 PyTorch 的 Github 上提出问题以请求支持。

参数

+   **输入**（*张量*）- 第一个非负输入张量

+   **other**（*张量*）- 第二个非负输入张量

关键参数

**out**（*张量**，* *可选*）- 输出张量。

示例：

```py
>>> a1 = torch.tensor([4.0])
>>> a2 = torch.tensor([3.0, 4.0, 5.0])
>>> a = torch.special.gammaincc(a1, a2)
tensor([0.3528, 0.5665, 0.7350])
tensor([0.3528, 0.5665, 0.7350])
>>> b = torch.special.gammainc(a1, a2) + torch.special.gammaincc(a1, a2)
tensor([1., 1., 1.]) 
```

```py
torch.special.gammaincc(input, other, *, out=None) → Tensor
```

计算正则化的上不完全伽玛函数：

$\text{out}_{i} = \frac{1}{\Gamma(\text{input}_i)} \int_{\text{other}_i}^{\infty} t^{\text{input}_i-1} e^{-t} dt$ outi​=Γ(inputi​)1​∫otheri​∞​tinputi​−1e−tdt

其中$\text{input}_i$​和$\text{other}_i$​均为弱正且至少一个严格为正。如果两者都为零或其中一个为负，则$\text{out}_i=\text{nan}$。上述方程中的$\Gamma(\cdot)$是伽玛函数，

$\Gamma(\text{input}_i) = \int_0^\infty t^{(\text{input}_i-1)} e^{-t} dt.$ Γ(inputi​)=∫0∞​t(inputi​−1)e−tdt.

有关相关函数，请参见`torch.special.gammainc()`和`torch.special.gammaln()`。

支持广播到公共形状和浮点输入。

注意

目前不支持对`input`的反向传播。请在 PyTorch 的 Github 上提出问题以请求支持。

参数

+   **输入**（*张量*）- 第一个非负输入张量

+   **other**（*张量*）- 第二个非负输入张量

关键参数

**out**（*张量**，* *可选*）- 输出张量。

示例：

```py
>>> a1 = torch.tensor([4.0])
>>> a2 = torch.tensor([3.0, 4.0, 5.0])
>>> a = torch.special.gammaincc(a1, a2)
tensor([0.6472, 0.4335, 0.2650])
>>> b = torch.special.gammainc(a1, a2) + torch.special.gammaincc(a1, a2)
tensor([1., 1., 1.]) 
```

```py
torch.special.gammaln(input, *, out=None) → Tensor
```

计算`input`上伽玛函数绝对值的自然对数。

$\text{out}_{i} = \ln \Gamma(|\text{input}_{i}|)$ outi​=lnΓ(∣inputi​∣)

参数

**输入**（*张量*）- 输入张量。

关键参数

**out**（*张量**，* *可选*）- 输出张量。

示例：

```py
>>> a = torch.arange(0.5, 2, 0.5)
>>> torch.special.gammaln(a)
tensor([ 0.5724,  0.0000, -0.1208]) 
```

```py
torch.special.i0(input, *, out=None) → Tensor
```

计算每个`input`元素的第零阶修正贝塞尔函数（如下所定义）。

$\text{out}_{i} = I_0(\text{input}_{i}) = \sum_{k=0}^{\infty} \frac{(\text{input}_{i}²/4)^k}{(k!)²}$

参数

**input**（*Tensor*） - 输入张量

关键参数

**out**（*Tensor**，*可选） - 输出张量。

示例：

```py
>>> torch.i0(torch.arange(5, dtype=torch.float32))
tensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019]) 
```

```py
torch.special.i0e(input, *, out=None) → Tensor
```

计算每个`input`元素的指数缩放的第零阶修正贝塞尔函数（如下所定义）。

$\text{out}_{i} = \exp(-|x|) * i0(x) = \exp(-|x|) * \sum_{k=0}^{\infty} \frac{(\text{input}_{i}²/4)^k}{(k!)²}$

参数

**input**（*Tensor*） - 输入张量。

关键参数

**out**（*Tensor**，*可选） - 输出张量。

示例：

```py
>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))
tensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070]) 
```

```py
torch.special.i1(input, *, out=None) → Tensor
```

计算每个`input`元素的第一类修正贝塞尔函数（如下所定义）。

$\text{out}_{i} = \frac{(\text{input}_{i})}{2} * \sum_{k=0}^{\infty} \frac{(\text{input}_{i}²/4)^k}{(k!) * (k+1)!}$

参数

**input**（*Tensor*） - 输入张量。

关键参数

**out**（*Tensor**，*可选） - 输出张量。

示例：

```py
>>> torch.special.i1(torch.arange(5, dtype=torch.float32))
tensor([0.0000, 0.5652, 1.5906, 3.9534, 9.7595]) 
```

```py
torch.special.i1e(input, *, out=None) → Tensor
```

计算每个`input`元素的指数缩放的第一类修正贝塞尔函数（如下所定义）。

$\text{out}_{i} = \exp(-|x|) * i1(x) = \exp(-|x|) * \frac{(\text{input}_{i})}{2} * \sum_{k=0}^{\infty} \frac{(\text{input}_{i}²/4)^k}{(k!) * (k+1)!}$

参数

输入张量。

关键参数

**out**（*Tensor**，*可选） - 输出张量。

示例：

```py
>>> torch.special.i1e(torch.arange(5, dtype=torch.float32))
tensor([0.0000, 0.2079, 0.2153, 0.1968, 0.1788]) 
```

```py
torch.special.log1p(input, *, out=None) → Tensor
```

`torch.log1p()`的别名。

```py
torch.special.log_ndtr(input, *, out=None) → Tensor
```

计算标准高斯概率密度函数下面积的对数，从负无穷到`input`，逐元素。

$\text{log\_ndtr}(x) = \log\left(\frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{x} e^{-\frac{1}{2}t²} dt \right)$

参数

**input**（*Tensor*） - 输入张量。

关键参数

**out**（*Tensor**，*可选） - 输出张量。

示例：

```py
>>> torch.special.log_ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))
tensor([-6.6077 -3.7832 -1.841  -0.6931 -0.1728 -0.023  -0.0014]) 
```

```py
torch.special.log_softmax(input, dim, *, dtype=None) → Tensor
```

计算 softmax 后跟对数。

虽然在数学上等价于 log(softmax(x))，但分别执行这两个操作会更慢且数值不稳定。此函数计算如下：

$\text{log\_softmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)$

参数

+   **input**（*Tensor*） - 输入

+   **dim**（[*int*](https://docs.python.org/3/library/functions.html#int "(在 Python v3.12 中)")） - log_softmax 将计算的维度。

+   **dtype**（`torch.dtype`，可选） - 返回张量的期望数据类型。如果指定，操作执行前将输入张量转换为`dtype`。这对于防止数据类型溢出很有用。默认值：无。

示例：

```py
>>> t = torch.ones(2, 2)
>>> torch.special.log_softmax(t, 0)
tensor([[-0.6931, -0.6931],
 [-0.6931, -0.6931]]) 
```

```py
torch.special.logit(input, eps=None, *, out=None) → Tensor
```

返回一个新张量，其中包含`input`元素的 logit。当 eps 不为 None 时，`input`被夹在[eps, 1 - eps]之间。当 eps 为 None 且`input` < 0 或`input` > 1 时，函数将产生 NaN。

$\begin{align} y_{i} &= \ln(\frac{z_{i}}{1 - z_{i}}) \\ z_{i} &= \begin{cases} x_{i} & \text{if eps is None} \\ \text{eps} & \text{if } x_{i} < \text{eps} \\ x_{i} & \text{if } \text{eps} \leq x_{i} \leq 1 - \text{eps} \\ 1 - \text{eps} & \text{if } x_{i} > 1 - \text{eps} \end{cases} \end{align}$

参数

+   **输入**（*张量*")*,* *可选*）- 输入夹紧边界的 epsilon。默认值：`None`

关键参数

**输出**（*张量**,* *可选*）- 输出张量。

示例:

```py
>>> a = torch.rand(5)
>>> a
tensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])
>>> torch.special.logit(a, eps=1e-6)
tensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261]) 
```

```py
torch.special.logsumexp(input, dim, keepdim=False, *, out=None)
```

`torch.logsumexp()`的别名。

```py
torch.special.multigammaln(input, p, *, out=None) → Tensor
```

计算多元对数伽玛函数，维度为$p$，逐元素给出

$\log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)$

其中$C = \log(\pi) \cdot \frac{p (p - 1)}{4}$，$\Gamma(-)$是 Gamma 函数。

所有元素必须大于$\frac{p - 1}{2}$，否则行为未定义。

参数

+   **输入**（*张量*")）- 维度的数量

关键参数

**输出**（*张量**,* *可选*）- 输出张量。

示例:

```py
>>> a = torch.empty(2, 3).uniform_(1, 2)
>>> a
tensor([[1.6835, 1.8474, 1.1929],
 [1.0475, 1.7162, 1.4180]])
>>> torch.special.multigammaln(a, 2)
tensor([[0.3928, 0.4007, 0.7586],
 [1.0311, 0.3901, 0.5049]]) 
```

```py
torch.special.ndtr(input, *, out=None) → Tensor
```

计算标准高斯概率密度函数下的面积，从负无穷积分到`输入`，逐元素计算。

$\text{ndtr}(x) = \frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{x} e^{-\frac{1}{2}t²} dt$

参数

**输入**（*张量**,* *可选*）- 输出张量。

示例::

```py
>>> torch.special.ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))
tensor([0.0013, 0.0228, 0.1587, 0.5000, 0.8413, 0.9772, 0.9987]) 
```

```py
torch.special.ndtri(input, *, out=None) → Tensor
```

计算参数$x$，使得高斯概率密度函数下的面积（从负无穷积分到 x）等于`输入`，逐元素计算。

$\text{ndtri}(p) = \sqrt{2}\text{erf}^{-1}(2p - 1)$

注意

也称为正态分布的分位函数。

参数

**输入**（*张量**,* *可选*）- 输出张量。

示例::

```py
>>> torch.special.ndtri(torch.tensor([0, 0.25, 0.5, 0.75, 1]))
tensor([   -inf, -0.6745,  0.0000,  0.6745,     inf]) 
```

```py
torch.special.polygamma(n, input, *, out=None) → Tensor
```

计算`输入`上的第$n$阶 digamma 函数的导数。$n \geq 0$

$\psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x)$

注意

此函数仅实现非负整数$n \geq 0$。

参数

+   **n**（[*int*](https://docs.python.org/3/library/functions.html#int "(在 Python v3.12 中)")）- polygamma 函数的阶数

+   **输入**（*张量**,* *可选*）- 输出张量。

示例::

```py
>>> a = torch.tensor([1, 0.5])
>>> torch.special.polygamma(1, a)
tensor([1.64493, 4.9348])
>>> torch.special.polygamma(2, a)
tensor([ -2.4041, -16.8288])
>>> torch.special.polygamma(3, a)
tensor([ 6.4939, 97.4091])
>>> torch.special.polygamma(4, a)
tensor([ -24.8863, -771.4742]) 
```

```py
torch.special.psi(input, *, out=None) → Tensor
```

`torch.special.digamma()`的别名。

```py
torch.special.round(input, *, out=None) → Tensor
```

`torch.round()`的别名。

```py
torch.special.scaled_modified_bessel_k0(input, *, out=None) → Tensor
```

第二类修正贝塞尔函数的缩放形式，阶数为$0$。

参数

**输入**（*张量**,* *可选*）- 输出张量。

```py
torch.special.scaled_modified_bessel_k1(input, *, out=None) → Tensor
```

第二类修正贝塞尔函数的缩放形式，阶数为$1$。

参数

**输入**（*张量**,* *可选*） - 输出张量。

```py
torch.special.sinc(input, *, out=None) → Tensor
```

计算`input`的归一化 sinc。

$\text{out}_{i} = \begin{cases} 1, & \text{if}\ \text{input}_{i}=0 \\ \sin(\pi \text{input}_{i}) / (\pi \text{input}_{i}), & \text{otherwise} \end{cases}$

参数

**input**（*Tensor*） - 输入张量。

关键字参数

**out**（*Tensor**,* *可选*） - 输出张量。

示例::

```py
>>> t = torch.randn(4)
>>> t
tensor([ 0.2252, -0.2948,  1.0267, -1.1566])
>>> torch.special.sinc(t)
tensor([ 0.9186,  0.8631, -0.0259, -0.1300]) 
```

```py
torch.special.softmax(input, dim, *, dtype=None) → Tensor
```

计算 softmax 函数。

Softmax 定义为：

$\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$

应用于沿 dim 的所有切片，并将它们重新缩放，使元素位于范围[0, 1]并总和为 1。

参数

+   **input**（*Tensor*） - 输入

+   **dim**（[*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)")） - 将进行 softmax 计算的维度。

+   **dtype**（`torch.dtype`，可选） - 返回张量的期望数据类型。如果指定，操作执行之前将输入张量转换为`dtype`。这对于防止数据类型溢出很有用。默认值：无。

示例::

```py
>>> t = torch.ones(2, 2)
>>> torch.special.softmax(t, 0)
tensor([[0.5000, 0.5000],
 [0.5000, 0.5000]]) 
```

```py
torch.special.spherical_bessel_j0(input, *, out=None) → Tensor
```

第一类零阶球贝塞尔函数。

参数

**input**（*Tensor*） - 输入张量。

关键字参数

**out**（*Tensor**,* *可选*） - 输出张量。

```py
torch.special.xlog1py(input, other, *, out=None) → Tensor
```

使用以下情况计算`input * log1p(other)`。

$\text{out}_{i} = \begin{cases} \text{NaN} & \text{if } \text{other}_{i} = \text{NaN} \\ 0 & \text{if } \text{input}_{i} = 0.0 \text{ and } \text{other}_{i} != \text{NaN} \\ \text{input}_{i} * \text{log1p}(\text{other}_{i})& \text{otherwise} \end{cases}$

类似于 SciPy 的 scipy.special.xlog1py。

参数

+   **input**（*Number* *或* *Tensor*） - 乘数

+   **other**（*Number* *或* *Tensor*） - 参数

注意

`input`或`other`中至少有一个必须是张量。

关键字参数

**out**（*Tensor**,* *可选*） - 输出张量。

示例：

```py
>>> x = torch.zeros(5,)
>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])
>>> torch.special.xlog1py(x, y)
tensor([0., 0., 0., 0., nan])
>>> x = torch.tensor([1, 2, 3])
>>> y = torch.tensor([3, 2, 1])
>>> torch.special.xlog1py(x, y)
tensor([1.3863, 2.1972, 2.0794])
>>> torch.special.xlog1py(x, 4)
tensor([1.6094, 3.2189, 4.8283])
>>> torch.special.xlog1py(2, y)
tensor([2.7726, 2.1972, 1.3863]) 
```

```py
torch.special.xlogy(input, other, *, out=None) → Tensor
```

使用以下情况计算`input * log(other)`。

$\text{out}_{i} = \begin{cases} \text{NaN} & \text{if } \text{other}_{i} = \text{NaN} \\ 0 & \text{if } \text{input}_{i} = 0.0 \\ \text{input}_{i} * \log{(\text{other}_{i})} & \text{otherwise} \end{cases}$

类似于 SciPy 的 scipy.special.xlogy。

参数

+   **input**（*Number* *或* *Tensor*） - 乘数

+   **other**（*Number* *或* *Tensor*） - 参数

注意

`input`或`other`中至少有一个必须是张量。

关键字参数

**out**（*Tensor**,* *可选*） - 输出张量。

示例：

```py
>>> x = torch.zeros(5,)
>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])
>>> torch.special.xlogy(x, y)
tensor([0., 0., 0., 0., nan])
>>> x = torch.tensor([1, 2, 3])
>>> y = torch.tensor([3, 2, 1])
>>> torch.special.xlogy(x, y)
tensor([1.0986, 1.3863, 0.0000])
>>> torch.special.xlogy(x, 4)
tensor([1.3863, 2.7726, 4.1589])
>>> torch.special.xlogy(2, y)
tensor([2.1972, 1.3863, 0.0000]) 
```

```py
torch.special.zeta(input, other, *, out=None) → Tensor
```

逐元素计算 Hurwitz zeta 函数。

$\zeta(x, q) = \sum_{k=0}^{\infty} \frac{1}{(k + q)^x}$

参数

+   **input**（*Tensor*） - 与 x 对应的输入张量。

+   **other**（*Tensor*） - 与 q 对应的输入张量。

注意

Riemann zeta 函数对应于 q = 1 的情况

关键字参数

**out**（*Tensor**,* *可选*） - 输出张量。

示例::

```py
>>> x = torch.tensor([2., 4.])
>>> torch.special.zeta(x, 1)
tensor([1.6449, 1.0823])
>>> torch.special.zeta(x, torch.tensor([1., 2.]))
tensor([1.6449, 0.0823])
>>> torch.special.zeta(2, torch.tensor([1., 2.]))
tensor([1.6449, 0.6449]) 
```
