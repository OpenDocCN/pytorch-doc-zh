["```py\n\"\"\"Import modules required to run the Jupyter notebook.\"\"\"\nfrom __future__ import annotations\n\n# Configure logging\nimport logging\nimport warnings\nif logging.getLogger().hasHandlers():\n    logging.getLogger().handlers.clear()\nwarnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")\n\n# Downloading data and files\nimport shutil\nfrom pathlib import Path\nfrom zipfile import ZipFile\n\n# Data processing and visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\nimport PIL\nimport contextlib\nimport io\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# TIAToolbox for WSI loading and processing\nfrom tiatoolbox import logger\nfrom tiatoolbox.models.architecture import vanilla\nfrom tiatoolbox.models.engine.patch_predictor import (\n    IOPatchPredictorConfig,\n    PatchPredictor,\n)\nfrom tiatoolbox.utils.misc import download_data, grab_files_from_dir\nfrom tiatoolbox.utils.visualization import overlay_prediction_mask\nfrom tiatoolbox.wsicore.wsireader import WSIReader\n\n# Torch-related\nimport torch\nfrom torchvision import transforms\n\n# Configure plotting\nmpl.rcParams[\"figure.dpi\"] = 160  # for high resolution figure in notebook\nmpl.rcParams[\"figure.facecolor\"] = \"white\"  # To make sure text is visible in dark mode\n\n# If you are not using GPU, change ON_GPU to False\nON_GPU = True\n\n# Function to suppress console output for overly verbose code blocks\ndef suppress_console_output():\n    return contextlib.redirect_stderr(io.StringIO()) \n```", "```py\nwarnings.filterwarnings(\"ignore\")\nglobal_save_dir = Path(\"./tmp/\")\n\ndef rmdir(dir_path: str | Path) -> None:\n  \"\"\"Helper function to delete directory.\"\"\"\n    if Path(dir_path).is_dir():\n        shutil.rmtree(dir_path)\n        logger.info(\"Removing directory %s\", dir_path)\n\nrmdir(global_save_dir)  # remove  directory if it exists from previous runs\nglobal_save_dir.mkdir()\nlogger.info(\"Creating new directory %s\", global_save_dir) \n```", "```py\nwsi_path = global_save_dir / \"sample_wsi.svs\"\npatches_path = global_save_dir / \"kather100k-validation-sample.zip\"\nweights_path = global_save_dir / \"resnet18-kather100k.pth\"\n\nlogger.info(\"Download has started. Please wait...\")\n\n# Downloading and unzip a sample whole-slide image\ndownload_data(\n    \"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs\",\n    wsi_path,\n)\n\n# Download and unzip a sample of the validation set used to train the Kather 100K dataset\ndownload_data(\n    \"https://tiatoolbox.dcs.warwick.ac.uk/datasets/kather100k-validation-sample.zip\",\n    patches_path,\n)\nwith ZipFile(patches_path, \"r\") as zipfile:\n    zipfile.extractall(path=global_save_dir)\n\n# Download pretrained model weights for WSI classification using ResNet18 architecture\ndownload_data(\n    \"https://tiatoolbox.dcs.warwick.ac.uk/models/pc/resnet18-kather100k.pth\",\n    weights_path,\n)\n\nlogger.info(\"Download is complete.\") \n```", "```py\n# Read the patch data and create a list of patches and a list of corresponding labels\ndataset_path = global_save_dir / \"kather100k-validation-sample\"\n\n# Set the path to the dataset\nimage_ext = \".tif\"  # file extension of each image\n\n# Obtain the mapping between the label ID and the class name\nlabel_dict = {\n    \"BACK\": 0, # Background (empty glass region)\n    \"NORM\": 1, # Normal colon mucosa\n    \"DEB\": 2,  # Debris\n    \"TUM\": 3,  # Colorectal adenocarcinoma epithelium\n    \"ADI\": 4,  # Adipose\n    \"MUC\": 5,  # Mucus\n    \"MUS\": 6,  # Smooth muscle\n    \"STR\": 7,  # Cancer-associated stroma\n    \"LYM\": 8,  # Lymphocytes\n}\n\nclass_names = list(label_dict.keys())\nclass_labels = list(label_dict.values())\n\n# Generate a list of patches and generate the label from the filename\npatch_list = []\nlabel_list = []\nfor class_name, label in label_dict.items():\n    dataset_class_path = dataset_path / class_name\n    patch_list_single_class = grab_files_from_dir(\n        dataset_class_path,\n        file_types=\"*\" + image_ext,\n    )\n    patch_list.extend(patch_list_single_class)\n    label_list.extend([label] * len(patch_list_single_class))\n\n# Show some dataset statistics\nplt.bar(class_names, [label_list.count(label) for label in class_labels])\nplt.xlabel(\"Patch types\")\nplt.ylabel(\"Number of patches\")\n\n# Count the number of examples per class\nfor class_name, label in label_dict.items():\n    logger.info(\n        \"Class ID: %d -- Class Name: %s -- Number of images: %d\",\n        label,\n        class_name,\n        label_list.count(label),\n    )\n\n# Overall dataset statistics\nlogger.info(\"Total number of patches: %d\", (len(patch_list))) \n```", "```py\n|2023-11-14|13:15:59.299| [INFO] Class ID: 0 -- Class Name: BACK -- Number of images: 211\n|2023-11-14|13:15:59.299| [INFO] Class ID: 1 -- Class Name: NORM -- Number of images: 176\n|2023-11-14|13:15:59.299| [INFO] Class ID: 2 -- Class Name: DEB -- Number of images: 230\n|2023-11-14|13:15:59.299| [INFO] Class ID: 3 -- Class Name: TUM -- Number of images: 286\n|2023-11-14|13:15:59.299| [INFO] Class ID: 4 -- Class Name: ADI -- Number of images: 208\n|2023-11-14|13:15:59.299| [INFO] Class ID: 5 -- Class Name: MUC -- Number of images: 178\n|2023-11-14|13:15:59.299| [INFO] Class ID: 6 -- Class Name: MUS -- Number of images: 270\n|2023-11-14|13:15:59.299| [INFO] Class ID: 7 -- Class Name: STR -- Number of images: 209\n|2023-11-14|13:15:59.299| [INFO] Class ID: 8 -- Class Name: LYM -- Number of images: 232\n|2023-11-14|13:15:59.299| [INFO] Total number of patches: 2000 \n```", "```py\n# Importing a pretrained PyTorch model from TIAToolbox\npredictor = PatchPredictor(pretrained_model='resnet18-kather100k', batch_size=32)\n\n# Users can load any PyTorch model architecture instead using the following script\nmodel = vanilla.CNNModel(backbone=\"resnet18\", num_classes=9) # Importing model from torchvision.models.resnet18\nmodel.load_state_dict(torch.load(weights_path, map_location=\"cpu\"), strict=True)\ndef preproc_func(img):\n    img = PIL.Image.fromarray(img)\n    img = transforms.ToTensor()(img)\n    return img.permute(1, 2, 0)\nmodel.preproc_func = preproc_func\npredictor = PatchPredictor(model=model, batch_size=32) \n```", "```py\nwith suppress_console_output():\n    output = predictor.predict(imgs=patch_list, mode=\"patch\", on_gpu=ON_GPU)\n\nacc = accuracy_score(label_list, output[\"predictions\"])\nlogger.info(\"Classification accuracy: %f\", acc)\n\n# Creating and visualizing the confusion matrix for patch classification results\nconf = confusion_matrix(label_list, output[\"predictions\"], normalize=\"true\")\ndf_cm = pd.DataFrame(conf, index=class_names, columns=class_names)\ndf_cm \n```", "```py\n|2023-11-14|13:16:03.215| [INFO] Classification accuracy: 0.993000 \n```", "```py\nwsi_ioconfig = IOPatchPredictorConfig(\n    input_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}],\n    patch_input_shape=[224, 224],\n    stride_shape=[224, 224],\n) \n```", "```py\nwith suppress_console_output():\n    wsi_output = predictor.predict(\n        imgs=[wsi_path],\n        masks=None,\n        mode=\"wsi\",\n        merge_predictions=False,\n        ioconfig=wsi_ioconfig,\n        return_probabilities=True,\n        save_dir=global_save_dir / \"wsi_predictions\",\n        on_gpu=ON_GPU,\n    ) \n```", "```py\noverview_resolution = (\n    4  # the resolution in which we desire to merge and visualize the patch predictions\n)\n# the unit of the `resolution` parameter. Can be \"power\", \"level\", \"mpp\", or \"baseline\"\noverview_unit = \"mpp\"\nwsi = WSIReader.open(wsi_path)\nwsi_overview = wsi.slide_thumbnail(resolution=overview_resolution, units=overview_unit)\nplt.figure(), plt.imshow(wsi_overview)\nplt.axis(\"off\") \n```", "```py\n# Visualization of whole-slide image patch-level prediction\n# first set up a label to color mapping\nlabel_color_dict = {}\nlabel_color_dict[0] = (\"empty\", (0, 0, 0))\ncolors = cm.get_cmap(\"Set1\").colors\nfor class_name, label in label_dict.items():\n    label_color_dict[label + 1] = (class_name, 255 * np.array(colors[label]))\n\npred_map = predictor.merge_predictions(\n    wsi_path,\n    wsi_output[0],\n    resolution=overview_resolution,\n    units=overview_unit,\n)\noverlay = overlay_prediction_mask(\n    wsi_overview,\n    pred_map,\n    alpha=0.5,\n    label_info=label_color_dict,\n    return_ax=True,\n)\nplt.show() \n```", "```py\n# Import some extra modules\nimport histoencoder.functional as F\nimport torch.nn as nn\n\nfrom tiatoolbox.models.engine.semantic_segmentor import DeepFeatureExtractor, IOSegmentorConfig\nfrom tiatoolbox.models.models_abc import ModelABC\nimport umap \n```", "```py\nclass HistoEncWrapper(ModelABC):\n  \"\"\"Wrapper for HistoEnc model that conforms to tiatoolbox ModelABC interface.\"\"\"\n\n    def __init__(self: HistoEncWrapper, encoder) -> None:\n        super().__init__()\n        self.feat_extract = encoder\n\n    def forward(self: HistoEncWrapper, imgs: torch.Tensor) -> torch.Tensor:\n  \"\"\"Pass input data through the model.\n\n Args:\n imgs (torch.Tensor):\n Model input.\n\n \"\"\"\n        out = F.extract_features(self.feat_extract, imgs, num_blocks=2, avg_pool=True)\n        return out\n\n    @staticmethod\n    def infer_batch(\n        model: nn.Module,\n        batch_data: torch.Tensor,\n        *,\n        on_gpu: bool,\n    ) -> list[np.ndarray]:\n  \"\"\"Run inference on an input batch.\n\n Contains logic for forward operation as well as i/o aggregation.\n\n Args:\n model (nn.Module):\n PyTorch defined model.\n batch_data (torch.Tensor):\n A batch of data generated by\n `torch.utils.data.DataLoader`.\n on_gpu (bool):\n Whether to run inference on a GPU.\n\n \"\"\"\n        img_patches_device = batch_data.to('cuda') if on_gpu else batch_data\n        model.eval()\n        # Do not compute the gradient (not training)\n        with torch.inference_mode():\n            output = model(img_patches_device)\n        return [output.cpu().numpy()] \n```", "```py\n# create the model\nencoder = F.create_encoder(\"prostate_medium\")\nmodel = HistoEncWrapper(encoder)\n\n# set the pre-processing function\nnorm=transforms.Normalize(mean=[0.662, 0.446, 0.605],std=[0.169, 0.190, 0.155])\ntrans = [\n    transforms.ToTensor(),\n    norm,\n]\nmodel.preproc_func = transforms.Compose(trans)\n\nwsi_ioconfig = IOSegmentorConfig(\n    input_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}],\n    patch_input_shape=[224, 224],\n    output_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}],\n    patch_output_shape=[224, 224],\n    stride_shape=[224, 224],\n) \n```", "```py\n# create the feature extractor and run it on the WSI\nextractor = DeepFeatureExtractor(model=model, auto_generate_mask=True, batch_size=32, num_loader_workers=4, num_postproc_workers=4)\nwith suppress_console_output():\n    out = extractor.predict(imgs=[wsi_path], mode=\"wsi\", ioconfig=wsi_ioconfig, save_dir=global_save_dir / \"wsi_features\",) \n```", "```py\n# First we define a function to calculate the umap reduction\ndef umap_reducer(x, dims=3, nns=10):\n  \"\"\"UMAP reduction of the input data.\"\"\"\n    reducer = umap.UMAP(n_neighbors=nns, n_components=dims, metric=\"manhattan\", spread=0.5, random_state=2)\n    reduced = reducer.fit_transform(x)\n    reduced -= reduced.min(axis=0)\n    reduced /= reduced.max(axis=0)\n    return reduced\n\n# load the features output by our feature extractor\npos = np.load(global_save_dir / \"wsi_features\" / \"0.position.npy\")\nfeats = np.load(global_save_dir / \"wsi_features\" / \"0.features.0.npy\")\npos = pos / 8 # as we extracted at 0.5mpp, and we are overlaying on a thumbnail at 4mpp\n\n# reduce the features into 3 dimensional (rgb) space\nreduced = umap_reducer(feats)\n\n# plot the prediction map the classifier again\noverlay = overlay_prediction_mask(\n    wsi_overview,\n    pred_map,\n    alpha=0.5,\n    label_info=label_color_dict,\n    return_ax=True,\n)\n\n# plot the feature map reduction\nplt.figure()\nplt.imshow(wsi_overview)\nplt.scatter(pos[:,0], pos[:,1], c=reduced, s=1, alpha=0.5)\nplt.axis(\"off\")\nplt.title(\"UMAP reduction of HistoEnc features\")\nplt.show() \n```"]