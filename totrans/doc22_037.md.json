["```py\nclass torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)\u00b6\n```", "```py\n# Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with torch.autocast(device_type=\"cuda\"):\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step() \n```", "```py\nclass AutocastModel(nn.Module):\n    ...\n    @torch.autocast(device_type=\"cuda\")\n    def forward(self, input):\n        ... \n```", "```py\n# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float()) \n```", "```py\n# Creates model and optimizer in default precision\nmodel = Net()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n\n        # Runs the forward pass with autocasting.\n        with torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        loss.backward()\n        optimizer.step() \n```", "```py\n# Creates model in default precision\nmodel = Net().eval()\n\nwith torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n    for input in data:\n        # Runs the forward pass with autocasting.\n        output = model(input) \n```", "```py\nclass TestModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, num_classes)\n    def forward(self, x):\n        return self.fc1(x)\n\ninput_size = 2\nnum_classes = 2\nmodel = TestModel(input_size, num_classes).eval()\n\n# For now, we suggest to disable the Jit Autocast Pass,\n# As the issue: https://github.com/pytorch/pytorch/issues/75956\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.cpu.amp.autocast(cache_enabled=False):\n    model = torch.jit.trace(model, torch.randn(1, input_size))\nmodel = torch.jit.freeze(model)\n# Models Run\nfor _ in range(3):\n    model(torch.randn(1, input_size)) \n```", "```py\n# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    e_float16 = torch.mm(a_float32, b_float32)\n    with torch.autocast(device_type=\"cuda\", enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32) \n```", "```py\nclass torch.cuda.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=True)\u00b6\n```", "```py\ntorch.cuda.amp.custom_fwd(fwd=None, *, cast_inputs=None)\u00b6\n```", "```py\ntorch.cuda.amp.custom_bwd(bwd)\u00b6\n```", "```py\nclass torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16, cache_enabled=True)\u00b6\n```", "```py\nclass torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True)\u00b6\n```", "```py\n# Creates a GradScaler once at the beginning of training.\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n\n        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n        scaler.scale(loss).backward()\n\n        # scaler.step() first unscales gradients of the optimizer's params.\n        # If gradients don't contain infs/NaNs, optimizer.step() is then called,\n        # otherwise, optimizer.step() is skipped.\n        scaler.step(optimizer)\n\n        # Updates the scale for next iteration.\n        scaler.update() \n```", "```py\nget_backoff_factor()\u00b6\n```", "```py\nget_growth_factor()\u00b6\n```", "```py\nget_growth_interval()\u00b6\n```", "```py\nget_scale()\u00b6\n```", "```py\nis_enabled()\u00b6\n```", "```py\nload_state_dict(state_dict)\u00b6\n```", "```py\nscale(outputs: Tensor) \u2192 Tensor\u00b6\n```", "```py\nscale(outputs: List[Tensor]) \u2192 List[Tensor]\n```", "```py\nscale(outputs: Tuple[Tensor, ...]) \u2192 Tuple[Tensor, ...]\n```", "```py\nscale(outputs: Iterable[Tensor]) \u2192 Iterable[Tensor]\n```", "```py\nset_backoff_factor(new_factor)\u00b6\n```", "```py\nset_growth_factor(new_factor)\u00b6\n```", "```py\nset_growth_interval(new_interval)\u00b6\n```", "```py\nstate_dict()\u00b6\n```", "```py\nstep(optimizer, *args, **kwargs)\u00b6\n```", "```py\nunscale_(optimizer)\u00b6\n```", "```py\n...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update() \n```", "```py\nupdate(new_scale=None)\u00b6\n```"]