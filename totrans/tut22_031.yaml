- en: Adversarial Example Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗性示例生成
- en: 原文：[https://pytorch.org/tutorials/beginner/fgsm_tutorial.html](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/beginner/fgsm_tutorial.html](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-beginner-fgsm-tutorial-py) to download the full
    example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-beginner-fgsm-tutorial-py)下载完整的示例代码
- en: '**Author:** [Nathan Inkawhich](https://github.com/inkawhich)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：** [Nathan Inkawhich](https://github.com/inkawhich)'
- en: If you are reading this, hopefully you can appreciate how effective some machine
    learning models are. Research is constantly pushing ML models to be faster, more
    accurate, and more efficient. However, an often overlooked aspect of designing
    and training models is security and robustness, especially in the face of an adversary
    who wishes to fool the model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在阅读本文，希望您能欣赏一些机器学习模型的有效性。研究不断推动机器学习模型变得更快、更准确和更高效。然而，设计和训练模型时经常被忽视的一个方面是安全性和稳健性，尤其是面对希望欺骗模型的对手时。
- en: This tutorial will raise your awareness to the security vulnerabilities of ML
    models, and will give insight into the hot topic of adversarial machine learning.
    You may be surprised to find that adding imperceptible perturbations to an image
    *can* cause drastically different model performance. Given that this is a tutorial,
    we will explore the topic via example on an image classifier. Specifically, we
    will use one of the first and most popular attack methods, the Fast Gradient Sign
    Attack (FGSM), to fool an MNIST classifier.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将提高您对机器学习模型安全漏洞的认识，并深入探讨对抗机器学习这一热门话题。您可能会惊讶地发现，向图像添加几乎不可察觉的扰动*可以*导致截然不同的模型性能。鉴于这是一个教程，我们将通过一个图像分类器的示例来探讨这个主题。具体来说，我们将使用第一个和最流行的攻击方法之一，即快速梯度符号攻击（FGSM），来欺骗一个MNIST分类器。
- en: Threat Model
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 威胁模型
- en: 'For context, there are many categories of adversarial attacks, each with a
    different goal and assumption of the attacker’s knowledge. However, in general
    the overarching goal is to add the least amount of perturbation to the input data
    to cause the desired misclassification. There are several kinds of assumptions
    of the attacker’s knowledge, two of which are: **white-box** and **black-box**.
    A *white-box* attack assumes the attacker has full knowledge and access to the
    model, including architecture, inputs, outputs, and weights. A *black-box* attack
    assumes the attacker only has access to the inputs and outputs of the model, and
    knows nothing about the underlying architecture or weights. There are also several
    types of goals, including **misclassification** and **source/target misclassification**.
    A goal of *misclassification* means the adversary only wants the output classification
    to be wrong but does not care what the new classification is. A *source/target
    misclassification* means the adversary wants to alter an image that is originally
    of a specific source class so that it is classified as a specific target class.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，有许多种类的对抗性攻击，每种攻击都有不同的目标和对攻击者知识的假设。然而，总体目标通常是向输入数据添加最少量的扰动，以导致所需的错误分类。攻击者知识的假设有几种类型，其中两种是：**白盒**和**黑盒**。*白盒*攻击假设攻击者对模型具有完全的知识和访问权限，包括架构、输入、输出和权重。*黑盒*攻击假设攻击者只能访问模型的输入和输出，对底层架构或权重一无所知。还有几种目标类型，包括**错误分类**和**源/目标错误分类**。*错误分类*的目标意味着对手只希望输出分类错误，但不在乎新的分类是什么。*源/目标错误分类*意味着对手希望修改原始属于特定源类别的图像，使其被分类为特定目标类别。
- en: In this case, the FGSM attack is a *white-box* attack with the goal of *misclassification*.
    With this background information, we can now discuss the attack in detail.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，FGSM攻击是一个*白盒*攻击，其目标是*错误分类*。有了这些背景信息，我们现在可以详细讨论攻击。
- en: Fast Gradient Sign Attack
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速梯度符号攻击
- en: One of the first and most popular adversarial attacks to date is referred to
    as the *Fast Gradient Sign Attack (FGSM)* and is described by Goodfellow et. al. in
    [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572).
    The attack is remarkably powerful, and yet intuitive. It is designed to attack
    neural networks by leveraging the way they learn, *gradients*. The idea is simple,
    rather than working to minimize the loss by adjusting the weights based on the
    backpropagated gradients, the attack *adjusts the input data to maximize the loss*
    based on the same backpropagated gradients. In other words, the attack uses the
    gradient of the loss w.r.t the input data, then adjusts the input data to maximize
    the loss.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，最早和最流行的对抗性攻击之一被称为*快速梯度符号攻击（FGSM）*，由Goodfellow等人在[解释和利用对抗性示例](https://arxiv.org/abs/1412.6572)中描述。这种攻击非常强大，同时又直观。它旨在通过利用神经网络学习的方式，即*梯度*，来攻击神经网络。其思想很简单，不是通过根据反向传播的梯度调整权重来最小化损失，而是根据相同的反向传播梯度*调整输入数据以最大化损失*。换句话说，攻击使用损失相对于输入数据的梯度，然后调整输入数据以最大化损失。
- en: Before we jump into the code, let’s look at the famous [FGSM](https://arxiv.org/abs/1412.6572)
    panda example and extract some notation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入代码之前，让我们看看著名的[FGSM](https://arxiv.org/abs/1412.6572)熊猫示例，并提取一些符号。
- en: '![fgsm_panda_image](../Images/d74012096c3134b776b5e9f70e8178f3.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![fgsm_panda_image](../Images/d74012096c3134b776b5e9f70e8178f3.png)'
- en: From the figure, \(\mathbf{x}\) is the original input image correctly classified
    as a “panda”, \(y\) is the ground truth label for \(\mathbf{x}\), \(\mathbf{\theta}\)
    represents the model parameters, and \(J(\mathbf{\theta}, \mathbf{x}, y)\) is
    the loss that is used to train the network. The attack backpropagates the gradient
    back to the input data to calculate \(\nabla_{x} J(\mathbf{\theta}, \mathbf{x},
    y)\). Then, it adjusts the input data by a small step (\(\epsilon\) or \(0.007\)
    in the picture) in the direction (i.e. \(sign(\nabla_{x} J(\mathbf{\theta}, \mathbf{x},
    y))\)) that will maximize the loss. The resulting perturbed image, \(x'\), is
    then *misclassified* by the target network as a “gibbon” when it is still clearly
    a “panda”.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，\(\mathbf{x}\) 是原始输入图像，被正确分类为“熊猫”，\(y\) 是\(\mathbf{x}\)的地面真实标签，\(\mathbf{\theta}\)
    代表模型参数，\(J(\mathbf{\theta}, \mathbf{x}, y)\) 是用于训练网络的损失。攻击将梯度反向传播回输入数据，计算\(\nabla_{x}
    J(\mathbf{\theta}, \mathbf{x}, y)\)。然后，它通过一个小步骤（即\(\epsilon\) 或图片中的 \(0.007\)）调整输入数据的方向（即\(sign(\nabla_{x}
    J(\mathbf{\theta}, \mathbf{x}, y))\)），以最大化损失。得到的扰动图像\(x'\)，然后被目标网络误分类为“长臂猿”，而实际上仍然是“熊猫”。
- en: Hopefully now the motivation for this tutorial is clear, so lets jump into the
    implementation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 希望现在这个教程的动机已经清楚了，让我们开始实施吧。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Implementation
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: In this section, we will discuss the input parameters for the tutorial, define
    the model under attack, then code the attack and run some tests.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论教程的输入参数，定义受攻击的模型，然后编写攻击代码并运行一些测试。
- en: Inputs
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入
- en: 'There are only three inputs for this tutorial, and are defined as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程只有三个输入，并定义如下：
- en: '`epsilons` - List of epsilon values to use for the run. It is important to
    keep 0 in the list because it represents the model performance on the original
    test set. Also, intuitively we would expect the larger the epsilon, the more noticeable
    the perturbations but the more effective the attack in terms of degrading model
    accuracy. Since the data range here is \([0,1]\), no epsilon value should exceed
    1.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilons` - 用于运行的 epsilon 值列表。在列表中保留 0 是重要的，因为它代表了模型在原始测试集上的性能。直观上，我们会期望 epsilon
    越大，扰动越明显，但攻击在降低模型准确性方面更有效。由于数据范围在 \([0,1]\) 这里，没有 epsilon 值应超过 1。'
- en: '`pretrained_model` - path to the pretrained MNIST model which was trained with
    [pytorch/examples/mnist](https://github.com/pytorch/examples/tree/master/mnist).
    For simplicity, download the pretrained model [here](https://drive.google.com/file/d/1HJV2nUHJqclXQ8flKvcWmjZ-OU5DGatl/view?usp=drive_link).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model` - 预训练的 MNIST 模型的路径，该模型是使用 [pytorch/examples/mnist](https://github.com/pytorch/examples/tree/master/mnist)
    训练的。为简单起见，可以在[这里](https://drive.google.com/file/d/1HJV2nUHJqclXQ8flKvcWmjZ-OU5DGatl/view?usp=drive_link)下载预训练模型。'
- en: '`use_cuda` - boolean flag to use CUDA if desired and available. Note, a GPU
    with CUDA is not critical for this tutorial as a CPU will not take much time.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cuda` - 一个布尔标志，用于在需要时使用 CUDA。请注意，对于本教程，具有 CUDA 的 GPU 不是必需的，因为 CPU 不会花费太多时间。'
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Model Under Attack
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 受攻击的模型
- en: As mentioned, the model under attack is the same MNIST model from [pytorch/examples/mnist](https://github.com/pytorch/examples/tree/master/mnist).
    You may train and save your own MNIST model or you can download and use the provided
    model. The *Net* definition and test dataloader here have been copied from the
    MNIST example. The purpose of this section is to define the model and dataloader,
    then initialize the model and load the pretrained weights.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，受攻击的模型是来自 [pytorch/examples/mnist](https://github.com/pytorch/examples/tree/master/mnist)
    的相同的 MNIST 模型。您可以训练和保存自己的 MNIST 模型，或者可以下载并使用提供的模型。这里的 *Net* 定义和测试数据加载器已从 MNIST
    示例中复制。本节的目的是定义模型和数据加载器，然后初始化模型并加载预训练权重。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: FGSM Attack
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FGSM 攻击
- en: Now, we can define the function that creates the adversarial examples by perturbing
    the original inputs. The `fgsm_attack` function takes three inputs, *image* is
    the original clean image (\(x\)), *epsilon* is the pixel-wise perturbation amount
    (\(\epsilon\)), and *data_grad* is gradient of the loss w.r.t the input image
    (\(\nabla_{x} J(\mathbf{\theta}, \mathbf{x}, y)\)). The function then creates
    perturbed image as
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义一个函数，通过扰动原始输入来创建对抗性示例。`fgsm_attack` 函数接受三个输入，*image* 是原始干净图像（\(x\)），*epsilon*
    是像素级扰动量（\(\epsilon\)），*data_grad* 是损失相对于输入图像的梯度（\(\nabla_{x} J(\mathbf{\theta},
    \mathbf{x}, y)\)）。然后，函数创建扰动图像如下：
- en: \[perturbed\_image = image + epsilon*sign(data\_grad) = x + \epsilon * sign(\nabla_{x}
    J(\mathbf{\theta}, \mathbf{x}, y)) \]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \[perturbed\_image = image + epsilon*sign(data\_grad) = x + \epsilon * sign(\nabla_{x}
    J(\mathbf{\theta}, \mathbf{x}, y)) \]
- en: Finally, in order to maintain the original range of the data, the perturbed
    image is clipped to range \([0,1]\).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了保持数据的原始范围，扰动图像被剪切到范围 \([0,1]\)。
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Testing Function
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试函数
- en: Finally, the central result of this tutorial comes from the `test` function.
    Each call to this test function performs a full test step on the MNIST test set
    and reports a final accuracy. However, notice that this function also takes an
    *epsilon* input. This is because the `test` function reports the accuracy of a
    model that is under attack from an adversary with strength \(\epsilon\). More
    specifically, for each sample in the test set, the function computes the gradient
    of the loss w.r.t the input data (\(data\_grad\)), creates a perturbed image with
    `fgsm_attack` (\(perturbed\_data\)), then checks to see if the perturbed example
    is adversarial. In addition to testing the accuracy of the model, the function
    also saves and returns some successful adversarial examples to be visualized later.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这个教程的核心结果来自 `test` 函数。每次调用此测试函数都会在 MNIST 测试集上执行完整的测试步骤，并报告最终准确性。但请注意，此函数还接受一个
    *epsilon* 输入。这是因为 `test` 函数报告了受到强度为 \(\epsilon\) 的对手攻击的模型的准确性。更具体地说，对于测试集中的每个样本，该函数计算损失相对于输入数据的梯度（\(data\_grad\)），使用
    `fgsm_attack` 创建扰动图像（\(perturbed\_data\)），然后检查扰动示例是否是对抗性的。除了测试模型的准确性外，该函数还保存并返回一些成功的对抗性示例，以便稍后进行可视化。
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Run Attack
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行攻击
- en: The last part of the implementation is to actually run the attack. Here, we
    run a full test step for each epsilon value in the *epsilons* input. For each
    epsilon we also save the final accuracy and some successful adversarial examples
    to be plotted in the coming sections. Notice how the printed accuracies decrease
    as the epsilon value increases. Also, note the \(\epsilon=0\) case represents
    the original test accuracy, with no attack.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的最后一部分是实际运行攻击。在这里，我们对*epsilons*输入中的每个epsilon值运行完整的测试步骤。对于每个epsilon值，我们还保存最终的准确率和一些成功的对抗性示例，以便在接下来的部分中绘制。请注意，随着epsilon值的增加，打印出的准确率也在降低。另外，请注意\(\epsilon=0\)的情况代表原始的测试准确率，没有攻击。
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Results
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: Accuracy vs Epsilon
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确率 vs Epsilon
- en: The first result is the accuracy versus epsilon plot. As alluded to earlier,
    as epsilon increases we expect the test accuracy to decrease. This is because
    larger epsilons mean we take a larger step in the direction that will maximize
    the loss. Notice the trend in the curve is not linear even though the epsilon
    values are linearly spaced. For example, the accuracy at \(\epsilon=0.05\) is
    only about 4% lower than \(\epsilon=0\), but the accuracy at \(\epsilon=0.2\)
    is 25% lower than \(\epsilon=0.15\). Also, notice the accuracy of the model hits
    random accuracy for a 10-class classifier between \(\epsilon=0.25\) and \(\epsilon=0.3\).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个结果是准确率与epsilon的图。正如前面提到的，随着epsilon的增加，我们预计测试准确率会降低。这是因为更大的epsilon意味着我们朝着最大化损失的方向迈出更大的一步。请注意，尽管epsilon值是线性间隔的，但曲线的趋势并不是线性的。例如，在\(\epsilon=0.05\)时的准确率仅比\(\epsilon=0\)时低约4%，但在\(\epsilon=0.2\)时的准确率比\(\epsilon=0.15\)低25%。另外，请注意，在\(\epsilon=0.25\)和\(\epsilon=0.3\)之间，模型的准确率达到了一个随机准确率，这是一个10类分类器。
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Accuracy vs Epsilon](../Images/7633144b009ac008488a6bd051f404c9.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![准确率 vs Epsilon](../Images/7633144b009ac008488a6bd051f404c9.png)'
- en: Sample Adversarial Examples
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例对抗性示例
- en: Remember the idea of no free lunch? In this case, as epsilon increases the test
    accuracy decreases **BUT** the perturbations become more easily perceptible. In
    reality, there is a tradeoff between accuracy degradation and perceptibility that
    an attacker must consider. Here, we show some examples of successful adversarial
    examples at each epsilon value. Each row of the plot shows a different epsilon
    value. The first row is the \(\epsilon=0\) examples which represent the original
    “clean” images with no perturbation. The title of each image shows the “original
    classification -> adversarial classification.” Notice, the perturbations start
    to become evident at \(\epsilon=0.15\) and are quite evident at \(\epsilon=0.3\).
    However, in all cases humans are still capable of identifying the correct class
    despite the added noise.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '记住没有免费午餐的概念吗？在这种情况下，随着epsilon的增加，测试准确率降低**但**扰动变得更容易察觉。实际上，攻击者必须考虑准确率降低和可察觉性之间的权衡。在这里，我们展示了每个epsilon值下一些成功的对抗性示例的示例。图的每一行显示不同的epsilon值。第一行是\(\epsilon=0\)的示例，代表没有扰动的原始“干净”图像。每个图像的标题显示“原始分类
    -> 对抗性分类”。请注意，在\(\epsilon=0.15\)时，扰动开始变得明显，在\(\epsilon=0.3\)时非常明显。然而，在所有情况下，人类仍然能够识别出正确的类别，尽管增加了噪音。 '
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![7 -> 7, 9 -> 9, 0 -> 0, 3 -> 3, 5 -> 5, 2 -> 8, 1 -> 3, 3 -> 5, 4 -> 6, 4
    -> 9, 9 -> 4, 5 -> 6, 9 -> 5, 9 -> 5, 3 -> 2, 3 -> 5, 5 -> 3, 1 -> 6, 4 -> 9,
    7 -> 9, 7 -> 2, 8 -> 2, 4 -> 8, 3 -> 7, 5 -> 3, 8 -> 3, 0 -> 8, 6 -> 5, 2 -> 3,
    1 -> 8, 1 -> 9, 1 -> 8, 5 -> 8, 7 -> 8, 0 -> 2](../Images/049e79b05a41598709a2aeef166e4a2a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![7 -> 7, 9 -> 9, 0 -> 0, 3 -> 3, 5 -> 5, 2 -> 8, 1 -> 3, 3 -> 5, 4 -> 6, 4
    -> 9, 9 -> 4, 5 -> 6, 9 -> 5, 9 -> 5, 3 -> 2, 3 -> 5, 5 -> 3, 1 -> 6, 4 -> 9,
    7 -> 9, 7 -> 2, 8 -> 2, 4 -> 8, 3 -> 7, 5 -> 3, 8 -> 3, 0 -> 8, 6 -> 5, 2 -> 3,
    1 -> 8, 1 -> 9, 1 -> 8, 5 -> 8, 7 -> 8, 0 -> 2](../Images/049e79b05a41598709a2aeef166e4a2a.png)'
- en: Where to go next?
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来去哪里？
- en: 'Hopefully this tutorial gives some insight into the topic of adversarial machine
    learning. There are many potential directions to go from here. This attack represents
    the very beginning of adversarial attack research and since there have been many
    subsequent ideas for how to attack and defend ML models from an adversary. In
    fact, at NIPS 2017 there was an adversarial attack and defense competition and
    many of the methods used in the competition are described in this paper: [Adversarial
    Attacks and Defences Competition](https://arxiv.org/pdf/1804.00097.pdf). The work
    on defense also leads into the idea of making machine learning models more *robust*
    in general, to both naturally perturbed and adversarially crafted inputs.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 希望本教程能够为对抗性机器学习的主题提供一些见解。从这里出发有许多潜在的方向。这种攻击代表了对抗性攻击研究的最初阶段，自那时以来，已经有许多关于如何攻击和防御ML模型的后续想法。事实上，在NIPS
    2017年有一个对抗性攻击和防御竞赛，许多竞赛中使用的方法在这篇论文中有描述：[对抗性攻击和防御竞赛](https://arxiv.org/pdf/1804.00097.pdf)。对防御的工作也引出了使机器学习模型更加*健壮*的想法，既对自然扰动又对对抗性制作的输入。
- en: Another direction to go is adversarial attacks and defense in different domains.
    Adversarial research is not limited to the image domain, check out [this](https://arxiv.org/pdf/1801.01944.pdf)
    attack on speech-to-text models. But perhaps the best way to learn more about
    adversarial machine learning is to get your hands dirty. Try to implement a different
    attack from the NIPS 2017 competition, and see how it differs from FGSM. Then,
    try to defend the model from your own attacks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个前进方向是在不同领域进行对抗性攻击和防御。对抗性研究不仅限于图像领域，可以查看[这篇](https://arxiv.org/pdf/1801.01944.pdf)关于语音转文本模型的攻击。但也许了解更多关于对抗性机器学习的最佳方法是动手实践。尝试实现来自NIPS
    2017竞赛的不同攻击，看看它与FGSM有何不同。然后，尝试防御模型免受您自己的攻击。
- en: A further direction to go, depending on available resources, is to modify the
    code to support processing work in batch, in parallel, and or distributed vs working
    on one attack at a time in the above for each `epsilon test()` loop.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 根据可用资源，另一个前进方向是修改代码以支持批处理、并行处理或分布式处理，而不是在上面的每个`epsilon test()`循环中一次处理一个攻击。
- en: '**Total running time of the script:** ( 3 minutes 52.817 seconds)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间:** (3分钟52.817秒)'
- en: '[`Download Python source code: fgsm_tutorial.py`](../_downloads/377bf4a7b1761e5f081e057385870d8e/fgsm_tutorial.py)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：fgsm_tutorial.py`](../_downloads/377bf4a7b1761e5f081e057385870d8e/fgsm_tutorial.py)'
- en: '[`Download Jupyter notebook: fgsm_tutorial.ipynb`](../_downloads/56c122e1c18e5e07666673e900acaed5/fgsm_tutorial.ipynb)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：fgsm_tutorial.ipynb`](../_downloads/56c122e1c18e5e07666673e900acaed5/fgsm_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery生成的画廊](https://sphinx-gallery.github.io)'
