- en: Exploring TorchRec sharding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/advanced/sharding.html](https://pytorch.org/tutorials/advanced/sharding.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This tutorial will mainly cover the sharding schemes of embedding tables via
    `EmbeddingPlanner` and `DistributedModelParallel` API and explore the benefits
    of different sharding schemes for the embedding tables by explicitly configuring
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Requirements: - python >= 3.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'We highly recommend CUDA when using torchRec. If using CUDA: - cuda >= 11.0'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Installing torchRec will also install [FBGEMM](https://github.com/pytorch/fbgemm),
    a collection of CUDA kernels and GPU enabled operations to run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Install multiprocess which works with ipython to for multi-processing programming
    within colab
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The following steps are needed for the Colab runtime to detect the added shared
    libraries. The runtime searches for shared libraries in /usr/lib, so we copy over
    the libraries which were installed in /usr/local/lib/. **This is a very necessary
    step, only in the colab runtime**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Restart your runtime at this point for the newly installed packages to be
    seen.** Run the step below immediately after restarting so that python knows where
    to look for packages. **Always run this step after restarting the runtime.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Distributed Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the notebook enviroment, we cannot run [SPMD](https://en.wikipedia.org/wiki/SPMD)
    program here but we can do multiprocessing inside the notebook to mimic the setup.
    Users should be responsible for setting up their own [SPMD](https://en.wikipedia.org/wiki/SPMD)
    launcher when using Torchrec. We setup our environment so that torch distributed
    based communication backend can work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Constructing our embedding model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we use TorchRec offering of [EmbeddingBagCollection](https://github.com/facebookresearch/torchrec/blob/main/torchrec/modules/embedding_modules.py#L59)
    to construct our embedding bag model with embedding tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we create an EmbeddingBagCollection (EBC) with four embedding bags. We
    have two types of tables: large tables and small tables differentiated by their
    row size difference: 4096 vs 1024\. Each table is still represented by 64 dimension
    embedding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We configure the `ParameterConstraints` data structure for the tables, which
    provides hints for the model parallel API to help decide the sharding and placement
    strategy for the tables. In TorchRec, we support * `table-wise`: place the entire
    table on one device; * `row-wise`: shard the table evenly by row dimension and
    place one shard on each device of the communication world; * `column-wise`: shard
    the table evenly by embedding dimension, and place one shard on each device of
    the communication world; * `table-row-wise`: special sharding optimized for intra-host
    communication for available fast intra-machine device interconnect, e.g. NVLink;
    * `data_parallel`: replicate the tables for every device;'
  prefs: []
  type: TYPE_NORMAL
- en: Note how we initially allocate the EBC on device “meta”. This will tell EBC
    to not allocate memory yet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: DistributedModelParallel in multiprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we have a single process execution function for mimicking one rank’s work
    during [SPMD](https://en.wikipedia.org/wiki/SPMD) execution.
  prefs: []
  type: TYPE_NORMAL
- en: This code will shard the model collectively with other processes and allocate
    memories accordingly. It first sets up process groups and do embedding table placement
    using planner and generate sharded model using `DistributedModelParallel`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Multiprocessing Execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s execute the code in multi-processes representing multiple GPU ranks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Table Wise Sharding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s execute the code in two processes for 2 GPUs. We can see in the plan
    print that how our tables are sharded across GPUs. Each node will have one large
    table and one small which shows our planner tries for load balance for the embedding
    tables. Table-wise is the de-factor go-to sharding schemes for many small-medium
    size tables for load balancing over the devices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Explore other sharding modes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have initially explored what table-wise sharding would look like and how
    it balances the tables placement. Now we explore sharding modes with finer focus
    on load balance: row-wise. Row-wise is specifically addressing large tables which
    a single device cannot hold due to the memory size increase from large embedding
    row numbers. It can address the placement of the super large tables in your models.
    Users can see that in the `shard_sizes` section in the printed plan log, the tables
    are halved by row dimension to be distributed onto two GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Column-wise on the other hand, address the load imbalance problems for tables
    with large embedding dimensions. We will split the table vertically. Users can
    see that in the `shard_sizes` section in the printed plan log, the tables are
    halved by embedding dimension to be distributed onto two GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For `table-row-wise`, unfortuately we cannot simulate it due to its nature of
    operating under multi-host setup. We will present a python [SPMD](https://en.wikipedia.org/wiki/SPMD)
    example in the future to train models with `table-row-wise`.
  prefs: []
  type: TYPE_NORMAL
- en: With data parallel, we will repeat the tables for all devices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
