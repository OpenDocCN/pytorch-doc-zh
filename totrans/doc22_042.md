# 了解CUDA内存使用情况

> 原文：[https://pytorch.org/docs/stable/torch_cuda_memory.html](https://pytorch.org/docs/stable/torch_cuda_memory.html)

为了调试CUDA内存使用，PyTorch提供了一种生成内存快照的方法，记录分配的CUDA内存在任何时间点的状态，并可选择记录导致该快照的分配事件的历史记录。

然后，可以将生成的快照拖放到托管在[pytorch.org/memory_viz](https://pytorch.org/memory_viz)上的交互式查看器中，用于探索快照。

# 生成快照

记录快照的常见模式是启用内存历史记录，运行要观察的代码，然后保存一个包含pickled快照的文件：

```py
# enable memory history, which will
# add tracebacks and event history to snapshots
torch.cuda.memory._record_memory_history()

run_your_code()
torch.cuda.memory._dump_snapshot("my_snapshot.pickle") 
```

# 使用可视化工具

打开[pytorch.org/memory_viz](https://pytorch.org/memory_viz)，将pickled快照文件拖放到可视化工具中。该可视化工具是一个在您的计算机上本地运行的JavaScript应用程序。它不会上传任何快照数据。

## 活动内存时间线

活动内存时间线显示了快照中特定GPU上的所有张量随时间的变化。在图表上进行平移/缩放，以查看较小的分配。将鼠标悬停在分配的块上，以查看分配该块时的堆栈跟踪，以及其地址等详细信息。可以调整详细滑块以渲染更少的分配，并在数据量较大时提高性能。

![_images/active_memory_timeline.png](../Images/eed5d13530c32a9ffc147fbef83865d2.png)

## 分配器状态历史

分配器状态历史在左侧的时间轴上显示了单个分配器事件。在时间轴中选择一个事件，以查看该事件时的分配器状态的可视摘要。此摘要显示了从cudaMalloc返回的每个单独段以及如何将其分割为单个分配或空闲空间的块。将鼠标悬停在段和块上，以查看内存分配时的堆栈跟踪。将鼠标悬停在事件上，以查看事件发生时的堆栈跟踪，例如张量何时被释放。内存不足错误报告为OOM事件。查看OOM时的内存状态可能有助于了解为什么分配失败，即使保留的内存仍然存在。

![_images/allocator_state_history.png](../Images/330e9ea5e1c7c9cf54145afa5cde9d6e.png)

堆栈跟踪信息还报告了分配发生的地址。地址b7f064c000000_0指的是地址为7f064c000000的块，这是该地址被分配的“_0”次。可以在活动内存时间线中查找此唯一字符串，并在活动状态历史中搜索，以检查张量分配或释放时的内存状态。

# 快照API参考

```py
torch.cuda.memory._record_memory_history(enabled='all', context='all', stacks='all', max_entries=9223372036854775807, device=None)¶
```

启用与内存分配相关的堆栈跟踪记录，这样您就可以知道[`torch.cuda.memory._snapshot()`](#torch.cuda.memory._snapshot "torch.cuda.memory._snapshot")中分配了哪些内存片段。

除了保留每个当前分配和释放的堆栈跟踪，这也将启用记录所有分配/释放事件的历史记录。

使用[`torch.cuda.memory._snapshot()`](#torch.cuda.memory._snapshot "torch.cuda.memory._snapshot")来检索此信息，并使用_memory_viz.py中的工具来可视化快照。

Python跟踪收集速度快（每个跟踪为2us），因此如果您预计需要调试内存问题，则可以考虑在生产作业中启用此功能。

C++跟踪收集也很快（~50ns/帧），对于许多典型程序来说，每个跟踪大约为2us，但可能会因堆栈深度而有所变化。

参数

+   **enabled**（*字面值*[**None**,* *"state"**,* *"all"**]**,* *可选*) – None，禁用记录内存历史。 “state”，保留当前分配内存的信息。 “all”，此外还保留所有分配/释放调用的历史记录。默认为“all”。

+   **context**（*Literal**[**None**,* *"state"**,* *"alloc"**,* *"all"**]**，“可选”） - None，不记录任何tracebacks。“state”，记录当前分配的内存的tracebacks。“alloc”，另外保留分配调用的tracebacks。“all”，另外保留释放调用的tracebacks。默认为“all”。

+   **stacks**（*Literal**[**"python"**,* *"all"**]**，“可选”） - “python”，包括Python、TorchScript和感应器帧在tracebacks中，“all”，另外包括C++帧，默认为“all”。

+   **max_entries**（[*int*](https://docs.python.org/3/library/functions.html#int)，“可选”） - 在记录的历史记录中保留最多max_entries个分配/释放事件。

```py
torch.cuda.memory._snapshot(device=None)¶
```

在调用时保存CUDA内存状态的快照。

状态表示为具有以下结构的字典。

```py
class Snapshot(TypedDict):
    segments : List[Segment]
    device_traces: List[List[TraceEntry]]

class Segment(TypedDict):
    # Segments are memory returned from a cudaMalloc call.
    # The size of reserved memory is the sum of all Segments.
    # Segments are cached and reused for future allocations.
    # If the reuse is smaller than the segment, the segment
    # is split into more then one Block.
    # empty_cache() frees Segments that are entirely inactive.
    address: int
    total_size: int #  cudaMalloc'd size of segment
    stream: int
    segment_type: Literal['small', 'large'] # 'large' (>1MB)
    allocated_size: int # size of memory in use
    active_size: int # size of memory in use or in active_awaiting_free state
    blocks : List[Block]

class Block(TypedDict):
    # A piece of memory returned from the allocator, or
    # current cached but inactive.
    size: int
    requested_size: int # size requested during malloc, may be smaller than
                        # size due to rounding
    address: int
    state: Literal['active_allocated', # used by a tensor
                'active_awaiting_free', # waiting for another stream to finish using
                                        # this, then it will become free
                'inactive',] # free for reuse
    frames: List[Frame] # stack trace from where the allocation occurred

class Frame(TypedDict):
        filename: str
        line: int
        name: str

class TraceEntry(TypedDict):
    # When `torch.cuda.memory._record_memory_history()` is enabled,
    # the snapshot will contain TraceEntry objects that record each
    # action the allocator took.
    action: Literal[
    'alloc'  # memory allocated
    'free_requested', # the allocated received a call to free memory
    'free_completed', # the memory that was requested to be freed is now
                    # able to be used in future allocation calls
    'segment_alloc', # the caching allocator ask cudaMalloc for more memory
                    # and added it as a segment in its cache
    'segment_free',  # the caching allocator called cudaFree to return memory
                    # to cuda possibly trying free up memory to
                    # allocate more segments or because empty_caches was called
    'oom',          # the allocator threw an OOM exception. 'size' is
                    # the requested number of bytes that did not succeed
    'snapshot'      # the allocator generated a memory snapshot
                    # useful to coorelate a previously taken
                    # snapshot with this trace
    ]
    addr: int # not present for OOM
    frames: List[Frame]
    size: int
    stream: int
    device_free: int # only present for OOM, the amount of
                    # memory cuda still reports to be free 
```

返回值

快照字典对象

```py
torch.cuda.memory._dump_snapshot(filename='dump_snapshot.pickle')¶
```

将torch.memory._snapshot()字典的pickled版本保存到文件中。

此文件可以由pytorch.org/memory_viz上的交互式快照查看器打开

参数

**filename**（[*str*](https://docs.python.org/3/library/stdtypes.html#str)，“可选”） - 要创建的文件名。默认为“dump_snapshot.pickle”。
