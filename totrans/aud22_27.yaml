- en: CTC forced alignment API tutorial
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/audio/stable/tutorials/ctc_forced_alignment_api_tutorial.html](https://pytorch.org/audio/stable/tutorials/ctc_forced_alignment_api_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-tutorials-ctc-forced-alignment-api-tutorial-py)
    to download the full example code
  prefs: []
  type: TYPE_NORMAL
- en: '**Author**: [Xiaohui Zhang](mailto:xiaohuizhang%40meta.com), [Moto Hira](mailto:moto%40meta.com)'
  prefs: []
  type: TYPE_NORMAL
- en: The forced alignment is a process to align transcript with speech. This tutorial
    shows how to align transcripts to speech using [`torchaudio.functional.forced_align()`](../generated/torchaudio.functional.forced_align.html#torchaudio.functional.forced_align
    "torchaudio.functional.forced_align") which was developed along the work of [Scaling
    Speech Technology to 1,000+ Languages](https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/).
  prefs: []
  type: TYPE_NORMAL
- en: '[`forced_align()`](../generated/torchaudio.functional.forced_align.html#torchaudio.functional.forced_align
    "torchaudio.functional.forced_align") has custom CPU and CUDA implementations
    which are more performant than the vanilla Python implementation above, and are
    more accurate. It can also handle missing transcript with special `<star>` token.'
  prefs: []
  type: TYPE_NORMAL
- en: There is also a high-level API, [`torchaudio.pipelines.Wav2Vec2FABundle`](../generated/torchaudio.pipelines.Wav2Vec2FABundle.html#torchaudio.pipelines.Wav2Vec2FABundle
    "torchaudio.pipelines.Wav2Vec2FABundle"), which wraps the pre/post-processing
    explained in this tutorial and makes it easy to run forced-alignments. [Forced
    alignment for multilingual data](./forced_alignment_for_multilingual_data_tutorial.html)
    uses this API to illustrate how to align non-English transcripts.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation[](#preparation "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: First we prepare the speech data and the transcript we area going to use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Generating emissions[](#generating-emissions "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[`forced_align()`](../generated/torchaudio.functional.forced_align.html#torchaudio.functional.forced_align
    "torchaudio.functional.forced_align") takes emission and token sequences and outputs
    timestaps of the tokens and their scores.'
  prefs: []
  type: TYPE_NORMAL
- en: Emission reperesents the frame-wise probability distribution over tokens, and
    it can be obtained by passing waveform to an acoustic model.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens are numerical expression of transcripts. There are many ways to tokenize
    transcripts, but here, we simply map alphabets into integer, which is how labels
    were constructed when the acoustice model we are going to use was trained.
  prefs: []
  type: TYPE_NORMAL
- en: We will use a pre-trained Wav2Vec2 model, [`torchaudio.pipelines.MMS_FA`](../generated/torchaudio.pipelines.MMS_FA.html#torchaudio.pipelines.MMS_FA
    "torchaudio.pipelines.MMS_FA"), to obtain emission and tokenize the transcript.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Frame-wise class probabilities](../Images/c32f061e6dd78030a0acb0683cc73658.png)'
  prefs: []
  type: TYPE_IMG
- en: Tokenize the transcript[](#tokenize-the-transcript "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We create a dictionary, which maps each label into token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: converting transcript to tokens is as simple as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Computing alignments[](#computing-alignments "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Frame-level alignments[](#frame-level-alignments "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we call TorchAudio’s forced alignment API to compute the frame-level alignment.
    For the detail of function signature, please refer to [`forced_align()`](../generated/torchaudio.functional.forced_align.html#torchaudio.functional.forced_align
    "torchaudio.functional.forced_align").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s look at the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The alignment is expressed in the frame cordinate of the emission, which is
    different from the original waveform.
  prefs: []
  type: TYPE_NORMAL
- en: It contains blank tokens and repeated tokens. The following is the interpretation
    of the non-blank tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When same token occured after blank tokens, it is not treated as a repeat, but
    as a new occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Token-level alignments[](#token-level-alignments "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next step is to resolve the repetation, so that each alignment does not depend
    on previous alignments. [`torchaudio.functional.merge_tokens()`](../generated/torchaudio.functional.merge_tokens.html#torchaudio.functional.merge_tokens
    "torchaudio.functional.merge_tokens") computes the [`TokenSpan`](../generated/torchaudio.functional.TokenSpan.html#torchaudio.functional.TokenSpan
    "torchaudio.functional.TokenSpan") object, which represents which token from the
    transcript is present at what time span.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Word-level alignments[](#word-level-alignments "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s group the token-level alignments into word-level alignments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Audio previews[](#audio-previews "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization[](#visualization "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s look at the alignment result and segment the original speech into
    words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![Emission](../Images/d047a1551a466fc4548b45cea0074e1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Inconsistent treatment of `blank` token[](#inconsistent-treatment-of-blank-token
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When splitting the token-level alignments into words, you will notice that some
    blank tokens are treated differently, and this makes the interpretation of the
    result somehwat ambigious.
  prefs: []
  type: TYPE_NORMAL
- en: This is easy to see when we plot the scores. The following figure shows word
    regions and non-word regions, with the frame-level scores of non-blank tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![Frame-level scores and word segments](../Images/fecac1c5bc6119c808419fbce87e00c0.png)'
  prefs: []
  type: TYPE_IMG
- en: In this plot, the blank tokens are those highlighted area without vertical bar.
    You can see that there are blank tokens which are interpreted as part of a word
    (highlighted red), while the others (highlighted blue) are not.
  prefs: []
  type: TYPE_NORMAL
- en: One reason for this is because the model was trained without a label for the
    word boundary. The blank tokens are treated not just as repeatation but also as
    silence between words.
  prefs: []
  type: TYPE_NORMAL
- en: But then, a question arises. Should frames immediately after or near the end
    of a word be silent or repeat?
  prefs: []
  type: TYPE_NORMAL
- en: In the above example, if you go back to the previous plot of spectrogram and
    word regions, you see that after “y” in “curiosity”, there is still some activities
    in multiple frequency buckets.
  prefs: []
  type: TYPE_NORMAL
- en: Would it be more accurate if that frame was included in the word?
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, CTC does not provide a comprehensive solution to this. Models
    trained with CTC are known to exhibit “peaky” response, that is, they tend to
    spike for an aoccurance of a label, but the spike does not last for the duration
    of the label. (Note: Pre-trained Wav2Vec2 models tend to spike at the beginning
    of label occurances, but this not always the case.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[Zeyer *et al.*, 2021](../references.html#id2 "Albert Zeyer, Ralf Schlüter,
    and Hermann Ney. Why does ctc result in peaky behavior? 2021\. arXiv:2105.14849.")]
    has in-depth alanysis on the peaky behavior of CTC. We encourage those who are
    interested understanding more to refer to the paper. The following is a quote
    from the paper, which is the exact issue we are facing here.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Peaky behavior can be problematic in certain cases,* *e.g. when an application
    requires to not use the blank label,* *e.g. to get meaningful time accurate alignments
    of phonemes* *to a transcription.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Advanced: Handling transcripts with `<star>` token[](#advanced-handling-transcripts-with-star-token
    "Permalink to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s look at when the transcript is partially missing, how can we improve
    alignment quality using the `<star>` token, which is capable of modeling any token.
  prefs: []
  type: TYPE_NORMAL
- en: Here we use the same English example as used above. But we remove the beginning
    text `“i had that curiosity beside me at”` from the transcript. Aligning audio
    with such transcript results in wrong alignments of the existing word “this”.
    However, this issue can be mitigated by using the `<star>` token to model the
    missing text.
  prefs: []
  type: TYPE_NORMAL
- en: First, we extend the dictionary to include the `<star>` token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Next, we extend the emission tensor with the extra dimension corresponding to
    the `<star>` token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![Frame-wise class probabilities](../Images/8398bfb2cbf4975411aad5762becbb24.png)'
  prefs: []
  type: TYPE_IMG
- en: The following function combines all the processes, and compute word segments
    from emission in one-go.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Full Transcript[](#full-transcript "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![Emission](../Images/06c55215ab23aa7e45ffbdb39ec92005.png)'
  prefs: []
  type: TYPE_IMG
- en: Partial Transcript with `<star>` token[](#partial-transcript-with-star-token
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we replace the first part of the transcript with the `<star>` token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![Emission](../Images/758a546451a8a63d8eaab41dceca0333.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: Partial Transcript without `<star>` token[](#partial-transcript-without-star-token
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a comparison, the following aligns the partial transcript without using `<star>`
    token. It demonstrates the effect of `<star>` token for dealing with deletion
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![Emission](../Images/e7fe5710f77da40ba8454960b5ecebfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion[](#conclusion "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this tutorial, we looked at how to use torchaudio’s forced alignment API
    to align and segment speech files, and demonstrated one advanced usage: How introducing
    a `<star>` token could improve alignment accuracy when transcription errors exist.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement[](#acknowledgement "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks to [Vineel Pratap](mailto:vineelkpratap%40meta.com) and [Zhaoheng Ni](mailto:zni%40meta.com)
    for developing and open-sourcing the forced aligner API.
  prefs: []
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 0 minutes 8.811 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: ctc_forced_alignment_api_tutorial.py`](../_downloads/fd312a07c77ccd892cb337379bf91f16/ctc_forced_alignment_api_tutorial.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: ctc_forced_alignment_api_tutorial.ipynb`](../_downloads/97729a601eea05725da9715649633311/ctc_forced_alignment_api_tutorial.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
