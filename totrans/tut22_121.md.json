["```py\nimport argparse\nimport os\nimport time\nfrom threading import Lock\n\nimport torch\nimport torch.distributed.autograd as dist_autograd\nimport torch.distributed.rpc as rpc\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.distributed.optim import DistributedOptimizer\nfrom torchvision import datasets, transforms\n\n# --------- MNIST Network to train, from pytorch/examples -----\n\nclass Net(nn.Module):\n    def __init__(self, num_gpus=0):\n        super(Net, self).__init__()\n        print(f\"Using {num_gpus} GPUs to train\")\n        self.num_gpus = num_gpus\n        device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() and self.num_gpus > 0 else \"cpu\")\n        print(f\"Putting first 2 convs on {str(device)}\")\n        # Put conv layers on the first cuda device, or CPU if no cuda device\n        self.conv1 = nn.Conv2d(1, 32, 3, 1).to(device)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1).to(device)\n        # Put rest of the network on the 2nd cuda device, if there is one\n        if \"cuda\" in str(device) and num_gpus > 1:\n            device = torch.device(\"cuda:1\")\n\n        print(f\"Putting rest of layers on {str(device)}\")\n        self.dropout1 = nn.Dropout2d(0.25).to(device)\n        self.dropout2 = nn.Dropout2d(0.5).to(device)\n        self.fc1 = nn.Linear(9216, 128).to(device)\n        self.fc2 = nn.Linear(128, 10).to(device)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        # Move tensor to next device if necessary\n        next_device = next(self.fc1.parameters()).device\n        x = x.to(next_device)\n\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output \n```", "```py\n# --------- Helper Methods --------------------\n\n# On the local node, call a method with first arg as the value held by the\n# RRef. Other args are passed in as arguments to the function called.\n# Useful for calling instance methods. method could be any matching function, including\n# class methods.\ndef call_method(method, rref, *args, **kwargs):\n    return method(rref.local_value(), *args, **kwargs)\n\n# Given an RRef, return the result of calling the passed in method on the value\n# held by the RRef. This call is done on the remote node that owns\n# the RRef and passes along the given argument.\n# Example: If the value held by the RRef is of type Foo, then\n# remote_method(Foo.bar, rref, arg1, arg2) is equivalent to calling\n# <foo_instance>.bar(arg1, arg2) on the remote node and getting the result\n# back.\n\ndef remote_method(method, rref, *args, **kwargs):\n    args = [method, rref] + list(args)\n    return rpc.rpc_sync(rref.owner(), call_method, args=args, kwargs=kwargs) \n```", "```py\n# --------- Parameter Server --------------------\nclass ParameterServer(nn.Module):\n    def __init__(self, num_gpus=0):\n        super().__init__()\n        model = Net(num_gpus=num_gpus)\n        self.model = model\n        self.input_device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() and num_gpus > 0 else \"cpu\") \n```", "```py\nclass ParameterServer(nn.Module):\n...\n    def forward(self, inp):\n        inp = inp.to(self.input_device)\n        out = self.model(inp)\n        # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.\n        # Tensors must be moved in and out of GPU memory due to this.\n        out = out.to(\"cpu\")\n        return out \n```", "```py\n# Use dist autograd to retrieve gradients accumulated for this model.\n# Primarily used for verification.\ndef get_dist_gradients(self, cid):\n    grads = dist_autograd.get_gradients(cid)\n    # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.\n    # Tensors must be moved in and out of GPU memory due to this.\n    cpu_grads = {}\n    for k, v in grads.items():\n        k_cpu, v_cpu = k.to(\"cpu\"), v.to(\"cpu\")\n        cpu_grads[k_cpu] = v_cpu\n    return cpu_grads\n\n# Wrap local parameters in a RRef. Needed for building the\n# DistributedOptimizer which optimizes paramters remotely.\ndef get_param_rrefs(self):\n    param_rrefs = [rpc.RRef(param) for param in self.model.parameters()]\n    return param_rrefs \n```", "```py\n# The global parameter server instance.\nparam_server = None\n# A lock to ensure we only have one parameter server.\nglobal_lock = Lock()\n\ndef get_parameter_server(num_gpus=0):\n  \"\"\"\n Returns a singleton parameter server to all trainer processes\n \"\"\"\n    global param_server\n    # Ensure that we get only one handle to the ParameterServer.\n    with global_lock:\n        if not param_server:\n            # construct it once\n            param_server = ParameterServer(num_gpus=num_gpus)\n        return param_server\n\ndef run_parameter_server(rank, world_size):\n    # The parameter server just acts as a host for the model and responds to\n    # requests from trainers.\n    # rpc.shutdown() will wait for all workers to complete by default, which\n    # in this case means that the parameter server will wait for all trainers\n    # to complete, and then exit.\n    print(\"PS master initializing RPC\")\n    rpc.init_rpc(name=\"parameter_server\", rank=rank, world_size=world_size)\n    print(\"RPC initialized! Running parameter server...\")\n    rpc.shutdown()\n    print(\"RPC shutdown on parameter server.\") \n```", "```py\n# --------- Trainers --------------------\n\n# nn.Module corresponding to the network trained by this trainer. The\n# forward() method simply invokes the network on the given parameter\n# server.\nclass TrainerNet(nn.Module):\n    def __init__(self, num_gpus=0):\n        super().__init__()\n        self.num_gpus = num_gpus\n        self.param_server_rref = rpc.remote(\n            \"parameter_server\", get_parameter_server, args=(num_gpus,)) \n```", "```py\nclass TrainerNet(nn.Module):\n...\n    def get_global_param_rrefs(self):\n        remote_params = remote_method(\n            ParameterServer.get_param_rrefs,\n            self.param_server_rref)\n        return remote_params \n```", "```py\nclass TrainerNet(nn.Module):\n...\n    def forward(self, x):\n        model_output = remote_method(\n            ParameterServer.forward, self.param_server_rref, x)\n        return model_output \n```", "```py\ndef run_training_loop(rank, num_gpus, train_loader, test_loader):\n    # Runs the typical nueral network forward + backward + optimizer step, but\n    # in a distributed fashion.\n    net = TrainerNet(num_gpus=num_gpus)\n    # Build DistributedOptimizer.\n    param_rrefs = net.get_global_param_rrefs()\n    opt = DistributedOptimizer(optim.SGD, param_rrefs, lr=0.03) \n```", "```py\ndef run_training_loop(rank, num_gpus, train_loader, test_loader):\n...\n    for i, (data, target) in enumerate(train_loader):\n        with dist_autograd.context() as cid:\n            model_output = net(data)\n            target = target.to(model_output.device)\n            loss = F.nll_loss(model_output, target)\n            if i % 5 == 0:\n                print(f\"Rank {rank} training batch {i} loss {loss.item()}\")\n            dist_autograd.backward(cid, [loss])\n            # Ensure that dist autograd ran successfully and gradients were\n            # returned.\n            assert remote_method(\n                ParameterServer.get_dist_gradients,\n                net.param_server_rref,\n                cid) != {}\n            opt.step(cid)\n\n     print(\"Training complete!\")\n     print(\"Getting accuracy....\")\n     get_accuracy(test_loader, net) \n```", "```py\ndef get_accuracy(test_loader, model):\n    model.eval()\n    correct_sum = 0\n    # Use GPU to evaluate if possible\n    device = torch.device(\"cuda:0\" if model.num_gpus > 0\n        and torch.cuda.is_available() else \"cpu\")\n    with torch.no_grad():\n        for i, (data, target) in enumerate(test_loader):\n            out = model(data, -1)\n            pred = out.argmax(dim=1, keepdim=True)\n            pred, target = pred.to(device), target.to(device)\n            correct = pred.eq(target.view_as(pred)).sum().item()\n            correct_sum += correct\n\n    print(f\"Accuracy {correct_sum  /  len(test_loader.dataset)}\") \n```", "```py\n# Main loop for trainers.\ndef run_worker(rank, world_size, num_gpus, train_loader, test_loader):\n    print(f\"Worker rank {rank} initializing RPC\")\n    rpc.init_rpc(\n        name=f\"trainer_{rank}\",\n        rank=rank,\n        world_size=world_size)\n\n    print(f\"Worker {rank} done initializing RPC\")\n\n    run_training_loop(rank, num_gpus, train_loader, test_loader)\n    rpc.shutdown() \n```", "```py\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=\"Parameter-Server RPC based training\")\n    parser.add_argument(\n        \"--world_size\",\n        type=int,\n        default=4,\n        help=\"\"\"Total number of participating processes. Should be the sum of\n master node and all training nodes.\"\"\")\n    parser.add_argument(\n        \"rank\",\n        type=int,\n        default=None,\n        help=\"Global rank of this process. Pass in 0 for master.\")\n    parser.add_argument(\n        \"num_gpus\",\n        type=int,\n        default=0,\n        help=\"\"\"Number of GPUs to use for training, Currently supports between 0\n and 2 GPUs. Note that this argument will be passed to the parameter servers.\"\"\")\n    parser.add_argument(\n        \"--master_addr\",\n        type=str,\n        default=\"localhost\",\n        help=\"\"\"Address of master, will default to localhost if not provided.\n Master must be able to accept network traffic on the address + port.\"\"\")\n    parser.add_argument(\n        \"--master_port\",\n        type=str,\n        default=\"29500\",\n        help=\"\"\"Port that master is listening on, will default to 29500 if not\n provided. Master must be able to accept network traffic on the host and port.\"\"\")\n\n    args = parser.parse_args()\n    assert args.rank is not None, \"must provide rank argument.\"\n    assert args.num_gpus <= 3, f\"Only 0-2 GPUs currently supported (got {args.num_gpus}).\"\n    os.environ['MASTER_ADDR'] = args.master_addr\n    os.environ[\"MASTER_PORT\"] = args.master_port \n```", "```py\nprocesses = []\nworld_size = args.world_size\nif args.rank == 0:\n    p = mp.Process(target=run_parameter_server, args=(0, world_size))\n    p.start()\n    processes.append(p)\nelse:\n    # Get data to train on\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=32, shuffle=True,)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            '../data',\n            train=False,\n            transform=transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.1307,), (0.3081,))\n                        ])),\n        batch_size=32,\n        shuffle=True,\n    )\n    # start training worker on this node\n    p = mp.Process(\n        target=run_worker,\n        args=(\n            args.rank,\n            world_size, args.num_gpus,\n            train_loader,\n            test_loader))\n    p.start()\n    processes.append(p)\n\nfor p in processes:\n    p.join() \n```"]