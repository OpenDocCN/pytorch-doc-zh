["```py\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\nrn18 = [models.resnet18](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18 \"torchvision.models.resnet18\")()\n[rn18.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")() \n```", "```py\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n) \n```", "```py\ninput = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(5, 3, 224, 224)\n[output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = rn18(input) \n```", "```py\nimport statistics, tabulate, time\nfrom typing import Any, Dict, List\nfrom torch.fx import [Interpreter](https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter \"torch.fx.Interpreter\") \n```", "```py\ntraced_rn18 = [torch.fx.symbolic_trace](https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace \"torch.fx.symbolic_trace\")(rn18)\nprint([traced_rn18.graph](https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule.graph \"torch.fx.GraphModule.graph\")) \n```", "```py\ngraph():\n    %x : torch.Tensor [num_users=1] = placeholder[target=x]\n    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n    %bn1 : [num_users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n    %relu : [num_users=1] = call_module[target=relu](args = (%bn1,), kwargs = {})\n    %maxpool : [num_users=2] = call_module[target=maxpool](args = (%relu,), kwargs = {})\n    %layer1_0_conv1 : [num_users=1] = call_module[target=layer1.0.conv1](args = (%maxpool,), kwargs = {})\n    %layer1_0_bn1 : [num_users=1] = call_module[target=layer1.0.bn1](args = (%layer1_0_conv1,), kwargs = {})\n    %layer1_0_relu : [num_users=1] = call_module[target=layer1.0.relu](args = (%layer1_0_bn1,), kwargs = {})\n    %layer1_0_conv2 : [num_users=1] = call_module[target=layer1.0.conv2](args = (%layer1_0_relu,), kwargs = {})\n    %layer1_0_bn2 : [num_users=1] = call_module[target=layer1.0.bn2](args = (%layer1_0_conv2,), kwargs = {})\n    %add : [num_users=1] = call_function[target=operator.add](args = (%layer1_0_bn2, %maxpool), kwargs = {})\n    %layer1_0_relu_1 : [num_users=2] = call_module[target=layer1.0.relu](args = (%add,), kwargs = {})\n    %layer1_1_conv1 : [num_users=1] = call_module[target=layer1.1.conv1](args = (%layer1_0_relu_1,), kwargs = {})\n    %layer1_1_bn1 : [num_users=1] = call_module[target=layer1.1.bn1](args = (%layer1_1_conv1,), kwargs = {})\n    %layer1_1_relu : [num_users=1] = call_module[target=layer1.1.relu](args = (%layer1_1_bn1,), kwargs = {})\n    %layer1_1_conv2 : [num_users=1] = call_module[target=layer1.1.conv2](args = (%layer1_1_relu,), kwargs = {})\n    %layer1_1_bn2 : [num_users=1] = call_module[target=layer1.1.bn2](args = (%layer1_1_conv2,), kwargs = {})\n    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%layer1_1_bn2, %layer1_0_relu_1), kwargs = {})\n    %layer1_1_relu_1 : [num_users=2] = call_module[target=layer1.1.relu](args = (%add_1,), kwargs = {})\n    %layer2_0_conv1 : [num_users=1] = call_module[target=layer2.0.conv1](args = (%layer1_1_relu_1,), kwargs = {})\n    %layer2_0_bn1 : [num_users=1] = call_module[target=layer2.0.bn1](args = (%layer2_0_conv1,), kwargs = {})\n    %layer2_0_relu : [num_users=1] = call_module[target=layer2.0.relu](args = (%layer2_0_bn1,), kwargs = {})\n    %layer2_0_conv2 : [num_users=1] = call_module[target=layer2.0.conv2](args = (%layer2_0_relu,), kwargs = {})\n    %layer2_0_bn2 : [num_users=1] = call_module[target=layer2.0.bn2](args = (%layer2_0_conv2,), kwargs = {})\n    %layer2_0_downsample_0 : [num_users=1] = call_module[target=layer2.0.downsample.0](args = (%layer1_1_relu_1,), kwargs = {})\n    %layer2_0_downsample_1 : [num_users=1] = call_module[target=layer2.0.downsample.1](args = (%layer2_0_downsample_0,), kwargs = {})\n    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%layer2_0_bn2, %layer2_0_downsample_1), kwargs = {})\n    %layer2_0_relu_1 : [num_users=2] = call_module[target=layer2.0.relu](args = (%add_2,), kwargs = {})\n    %layer2_1_conv1 : [num_users=1] = call_module[target=layer2.1.conv1](args = (%layer2_0_relu_1,), kwargs = {})\n    %layer2_1_bn1 : [num_users=1] = call_module[target=layer2.1.bn1](args = (%layer2_1_conv1,), kwargs = {})\n    %layer2_1_relu : [num_users=1] = call_module[target=layer2.1.relu](args = (%layer2_1_bn1,), kwargs = {})\n    %layer2_1_conv2 : [num_users=1] = call_module[target=layer2.1.conv2](args = (%layer2_1_relu,), kwargs = {})\n    %layer2_1_bn2 : [num_users=1] = call_module[target=layer2.1.bn2](args = (%layer2_1_conv2,), kwargs = {})\n    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%layer2_1_bn2, %layer2_0_relu_1), kwargs = {})\n    %layer2_1_relu_1 : [num_users=2] = call_module[target=layer2.1.relu](args = (%add_3,), kwargs = {})\n    %layer3_0_conv1 : [num_users=1] = call_module[target=layer3.0.conv1](args = (%layer2_1_relu_1,), kwargs = {})\n    %layer3_0_bn1 : [num_users=1] = call_module[target=layer3.0.bn1](args = (%layer3_0_conv1,), kwargs = {})\n    %layer3_0_relu : [num_users=1] = call_module[target=layer3.0.relu](args = (%layer3_0_bn1,), kwargs = {})\n    %layer3_0_conv2 : [num_users=1] = call_module[target=layer3.0.conv2](args = (%layer3_0_relu,), kwargs = {})\n    %layer3_0_bn2 : [num_users=1] = call_module[target=layer3.0.bn2](args = (%layer3_0_conv2,), kwargs = {})\n    %layer3_0_downsample_0 : [num_users=1] = call_module[target=layer3.0.downsample.0](args = (%layer2_1_relu_1,), kwargs = {})\n    %layer3_0_downsample_1 : [num_users=1] = call_module[target=layer3.0.downsample.1](args = (%layer3_0_downsample_0,), kwargs = {})\n    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%layer3_0_bn2, %layer3_0_downsample_1), kwargs = {})\n    %layer3_0_relu_1 : [num_users=2] = call_module[target=layer3.0.relu](args = (%add_4,), kwargs = {})\n    %layer3_1_conv1 : [num_users=1] = call_module[target=layer3.1.conv1](args = (%layer3_0_relu_1,), kwargs = {})\n    %layer3_1_bn1 : [num_users=1] = call_module[target=layer3.1.bn1](args = (%layer3_1_conv1,), kwargs = {})\n    %layer3_1_relu : [num_users=1] = call_module[target=layer3.1.relu](args = (%layer3_1_bn1,), kwargs = {})\n    %layer3_1_conv2 : [num_users=1] = call_module[target=layer3.1.conv2](args = (%layer3_1_relu,), kwargs = {})\n    %layer3_1_bn2 : [num_users=1] = call_module[target=layer3.1.bn2](args = (%layer3_1_conv2,), kwargs = {})\n    %add_5 : [num_users=1] = call_function[target=operator.add](args = (%layer3_1_bn2, %layer3_0_relu_1), kwargs = {})\n    %layer3_1_relu_1 : [num_users=2] = call_module[target=layer3.1.relu](args = (%add_5,), kwargs = {})\n    %layer4_0_conv1 : [num_users=1] = call_module[target=layer4.0.conv1](args = (%layer3_1_relu_1,), kwargs = {})\n    %layer4_0_bn1 : [num_users=1] = call_module[target=layer4.0.bn1](args = (%layer4_0_conv1,), kwargs = {})\n    %layer4_0_relu : [num_users=1] = call_module[target=layer4.0.relu](args = (%layer4_0_bn1,), kwargs = {})\n    %layer4_0_conv2 : [num_users=1] = call_module[target=layer4.0.conv2](args = (%layer4_0_relu,), kwargs = {})\n    %layer4_0_bn2 : [num_users=1] = call_module[target=layer4.0.bn2](args = (%layer4_0_conv2,), kwargs = {})\n    %layer4_0_downsample_0 : [num_users=1] = call_module[target=layer4.0.downsample.0](args = (%layer3_1_relu_1,), kwargs = {})\n    %layer4_0_downsample_1 : [num_users=1] = call_module[target=layer4.0.downsample.1](args = (%layer4_0_downsample_0,), kwargs = {})\n    %add_6 : [num_users=1] = call_function[target=operator.add](args = (%layer4_0_bn2, %layer4_0_downsample_1), kwargs = {})\n    %layer4_0_relu_1 : [num_users=2] = call_module[target=layer4.0.relu](args = (%add_6,), kwargs = {})\n    %layer4_1_conv1 : [num_users=1] = call_module[target=layer4.1.conv1](args = (%layer4_0_relu_1,), kwargs = {})\n    %layer4_1_bn1 : [num_users=1] = call_module[target=layer4.1.bn1](args = (%layer4_1_conv1,), kwargs = {})\n    %layer4_1_relu : [num_users=1] = call_module[target=layer4.1.relu](args = (%layer4_1_bn1,), kwargs = {})\n    %layer4_1_conv2 : [num_users=1] = call_module[target=layer4.1.conv2](args = (%layer4_1_relu,), kwargs = {})\n    %layer4_1_bn2 : [num_users=1] = call_module[target=layer4.1.bn2](args = (%layer4_1_conv2,), kwargs = {})\n    %add_7 : [num_users=1] = call_function[target=operator.add](args = (%layer4_1_bn2, %layer4_0_relu_1), kwargs = {})\n    %layer4_1_relu_1 : [num_users=1] = call_module[target=layer4.1.relu](args = (%add_7,), kwargs = {})\n    %avgpool : [num_users=1] = call_module[target=avgpool](args = (%layer4_1_relu_1,), kwargs = {})\n    %flatten : [num_users=1] = call_function[target=torch.flatten](args = (%avgpool, 1), kwargs = {})\n    %fc : [num_users=1] = call_module[target=fc](args = (%flatten,), kwargs = {})\n    return fc \n```", "```py\nclass ProfilingInterpreter([Interpreter](https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter \"torch.fx.Interpreter\")):\n    def __init__(self, mod : [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n        # Rather than have the user symbolically trace their model,\n        # we're going to do it in the constructor. As a result, the\n        # user can pass in any ``Module`` without having to worry about\n        # symbolic tracing APIs\n        gm = [torch.fx.symbolic_trace](https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace \"torch.fx.symbolic_trace\")(mod)\n        super().__init__(gm)\n\n        # We are going to store away two things here:\n        #\n        # 1\\. A list of total runtimes for ``mod``. In other words, we are\n        #    storing away the time ``mod(...)`` took each time this\n        #    interpreter is called.\n        self.total_runtime_sec : List[float] = []\n        # 2\\. A map from ``Node`` to a list of times (in seconds) that\n        #    node took to run. This can be seen as similar to (1) but\n        #    for specific sub-parts of the model.\n        self.runtimes_sec : Dict[[torch.fx.Node](https://pytorch.org/docs/stable/fx.html#torch.fx.Node \"torch.fx.Node\"), List[float]] = {}\n\n    ######################################################################\n    # Next, let's override our first method: ``run()``. ``Interpreter``'s ``run``\n    # method is the top-level entry point for execution of the model. We will\n    # want to intercept this so that we can record the total runtime of the\n    # model.\n\n    def run(self, *args) -> Any:\n        # Record the time we started running the model\n        t_start = time.time()\n        # Run the model by delegating back into Interpreter.run()\n        return_val = super().run(*args)\n        # Record the time we finished running the model\n        t_end = time.time()\n        # Store the total elapsed time this model execution took in the\n        # ``ProfilingInterpreter``\n        self.total_runtime_sec.append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Now, let's override ``run_node``. ``Interpreter`` calls ``run_node`` each\n    # time it executes a single node. We will intercept this so that we\n    # can measure and record the time taken for each individual call in\n    # the model.\n\n    def run_node(self, n : [torch.fx.Node](https://pytorch.org/docs/stable/fx.html#torch.fx.Node \"torch.fx.Node\")) -> Any:\n        # Record the time we started running the op\n        t_start = time.time()\n        # Run the op by delegating back into Interpreter.run_node()\n        return_val = super().run_node(n)\n        # Record the time we finished running the op\n        t_end = time.time()\n        # If we don't have an entry for this node in our runtimes_sec\n        # data structure, add one with an empty list value.\n        self.runtimes_sec.setdefault(n, [])\n        # Record the total elapsed time for this single invocation\n        # in the runtimes_sec data structure\n        self.runtimes_sec[n].append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Finally, we are going to define a method (one which doesn't override\n    # any ``Interpreter`` method) that provides us a nice, organized view of\n    # the data we have collected.\n\n    def summary(self, should_sort : bool = False) -> str:\n        # Build up a list of summary information for each node\n        node_summaries : List[List[Any]] = []\n        # Calculate the mean runtime for the whole network. Because the\n        # network may have been called multiple times during profiling,\n        # we need to summarize the runtimes. We choose to use the\n        # arithmetic mean for this.\n        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n\n        # For each node, record summary statistics\n        for node, runtimes in self.runtimes_sec.items():\n            # Similarly, compute the mean runtime for ``node``\n            mean_runtime = statistics.mean(runtimes)\n            # For easier understanding, we also compute the percentage\n            # time each node took with respect to the whole network.\n            pct_total = mean_runtime / mean_total_runtime * 100\n            # Record the node's type, name of the node, mean runtime, and\n            # percent runtime.\n            node_summaries.append(\n                [node.op, str(node), mean_runtime, pct_total])\n\n        # One of the most important questions to answer when doing performance\n        # profiling is \"Which op(s) took the longest?\". We can make this easy\n        # to see by providing sorting functionality in our summary view\n        if should_sort:\n            node_summaries.sort(key=lambda s: s[2], reverse=True)\n\n        # Use the ``tabulate`` library to create a well-formatted table\n        # presenting our summary information\n        headers : List[str] = [\n            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n        ]\n        return tabulate.tabulate(node_summaries, headers=headers) \n```", "```py\ninterp = [ProfilingInterpreter](https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter \"torch.fx.Interpreter\")(rn18)\n[interp.run](https://pytorch.org/docs/stable/fx.html#torch.fx.Interpreter.run \"torch.fx.Interpreter.run\")(input)\nprint(interp.summary(True)) \n```", "```py\nOp type        Op                       Average runtime (s)    Pct total runtime\n-------------  ---------------------  ---------------------  -------------------\ncall_module    maxpool                          0.0058043              9.43883\ncall_module    conv1                            0.00556087             9.04297\ncall_module    layer4_0_conv2                   0.00342155             5.56404\ncall_module    layer4_1_conv2                   0.00325394             5.29148\ncall_module    layer4_1_conv1                   0.00316119             5.14066\ncall_module    layer1_0_conv2                   0.00267935             4.3571\ncall_module    layer1_1_conv1                   0.00267816             4.35516\ncall_module    layer3_0_conv2                   0.00267792             4.35477\ncall_module    layer3_1_conv1                   0.00261283             4.24893\ncall_module    layer3_1_conv2                   0.00259137             4.21403\ncall_module    layer1_0_conv1                   0.00256515             4.17138\ncall_module    layer2_1_conv1                   0.00249219             4.05274\ncall_module    layer2_1_conv2                   0.0024581              3.9973\ncall_module    layer2_0_conv2                   0.00242114             3.93721\ncall_module    layer1_1_conv2                   0.00241613             3.92906\ncall_module    layer4_0_conv1                   0.00203657             3.31183\ncall_module    layer3_0_conv1                   0.00165725             2.69498\ncall_module    layer2_0_conv1                   0.00164604             2.67676\ncall_module    bn1                              0.00133991             2.17894\ncall_module    layer2_0_downsample_0            0.000616312            1.00223\ncall_module    layer3_0_downsample_0            0.000507832            0.825825\ncall_module    layer4_0_downsample_0            0.000471115            0.766117\ncall_function  add                              0.00034976             0.568772\ncall_module    relu                             0.000216722            0.352429\ncall_function  add_1                            0.000201702            0.328004\ncall_module    fc                               0.000183105            0.297762\ncall_module    layer1_0_bn1                     0.000178337            0.290008\ncall_module    layer1_0_bn2                     0.000164032            0.266745\ncall_module    layer1_1_bn1                     0.000163794            0.266358\ncall_module    layer1_1_bn2                     0.000160933            0.261705\ncall_module    avgpool                          0.000149012            0.242319\ncall_module    layer2_1_bn2                     0.000141621            0.2303\ncall_module    layer2_0_downsample_1            0.000141382            0.229913\ncall_module    layer4_0_bn2                     0.000140429            0.228362\ncall_module    layer2_0_bn1                     0.000137806            0.224097\ncall_module    layer4_1_bn2                     0.000136852            0.222546\ncall_module    layer2_1_bn1                     0.000136137            0.221383\ncall_module    layer2_0_bn2                     0.000132799            0.215955\ncall_module    layer1_1_relu                    0.000128984            0.209752\ncall_function  add_2                            0.000127316            0.207038\ncall_module    layer3_1_bn1                     0.000127316            0.207038\ncall_module    layer3_0_downsample_1            0.0001266              0.205875\ncall_module    layer3_0_bn1                     0.000126362            0.205487\ncall_module    layer3_0_bn2                     0.000125647            0.204324\ncall_function  add_3                            0.000124454            0.202385\ncall_module    layer3_1_bn2                     0.000123978            0.20161\ncall_module    layer4_1_bn1                     0.000119686            0.194631\ncall_module    layer4_0_downsample_1            0.000118017            0.191917\ncall_module    layer4_0_bn1                     0.000117779            0.191529\ncall_module    layer1_0_relu                    0.000107288            0.17447\ncall_module    layer1_0_relu_1                  9.91821e-05            0.161288\ncall_module    layer1_1_relu_1                  9.63211e-05            0.156635\ncall_module    layer4_0_relu                    8.51154e-05            0.138413\ncall_function  add_5                            8.46386e-05            0.137637\ncall_module    layer4_1_relu                    8.44002e-05            0.13725\ncall_module    layer2_1_relu                    8.36849e-05            0.136087\ncall_function  add_4                            8.24928e-05            0.134148\ncall_module    layer2_0_relu                    8.10623e-05            0.131822\ncall_module    layer2_1_relu_1                  8.01086e-05            0.130271\ncall_module    layer2_0_relu_1                  7.96318e-05            0.129496\ncall_module    layer3_0_relu_1                  7.9155e-05             0.12872\ncall_module    layer4_0_relu_1                  7.7486e-05             0.126006\ncall_function  add_7                            7.7486e-05             0.126006\ncall_module    layer3_1_relu                    7.70092e-05            0.125231\ncall_function  add_6                            7.67708e-05            0.124843\ncall_module    layer4_1_relu_1                  7.67708e-05            0.124843\ncall_module    layer3_0_relu                    7.65324e-05            0.124455\ncall_module    layer3_1_relu_1                  7.10487e-05            0.115538\ncall_function  flatten                          4.3869e-05             0.0713388\nplaceholder    x                                2.59876e-05            0.0422605\noutput         output                           1.95503e-05            0.0317923 \n```"]