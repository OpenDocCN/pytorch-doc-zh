["```py\nimport torch\nimport torch.nn as nn\n\nDEVICE = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")(\"cuda\") if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available \"torch.cuda.is_available\")() else [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")(\"cpu\") \n```", "```py\nimport torchtext.transforms as T\nfrom torch.hub import [load_state_dict_from_url](https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url \"torch.hub.load_state_dict_from_url\")\n\npadding_idx = 1\nbos_idx = 0\neos_idx = 2\nmax_seq_len = 256\nxlmr_vocab_path = r\"https://download.pytorch.org/models/text/xlmr.vocab.pt\"\nxlmr_spm_model_path = r\"https://download.pytorch.org/models/text/xlmr.sentencepiece.bpe.model\"\n\ntext_transform = T.Sequential(\n    T.SentencePieceTokenizer(xlmr_spm_model_path),\n    T.VocabTransform([load_state_dict_from_url](https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url \"torch.hub.load_state_dict_from_url\")(xlmr_vocab_path)),\n    T.Truncate(max_seq_len - 2),\n    T.AddToken(token=bos_idx, begin=True),\n    T.AddToken(token=eos_idx, begin=False),\n)\n\nfrom torch.utils.data import [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") \n```", "```py\ntext_transform = XLMR_BASE_ENCODER.transform() \n```", "```py\nfrom torchtext.datasets import SST2\n\nbatch_size = 16\n\ntrain_datapipe = SST2(split=\"train\")\ndev_datapipe = SST2(split=\"dev\")\n\n# Transform the raw dataset using non-batched API (i.e apply transformation line by line)\ndef apply_transform(x):\n    return text_transform(x[0]), x[1]\n\ntrain_datapipe = train_datapipe.map(apply_transform)\ntrain_datapipe = train_datapipe.batch(batch_size)\ntrain_datapipe = train_datapipe.rows2columnar([\"token_ids\", \"target\"])\ntrain_dataloader = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")(train_datapipe, batch_size=None)\n\ndev_datapipe = dev_datapipe.map(apply_transform)\ndev_datapipe = dev_datapipe.batch(batch_size)\ndev_datapipe = dev_datapipe.rows2columnar([\"token_ids\", \"target\"])\ndev_dataloader = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")(dev_datapipe, batch_size=None) \n```", "```py\ndef batch_transform(x):\n    return {\"token_ids\": text_transform(x[\"text\"]), \"target\": x[\"label\"]}\n\ntrain_datapipe = train_datapipe.batch(batch_size).rows2columnar([\"text\", \"label\"])\ntrain_datapipe = train_datapipe.map(lambda x: batch_transform)\ndev_datapipe = dev_datapipe.batch(batch_size).rows2columnar([\"text\", \"label\"])\ndev_datapipe = dev_datapipe.map(lambda x: batch_transform) \n```", "```py\nnum_classes = 2\ninput_dim = 768\n\nfrom torchtext.models import RobertaClassificationHead, XLMR_BASE_ENCODER\n\nclassifier_head = RobertaClassificationHead(num_classes=num_classes, input_dim=input_dim)\nmodel = XLMR_BASE_ENCODER.get_model(head=classifier_head)\nmodel.to(DEVICE) \n```", "```py\nimport torchtext.functional as F\nfrom torch.optim import [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW \"torch.optim.AdamW\")\n\nlearning_rate = 1e-5\noptim = [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW \"torch.optim.AdamW\")(model.parameters(), lr=learning_rate)\ncriteria = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\n\ndef train_step(input, target):\n    output = model(input)\n    loss = criteria(output, target)\n    optim.zero_grad()\n    loss.backward()\n    optim.step()\n\ndef eval_step(input, target):\n    output = model(input)\n    loss = criteria(output, target).item()\n    return float(loss), (output.argmax(1) == target).type(torch.float).sum().item()\n\ndef evaluate():\n    model.eval()\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n    counter = 0\n    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n        for batch in dev_dataloader:\n            input = F.to_tensor(batch[\"token_ids\"], padding_value=padding_idx).to(DEVICE)\n            target = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")(batch[\"target\"]).to(DEVICE)\n            loss, predictions = eval_step(input, target)\n            total_loss += loss\n            correct_predictions += predictions\n            total_predictions += len(target)\n            counter += 1\n\n    return total_loss / counter, correct_predictions / total_predictions \n```", "```py\nnum_epochs = 1\n\nfor e in range(num_epochs):\n    for batch in train_dataloader:\n        input = F.to_tensor(batch[\"token_ids\"], padding_value=padding_idx).to(DEVICE)\n        target = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")(batch[\"target\"]).to(DEVICE)\n        train_step(input, target)\n\n    loss, accuracy = evaluate()\n    print(\"Epoch = [{}], loss = [{}], accuracy = [{}]\".format(e, loss, accuracy)) \n```", "```py\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|5.07M/5.07M [00:00<00:00, 40.8MB/s]\nDownloading: \"https://download.pytorch.org/models/text/xlmr.vocab.pt\" to /root/.cache/torch/hub/checkpoints/xlmr.vocab.pt\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|4.85M/4.85M [00:00<00:00, 16.8MB/s]\nDownloading: \"https://download.pytorch.org/models/text/xlmr.base.encoder.pt\" to /root/.cache/torch/hub/checkpoints/xlmr.base.encoder.pt\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|1.03G/1.03G [00:26<00:00, 47.1MB/s]\nEpoch = [0], loss = [0.2629831412637776], accuracy = [0.9105504587155964] \n```"]