- en: TorchVision Object Detection Finetuning Tutorial
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-intermediate-torchvision-tutorial-py) to download
    the full example code
  prefs: []
  type: TYPE_NORMAL
- en: For this tutorial, we will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870)
    model on the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/).
    It contains 170 images with 345 instances of pedestrians, and we will use it to
    illustrate how to use the new features in torchvision in order to train an object
    detection and instance segmentation model on a custom dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial works only with torchvision version >=0.16 or nightly. If you’re
    using torchvision<=0.15, please follow [this tutorial instead](https://github.com/pytorch/tutorials/blob/d686b662932a380a58b7683425faa00c06bcf502/intermediate_source/torchvision_tutorial.rst).
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reference scripts for training object detection, instance segmentation and
    person keypoint detection allows for easily supporting adding new custom datasets.
    The dataset should inherit from the standard [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset
    "(in PyTorch v2.2)") class, and implement `__len__` and `__getitem__`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only specificity that we require is that the dataset `__getitem__` should
    return a tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: 'image: [`torchvision.tv_tensors.Image`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Image.html#torchvision.tv_tensors.Image
    "(in Torchvision v0.17)") of shape `[3, H, W]`, a pure tensor, or a PIL Image
    of size `(H, W)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'target: a dict containing the following fields'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boxes`, [`torchvision.tv_tensors.BoundingBoxes`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.BoundingBoxes.html#torchvision.tv_tensors.BoundingBoxes
    "(in Torchvision v0.17)") of shape `[N, 4]`: the coordinates of the `N` bounding
    boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels`, integer [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor
    "(in PyTorch v2.2)") of shape `[N]`: the label for each bounding box. `0` represents
    always the background class.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_id`, int: an image identifier. It should be unique between all the images
    in the dataset, and is used during evaluation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`area`, float [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor
    "(in PyTorch v2.2)") of shape `[N]`: the area of the bounding box. This is used
    during evaluation with the COCO metric, to separate the metric scores between
    small, medium and large boxes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iscrowd`, uint8 [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor
    "(in PyTorch v2.2)") of shape `[N]`: instances with `iscrowd=True` will be ignored
    during evaluation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '(optionally) `masks`, [`torchvision.tv_tensors.Mask`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Mask.html#torchvision.tv_tensors.Mask
    "(in Torchvision v0.17)") of shape `[N, H, W]`: the segmentation masks for each
    one of the objects'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If your dataset is compliant with above requirements then it will work for both
    training and evaluation codes from the reference script. Evaluation code will
    use scripts from `pycocotools` which can be installed with `pip install pycocotools`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For Windows, please install `pycocotools` from [gautamchitnis](https://github.com/gautamchitnis/cocoapi)
    with command
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI`'
  prefs: []
  type: TYPE_NORMAL
- en: One note on the `labels`. The model considers class `0` as background. If your
    dataset does not contain the background class, you should not have `0` in your
    `labels`. For example, assuming you have just two classes, *cat* and *dog*, you
    can define `1` (not `0`) to represent *cats* and `2` to represent *dogs*. So,
    for instance, if one of the images has both classes, your `labels` tensor should
    look like `[1, 2]`.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, if you want to use aspect ratio grouping during training (so that
    each batch only contains images with similar aspect ratios), then it is recommended
    to also implement a `get_height_and_width` method, which returns the height and
    the width of the image. If this method is not provided, we query all elements
    of the dataset via `__getitem__` , which loads the image in memory and is slower
    than if a custom method is provided.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a custom dataset for PennFudan
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s write a dataset for the PennFudan dataset. First, let’s download the
    dataset and extract the [zip file](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the following folder structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here is one example of a pair of images and segmentation masks
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Image, Mask](../Images/af4053119e5bd4687c7021a5d001f282.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So each image has a corresponding segmentation mask, where each color correspond
    to a different instance. Let’s write a [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset
    "(in PyTorch v2.2)") class for this dataset. In the code below, we are wrapping
    images, bounding boxes and masks into [`torchvision.tv_tensors.TVTensor`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.TVTensor.html#torchvision.tv_tensors.TVTensor
    "(in Torchvision v0.17)") classes so that we will be able to apply torchvision
    built-in transformations ([new Transforms API](https://pytorch.org/vision/stable/transforms.html))
    for the given object detection and segmentation task. Namely, image tensors will
    be wrapped by [`torchvision.tv_tensors.Image`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Image.html#torchvision.tv_tensors.Image
    "(in Torchvision v0.17)"), bounding boxes into [`torchvision.tv_tensors.BoundingBoxes`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.BoundingBoxes.html#torchvision.tv_tensors.BoundingBoxes
    "(in Torchvision v0.17)") and masks into [`torchvision.tv_tensors.Mask`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Mask.html#torchvision.tv_tensors.Mask
    "(in Torchvision v0.17)"). As [`torchvision.tv_tensors.TVTensor`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.TVTensor.html#torchvision.tv_tensors.TVTensor
    "(in Torchvision v0.17)") are [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor
    "(in PyTorch v2.2)") subclasses, wrapped objects are also tensors and inherit
    the plain [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor
    "(in PyTorch v2.2)") API. For more information about torchvision `tv_tensors`
    see [this documentation](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-are-tvtensors).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: That’s all for the dataset. Now let’s define a model that can perform predictions
    on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Defining your model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we will be using [Mask R-CNN](https://arxiv.org/abs/1703.06870),
    which is based on top of [Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster
    R-CNN is a model that predicts both bounding boxes and class scores for potential
    objects in the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![../_static/img/tv_tutorial/tv_image03.png](../Images/611c2725bdfb89e258da9a99fca53433.png)'
  prefs: []
  type: TYPE_IMG
- en: Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation
    masks for each instance.
  prefs: []
  type: TYPE_NORMAL
- en: '![../_static/img/tv_tutorial/tv_image04.png](../Images/afd408b97567c661cc8cb8a80c7c777c.png)'
  prefs: []
  type: TYPE_IMG
- en: There are two common situations where one might want to modify one of the available
    models in TorchVision Model Zoo. The first is when we want to start from a pre-trained
    model, and just finetune the last layer. The other is when we want to replace
    the backbone of the model with a different one (for faster predictions, for example).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go see how we would do one or another in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 1 - Finetuning from a pretrained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s suppose that you want to start from a model pre-trained on COCO and want
    to finetune it for your particular classes. Here is a possible way of doing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 2 - Modifying the model to add a different backbone
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Object detection and instance segmentation model for PennFudan Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our case, we want to finetune from a pre-trained model, given that our dataset
    is very small, so we will be following approach number 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we want to also compute the instance segmentation masks, so we will be
    using Mask R-CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: That’s it, this will make `model` be ready to be trained and evaluated on your
    custom dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In `references/detection/`, we have a number of helper functions to simplify
    training and evaluating detection models. Here, we will use `references/detection/engine.py`
    and `references/detection/utils.py`. Just download everything under `references/detection`
    to your folder and use them here. On Linux if you have `wget`, you can download
    them using below commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Since v0.15.0 torchvision provides [new Transforms API](https://pytorch.org/vision/stable/transforms.html)
    to easily write data augmentation pipelines for Object Detection and Segmentation
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write some helper functions for data augmentation / transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Testing `forward()` method (Optional)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before iterating over the dataset, it’s good to see what the model expects during
    training and inference time on sample data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now write the main function which performs the training and the validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: So after one epoch of training, we obtain a COCO-style mAP > 50, and a mask
    mAP of 65.
  prefs: []
  type: TYPE_NORMAL
- en: But what do the predictions look like? Let’s take one image in the dataset and
    verify
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![torchvision tutorial](../Images/ece431a9b916de8c06f03c6efa4b7cc4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The results look good!
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, you have learned how to create your own training pipeline
    for object detection models on a custom dataset. For that, you wrote a [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset
    "(in PyTorch v2.2)") class that returns the images and the ground truth boxes
    and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO
    train2017 in order to perform transfer learning on this new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For a more complete example, which includes multi-machine / multi-GPU training,
    check `references/detection/train.py`, which is present in the torchvision repository.
  prefs: []
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 2 minutes 27.747 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: torchvision_tutorial.py`](../_downloads/7590258df9f28b5ae0994c3b5b035edf/torchvision_tutorial.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: torchvision_tutorial.ipynb`](../_downloads/4a542c9f39bedbfe7de5061767181d36/torchvision_tutorial.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
