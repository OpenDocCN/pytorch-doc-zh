# 广播语义

> 原文：[https://pytorch.org/docs/stable/notes/broadcasting.html](https://pytorch.org/docs/stable/notes/broadcasting.html)

许多PyTorch操作支持NumPy的广播语义。有关详细信息，请参阅[https://numpy.org/doc/stable/user/basics.broadcasting.html](https://numpy.org/doc/stable/user/basics.broadcasting.html)。

简而言之，如果PyTorch操作支持广播，则其张量参数可以自动扩展为相等大小（而不会复制数据）。

## 一般语义

如果满足以下规则，则两个张量是“可广播的”：

+   每个张量至少有一个维度。

+   在迭代维度大小时，从尾部维度开始，维度大小必须要么相等，要么其中一个为1，要么其中一个不存在。

例如：

```py
>>> x=torch.empty(5,7,3)
>>> y=torch.empty(5,7,3)
# same shapes are always broadcastable (i.e. the above rules always hold)

>>> x=torch.empty((0,))
>>> y=torch.empty(2,2)
# x and y are not broadcastable, because x does not have at least 1 dimension

# can line up trailing dimensions
>>> x=torch.empty(5,3,4,1)
>>> y=torch.empty(  3,1,1)
# x and y are broadcastable.
# 1st trailing dimension: both have size 1
# 2nd trailing dimension: y has size 1
# 3rd trailing dimension: x size == y size
# 4th trailing dimension: y dimension doesn't exist

# but:
>>> x=torch.empty(5,2,4,1)
>>> y=torch.empty(  3,1,1)
# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3 
```

如果两个张量`x`，`y`是“可广播的”，则结果张量大小计算如下：

+   如果`x`和`y`的维度数不相等，则在较少维度的张量的维度前添加1，使它们长度相等。

+   然后，对于每个维度大小，结果维度大小是沿该维度的`x`和`y`的大小的最大值。

例如：

```py
# can line up trailing dimensions to make reading easier
>>> x=torch.empty(5,1,4,1)
>>> y=torch.empty(  3,1,1)
>>> (x+y).size()
torch.Size([5, 3, 4, 1])

# but not necessary:
>>> x=torch.empty(1)
>>> y=torch.empty(3,1,7)
>>> (x+y).size()
torch.Size([3, 1, 7])

>>> x=torch.empty(5,2,4,1)
>>> y=torch.empty(3,1,1)
>>> (x+y).size()
RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1 
```

## 就地语义

一个复杂之处在于就地操作不允许就地张量由于广播而改变形状。

例如：

```py
>>> x=torch.empty(5,3,4,1)
>>> y=torch.empty(3,1,1)
>>> (x.add_(y)).size()
torch.Size([5, 3, 4, 1])

# but:
>>> x=torch.empty(1,3,1)
>>> y=torch.empty(3,1,7)
>>> (x.add_(y)).size()
RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2. 
```

## 向后兼容性

PyTorch的先前版本允许某些逐点函数在具有不同形状的张量上执行，只要每个张量中的元素数量相等即可。然后，逐点操作将通过将每个张量视为1维来执行。PyTorch现在支持广播，而“1维”逐点行为被视为已弃用，并且在张量不可广播但具有相同数量的元素的情况下会生成Python警告。

请注意，广播的引入可能会导致向后不兼容的更改，即两个张量的形状不相同，但可以广播并且具有相同数量的元素的情况。例如：

```py
>>> torch.add(torch.ones(4,1), torch.randn(4)) 
```

以前会产生一个大小为torch.Size([4,1])的张量，但现在会产生一个大小为torch.Size([4,4])的张量。为了帮助识别代码中可能存在的由广播引入的向后不兼容性，您可以将torch.utils.backcompat.broadcast_warning.enabled设置为True，在这种情况下会生成一个Python警告。

例如：

```py
>>> torch.utils.backcompat.broadcast_warning.enabled=True
>>> torch.add(torch.ones(4,1), torch.ones(4))
__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.
Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional. 
```
