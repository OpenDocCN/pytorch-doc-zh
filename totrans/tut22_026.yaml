- en: What is torch.nn really?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torch.nn到底是什么？
- en: 原文：[https://pytorch.org/tutorials/beginner/nn_tutorial.html](https://pytorch.org/tutorials/beginner/nn_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/beginner/nn_tutorial.html](https://pytorch.org/tutorials/beginner/nn_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-beginner-nn-tutorial-py) to download the full
    example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-beginner-nn-tutorial-py)下载完整示例代码
- en: '**Authors:** Jeremy Howard, [fast.ai](https://www.fast.ai). Thanks to Rachel
    Thomas and Francisco Ingham.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：** Jeremy Howard，[fast.ai](https://www.fast.ai)。感谢Rachel Thomas和Francisco
    Ingham。'
- en: We recommend running this tutorial as a notebook, not a script. To download
    the notebook (`.ipynb`) file, click the link at the top of the page.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议将此教程作为笔记本运行，而不是脚本。要下载笔记本（`.ipynb`）文件，请点击页面顶部的链接。
- en: PyTorch provides the elegantly designed modules and classes [torch.nn](https://pytorch.org/docs/stable/nn.html)
    , [torch.optim](https://pytorch.org/docs/stable/optim.html) , [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)
    , and [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)
    to help you create and train neural networks. In order to fully utilize their
    power and customize them for your problem, you need to really understand exactly
    what they’re doing. To develop this understanding, we will first train basic neural
    net on the MNIST data set without using any features from these models; we will
    initially only use the most basic PyTorch tensor functionality. Then, we will
    incrementally add one feature from `torch.nn`, `torch.optim`, `Dataset`, or `DataLoader`
    at a time, showing exactly what each piece does, and how it works to make the
    code either more concise, or more flexible.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了优雅设计的模块和类[torch.nn](https://pytorch.org/docs/stable/nn.html)、[torch.optim](https://pytorch.org/docs/stable/optim.html)、[Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)和[DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)来帮助您创建和训练神经网络。为了充分利用它们的功能并为您的问题定制它们，您需要真正了解它们在做什么。为了培养这种理解，我们将首先在MNIST数据集上训练基本的神经网络，而不使用这些模型的任何特性；最初我们只使用最基本的PyTorch张量功能。然后，我们将逐步添加一个来自`torch.nn`、`torch.optim`、`Dataset`或`DataLoader`的特性，展示每个部分的确切作用，以及它如何使代码更简洁或更灵活。
- en: '**This tutorial assumes you already have PyTorch installed, and are familiar
    with the basics of tensor operations.** (If you’re familiar with Numpy array operations,
    you’ll find the PyTorch tensor operations used here nearly identical).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**本教程假定您已经安装了PyTorch，并熟悉张量操作的基础知识。**（如果您熟悉Numpy数组操作，您会发现这里使用的PyTorch张量操作几乎相同）。'
- en: MNIST data setup
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST数据设置
- en: We will use the classic [MNIST](http://deeplearning.net/data/mnist/) dataset,
    which consists of black-and-white images of hand-drawn digits (between 0 and 9).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用经典的[MNIST](http://deeplearning.net/data/mnist/)数据集，其中包含手绘数字（介于0和9之间）的黑白图像。
- en: We will use [pathlib](https://docs.python.org/3/library/pathlib.html) for dealing
    with paths (part of the Python 3 standard library), and will download the dataset
    using [requests](http://docs.python-requests.org/en/master/). We will only import
    modules when we use them, so you can see exactly what’s being used at each point.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[pathlib](https://docs.python.org/3/library/pathlib.html)处理路径（Python 3标准库的一部分），并将使用[requests](http://docs.python-requests.org/en/master/)下载数据集。我们只在使用时导入模块，这样您可以清楚地看到每个时刻使用的内容。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This dataset is in numpy array format, and has been stored using pickle, a python-specific
    format for serializing data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是以numpy数组格式存储的，并且使用pickle进行存储，pickle是一种Python特定的序列化数据的格式。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Each image is 28 x 28, and is being stored as a flattened row of length 784
    (=28x28). Let’s take a look at one; we need to reshape it to 2d first.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像是28 x 28，被存储为长度为784（=28x28）的扁平化行。让我们看一个；我们需要先将其重塑为2D。
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![nn tutorial](../Images/7c783def0bbe536f41ed172041b7e89e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![nn教程](../Images/7c783def0bbe536f41ed172041b7e89e.png)'
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PyTorch uses `torch.tensor`, rather than numpy arrays, so we need to convert
    our data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch使用`torch.tensor`，而不是numpy数组，所以我们需要转换我们的数据。
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Neural net from scratch (without `torch.nn`)
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始构建神经网络（不使用`torch.nn`）
- en: Let’s first create a model using nothing but PyTorch tensor operations. We’re
    assuming you’re already familiar with the basics of neural networks. (If you’re
    not, you can learn them at [course.fast.ai](https://course.fast.ai)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用纯粹的PyTorch张量操作创建一个模型。我们假设您已经熟悉神经网络的基础知识。（如果您不熟悉，您可以在[course.fast.ai](https://course.fast.ai)学习）。
- en: 'PyTorch provides methods to create random or zero-filled tensors, which we
    will use to create our weights and bias for a simple linear model. These are just
    regular tensors, with one very special addition: we tell PyTorch that they require
    a gradient. This causes PyTorch to record all of the operations done on the tensor,
    so that it can calculate the gradient during back-propagation *automatically*!'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了创建随机或零填充张量的方法，我们将使用它们来创建简单线性模型的权重和偏置。这些只是常规张量，但有一个非常特殊的附加功能：我们告诉PyTorch它们需要梯度。这会导致PyTorch记录在张量上执行的所有操作，以便在反向传播期间*自动*计算梯度！
- en: For the weights, we set `requires_grad` **after** the initialization, since
    we don’t want that step included in the gradient. (Note that a trailing `_` in
    PyTorch signifies that the operation is performed in-place.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于权重，我们在初始化之后设置`requires_grad`，因为我们不希望该步骤包含在梯度中。（请注意，PyTorch中的下划线`_`表示该操作是原地执行的。）
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We are initializing the weights here with [Xavier initialisation](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
    (by multiplying with `1/sqrt(n)`).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用[Xavier初始化](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)来初始化权重（通过乘以`1/sqrt(n)`）。
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Thanks to PyTorch’s ability to calculate gradients automatically, we can use
    any standard Python function (or callable object) as a model! So let’s just write
    a plain matrix multiplication and broadcasted addition to create a simple linear
    model. We also need an activation function, so we’ll write log_softmax and use
    it. Remember: although PyTorch provides lots of prewritten loss functions, activation
    functions, and so forth, you can easily write your own using plain python. PyTorch
    will even create fast GPU or vectorized CPU code for your function automatically.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PyTorch能够自动计算梯度，我们可以使用任何标准的Python函数（或可调用对象）作为模型！因此，让我们只需编写一个简单的矩阵乘法和广播加法来创建一个简单的线性模型。我们还需要一个激活函数，所以我们将编写log_softmax并使用它。请记住：尽管PyTorch提供了许多预先编写的损失函数、激活函数等，但您可以轻松使用纯Python编写自己的函数。PyTorch甚至会自动为您的函数创建快速的GPU或矢量化CPU代码。
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the above, the `@` stands for the matrix multiplication operation. We will
    call our function on one batch of data (in this case, 64 images). This is one
    *forward pass*. Note that our predictions won’t be any better than random at this
    stage, since we start with random weights.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面，`@`代表矩阵乘法运算。我们将在一批数据（在本例中为64张图像）上调用我们的函数。这是一个*前向传递*。请注意，在这个阶段我们的预测不会比随机更好，因为我们从随机权重开始。
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you see, the `preds` tensor contains not only the tensor values, but also
    a gradient function. We’ll use this later to do backprop.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，`preds`张量不仅包含张量值，还包含一个梯度函数。我们稍后将使用这个函数进行反向传播。
- en: 'Let’s implement negative log-likelihood to use as the loss function (again,
    we can just use standard Python):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现负对数似然作为损失函数（同样，我们可以直接使用标准的Python）：
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s check our loss with our random model, so we can see if we improve after
    a backprop pass later.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们的随机模型的损失，这样我们就可以看到在后续的反向传播过程中是否有改善。
- en: '[PRE11]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s also implement a function to calculate the accuracy of our model. For
    each prediction, if the index with the largest value matches the target value,
    then the prediction was correct.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还实现一个函数来计算模型的准确性。对于每个预测，如果具有最大值的索引与目标值匹配，则预测是正确的。
- en: '[PRE13]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s check the accuracy of our random model, so we can see if our accuracy
    improves as our loss improves.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们的随机模型的准确性，这样我们就可以看到随着损失的改善，我们的准确性是否也在提高。
- en: '[PRE14]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now run a training loop. For each iteration, we will:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行训练循环。对于每次迭代，我们将：
- en: select a mini-batch of data (of size `bs`)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个大小为`bs`的数据小批量
- en: use the model to make predictions
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型进行预测
- en: calculate the loss
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失
- en: '`loss.backward()` updates the gradients of the model, in this case, `weights`
    and `bias`.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss.backward()`更新模型的梯度，在这种情况下是`weights`和`bias`。'
- en: We now use these gradients to update the weights and bias. We do this within
    the `torch.no_grad()` context manager, because we do not want these actions to
    be recorded for our next calculation of the gradient. You can read more about
    how PyTorch’s Autograd records operations [here](https://pytorch.org/docs/stable/notes/autograd.html).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用这些梯度来更新权重和偏置。我们在`torch.no_grad()`上下文管理器中执行此操作，因为我们不希望这些操作被记录下来用于下一次计算梯度。您可以在这里阅读更多关于PyTorch的Autograd如何记录操作的信息。
- en: We then set the gradients to zero, so that we are ready for the next loop. Otherwise,
    our gradients would record a running tally of all the operations that had happened
    (i.e. `loss.backward()` *adds* the gradients to whatever is already stored, rather
    than replacing them).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将梯度设置为零，这样我们就准备好进行下一次循环。否则，我们的梯度会记录所有已发生的操作的累计总数（即`loss.backward()` *添加*梯度到已经存储的内容，而不是替换它们）。
- en: Tip
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: You can use the standard python debugger to step through PyTorch code, allowing
    you to check the various variable values at each step. Uncomment `set_trace()`
    below to try it out.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用标准的Python调试器逐步执行PyTorch代码，从而可以在每个步骤检查各种变量的值。取消下面的`set_trace()`注释以尝试。
- en: '[PRE16]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'That’s it: we’ve created and trained a minimal neural network (in this case,
    a logistic regression, since we have no hidden layers) entirely from scratch!'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样：我们已经从头开始创建和训练了一个最小的神经网络（在这种情况下，是一个逻辑回归，因为我们没有隐藏层）！
- en: Let’s check the loss and accuracy and compare those to what we got earlier.
    We expect that the loss will have decreased and accuracy to have increased, and
    they have.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查损失和准确性，并将其与之前的结果进行比较。我们预计损失会减少，准确性会增加，事实也是如此。
- en: '[PRE17]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Using `torch.nn.functional`
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`torch.nn.functional`
- en: 'We will now refactor our code, so that it does the same thing as before, only
    we’ll start taking advantage of PyTorch’s `nn` classes to make it more concise
    and flexible. At each step from here, we should be making our code one or more
    of: shorter, more understandable, and/or more flexible.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将重构我们的代码，使其与以前的代码执行相同的操作，只是我们将开始利用PyTorch的`nn`类使其更简洁和灵活。从这里开始的每一步，我们应该使我们的代码更短、更易理解和/或更灵活。
- en: The first and easiest step is to make our code shorter by replacing our hand-written
    activation and loss functions with those from `torch.nn.functional` (which is
    generally imported into the namespace `F` by convention). This module contains
    all the functions in the `torch.nn` library (whereas other parts of the library
    contain classes). As well as a wide range of loss and activation functions, you’ll
    also find here some convenient functions for creating neural nets, such as pooling
    functions. (There are also functions for doing convolutions, linear layers, etc,
    but as we’ll see, these are usually better handled using other parts of the library.)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个最简单的步骤是通过用`torch.nn.functional`中的函数替换我们手写的激活和损失函数来缩短我们的代码（通常按照惯例，这个模块被导入到`F`命名空间中）。该模块包含`torch.nn`库中的所有函数（而库的其他部分包含类）。除了各种损失和激活函数外，您还会在这里找到一些方便创建神经网络的函数，如池化函数。（还有用于执行卷积、线性层等操作的函数，但正如我们将看到的，这些通常更好地使用库的其他部分处理。）
- en: If you’re using negative log likelihood loss and log softmax activation, then
    Pytorch provides a single function `F.cross_entropy` that combines the two. So
    we can even remove the activation function from our model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用负对数似然损失和对数softmax激活函数，那么Pytorch提供了一个结合了两者的单个函数`F.cross_entropy`。因此，我们甚至可以从我们的模型中删除激活函数。
- en: '[PRE19]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that we no longer call `log_softmax` in the `model` function. Let’s confirm
    that our loss and accuracy are the same as before:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在`model`函数中我们不再调用`log_softmax`。让我们确认我们的损失和准确率与以前相同：
- en: '[PRE20]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Refactor using `nn.Module`
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`nn.Module`进行重构
- en: Next up, we’ll use `nn.Module` and `nn.Parameter`, for a clearer and more concise
    training loop. We subclass `nn.Module` (which itself is a class and able to keep
    track of state). In this case, we want to create a class that holds our weights,
    bias, and method for the forward step. `nn.Module` has a number of attributes
    and methods (such as `.parameters()` and `.zero_grad()`) which we will be using.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`nn.Module`和`nn.Parameter`，以获得更清晰、更简洁的训练循环。我们子类化`nn.Module`（它本身是一个类，能够跟踪状态）。在这种情况下，我们想要创建一个类来保存我们的权重、偏置和前向步骤的方法。`nn.Module`有许多属性和方法（如`.parameters()`和`.zero_grad()`），我们将使用它们。
- en: Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`nn.Module` (uppercase M) is a PyTorch specific concept, and is a class we’ll
    be using a lot. `nn.Module` is not to be confused with the Python concept of a
    (lowercase `m`) [module](https://docs.python.org/3/tutorial/modules.html), which
    is a file of Python code that can be imported.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Module`（大写M）是PyTorch特定的概念，是一个我们将经常使用的类。`nn.Module`不应与Python概念中的（小写m）[模块](https://docs.python.org/3/tutorial/modules.html)混淆，后者是一个可以被导入的Python代码文件。'
- en: '[PRE22]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since we’re now using an object instead of just using a function, we first
    have to instantiate our model:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在使用的是对象而不仅仅是函数，我们首先要实例化我们的模型：
- en: '[PRE23]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now we can calculate the loss in the same way as before. Note that `nn.Module`
    objects are used as if they are functions (i.e they are *callable*), but behind
    the scenes Pytorch will call our `forward` method automatically.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以像以前一样计算损失。请注意，`nn.Module`对象被用作函数（即它们是*可调用的*），但在幕后，Pytorch会自动调用我们的`forward`方法。
- en: '[PRE24]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Previously for our training loop we had to update the values for each parameter
    by name, and manually zero out the grads for each parameter separately, like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以前在我们的训练循环中，我们必须按名称更新每个参数的值，并手动将每个参数的梯度归零，就像这样：
- en: '[PRE26]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we can take advantage of model.parameters() and model.zero_grad() (which
    are both defined by PyTorch for `nn.Module`) to make those steps more concise
    and less prone to the error of forgetting some of our parameters, particularly
    if we had a more complicated model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以利用model.parameters()和model.zero_grad()（这两者都由PyTorch为`nn.Module`定义）来使这些步骤更简洁，更不容易出错，特别是如果我们有一个更复杂的模型：
- en: '[PRE27]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We’ll wrap our little training loop in a `fit` function so we can run it again
    later.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把我们的小训练循环封装在一个`fit`函数中，以便以后可以再次运行它。
- en: '[PRE28]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s double-check that our loss has gone down:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次确认我们的损失是否下降了：
- en: '[PRE29]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Refactor using `nn.Linear`
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`nn.Linear`进行重构
- en: We continue to refactor our code. Instead of manually defining and initializing
    `self.weights` and `self.bias`, and calculating `xb  @ self.weights + self.bias`,
    we will instead use the Pytorch class [nn.Linear](https://pytorch.org/docs/stable/nn.html#linear-layers)
    for a linear layer, which does all that for us. Pytorch has many types of predefined
    layers that can greatly simplify our code, and often makes it faster too.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续重构我们的代码。我们将使用Pytorch类[nn.Linear](https://pytorch.org/docs/stable/nn.html#linear-layers)来代替手动定义和初始化`self.weights`和`self.bias`，以及计算`xb
    @ self.weights + self.bias`，这个线性层会为我们完成所有这些工作。Pytorch有许多预定义的层类型，可以极大简化我们的代码，而且通常也会使其更快。
- en: '[PRE31]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We instantiate our model and calculate the loss in the same way as before:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化我们的模型，并像以前一样计算损失：
- en: '[PRE32]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We are still able to use our same `fit` method as before.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然可以像以前一样使用我们的`fit`方法。
- en: '[PRE34]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Refactor using `torch.optim`
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`torch.optim`进行重构
- en: Pytorch also has a package with various optimization algorithms, `torch.optim`.
    We can use the `step` method from our optimizer to take a forward step, instead
    of manually updating each parameter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch还有一个包含各种优化算法的包，`torch.optim`。我们可以使用优化器的`step`方法来进行前向步骤，而不是手动更新每个参数。
- en: 'This will let us replace our previous manually coded optimization step:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们能够替换以前手动编码的优化步骤：
- en: '[PRE36]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'and instead use just:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 而是使用：
- en: '[PRE37]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: (`optim.zero_grad()` resets the gradient to 0 and we need to call it before
    computing the gradient for the next minibatch.)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: （`optim.zero_grad()`将梯度重置为0，我们需要在计算下一个小批量的梯度之前调用它。）
- en: '[PRE38]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We’ll define a little function to create our model and optimizer so we can reuse
    it in the future.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个小函数来创建我们的模型和优化器，以便将来可以重复使用它。
- en: '[PRE39]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Refactor using Dataset
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Dataset进行重构
- en: PyTorch has an abstract Dataset class. A Dataset can be anything that has a
    `__len__` function (called by Python’s standard `len` function) and a `__getitem__`
    function as a way of indexing into it. [This tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)
    walks through a nice example of creating a custom `FacialLandmarkDataset` class
    as a subclass of `Dataset`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch有一个抽象的Dataset类。一个Dataset可以是任何具有`__len__`函数（由Python的标准`len`函数调用）和`__getitem__`函数作为索引方式的东西。[这个教程](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)演示了创建一个自定义的`FacialLandmarkDataset`类作为`Dataset`子类的一个很好的例子。
- en: PyTorch’s [TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset)
    is a Dataset wrapping tensors. By defining a length and way of indexing, this
    also gives us a way to iterate, index, and slice along the first dimension of
    a tensor. This will make it easier to access both the independent and dependent
    variables in the same line as we train.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的[TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset)是一个包装张量的数据集。通过定义长度和索引方式，这也为我们提供了一种在张量的第一维度上进行迭代、索引和切片的方式。这将使我们更容易在训练时同时访问独立变量和因变量。
- en: '[PRE41]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Both `x_train` and `y_train` can be combined in a single `TensorDataset`, which
    will be easier to iterate over and slice.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`x_train`和`y_train`可以合并在一个`TensorDataset`中，这样在迭代和切片时会更容易。'
- en: '[PRE42]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Previously, we had to iterate through minibatches of `x` and `y` values separately:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们必须分别迭代`x`和`y`值的小批次：
- en: '[PRE43]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, we can do these two steps together:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以一起完成这两个步骤：
- en: '[PRE44]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Refactor using `DataLoader`
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`DataLoader`进行重构
- en: 'PyTorch’s `DataLoader` is responsible for managing batches. You can create
    a `DataLoader` from any `Dataset`. `DataLoader` makes it easier to iterate over
    batches. Rather than having to use `train_ds[i*bs : i*bs+bs]`, the `DataLoader`
    gives us each minibatch automatically.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 'PyTorch的`DataLoader`负责管理批次。您可以从任何`Dataset`创建一个`DataLoader`。`DataLoader`使得批次迭代更容易。不需要使用`train_ds[i*bs
    : i*bs+bs]`，`DataLoader`会自动给我们每个小批次。'
- en: '[PRE47]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Previously, our loop iterated over batches `(xb, yb)` like this:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们的循环像这样迭代批次`(xb, yb)`：
- en: '[PRE48]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, our loop is much cleaner, as `(xb, yb)` are loaded automatically from
    the data loader:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的循环更加清晰，因为`(xb, yb)`会自动从数据加载器中加载：
- en: '[PRE49]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Thanks to PyTorch’s `nn.Module`, `nn.Parameter`, `Dataset`, and `DataLoader`,
    our training loop is now dramatically smaller and easier to understand. Let’s
    now try to add the basic features necessary to create effective models in practice.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PyTorch的`nn.Module`、`nn.Parameter`、`Dataset`和`DataLoader`，我们的训练循环现在变得更小更容易理解。现在让我们尝试添加创建有效模型所需的基本特性。
- en: Add validation
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加验证
- en: In section 1, we were just trying to get a reasonable training loop set up for
    use on our training data. In reality, you **always** should also have a [validation
    set](https://www.fast.ai/2017/11/13/validation-sets/), in order to identify if
    you are overfitting.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1节中，我们只是试图建立一个合理的训练循环来用于训练数据。实际上，您**总是**应该有一个[验证集](https://www.fast.ai/2017/11/13/validation-sets/)，以便确定是否过拟合。
- en: Shuffling the training data is [important](https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks)
    to prevent correlation between batches and overfitting. On the other hand, the
    validation loss will be identical whether we shuffle the validation set or not.
    Since shuffling takes extra time, it makes no sense to shuffle the validation
    data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对训练数据进行洗牌是[重要的](https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks)，以防止批次之间的相关性和过拟合。另一方面，验证损失无论我们是否对验证集进行洗牌都是相同的。由于洗牌需要额外的时间，对验证数据进行洗牌是没有意义的。
- en: We’ll use a batch size for the validation set that is twice as large as that
    for the training set. This is because the validation set does not need backpropagation
    and thus takes less memory (it doesn’t need to store the gradients). We take advantage
    of this to use a larger batch size and compute the loss more quickly.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用验证集的批量大小是训练集的两倍。这是因为验证集不需要反向传播，因此占用的内存较少（不需要存储梯度）。我们利用这一点使用更大的批量大小更快地计算损失。
- en: '[PRE52]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We will calculate and print the validation loss at the end of each epoch.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在每个epoch结束时计算并打印验证损失。
- en: (Note that we always call `model.train()` before training, and `model.eval()`
    before inference, because these are used by layers such as `nn.BatchNorm2d` and
    `nn.Dropout` to ensure appropriate behavior for these different phases.)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: （请注意，在训练之前我们总是调用`model.train()`，在推理之前我们总是调用`model.eval()`，因为这些被`nn.BatchNorm2d`和`nn.Dropout`等层使用以确保这些不同阶段的适当行为。）
- en: '[PRE53]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Create fit() and get_data()
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建fit()和get_data()
- en: We’ll now do a little refactoring of our own. Since we go through a similar
    process twice of calculating the loss for both the training set and the validation
    set, let’s make that into its own function, `loss_batch`, which computes the loss
    for one batch.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将进行一些重构。由于我们两次都要计算训练集和验证集的损失，让我们将其制作成自己的函数`loss_batch`，用于计算一个batch的损失。
- en: We pass an optimizer in for the training set, and use it to perform backprop.
    For the validation set, we don’t pass an optimizer, so the method doesn’t perform
    backprop.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为训练集传入一个优化器，并用它执行反向传播。对于验证集，我们不传入优化器，所以该方法不执行反向传播。
- en: '[PRE55]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '`fit` runs the necessary operations to train our model and compute the training
    and validation losses for each epoch.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`运行必要的操作来训练我们的模型，并计算每个epoch的训练和验证损失。'
- en: '[PRE56]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '`get_data` returns dataloaders for the training and validation sets.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_data`返回训练集和验证集的数据加载器。'
- en: '[PRE57]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, our whole process of obtaining the data loaders and fitting the model
    can be run in 3 lines of code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们整个获取数据加载器和拟合模型的过程可以在3行代码中运行：
- en: '[PRE58]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: You can use these basic 3 lines of code to train a wide variety of models. Let’s
    see if we can use them to train a convolutional neural network (CNN)!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用这基本的3行代码来训练各种模型。让我们看看是否可以使用它们来训练卷积神经网络（CNN）！
- en: Switch to CNN
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 切换到CNN
- en: We are now going to build our neural network with three convolutional layers.
    Because none of the functions in the previous section assume anything about the
    model form, we’ll be able to use them to train a CNN without any modification.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将建立一个具有三个卷积层的神经网络。由于上一节中的函数都不假设模型形式，我们将能够使用它们来训练CNN而无需任何修改。
- en: We will use PyTorch’s predefined [Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d)
    class as our convolutional layer. We define a CNN with 3 convolutional layers.
    Each convolution is followed by a ReLU. At the end, we perform an average pooling.
    (Note that `view` is PyTorch’s version of Numpy’s `reshape`)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用PyTorch预定义的[Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d)类作为我们的卷积层。我们定义了一个具有3个卷积层的CNN。每个卷积后面跟着一个ReLU。最后，我们执行平均池化。（注意`view`是PyTorch版本的Numpy的`reshape`）
- en: '[PRE60]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[Momentum](https://cs231n.github.io/neural-networks-3/#sgd) is a variation
    on stochastic gradient descent that takes previous updates into account as well
    and generally leads to faster training.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[动量](https://cs231n.github.io/neural-networks-3/#sgd)是随机梯度下降的一种变体，它考虑了先前的更新，通常导致更快的训练。'
- en: '[PRE61]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Using `nn.Sequential`
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`nn.Sequential`
- en: '`torch.nn` has another handy class we can use to simplify our code: [Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential)
    . A `Sequential` object runs each of the modules contained within it, in a sequential
    manner. This is a simpler way of writing our neural network.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn`还有另一个方便的类，我们可以用它来简化我们的代码：[Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential)。`Sequential`对象按顺序运行其中包含的每个模块。这是编写我们的神经网络的一种更简单的方法。'
- en: To take advantage of this, we need to be able to easily define a **custom layer**
    from a given function. For instance, PyTorch doesn’t have a view layer, and we
    need to create one for our network. `Lambda` will create a layer that we can then
    use when defining a network with `Sequential`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用这一点，我们需要能够轻松地从给定函数定义一个**自定义层**。例如，PyTorch没有一个视图层，我们需要为我们的网络创建一个。`Lambda`将创建一个层，然后我们可以在使用`Sequential`定义网络时使用它。
- en: '[PRE63]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The model created with `Sequential` is simple:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Sequential`创建的模型很简单：
- en: '[PRE64]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Wrapping `DataLoader`
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包装`DataLoader`
- en: 'Our CNN is fairly concise, but it only works with MNIST, because:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的CNN相当简洁，但只适用于MNIST，因为：
- en: It assumes the input is a 28*28 long vector
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它假设输入是一个28*28的长向量
- en: It assumes that the final CNN grid size is 4*4 (since that’s the average pooling
    kernel size we used)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它假设最终的CNN网格大小为4*4（因为这是我们使用的平均池化核大小）
- en: 'Let’s get rid of these two assumptions, so our model works with any 2d single
    channel image. First, we can remove the initial Lambda layer by moving the data
    preprocessing into a generator:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们摆脱这两个假设，这样我们的模型就可以处理任何2D单通道图像。首先，我们可以通过将数据预处理移入生成器来删除初始的Lambda层：
- en: '[PRE66]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Next, we can replace `nn.AvgPool2d` with `nn.AdaptiveAvgPool2d`, which allows
    us to define the size of the *output* tensor we want, rather than the *input*
    tensor we have. As a result, our model will work with any size input.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以用`nn.AdaptiveAvgPool2d`替换`nn.AvgPool2d`，这样我们可以定义我们想要的*输出*张量的大小，而不是我们拥有的*输入*张量。因此，我们的模型将适用于任何大小的输入。
- en: '[PRE67]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Let’s try it out:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试：
- en: '[PRE68]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Using your GPU
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用您的GPU
- en: 'If you’re lucky enough to have access to a CUDA-capable GPU (you can rent one
    for about $0.50/hour from most cloud providers) you can use it to speed up your
    code. First check that your GPU is working in Pytorch:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有幸拥有支持CUDA的GPU（您可以从大多数云提供商租用一个约0.50美元/小时），您可以使用它加速您的代码。首先检查您的GPU在Pytorch中是否正常工作：
- en: '[PRE70]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'And then create a device object for it:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后为其创建一个设备对象：
- en: '[PRE72]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Let’s update `preprocess` to move batches to the GPU:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新`preprocess`以将批次移动到GPU：
- en: '[PRE73]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Finally, we can move our model to the GPU.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将我们的模型移动到GPU上。
- en: '[PRE74]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'You should find it runs faster now:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该发现现在运行得更快了：
- en: '[PRE75]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Closing thoughts
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束思考
- en: We now have a general data pipeline and training loop which you can use for
    training many types of models using Pytorch. To see how simple training a model
    can now be, take a look at the [mnist_sample notebook](https://github.com/fastai/fastai_dev/blob/master/dev_nb/mnist_sample.ipynb).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个通用的数据管道和训练循环，您可以使用它来训练许多类型的模型使用Pytorch。要了解现在训练模型有多简单，请查看[mnist_sample笔记本](https://github.com/fastai/fastai_dev/blob/master/dev_nb/mnist_sample.ipynb)。
- en: Of course, there are many things you’ll want to add, such as data augmentation,
    hyperparameter tuning, monitoring training, transfer learning, and so forth. These
    features are available in the fastai library, which has been developed using the
    same design approach shown in this tutorial, providing a natural next step for
    practitioners looking to take their models further.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可能想要添加许多其他功能，例如数据增强、超参数调整、监视训练、迁移学习等。这些功能在fastai库中可用，该库是使用本教程中展示的相同设计方法开发的，为希望进一步发展其模型的从业者提供了一个自然的下一步。
- en: 'We promised at the start of this tutorial we’d explain through example each
    of `torch.nn`, `torch.optim`, `Dataset`, and `DataLoader`. So let’s summarize
    what we’ve seen:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本教程开始时承诺通过示例解释每个`torch.nn`，`torch.optim`，`Dataset`和`DataLoader`。所以让我们总结一下我们所看到的内容：
- en: '`torch.nn`:'
  id: totrans-190
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn`：'
- en: ''
  id: totrans-191
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-192
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Module`: creates a callable which behaves like a function, but can also contain
    state(such as neural net layer weights). It knows what `Parameter` (s) it contains
    and can zero all their gradients, loop through them for weight updates, etc.'
  id: totrans-193
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Module`：创建一个可调用的函数，但也可以包含状态（例如神经网络层权重）。它知道它包含的`Parameter`(s)，可以将它们的梯度清零，循环遍历它们进行权重更新等。'
- en: ''
  id: totrans-194
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-195
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Parameter`: a wrapper for a tensor that tells a `Module` that it has weights
    that need updating during backprop. Only tensors with the requires_grad attribute
    set are updated'
  id: totrans-196
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Parameter`：一个张量的包装器，告诉`Module`它有需要在反向传播过程中更新的权重。只有设置了requires_grad属性的张量才会被更新'
- en: ''
  id: totrans-197
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-198
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`functional`: a module(usually imported into the `F` namespace by convention)
    which contains activation functions, loss functions, etc, as well as non-stateful
    versions of layers such as convolutional and linear layers.'
  id: totrans-199
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`functional`：一个模块（通常按照惯例导入到`F`命名空间中），其中包含激活函数、损失函数等，以及卷积和线性层的非状态版本。'
- en: ''
  id: totrans-200
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-201
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`torch.optim`: Contains optimizers such as `SGD`, which update the weights
    of `Parameter` during the backward step'
  id: totrans-202
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.optim`：包含诸如`SGD`之类的优化器，在反向步骤中更新`Parameter`的权重'
- en: ''
  id: totrans-203
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-204
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Dataset`: An abstract interface of objects with a `__len__` and a `__getitem__`,
    including classes provided with Pytorch such as `TensorDataset`'
  id: totrans-205
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset`：具有`__len__`和`__getitem__`的对象的抽象接口，包括Pytorch提供的类，如`TensorDataset`'
- en: ''
  id: totrans-206
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-207
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '`DataLoader`: Takes any `Dataset` and creates an iterator which returns batches
    of data.'
  id: totrans-208
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataLoader`：接受任何`Dataset`并创建一个返回数据批次的迭代器。'
- en: '**Total running time of the script:** ( 0 minutes 36.765 seconds)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（0分钟36.765秒）'
- en: '[`Download Python source code: nn_tutorial.py`](../_downloads/f16255c783f9e487235b8eff6c8792b9/nn_tutorial.py)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：nn_tutorial.py`](../_downloads/f16255c783f9e487235b8eff6c8792b9/nn_tutorial.py)'
- en: '[`Download Jupyter notebook: nn_tutorial.ipynb`](../_downloads/d9398fce39ca80dc4bb8b8ea55b575a8/nn_tutorial.ipynb)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：nn_tutorial.ipynb`](../_downloads/d9398fce39ca80dc4bb8b8ea55b575a8/nn_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery生成的画廊](https://sphinx-gallery.github.io)'
