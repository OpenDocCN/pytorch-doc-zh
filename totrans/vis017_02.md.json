["```py\n# Image Classification\nimport torch\nfrom torchvision.transforms import v2\n\nH, W = 32, 32\nimg = torch.randint(0, 256, size=(3, H, W), dtype=torch.uint8)\n\ntransforms = v2.Compose([\n    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n    v2.RandomHorizontalFlip(p=0.5),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\nimg = transforms(img) \n```", "```py\n# Detection (re-using imports and transforms from above)\nfrom torchvision import tv_tensors\n\nimg = torch.randint(0, 256, size=(3, H, W), dtype=torch.uint8)\nboxes = torch.randint(0, H // 2, size=(3, 4))\nboxes[:, 2:] += boxes[:, :2]\nboxes = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(H, W))\n\n# The same transforms can be used!\nimg, boxes = transforms(img, boxes)\n# And you can pass arbitrary input structures\noutput_dict = transforms({\"image\": img, \"boxes\": boxes}) \n```", "```py\nfrom torchvision.transforms import v2\ntransforms = v2.Compose([\n    v2.ToImage(),  # Convert to tensor, only needed if you had a PIL image\n    v2.ToDtype(torch.uint8, scale=True),  # optional, most input are already uint8 at this point\n    # ...\n    v2.RandomResizedCrop(size=(224, 224), antialias=True),  # Or Resize(antialias=True)\n    # ...\n    v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n]) \n```", "```py\ntransforms = torch.nn.Sequential(\n    CenterCrop(10),\n    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n)\nscripted_transforms = torch.jit.script(transforms) \n```"]