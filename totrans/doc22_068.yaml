- en: torch.nn.init
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/nn.init.html](https://pytorch.org/docs/stable/nn.init.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: All the functions in this module are intended to be used to initialize neural
    network parameters, so they all run in [`torch.no_grad()`](generated/torch.no_grad.html#torch.no_grad
    "torch.no_grad") mode and will not be taken into account by autograd.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Return the recommended gain value for the given nonlinearity function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The values are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| nonlinearity | gain |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Linear / Identity | $1$1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Conv{1,2,3}D | $1$1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sigmoid | $1$1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tanh | $\frac{5}{3}$35​ |'
  prefs: []
  type: TYPE_TB
- en: '| ReLU | $\sqrt{2}$2​ |'
  prefs: []
  type: TYPE_TB
- en: '| Leaky Relu | $\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}$1+negative_slope22​​
    |'
  prefs: []
  type: TYPE_TB
- en: '| SELU | $\frac{3}{4}$43​ |'
  prefs: []
  type: TYPE_TB
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: In order to implement [Self-Normalizing Neural Networks](https://papers.nips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html)
    , you should use `nonlinearity='linear'` instead of `nonlinearity='selu'`. This
    gives the initial weights a variance of `1 / N`, which is necessary to induce
    a stable fixed point in the forward pass. In contrast, the default gain for `SELU`
    sacrifices the normalization effect for more stable gradient flow in rectangular
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**nonlinearity** – the non-linear function (nn.functional name)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**param** – optional parameter for the non-linear function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with values drawn from the uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: <math><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mi>a</mi><mo
    separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\mathcal{U}(a, b)</annotation></semantics></math>U(a,b).
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – an n-dimensional
    torch.Tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**a** ([*float*](https://docs.python.org/3/library/functions.html#float "(in
    Python v3.12)")) – the lower bound of the uniform distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b** ([*float*](https://docs.python.org/3/library/functions.html#float "(in
    Python v3.12)")) – the upper bound of the uniform distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator
    "torch._C.Generator")*]*) – the torch Generator to sample from (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with values drawn from the normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: <math><semantics><mrow><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mtext>mean</mtext><mo
    separator="true">,</mo><msup><mtext>std</mtext><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\mathcal{N}(\text{mean}, \text{std}^2)</annotation></semantics></math>N(mean,std2).
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – an n-dimensional
    torch.Tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mean** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")) – the mean of the normal distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std** ([*float*](https://docs.python.org/3/library/functions.html#float "(in
    Python v3.12)")) – the standard deviation of the normal distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator
    "torch._C.Generator")*]*) – the torch Generator to sample from (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with the value $\text{val}$val.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – an n-dimensional
    torch.Tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**val** ([*float*](https://docs.python.org/3/library/functions.html#float "(in
    Python v3.12)")) – the value to fill the tensor with'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with the scalar value 1.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – an n-dimensional
    torch.Tensor'
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with the scalar value 0.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – an n-dimensional
    torch.Tensor'
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Fill the 2-dimensional input Tensor with the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Preserves the identity of the inputs in Linear layers, where as many inputs
    are preserved as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** – a 2-dimensional torch.Tensor'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Fill the {3, 4, 5}-dimensional input Tensor with the Dirac delta function.
  prefs: []
  type: TYPE_NORMAL
- en: Preserves the identity of the inputs in Convolutional layers, where as many
    input channels are preserved as possible. In case of groups>1, each group of channels
    preserves identity
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** – a {3, 4, 5}-dimensional torch.Tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**groups** ([*int*](https://docs.python.org/3/library/functions.html#int "(in
    Python v3.12)")*,* *optional*) – number of groups in the conv layer (default:
    1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with values using a Xavier uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The method is described in Understanding the difficulty of training deep feedforward
    neural networks - Glorot, X. & Bengio, Y. (2010). The resulting tensor will have
    values sampled from <math><semantics><mrow><mi mathvariant="script">U</mi><mo
    stretchy="false">(</mo><mo>−</mo><mi>a</mi><mo separator="true">,</mo><mi>a</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-a,
    a)</annotation></semantics></math>U(−a,a) where
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><semantics><mrow><mi>a</mi><mo>=</mo><mtext>gain</mtext><mo>×</mo><msqrt><mfrac><mn>6</mn><mrow><mtext>fan_in</mtext><mo>+</mo><mtext>fan_out</mtext></mrow></mfrac></msqrt></mrow><annotation
    encoding="application/x-tex">a = \text{gain} \times \sqrt{\frac{6}{\text{fan\_in}
    + \text{fan\_out}}}</annotation></semantics></math> a=gain×fan_in+fan_out6​​
  prefs: []
  type: TYPE_NORMAL
- en: Also known as Glorot initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – an n-dimensional
    torch.Tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gain** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")) – an optional scaling factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator
    "torch._C.Generator")*]*) – the torch Generator to sample from (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with values using a Xavier normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The method is described in Understanding the difficulty of training deep feedforward
    neural networks - Glorot, X. & Bengio, Y. (2010). The resulting tensor will have
    values sampled from <math><semantics><mrow><mi mathvariant="script">N</mi><mo
    stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><msup><mtext>std</mtext><mn>2</mn></msup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0,
    \text{std}^2)</annotation></semantics></math>N(0,std2) where
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><semantics><mrow><mtext>std</mtext><mo>=</mo><mtext>gain</mtext><mo>×</mo><msqrt><mfrac><mn>2</mn><mrow><mtext>fan_in</mtext><mo>+</mo><mtext>fan_out</mtext></mrow></mfrac></msqrt></mrow><annotation
    encoding="application/x-tex">\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\_in}
    + \text{fan\_out}}}</annotation></semantics></math> std=gain×fan_in+fan_out2​​
  prefs: []
  type: TYPE_NORMAL
- en: Also known as Glorot initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – an n-dimensional
    torch.Tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gain** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")) – an optional scaling factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator
    "torch._C.Generator")*]*) – the torch Generator to sample from (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with values using a Kaiming uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method is described in Delving deep into rectifiers: Surpassing human-level
    performance on ImageNet classification - He, K. et al. (2015). The resulting tensor
    will have values sampled from <math><semantics><mrow><mi mathvariant="script">U</mi><mo
    stretchy="false">(</mo><mo>−</mo><mtext>bound</mtext><mo separator="true">,</mo><mtext>bound</mtext><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\text{bound},
    \text{bound})</annotation></semantics></math>U(−bound,bound) where'
  prefs: []
  type: TYPE_NORMAL
- en: $\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\_mode}}}$
    bound=gain×fan_mode3​​
  prefs: []
  type: TYPE_NORMAL
- en: Also known as He initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – an n-dimensional
    torch.Tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**a** ([*float*](https://docs.python.org/3/library/functions.html#float "(in
    Python v3.12)")) – the negative slope of the rectifier used after this layer (only
    used with `''leaky_relu''`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mode** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in
    Python v3.12)")) – either `''fan_in''` (default) or `''fan_out''`. Choosing `''fan_in''`
    preserves the magnitude of the variance of the weights in the forward pass. Choosing
    `''fan_out''` preserves the magnitudes in the backwards pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nonlinearity** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – the non-linear function (nn.functional name), recommended
    to use only with `''relu''` or `''leaky_relu''` (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator
    "torch._C.Generator")*]*) – the torch Generator to sample from (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with values using a Kaiming normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method is described in Delving deep into rectifiers: Surpassing human-level
    performance on ImageNet classification - He, K. et al. (2015). The resulting tensor
    will have values sampled from <math><semantics><mrow><mi mathvariant="script">N</mi><mo
    stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><msup><mtext>std</mtext><mn>2</mn></msup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0,
    \text{std}^2)</annotation></semantics></math>N(0,std2) where'
  prefs: []
  type: TYPE_NORMAL
- en: $\text{std} = \frac{\text{gain}}{\sqrt{\text{fan\_mode}}}$
    std=fan_mode​gain​
  prefs: []
  type: TYPE_NORMAL
- en: Also known as He initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – an n-dimensional
    torch.Tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**a** ([*float*](https://docs.python.org/3/library/functions.html#float "(in
    Python v3.12)")) – the negative slope of the rectifier used after this layer (only
    used with `''leaky_relu''`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mode** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in
    Python v3.12)")) – either `''fan_in''` (default) or `''fan_out''`. Choosing `''fan_in''`
    preserves the magnitude of the variance of the weights in the forward pass. Choosing
    `''fan_out''` preserves the magnitudes in the backwards pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nonlinearity** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – the non-linear function (nn.functional name), recommended
    to use only with `''relu''` or `''leaky_relu''` (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator
    "torch._C.Generator")*]*) – the torch Generator to sample from (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with values drawn from a truncated normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The values are effectively drawn from the normal distribution <math><semantics><mrow><mi
    mathvariant="script">N</mi><mo stretchy="false">(</mo><mtext>mean</mtext><mo separator="true">,</mo><msup><mtext>std</mtext><mn>2</mn></msup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(\text{mean},
    \text{std}^2)</annotation></semantics></math>N(mean,std2) with values outside
    <math><semantics><mrow><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo
    stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[a, b]</annotation></semantics></math>[a,b]
    redrawn until they are within the bounds. The method used for generating the random
    values works best when $a \leq \text{mean} \leq b$a≤mean≤b.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – an n-dimensional
    torch.Tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mean** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")) – the mean of the normal distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std** ([*float*](https://docs.python.org/3/library/functions.html#float "(in
    Python v3.12)")) – the standard deviation of the normal distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**a** ([*float*](https://docs.python.org/3/library/functions.html#float "(in
    Python v3.12)")) – the minimum cutoff value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b** ([*float*](https://docs.python.org/3/library/functions.html#float "(in
    Python v3.12)")) – the maximum cutoff value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator
    "torch._C.Generator")*]*) – the torch Generator to sample from (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Tensor*](tensors.html#torch.Tensor "torch.Tensor")'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Fill the input Tensor with a (semi) orthogonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Described in Exact solutions to the nonlinear dynamics of learning in deep linear
    neural networks - Saxe, A. et al. (2013). The input tensor must have at least
    2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions
    are flattened.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** – an n-dimensional torch.Tensor, where $n \geq 2$n≥2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gain** – optional scaling factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator
    "torch._C.Generator")*]*) – the torch Generator to sample from (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Fill the 2D input Tensor as a sparse matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The non-zero elements will be drawn from the normal distribution <math><semantics><mrow><mi
    mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>0.01</mn><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0,
    0.01)</annotation></semantics></math>N(0,0.01), as described in Deep learning
    via Hessian-free optimization - Martens, J. (2010).
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor** – an n-dimensional torch.Tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sparsity** – The fraction of elements in each column to be set to zero'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std** – the standard deviation of the normal distribution used to generate
    the non-zero values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Generator*](generated/torch.Generator.html#torch.Generator
    "torch._C.Generator")*]*) – the torch Generator to sample from (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
