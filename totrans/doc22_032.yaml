- en: torch.nn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torch.nn
- en: 原文：[https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)
- en: 'These are the basic building blocks for graphs:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是图的基本构建块：
- en: torch.nn
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: torch.nn
- en: '[Containers](#containers)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[容器](#containers)'
- en: '[Convolution Layers](#convolution-layers)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[卷积层](#convolution-layers)'
- en: '[Pooling layers](#pooling-layers)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[池化层](#pooling-layers)'
- en: '[Padding Layers](#padding-layers)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[填充层](#padding-layers)'
- en: '[Non-linear Activations (weighted sum, nonlinearity)](#non-linear-activations-weighted-sum-nonlinearity)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[非线性激活（加权和，非线性）](#non-linear-activations-weighted-sum-nonlinearity)'
- en: '[Non-linear Activations (other)](#non-linear-activations-other)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[非线性激活（其他）](#non-linear-activations-other)'
- en: '[Normalization Layers](#normalization-layers)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[归一化层](#normalization-layers)'
- en: '[Recurrent Layers](#recurrent-layers)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[循环层](#recurrent-layers)'
- en: '[Transformer Layers](#transformer-layers)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[变换器层](#transformer-layers)'
- en: '[Linear Layers](#linear-layers)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性层](#linear-layers)'
- en: '[Dropout Layers](#dropout-layers)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[丢弃层](#dropout-layers)'
- en: '[Sparse Layers](#sparse-layers)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[稀疏层](#sparse-layers)'
- en: '[Distance Functions](#distance-functions)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[距离函数](#distance-functions)'
- en: '[Loss Functions](#loss-functions)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[损失函数](#loss-functions)'
- en: '[Vision Layers](#vision-layers)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[视觉层](#vision-layers)'
- en: '[Shuffle Layers](#shuffle-layers)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[洗牌层](#shuffle-layers)'
- en: '[DataParallel Layers (multi-GPU, distributed)](#module-torch.nn.parallel)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DataParallel 层（多GPU，分布式）](#module-torch.nn.parallel)'
- en: '[Utilities](#module-torch.nn.utils)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实用工具](#module-torch.nn.utils)'
- en: '[Quantized Functions](#quantized-functions)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[量化函数](#quantized-functions)'
- en: '[Lazy Modules Initialization](#lazy-modules-initialization)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[延迟模块初始化](#lazy-modules-initialization)'
- en: '| [`Parameter`](generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter
    "torch.nn.parameter.Parameter") | A kind of Tensor that is to be considered a
    module parameter. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [`Parameter`](generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter
    "torch.nn.parameter.Parameter") | 一种被视为模块参数的张量。 |'
- en: '| [`UninitializedParameter`](generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter
    "torch.nn.parameter.UninitializedParameter") | A parameter that is not initialized.
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| [`UninitializedParameter`](generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter
    "torch.nn.parameter.UninitializedParameter") | 一个未初始化的参数。 |'
- en: '| [`UninitializedBuffer`](generated/torch.nn.parameter.UninitializedBuffer.html#torch.nn.parameter.UninitializedBuffer
    "torch.nn.parameter.UninitializedBuffer") | A buffer that is not initialized.
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| [`UninitializedBuffer`](generated/torch.nn.parameter.UninitializedBuffer.html#torch.nn.parameter.UninitializedBuffer
    "torch.nn.parameter.UninitializedBuffer") | 一个未初始化的缓冲区。 |'
- en: '[Containers](#id1)'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[容器](#id1)'
- en: '| [`Module`](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")
    | Base class for all neural network modules. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| [`Module`](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")
    | 所有神经网络模块的基类。 |'
- en: '| [`Sequential`](generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential")
    | A sequential container. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| [`Sequential`](generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential")
    | 一个顺序容器。 |'
- en: '| [`ModuleList`](generated/torch.nn.ModuleList.html#torch.nn.ModuleList "torch.nn.ModuleList")
    | Holds submodules in a list. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| [`ModuleList`](generated/torch.nn.ModuleList.html#torch.nn.ModuleList "torch.nn.ModuleList")
    | 在列表中保存子模块。 |'
- en: '| [`ModuleDict`](generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict "torch.nn.ModuleDict")
    | Holds submodules in a dictionary. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| [`ModuleDict`](generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict "torch.nn.ModuleDict")
    | 在字典中保存子模块。 |'
- en: '| [`ParameterList`](generated/torch.nn.ParameterList.html#torch.nn.ParameterList
    "torch.nn.ParameterList") | Holds parameters in a list. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| [`ParameterList`](generated/torch.nn.ParameterList.html#torch.nn.ParameterList
    "torch.nn.ParameterList") | 在列表中保存参数。 |'
- en: '| [`ParameterDict`](generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict
    "torch.nn.ParameterDict") | Holds parameters in a dictionary. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| [`ParameterDict`](generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict
    "torch.nn.ParameterDict") | 在字典中保存参数。 |'
- en: Global Hooks For Module
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 模块的全局钩子
- en: '| [`register_module_forward_pre_hook`](generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook
    "torch.nn.modules.module.register_module_forward_pre_hook") | Register a forward
    pre-hook common to all modules. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| [`register_module_forward_pre_hook`](generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook
    "torch.nn.modules.module.register_module_forward_pre_hook") | 注册一个对所有模块通用的前向预钩子。
    |'
- en: '| [`register_module_forward_hook`](generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook
    "torch.nn.modules.module.register_module_forward_hook") | Register a global forward
    hook for all the modules. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| [`register_module_forward_hook`](generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook
    "torch.nn.modules.module.register_module_forward_hook") | 为所有模块注册一个全局前向钩子。 |'
- en: '| [`register_module_backward_hook`](generated/torch.nn.modules.module.register_module_backward_hook.html#torch.nn.modules.module.register_module_backward_hook
    "torch.nn.modules.module.register_module_backward_hook") | Register a backward
    hook common to all the modules. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| [`register_module_backward_hook`](generated/torch.nn.modules.module.register_module_backward_hook.html#torch.nn.modules.module.register_module_backward_hook
    "torch.nn.modules.module.register_module_backward_hook") | 注册一个对所有模块通用的反向钩子。 |'
- en: '| [`register_module_full_backward_pre_hook`](generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook
    "torch.nn.modules.module.register_module_full_backward_pre_hook") | Register a
    backward pre-hook common to all the modules. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| [`register_module_full_backward_pre_hook`](generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook
    "torch.nn.modules.module.register_module_full_backward_pre_hook") | 注册一个对所有模块通用的反向预钩子。
    |'
- en: '| [`register_module_full_backward_hook`](generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook
    "torch.nn.modules.module.register_module_full_backward_hook") | Register a backward
    hook common to all the modules. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| [`register_module_full_backward_hook`](generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook
    "torch.nn.modules.module.register_module_full_backward_hook") | 注册一个对所有模块通用的反向钩子。
    |'
- en: '| [`register_module_buffer_registration_hook`](generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch.nn.modules.module.register_module_buffer_registration_hook
    "torch.nn.modules.module.register_module_buffer_registration_hook") | Register
    a buffer registration hook common to all modules. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| [`register_module_buffer_registration_hook`](generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch.nn.modules.module.register_module_buffer_registration_hook
    "torch.nn.modules.module.register_module_buffer_registration_hook") | 注册一个适用于所有模块的缓冲区注册钩子。
    |'
- en: '| [`register_module_module_registration_hook`](generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch.nn.modules.module.register_module_module_registration_hook
    "torch.nn.modules.module.register_module_module_registration_hook") | Register
    a module registration hook common to all modules. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| [`register_module_module_registration_hook`](generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch.nn.modules.module.register_module_module_registration_hook
    "torch.nn.modules.module.register_module_module_registration_hook") | 注册一个适用于所有模块的模块注册钩子。
    |'
- en: '| [`register_module_parameter_registration_hook`](generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch.nn.modules.module.register_module_parameter_registration_hook
    "torch.nn.modules.module.register_module_parameter_registration_hook") | Register
    a parameter registration hook common to all modules. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| [`register_module_parameter_registration_hook`](generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch.nn.modules.module.register_module_parameter_registration_hook
    "torch.nn.modules.module.register_module_parameter_registration_hook") | 注册一个适用于所有模块的参数注册钩子。
    |'
- en: '[Convolution Layers](#id1)'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[卷积层](#id1)'
- en: '| [`nn.Conv1d`](generated/torch.nn.Conv1d.html#torch.nn.Conv1d "torch.nn.Conv1d")
    | Applies a 1D convolution over an input signal composed of several input planes.
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Conv1d`](generated/torch.nn.Conv1d.html#torch.nn.Conv1d "torch.nn.Conv1d")
    | 对由多个输入平面组成的输入信号应用1D卷积。 |'
- en: '| [`nn.Conv2d`](generated/torch.nn.Conv2d.html#torch.nn.Conv2d "torch.nn.Conv2d")
    | Applies a 2D convolution over an input signal composed of several input planes.
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Conv2d`](generated/torch.nn.Conv2d.html#torch.nn.Conv2d "torch.nn.Conv2d")
    | 对由多个输入平面组成的输入信号应用2D卷积。 |'
- en: '| [`nn.Conv3d`](generated/torch.nn.Conv3d.html#torch.nn.Conv3d "torch.nn.Conv3d")
    | Applies a 3D convolution over an input signal composed of several input planes.
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Conv3d`](generated/torch.nn.Conv3d.html#torch.nn.Conv3d "torch.nn.Conv3d")
    | 对由多个输入平面组成的输入信号应用3D卷积。 |'
- en: '| [`nn.ConvTranspose1d`](generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d
    "torch.nn.ConvTranspose1d") | Applies a 1D transposed convolution operator over
    an input image composed of several input planes. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ConvTranspose1d`](generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d
    "torch.nn.ConvTranspose1d") | 对由多个输入平面组成的输入图像应用1D转置卷积运算符。 |'
- en: '| [`nn.ConvTranspose2d`](generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d
    "torch.nn.ConvTranspose2d") | Applies a 2D transposed convolution operator over
    an input image composed of several input planes. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ConvTranspose2d`](generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d
    "torch.nn.ConvTranspose2d") | 对由多个输入平面组成的输入图像应用2D转置卷积运算符。 |'
- en: '| [`nn.ConvTranspose3d`](generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d
    "torch.nn.ConvTranspose3d") | Applies a 3D transposed convolution operator over
    an input image composed of several input planes. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ConvTranspose3d`](generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d
    "torch.nn.ConvTranspose3d") | 对由多个输入平面组成的输入图像应用3D转置卷积运算符。 |'
- en: '| [`nn.LazyConv1d`](generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d
    "torch.nn.LazyConv1d") | A [`torch.nn.Conv1d`](generated/torch.nn.Conv1d.html#torch.nn.Conv1d
    "torch.nn.Conv1d") module with lazy initialization of the `in_channels` argument.
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyConv1d`](generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d
    "torch.nn.LazyConv1d") | 一个带有`in_channels`参数延迟初始化的[`torch.nn.Conv1d`](generated/torch.nn.Conv1d.html#torch.nn.Conv1d
    "torch.nn.Conv1d")模块。 |'
- en: '| [`nn.LazyConv2d`](generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d
    "torch.nn.LazyConv2d") | A [`torch.nn.Conv2d`](generated/torch.nn.Conv2d.html#torch.nn.Conv2d
    "torch.nn.Conv2d") module with lazy initialization of the `in_channels` argument.
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyConv2d`](generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d
    "torch.nn.LazyConv2d") | 一个带有`in_channels`参数延迟初始化的[`torch.nn.Conv2d`](generated/torch.nn.Conv2d.html#torch.nn.Conv2d
    "torch.nn.Conv2d")模块。 |'
- en: '| [`nn.LazyConv3d`](generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d
    "torch.nn.LazyConv3d") | A [`torch.nn.Conv3d`](generated/torch.nn.Conv3d.html#torch.nn.Conv3d
    "torch.nn.Conv3d") module with lazy initialization of the `in_channels` argument.
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyConv3d`](generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d
    "torch.nn.LazyConv3d") | 一个带有`in_channels`参数延迟初始化的[`torch.nn.Conv3d`](generated/torch.nn.Conv3d.html#torch.nn.Conv3d
    "torch.nn.Conv3d")模块。 |'
- en: '| [`nn.LazyConvTranspose1d`](generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d
    "torch.nn.LazyConvTranspose1d") | A [`torch.nn.ConvTranspose1d`](generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d
    "torch.nn.ConvTranspose1d") module with lazy initialization of the `in_channels`
    argument. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyConvTranspose1d`](generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d
    "torch.nn.LazyConvTranspose1d") | 一个带有`in_channels`参数延迟初始化的[`torch.nn.ConvTranspose1d`](generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d
    "torch.nn.ConvTranspose1d")模块。 |'
- en: '| [`nn.LazyConvTranspose2d`](generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d
    "torch.nn.LazyConvTranspose2d") | A [`torch.nn.ConvTranspose2d`](generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d
    "torch.nn.ConvTranspose2d") module with lazy initialization of the `in_channels`
    argument. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyConvTranspose2d`](generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d
    "torch.nn.LazyConvTranspose2d") | 一个带有`in_channels`参数延迟初始化的[`torch.nn.ConvTranspose2d`](generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d
    "torch.nn.ConvTranspose2d")模块。 |'
- en: '| [`nn.LazyConvTranspose3d`](generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d
    "torch.nn.LazyConvTranspose3d") | A [`torch.nn.ConvTranspose3d`](generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d
    "torch.nn.ConvTranspose3d") module with lazy initialization of the `in_channels`
    argument. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyConvTranspose3d`](generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d
    "torch.nn.LazyConvTranspose3d") | 一个带有`in_channels`参数延迟初始化的[`torch.nn.ConvTranspose3d`](generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d
    "torch.nn.ConvTranspose3d")模块。 |'
- en: '| [`nn.Unfold`](generated/torch.nn.Unfold.html#torch.nn.Unfold "torch.nn.Unfold")
    | Extracts sliding local blocks from a batched input tensor. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Unfold`](generated/torch.nn.Unfold.html#torch.nn.Unfold "torch.nn.Unfold")
    | 从批量输入张量中提取滑动局部块。 |'
- en: '| [`nn.Fold`](generated/torch.nn.Fold.html#torch.nn.Fold "torch.nn.Fold") |
    Combines an array of sliding local blocks into a large containing tensor. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Fold`](generated/torch.nn.Fold.html#torch.nn.Fold "torch.nn.Fold") |
    将一组滑动局部块组合成一个大的包含张量。 |'
- en: '[Pooling layers](#id1)'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[池化层](#id1)'
- en: '| [`nn.MaxPool1d`](generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d "torch.nn.MaxPool1d")
    | Applies a 1D max pooling over an input signal composed of several input planes.
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MaxPool1d`](generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d "torch.nn.MaxPool1d")
    | 对由多个输入平面组成的输入信号应用1D最大池化。 |'
- en: '| [`nn.MaxPool2d`](generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d "torch.nn.MaxPool2d")
    | Applies a 2D max pooling over an input signal composed of several input planes.
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MaxPool2d`](generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d "torch.nn.MaxPool2d")
    | 对由多个输入平面组成的输入信号应用2D最大池化。 |'
- en: '| [`nn.MaxPool3d`](generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d "torch.nn.MaxPool3d")
    | Applies a 3D max pooling over an input signal composed of several input planes.
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MaxPool3d`](generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d "torch.nn.MaxPool3d")
    | 对由多个输入平面组成的输入信号应用3D最大池化。 |'
- en: '| [`nn.MaxUnpool1d`](generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d
    "torch.nn.MaxUnpool1d") | Computes a partial inverse of `MaxPool1d`. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MaxUnpool1d`](generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d
    "torch.nn.MaxUnpool1d") | 计算`MaxPool1d`的部分逆操作。 |'
- en: '| [`nn.MaxUnpool2d`](generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d
    "torch.nn.MaxUnpool2d") | Computes a partial inverse of `MaxPool2d`. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MaxUnpool2d`](generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d
    "torch.nn.MaxUnpool2d") | 计算`MaxPool2d`的部分逆操作。 |'
- en: '| [`nn.MaxUnpool3d`](generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d
    "torch.nn.MaxUnpool3d") | Computes a partial inverse of `MaxPool3d`. |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MaxUnpool3d`](generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d
    "torch.nn.MaxUnpool3d") | 计算`MaxPool3d`的部分逆操作。 |'
- en: '| [`nn.AvgPool1d`](generated/torch.nn.AvgPool1d.html#torch.nn.AvgPool1d "torch.nn.AvgPool1d")
    | Applies a 1D average pooling over an input signal composed of several input
    planes. |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AvgPool1d`](generated/torch.nn.AvgPool1d.html#torch.nn.AvgPool1d "torch.nn.AvgPool1d")
    | 对由多个输入平面组成的输入信号应用1D平均池化。 |'
- en: '| [`nn.AvgPool2d`](generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d "torch.nn.AvgPool2d")
    | Applies a 2D average pooling over an input signal composed of several input
    planes. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AvgPool2d`](generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d "torch.nn.AvgPool2d")
    | 对由多个输入平面组成的输入信号应用2D平均池化。 |'
- en: '| [`nn.AvgPool3d`](generated/torch.nn.AvgPool3d.html#torch.nn.AvgPool3d "torch.nn.AvgPool3d")
    | Applies a 3D average pooling over an input signal composed of several input
    planes. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AvgPool3d`](generated/torch.nn.AvgPool3d.html#torch.nn.AvgPool3d "torch.nn.AvgPool3d")
    | 对由多个输入平面组成的输入信号应用3D平均池化。 |'
- en: '| [`nn.FractionalMaxPool2d`](generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d
    "torch.nn.FractionalMaxPool2d") | Applies a 2D fractional max pooling over an
    input signal composed of several input planes. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.FractionalMaxPool2d`](generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d
    "torch.nn.FractionalMaxPool2d") | 对由多个输入平面组成的输入信号应用2D分数最大池化。 |'
- en: '| [`nn.FractionalMaxPool3d`](generated/torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d
    "torch.nn.FractionalMaxPool3d") | Applies a 3D fractional max pooling over an
    input signal composed of several input planes. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.FractionalMaxPool3d`](generated/torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d
    "torch.nn.FractionalMaxPool3d") | 对由多个输入平面组成的输入信号应用3D分数最大池化。 |'
- en: '| [`nn.LPPool1d`](generated/torch.nn.LPPool1d.html#torch.nn.LPPool1d "torch.nn.LPPool1d")
    | Applies a 1D power-average pooling over an input signal composed of several
    input planes. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LPPool1d`](generated/torch.nn.LPPool1d.html#torch.nn.LPPool1d "torch.nn.LPPool1d")
    | 对由多个输入平面组成的输入信号应用1D幂平均池化。 |'
- en: '| [`nn.LPPool2d`](generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d "torch.nn.LPPool2d")
    | Applies a 2D power-average pooling over an input signal composed of several
    input planes. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LPPool2d`](generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d "torch.nn.LPPool2d")
    | 对由多个输入平面组成的输入信号应用2D幂平均池化。 |'
- en: '| [`nn.AdaptiveMaxPool1d`](generated/torch.nn.AdaptiveMaxPool1d.html#torch.nn.AdaptiveMaxPool1d
    "torch.nn.AdaptiveMaxPool1d") | Applies a 1D adaptive max pooling over an input
    signal composed of several input planes. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AdaptiveMaxPool1d`](generated/torch.nn.AdaptiveMaxPool1d.html#torch.nn.AdaptiveMaxPool1d
    "torch.nn.AdaptiveMaxPool1d") | 对由多个输入平面组成的输入信号应用1D自适应最大池化。 |'
- en: '| [`nn.AdaptiveMaxPool2d`](generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d
    "torch.nn.AdaptiveMaxPool2d") | Applies a 2D adaptive max pooling over an input
    signal composed of several input planes. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AdaptiveMaxPool2d`](generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d
    "torch.nn.AdaptiveMaxPool2d") | 对由多个输入平面组成的输入信号应用2D自适应最大池化。 |'
- en: '| [`nn.AdaptiveMaxPool3d`](generated/torch.nn.AdaptiveMaxPool3d.html#torch.nn.AdaptiveMaxPool3d
    "torch.nn.AdaptiveMaxPool3d") | Applies a 3D adaptive max pooling over an input
    signal composed of several input planes. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AdaptiveMaxPool3d`](generated/torch.nn.AdaptiveMaxPool3d.html#torch.nn.AdaptiveMaxPool3d
    "torch.nn.AdaptiveMaxPool3d") | 对由多个输入平面组成的输入信号应用3D自适应最大池化。 |'
- en: '| [`nn.AdaptiveAvgPool1d`](generated/torch.nn.AdaptiveAvgPool1d.html#torch.nn.AdaptiveAvgPool1d
    "torch.nn.AdaptiveAvgPool1d") | Applies a 1D adaptive average pooling over an
    input signal composed of several input planes. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AdaptiveAvgPool1d`](generated/torch.nn.AdaptiveAvgPool1d.html#torch.nn.AdaptiveAvgPool1d
    "torch.nn.AdaptiveAvgPool1d") | 对由多个输入平面组成的输入信号应用1D自适应平均池化。 |'
- en: '| [`nn.AdaptiveAvgPool2d`](generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d
    "torch.nn.AdaptiveAvgPool2d") | Applies a 2D adaptive average pooling over an
    input signal composed of several input planes. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AdaptiveAvgPool2d`](generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d
    "torch.nn.AdaptiveAvgPool2d") | 对由多个输入平面组成的输入信号应用2D自适应平均池化。 |'
- en: '| [`nn.AdaptiveAvgPool3d`](generated/torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d
    "torch.nn.AdaptiveAvgPool3d") | Applies a 3D adaptive average pooling over an
    input signal composed of several input planes. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AdaptiveAvgPool3d`](generated/torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d
    "torch.nn.AdaptiveAvgPool3d") | 对由多个输入平面组成的输入信号应用3D自适应平均池化。 |'
- en: '[Padding Layers](#id1)'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[填充层](#id1)'
- en: '| [`nn.ReflectionPad1d`](generated/torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d
    "torch.nn.ReflectionPad1d") | Pads the input tensor using the reflection of the
    input boundary. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ReflectionPad1d`](generated/torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d
    "torch.nn.ReflectionPad1d") | 使用输入边界的反射来填充输入张量。 |'
- en: '| [`nn.ReflectionPad2d`](generated/torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d
    "torch.nn.ReflectionPad2d") | Pads the input tensor using the reflection of the
    input boundary. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ReflectionPad2d`](generated/torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d
    "torch.nn.ReflectionPad2d") | 使用输入边界的反射来填充输入张量。 |'
- en: '| [`nn.ReflectionPad3d`](generated/torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d
    "torch.nn.ReflectionPad3d") | Pads the input tensor using the reflection of the
    input boundary. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ReflectionPad3d`](generated/torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d
    "torch.nn.ReflectionPad3d") | 使用输入边界的反射来填充输入张量。 |'
- en: '| [`nn.ReplicationPad1d`](generated/torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d
    "torch.nn.ReplicationPad1d") | Pads the input tensor using replication of the
    input boundary. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ReplicationPad1d`](generated/torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d
    "torch.nn.ReplicationPad1d") | 使用输入边界的复制来填充输入张量。 |'
- en: '| [`nn.ReplicationPad2d`](generated/torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d
    "torch.nn.ReplicationPad2d") | Pads the input tensor using replication of the
    input boundary. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ReplicationPad2d`](generated/torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d
    "torch.nn.ReplicationPad2d") | 使用输入边界的复制来填充输入张量。 |'
- en: '| [`nn.ReplicationPad3d`](generated/torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d
    "torch.nn.ReplicationPad3d") | Pads the input tensor using replication of the
    input boundary. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ReplicationPad3d`](generated/torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d
    "torch.nn.ReplicationPad3d") | 使用输入边界的复制来填充输入张量。 |'
- en: '| [`nn.ZeroPad1d`](generated/torch.nn.ZeroPad1d.html#torch.nn.ZeroPad1d "torch.nn.ZeroPad1d")
    | Pads the input tensor boundaries with zero. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ZeroPad1d`](generated/torch.nn.ZeroPad1d.html#torch.nn.ZeroPad1d "torch.nn.ZeroPad1d")
    | 使用零值填充输入张量的边界。 |'
- en: '| [`nn.ZeroPad2d`](generated/torch.nn.ZeroPad2d.html#torch.nn.ZeroPad2d "torch.nn.ZeroPad2d")
    | Pads the input tensor boundaries with zero. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ZeroPad2d`](generated/torch.nn.ZeroPad2d.html#torch.nn.ZeroPad2d "torch.nn.ZeroPad2d")
    | 使用零值填充输入张量的边界。 |'
- en: '| [`nn.ZeroPad3d`](generated/torch.nn.ZeroPad3d.html#torch.nn.ZeroPad3d "torch.nn.ZeroPad3d")
    | Pads the input tensor boundaries with zero. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ZeroPad3d`](generated/torch.nn.ZeroPad3d.html#torch.nn.ZeroPad3d "torch.nn.ZeroPad3d")
    | 使用零值填充输入张量的边界。 |'
- en: '| [`nn.ConstantPad1d`](generated/torch.nn.ConstantPad1d.html#torch.nn.ConstantPad1d
    "torch.nn.ConstantPad1d") | Pads the input tensor boundaries with a constant value.
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ConstantPad1d`](generated/torch.nn.ConstantPad1d.html#torch.nn.ConstantPad1d
    "torch.nn.ConstantPad1d") | 使用常数值填充输入张量的边界。 |'
- en: '| [`nn.ConstantPad2d`](generated/torch.nn.ConstantPad2d.html#torch.nn.ConstantPad2d
    "torch.nn.ConstantPad2d") | Pads the input tensor boundaries with a constant value.
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ConstantPad2d`](generated/torch.nn.ConstantPad2d.html#torch.nn.ConstantPad2d
    "torch.nn.ConstantPad2d") | 使用常数值填充输入张量的边界。 |'
- en: '| [`nn.ConstantPad3d`](generated/torch.nn.ConstantPad3d.html#torch.nn.ConstantPad3d
    "torch.nn.ConstantPad3d") | Pads the input tensor boundaries with a constant value.
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ConstantPad3d`](generated/torch.nn.ConstantPad3d.html#torch.nn.ConstantPad3d
    "torch.nn.ConstantPad3d") | 使用常数值填充输入张量的边界。 |'
- en: '[Non-linear Activations (weighted sum, nonlinearity)](#id1)[](#non-linear-activations-weighted-sum-nonlinearity
    "Permalink to this heading")'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[非线性激活函数（加权和，非线性）](#id1)[](#non-linear-activations-weighted-sum-nonlinearity
    "跳转到此标题")'
- en: '| [`nn.ELU`](generated/torch.nn.ELU.html#torch.nn.ELU "torch.nn.ELU") | Applies
    the Exponential Linear Unit (ELU) function, element-wise, as described in the
    paper: [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289).
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ELU`](generated/torch.nn.ELU.html#torch.nn.ELU "torch.nn.ELU") | 对每个元素应用指数线性单元（ELU）函数，如论文中所述：[通过指数线性单元（ELUs）实现快速准确的深度网络学习](https://arxiv.org/abs/1511.07289)。
    |'
- en: '| [`nn.Hardshrink`](generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink
    "torch.nn.Hardshrink") | Applies the Hard Shrinkage (Hardshrink) function element-wise.
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Hardshrink`](generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink
    "torch.nn.Hardshrink") | 对每个元素应用硬收缩（Hardshrink）函数。 |'
- en: '| [`nn.Hardsigmoid`](generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid
    "torch.nn.Hardsigmoid") | Applies the Hardsigmoid function element-wise. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Hardsigmoid`](generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid
    "torch.nn.Hardsigmoid") | 对每个元素应用硬Sigmoid函数。 |'
- en: '| [`nn.Hardtanh`](generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh "torch.nn.Hardtanh")
    | Applies the HardTanh function element-wise. |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Hardtanh`](generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh "torch.nn.Hardtanh")
    | 对每个元素应用HardTanh函数。 |'
- en: '| [`nn.Hardswish`](generated/torch.nn.Hardswish.html#torch.nn.Hardswish "torch.nn.Hardswish")
    | Applies the Hardswish function, element-wise, as described in the paper: [Searching
    for MobileNetV3](https://arxiv.org/abs/1905.02244). |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Hardswish`](generated/torch.nn.Hardswish.html#torch.nn.Hardswish "torch.nn.Hardswish")
    | 对每个元素应用Hardswish函数，如论文中所述：[搜索MobileNetV3](https://arxiv.org/abs/1905.02244)。
    |'
- en: '| [`nn.LeakyReLU`](generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU "torch.nn.LeakyReLU")
    | Applies the element-wise function: |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LeakyReLU`](generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU "torch.nn.LeakyReLU")
    | 应用逐元素函数： |'
- en: '| [`nn.LogSigmoid`](generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid
    "torch.nn.LogSigmoid") | Applies the element-wise function: |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LogSigmoid`](generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid
    "torch.nn.LogSigmoid") | 应用逐元素函数： |'
- en: '| [`nn.MultiheadAttention`](generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention
    "torch.nn.MultiheadAttention") | Allows the model to jointly attend to information
    from different representation subspaces as described in the paper: [Attention
    Is All You Need](https://arxiv.org/abs/1706.03762). |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MultiheadAttention`](generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention
    "torch.nn.MultiheadAttention") | 允许模型同时关注来自不同表示子空间的信息，如论文中所述：[注意力机制是你所需要的一切](https://arxiv.org/abs/1706.03762)。
    |'
- en: '| [`nn.PReLU`](generated/torch.nn.PReLU.html#torch.nn.PReLU "torch.nn.PReLU")
    | Applies the element-wise function: |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.PReLU`](generated/torch.nn.PReLU.html#torch.nn.PReLU "torch.nn.PReLU")
    | 应用逐元素函数： |'
- en: '| [`nn.ReLU`](generated/torch.nn.ReLU.html#torch.nn.ReLU "torch.nn.ReLU") |
    Applies the rectified linear unit function element-wise: |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ReLU`](generated/torch.nn.ReLU.html#torch.nn.ReLU "torch.nn.ReLU") |
    逐元素应用修正线性单元函数： |'
- en: '| [`nn.ReLU6`](generated/torch.nn.ReLU6.html#torch.nn.ReLU6 "torch.nn.ReLU6")
    | Applies the element-wise function: |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ReLU6`](generated/torch.nn.ReLU6.html#torch.nn.ReLU6 "torch.nn.ReLU6")
    | 应用逐元素函数： |'
- en: '| [`nn.RReLU`](generated/torch.nn.RReLU.html#torch.nn.RReLU "torch.nn.RReLU")
    | Applies the randomized leaky rectified linear unit function, element-wise, as
    described in the paper: |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.RReLU`](generated/torch.nn.RReLU.html#torch.nn.RReLU "torch.nn.RReLU")
    | 应用随机泄漏修正线性单元函数，逐元素地，如论文中所述： |'
- en: '| [`nn.SELU`](generated/torch.nn.SELU.html#torch.nn.SELU "torch.nn.SELU") |
    Applied element-wise, as: |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.SELU`](generated/torch.nn.SELU.html#torch.nn.SELU "torch.nn.SELU") |
    逐元素应用，如： |'
- en: '| [`nn.CELU`](generated/torch.nn.CELU.html#torch.nn.CELU "torch.nn.CELU") |
    Applies the element-wise function: |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.CELU`](generated/torch.nn.CELU.html#torch.nn.CELU "torch.nn.CELU") |
    应用逐元素函数： |'
- en: '| [`nn.GELU`](generated/torch.nn.GELU.html#torch.nn.GELU "torch.nn.GELU") |
    Applies the Gaussian Error Linear Units function: |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.GELU`](generated/torch.nn.GELU.html#torch.nn.GELU "torch.nn.GELU") |
    应用高斯误差线性单元函数： |'
- en: '| [`nn.Sigmoid`](generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid "torch.nn.Sigmoid")
    | Applies the element-wise function: |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Sigmoid`](generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid "torch.nn.Sigmoid")
    | 应用逐元素函数： |'
- en: '| [`nn.SiLU`](generated/torch.nn.SiLU.html#torch.nn.SiLU "torch.nn.SiLU") |
    Applies the Sigmoid Linear Unit (SiLU) function, element-wise. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.SiLU`](generated/torch.nn.SiLU.html#torch.nn.SiLU "torch.nn.SiLU") |
    应用Sigmoid线性单元（SiLU）函数，逐元素。 |'
- en: '| [`nn.Mish`](generated/torch.nn.Mish.html#torch.nn.Mish "torch.nn.Mish") |
    Applies the Mish function, element-wise. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Mish`](generated/torch.nn.Mish.html#torch.nn.Mish "torch.nn.Mish") |
    应用Mish函数，逐元素。 |'
- en: '| [`nn.Softplus`](generated/torch.nn.Softplus.html#torch.nn.Softplus "torch.nn.Softplus")
    | Applies the Softplus function $\text{Softplus}(x) = \frac{1}{\beta} * \log(1
    + \exp(\beta * x))$Softplus(x)=β1​∗log(1+exp(β∗x)) element-wise. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Softplus`](generated/torch.nn.Softplus.html#torch.nn.Softplus "torch.nn.Softplus")
    | 逐元素应用Softplus函数$\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta *
    x))$。 |'
- en: '| [`nn.Softshrink`](generated/torch.nn.Softshrink.html#torch.nn.Softshrink
    "torch.nn.Softshrink") | Applies the soft shrinkage function elementwise: |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Softshrink`](generated/torch.nn.Softshrink.html#torch.nn.Softshrink
    "torch.nn.Softshrink") | 应用软阈值函数，逐元素： |'
- en: '| [`nn.Softsign`](generated/torch.nn.Softsign.html#torch.nn.Softsign "torch.nn.Softsign")
    | Applies the element-wise function: |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Softsign`](generated/torch.nn.Softsign.html#torch.nn.Softsign "torch.nn.Softsign")
    | 应用逐元素函数： |'
- en: '| [`nn.Tanh`](generated/torch.nn.Tanh.html#torch.nn.Tanh "torch.nn.Tanh") |
    Applies the Hyperbolic Tangent (Tanh) function element-wise. |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Tanh`](generated/torch.nn.Tanh.html#torch.nn.Tanh "torch.nn.Tanh") |
    逐元素应用双曲正切（Tanh）函数。 |'
- en: '| [`nn.Tanhshrink`](generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink
    "torch.nn.Tanhshrink") | Applies the element-wise function: |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Tanhshrink`](generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink
    "torch.nn.Tanhshrink") | 应用逐元素函数： |'
- en: '| [`nn.Threshold`](generated/torch.nn.Threshold.html#torch.nn.Threshold "torch.nn.Threshold")
    | Thresholds each element of the input Tensor. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Threshold`](generated/torch.nn.Threshold.html#torch.nn.Threshold "torch.nn.Threshold")
    | 对输入张量的每个元素进行阈值处理。 |'
- en: '| [`nn.GLU`](generated/torch.nn.GLU.html#torch.nn.GLU "torch.nn.GLU") | Applies
    the gated linear unit function ${GLU}(a, b)= a \otimes \sigma(b)$GLU(a,b)=a⊗σ(b)
    where $a$a is the first half of the input matrices and $b$b is the second half.
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.GLU`](generated/torch.nn.GLU.html#torch.nn.GLU "torch.nn.GLU") | 应用门控线性单元函数${GLU}(a,
    b)= a \otimes \sigma(b)$，其中$a$是输入矩阵的前一半，$b$是后一半。 |'
- en: '[Non-linear Activations (other)](#id1)[](#non-linear-activations-other "Permalink
    to this heading")'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[非线性激活函数（其他）](#id1)'
- en: '| [`nn.Softmin`](generated/torch.nn.Softmin.html#torch.nn.Softmin "torch.nn.Softmin")
    | Applies the Softmin function to an n-dimensional input Tensor rescaling them
    so that the elements of the n-dimensional output Tensor lie in the range [0, 1]
    and sum to 1. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Softmin`](generated/torch.nn.Softmin.html#torch.nn.Softmin "torch.nn.Softmin")
    | 对n维输入张量应用Softmin函数，重新缩放它们，使得n维输出张量的元素位于范围[0, 1]并总和为1。 |'
- en: '| [`nn.Softmax`](generated/torch.nn.Softmax.html#torch.nn.Softmax "torch.nn.Softmax")
    | Applies the Softmax function to an n-dimensional input Tensor rescaling them
    so that the elements of the n-dimensional output Tensor lie in the range [0,1]
    and sum to 1. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Softmax`](generated/torch.nn.Softmax.html#torch.nn.Softmax "torch.nn.Softmax")
    | 对n维输入张量应用Softmax函数，重新缩放它们，使得n维输出张量的元素位于范围[0,1]并总和为1。 |'
- en: '| [`nn.Softmax2d`](generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d "torch.nn.Softmax2d")
    | Applies SoftMax over features to each spatial location. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Softmax2d`](generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d "torch.nn.Softmax2d")
    | 对每个空间位置的特征应用SoftMax。 |'
- en: '| [`nn.LogSoftmax`](generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax
    "torch.nn.LogSoftmax") | Applies the $\log(\text{Softmax}(x))$log(Softmax(x))
    function to an n-dimensional input Tensor. |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LogSoftmax`](generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax
    "torch.nn.LogSoftmax") | 对n维输入张量应用$\log(\text{Softmax}(x))$函数。 |'
- en: '| [`nn.AdaptiveLogSoftmaxWithLoss`](generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss
    "torch.nn.AdaptiveLogSoftmaxWithLoss") | Efficient softmax approximation. |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AdaptiveLogSoftmaxWithLoss`](generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss
    "torch.nn.AdaptiveLogSoftmaxWithLoss") | 高效的softmax近似。 |'
- en: '[Normalization Layers](#id1)'
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[归一化层](#id1)'
- en: '| [`nn.BatchNorm1d`](generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d
    "torch.nn.BatchNorm1d") | Applies Batch Normalization over a 2D or 3D input as
    described in the paper [Batch Normalization: Accelerating Deep Network Training
    by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) . |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.BatchNorm1d`](generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d
    "torch.nn.BatchNorm1d") | 对2D或3D输入应用批量归一化，如论文[Batch Normalization: Accelerating
    Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)中描述。
    |'
- en: '| [`nn.BatchNorm2d`](generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d
    "torch.nn.BatchNorm2d") | Applies Batch Normalization over a 4D input (a mini-batch
    of 2D inputs with additional channel dimension) as described in the paper [Batch
    Normalization: Accelerating Deep Network Training by Reducing Internal Covariate
    Shift](https://arxiv.org/abs/1502.03167) . |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.BatchNorm2d`](generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d
    "torch.nn.BatchNorm2d") | 对 4D 输入（带有额外通道维度的 2D 输入的小批量）应用批归一化，如论文 [Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
    中所述。 |'
- en: '| [`nn.BatchNorm3d`](generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d
    "torch.nn.BatchNorm3d") | Applies Batch Normalization over a 5D input (a mini-batch
    of 3D inputs with additional channel dimension) as described in the paper [Batch
    Normalization: Accelerating Deep Network Training by Reducing Internal Covariate
    Shift](https://arxiv.org/abs/1502.03167) . |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.BatchNorm3d`](generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d
    "torch.nn.BatchNorm3d") | 对 5D 输入（带有额外通道维度的 3D 输入的小批量）应用批归一化，如论文 [Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
    中所述。 |'
- en: '| [`nn.LazyBatchNorm1d`](generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d
    "torch.nn.LazyBatchNorm1d") | A [`torch.nn.BatchNorm1d`](generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d
    "torch.nn.BatchNorm1d") module with lazy initialization of the `num_features`
    argument of the `BatchNorm1d` that is inferred from the `input.size(1)`. |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyBatchNorm1d`](generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d
    "torch.nn.LazyBatchNorm1d") | 具有延迟初始化的 `num_features` 参数的 [`torch.nn.BatchNorm1d`](generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d
    "torch.nn.BatchNorm1d") 模块，该参数从 `input.size(1)` 推断而来。 |'
- en: '| [`nn.LazyBatchNorm2d`](generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d
    "torch.nn.LazyBatchNorm2d") | A [`torch.nn.BatchNorm2d`](generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d
    "torch.nn.BatchNorm2d") module with lazy initialization of the `num_features`
    argument of the `BatchNorm2d` that is inferred from the `input.size(1)`. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyBatchNorm2d`](generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d
    "torch.nn.LazyBatchNorm2d") | 具有延迟初始化的 `num_features` 参数的 [`torch.nn.BatchNorm2d`](generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d
    "torch.nn.BatchNorm2d") 模块，该参数从 `input.size(1)` 推断而来。 |'
- en: '| [`nn.LazyBatchNorm3d`](generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d
    "torch.nn.LazyBatchNorm3d") | A [`torch.nn.BatchNorm3d`](generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d
    "torch.nn.BatchNorm3d") module with lazy initialization of the `num_features`
    argument of the `BatchNorm3d` that is inferred from the `input.size(1)`. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyBatchNorm3d`](generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d
    "torch.nn.LazyBatchNorm3d") | 具有延迟初始化的 `num_features` 参数的 [`torch.nn.BatchNorm3d`](generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d
    "torch.nn.BatchNorm3d") 模块，该参数从 `input.size(1)` 推断而来。 |'
- en: '| [`nn.GroupNorm`](generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm "torch.nn.GroupNorm")
    | Applies Group Normalization over a mini-batch of inputs. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.GroupNorm`](generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm "torch.nn.GroupNorm")
    | 对输入的一个小批量应用组归一化。 |'
- en: '| [`nn.SyncBatchNorm`](generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm
    "torch.nn.SyncBatchNorm") | Applies Batch Normalization over a N-Dimensional input
    (a mini-batch of [N-2]D inputs with additional channel dimension) as described
    in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift](https://arxiv.org/abs/1502.03167) . |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.SyncBatchNorm`](generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm
    "torch.nn.SyncBatchNorm") | 对 N 维输入（带有额外通道维度的小批量 [N-2]D 输入）应用批归一化，如论文 [Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
    中所述。 |'
- en: '| [`nn.InstanceNorm1d`](generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d
    "torch.nn.InstanceNorm1d") | Applies Instance Normalization. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.InstanceNorm1d`](generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d
    "torch.nn.InstanceNorm1d") | 应用实例归一化。 |'
- en: '| [`nn.InstanceNorm2d`](generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d
    "torch.nn.InstanceNorm2d") | Applies Instance Normalization. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.InstanceNorm2d`](generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d
    "torch.nn.InstanceNorm2d") | 应用实例归一化。 |'
- en: '| [`nn.InstanceNorm3d`](generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d
    "torch.nn.InstanceNorm3d") | Applies Instance Normalization. |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.InstanceNorm3d`](generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d
    "torch.nn.InstanceNorm3d") | 应用实例归一化。 |'
- en: '| [`nn.LazyInstanceNorm1d`](generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d
    "torch.nn.LazyInstanceNorm1d") | A [`torch.nn.InstanceNorm1d`](generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d
    "torch.nn.InstanceNorm1d") module with lazy initialization of the `num_features`
    argument. |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyInstanceNorm1d`](generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d
    "torch.nn.LazyInstanceNorm1d") | 具有延迟初始化的 `num_features` 参数的 [`torch.nn.InstanceNorm1d`](generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d
    "torch.nn.InstanceNorm1d") 模块。 |'
- en: '| [`nn.LazyInstanceNorm2d`](generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d
    "torch.nn.LazyInstanceNorm2d") | A [`torch.nn.InstanceNorm2d`](generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d
    "torch.nn.InstanceNorm2d") module with lazy initialization of the `num_features`
    argument. |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyInstanceNorm2d`](generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d
    "torch.nn.LazyInstanceNorm2d") | 具有延迟初始化的 `num_features` 参数的 [`torch.nn.InstanceNorm2d`](generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d
    "torch.nn.InstanceNorm2d") 模块。 |'
- en: '| [`nn.LazyInstanceNorm3d`](generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d
    "torch.nn.LazyInstanceNorm3d") | A [`torch.nn.InstanceNorm3d`](generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d
    "torch.nn.InstanceNorm3d") module with lazy initialization of the `num_features`
    argument. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyInstanceNorm3d`](generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d
    "torch.nn.LazyInstanceNorm3d") | 具有 `num_features` 参数的延迟初始化的 [`torch.nn.InstanceNorm3d`](generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d
    "torch.nn.InstanceNorm3d") 模块。 |'
- en: '| [`nn.LayerNorm`](generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm "torch.nn.LayerNorm")
    | Applies Layer Normalization over a mini-batch of inputs. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LayerNorm`](generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm "torch.nn.LayerNorm")
    | 对输入的一个小批量应用层归一化。 |'
- en: '| [`nn.LocalResponseNorm`](generated/torch.nn.LocalResponseNorm.html#torch.nn.LocalResponseNorm
    "torch.nn.LocalResponseNorm") | Applies local response normalization over an input
    signal. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LocalResponseNorm`](generated/torch.nn.LocalResponseNorm.html#torch.nn.LocalResponseNorm
    "torch.nn.LocalResponseNorm") | 对输入信号应用局部响应归一化。 |'
- en: '[Recurrent Layers](#id1)'
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Recurrent Layers](#id1)'
- en: '| [`nn.RNNBase`](generated/torch.nn.RNNBase.html#torch.nn.RNNBase "torch.nn.RNNBase")
    | Base class for RNN modules (RNN, LSTM, GRU). |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.RNNBase`](generated/torch.nn.RNNBase.html#torch.nn.RNNBase "torch.nn.RNNBase")
    | RNN 模块（RNN、LSTM、GRU）的基类。 |'
- en: '| [`nn.RNN`](generated/torch.nn.RNN.html#torch.nn.RNN "torch.nn.RNN") | Apply
    a multi-layer Elman RNN with $\tanh$tanh or $\text{ReLU}$ReLU non-linearity to
    an input sequence. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.RNN`](generated/torch.nn.RNN.html#torch.nn.RNN "torch.nn.RNN") | 对输入序列应用多层
    Elman RNN，使用 $\tanh$ 或 $\text{ReLU}$ 非线性。 |'
- en: '| [`nn.LSTM`](generated/torch.nn.LSTM.html#torch.nn.LSTM "torch.nn.LSTM") |
    Apply a multi-layer long short-term memory (LSTM) RNN to an input sequence. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LSTM`](generated/torch.nn.LSTM.html#torch.nn.LSTM "torch.nn.LSTM") |
    对输入序列应用多层长短期记忆（LSTM）RNN。 |'
- en: '| [`nn.GRU`](generated/torch.nn.GRU.html#torch.nn.GRU "torch.nn.GRU") | Apply
    a multi-layer gated recurrent unit (GRU) RNN to an input sequence. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.GRU`](generated/torch.nn.GRU.html#torch.nn.GRU "torch.nn.GRU") | 对输入序列应用多层门控循环单元（GRU）RNN。
    |'
- en: '| [`nn.RNNCell`](generated/torch.nn.RNNCell.html#torch.nn.RNNCell "torch.nn.RNNCell")
    | An Elman RNN cell with tanh or ReLU non-linearity. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.RNNCell`](generated/torch.nn.RNNCell.html#torch.nn.RNNCell "torch.nn.RNNCell")
    | 一个具有 tanh 或 ReLU 非线性的 Elman RNN 单元。 |'
- en: '| [`nn.LSTMCell`](generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell "torch.nn.LSTMCell")
    | A long short-term memory (LSTM) cell. |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LSTMCell`](generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell "torch.nn.LSTMCell")
    | 长短期记忆（LSTM）单元。 |'
- en: '| [`nn.GRUCell`](generated/torch.nn.GRUCell.html#torch.nn.GRUCell "torch.nn.GRUCell")
    | A gated recurrent unit (GRU) cell. |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.GRUCell`](generated/torch.nn.GRUCell.html#torch.nn.GRUCell "torch.nn.GRUCell")
    | 门控循环单元（GRU）单元。 |'
- en: '[Transformer Layers](#id1)'
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Transformer Layers](#id1)'
- en: '| [`nn.Transformer`](generated/torch.nn.Transformer.html#torch.nn.Transformer
    "torch.nn.Transformer") | A transformer model. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Transformer`](generated/torch.nn.Transformer.html#torch.nn.Transformer
    "torch.nn.Transformer") | 一个 transformer 模型。 |'
- en: '| [`nn.TransformerEncoder`](generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder
    "torch.nn.TransformerEncoder") | TransformerEncoder is a stack of N encoder layers.
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.TransformerEncoder`](generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder
    "torch.nn.TransformerEncoder") | TransformerEncoder 是 N 个编码器层的堆叠。 |'
- en: '| [`nn.TransformerDecoder`](generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder
    "torch.nn.TransformerDecoder") | TransformerDecoder is a stack of N decoder layers.
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.TransformerDecoder`](generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder
    "torch.nn.TransformerDecoder") | TransformerDecoder 是 N 个解码器层的堆叠。 |'
- en: '| [`nn.TransformerEncoderLayer`](generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer
    "torch.nn.TransformerEncoderLayer") | TransformerEncoderLayer is made up of self-attn
    and feedforward network. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.TransformerEncoderLayer`](generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer
    "torch.nn.TransformerEncoderLayer") | TransformerEncoderLayer 由 self-attn 和前馈网络组成。
    |'
- en: '| [`nn.TransformerDecoderLayer`](generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer
    "torch.nn.TransformerDecoderLayer") | TransformerDecoderLayer is made up of self-attn,
    multi-head-attn and feedforward network. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.TransformerDecoderLayer`](generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer
    "torch.nn.TransformerDecoderLayer") | TransformerDecoderLayer 由 self-attn、multi-head-attn
    和前馈网络组成。 |'
- en: '[Linear Layers](#id1)'
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Linear Layers](#id1)'
- en: '| [`nn.Identity`](generated/torch.nn.Identity.html#torch.nn.Identity "torch.nn.Identity")
    | A placeholder identity operator that is argument-insensitive. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Identity`](generated/torch.nn.Identity.html#torch.nn.Identity "torch.nn.Identity")
    | 一个占位符身份运算符，不受参数影响。 |'
- en: '| [`nn.Linear`](generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")
    | Applies a linear transformation to the incoming data: $y = xA^T + b$y=xAT+b.
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Linear`](generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")
    | 对传入数据应用线性变换：$y = xA^T + b$。 |'
- en: '| [`nn.Bilinear`](generated/torch.nn.Bilinear.html#torch.nn.Bilinear "torch.nn.Bilinear")
    | Applies a bilinear transformation to the incoming data: $y = x_1^T A x_2 + b$y=x1T​Ax2​+b.
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Bilinear`](generated/torch.nn.Bilinear.html#torch.nn.Bilinear "torch.nn.Bilinear")
    | 对传入数据应用双线性变换：$y = x_1^T A x_2 + b$。 |'
- en: '| [`nn.LazyLinear`](generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear
    "torch.nn.LazyLinear") | A [`torch.nn.Linear`](generated/torch.nn.Linear.html#torch.nn.Linear
    "torch.nn.Linear") module where in_features is inferred. |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.LazyLinear`](generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear
    "torch.nn.LazyLinear") | 一个 [`torch.nn.Linear`](generated/torch.nn.Linear.html#torch.nn.Linear
    "torch.nn.Linear") 模块，其中的 in_features 是推断出来的。 |'
- en: '[Dropout Layers](#id1)'
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Dropout Layers](#id1)'
- en: '| [`nn.Dropout`](generated/torch.nn.Dropout.html#torch.nn.Dropout "torch.nn.Dropout")
    | During training, randomly zeroes some of the elements of the input tensor with
    probability `p`. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Dropout`](generated/torch.nn.Dropout.html#torch.nn.Dropout "torch.nn.Dropout")
    | 在训练期间，以概率 `p` 随机将输入张量的一些元素置零。 |'
- en: '| [`nn.Dropout1d`](generated/torch.nn.Dropout1d.html#torch.nn.Dropout1d "torch.nn.Dropout1d")
    | Randomly zero out entire channels. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Dropout1d`](generated/torch.nn.Dropout1d.html#torch.nn.Dropout1d "torch.nn.Dropout1d")
    | 随机将整个通道置零。 |'
- en: '| [`nn.Dropout2d`](generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d "torch.nn.Dropout2d")
    | Randomly zero out entire channels. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Dropout2d`](generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d "torch.nn.Dropout2d")
    | 随机将整个通道置零。 |'
- en: '| [`nn.Dropout3d`](generated/torch.nn.Dropout3d.html#torch.nn.Dropout3d "torch.nn.Dropout3d")
    | Randomly zero out entire channels. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Dropout3d`](generated/torch.nn.Dropout3d.html#torch.nn.Dropout3d "torch.nn.Dropout3d")
    | 随机将整个通道置零。 |'
- en: '| [`nn.AlphaDropout`](generated/torch.nn.AlphaDropout.html#torch.nn.AlphaDropout
    "torch.nn.AlphaDropout") | Applies Alpha Dropout over the input. |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.AlphaDropout`](generated/torch.nn.AlphaDropout.html#torch.nn.AlphaDropout
    "torch.nn.AlphaDropout") | 对输入应用 Alpha Dropout。 |'
- en: '| [`nn.FeatureAlphaDropout`](generated/torch.nn.FeatureAlphaDropout.html#torch.nn.FeatureAlphaDropout
    "torch.nn.FeatureAlphaDropout") | Randomly masks out entire channels. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.FeatureAlphaDropout`](generated/torch.nn.FeatureAlphaDropout.html#torch.nn.FeatureAlphaDropout
    "torch.nn.FeatureAlphaDropout") | 随机屏蔽整个通道。 |'
- en: '[Sparse Layers](#id1)'
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Sparse Layers](#id1)'
- en: '| [`nn.Embedding`](generated/torch.nn.Embedding.html#torch.nn.Embedding "torch.nn.Embedding")
    | A simple lookup table that stores embeddings of a fixed dictionary and size.
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Embedding`](generated/torch.nn.Embedding.html#torch.nn.Embedding "torch.nn.Embedding")
    | 存储固定字典和大小的嵌入的简单查找表。 |'
- en: '| [`nn.EmbeddingBag`](generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag
    "torch.nn.EmbeddingBag") | Compute sums or means of ''bags'' of embeddings, without
    instantiating the intermediate embeddings. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.EmbeddingBag`](generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag
    "torch.nn.EmbeddingBag") | 计算嵌入“bags”的和或均值，而不实例化中间嵌入。 |'
- en: '[Distance Functions](#id1)'
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[距离函数](#id1)'
- en: '| [`nn.CosineSimilarity`](generated/torch.nn.CosineSimilarity.html#torch.nn.CosineSimilarity
    "torch.nn.CosineSimilarity") | Returns cosine similarity between $x_1$x1​ and
    $x_2$x2​, computed along dim. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.CosineSimilarity`](generated/torch.nn.CosineSimilarity.html#torch.nn.CosineSimilarity
    "torch.nn.CosineSimilarity") | 返回 $x_1$ 和 $x_2$ 之间的余弦相似度，沿着维度计算。 |'
- en: '| [`nn.PairwiseDistance`](generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance
    "torch.nn.PairwiseDistance") | Computes the pairwise distance between input vectors,
    or between columns of input matrices. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.PairwiseDistance`](generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance
    "torch.nn.PairwiseDistance") | 计算输入向量之间的成对距离，或输入矩阵的列之间的距离。 |'
- en: '[Loss Functions](#id1)'
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[损失函数](#id1)'
- en: '| [`nn.L1Loss`](generated/torch.nn.L1Loss.html#torch.nn.L1Loss "torch.nn.L1Loss")
    | Creates a criterion that measures the mean absolute error (MAE) between each
    element in the input $x$x and target $y$y. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.L1Loss`](generated/torch.nn.L1Loss.html#torch.nn.L1Loss "torch.nn.L1Loss")
    | 创建一个标准，衡量输入 $x$ 和目标 $y$ 中每个元素的平均绝对误差（MAE）。 |'
- en: '| [`nn.MSELoss`](generated/torch.nn.MSELoss.html#torch.nn.MSELoss "torch.nn.MSELoss")
    | Creates a criterion that measures the mean squared error (squared L2 norm) between
    each element in the input $x$x and target $y$y. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MSELoss`](generated/torch.nn.MSELoss.html#torch.nn.MSELoss "torch.nn.MSELoss")
    | 创建一个标准，衡量输入 $x$ 和目标 $y$ 中每个元素的均方误差（平方L2范数）。 |'
- en: '| [`nn.CrossEntropyLoss`](generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss
    "torch.nn.CrossEntropyLoss") | This criterion computes the cross entropy loss
    between input logits and target. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.CrossEntropyLoss`](generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss
    "torch.nn.CrossEntropyLoss") | 此标准计算输入logits和目标之间的交叉熵损失。 |'
- en: '| [`nn.CTCLoss`](generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss "torch.nn.CTCLoss")
    | The Connectionist Temporal Classification loss. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.CTCLoss`](generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss "torch.nn.CTCLoss")
    | 连接主义时间分类损失。 |'
- en: '| [`nn.NLLLoss`](generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss "torch.nn.NLLLoss")
    | The negative log likelihood loss. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.NLLLoss`](generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss "torch.nn.NLLLoss")
    | 负对数似然损失。 |'
- en: '| [`nn.PoissonNLLLoss`](generated/torch.nn.PoissonNLLLoss.html#torch.nn.PoissonNLLLoss
    "torch.nn.PoissonNLLLoss") | Negative log likelihood loss with Poisson distribution
    of target. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.PoissonNLLLoss`](generated/torch.nn.PoissonNLLLoss.html#torch.nn.PoissonNLLLoss
    "torch.nn.PoissonNLLLoss") | 具有泊松分布目标的负对数似然损失。 |'
- en: '| [`nn.GaussianNLLLoss`](generated/torch.nn.GaussianNLLLoss.html#torch.nn.GaussianNLLLoss
    "torch.nn.GaussianNLLLoss") | Gaussian negative log likelihood loss. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.GaussianNLLLoss`](generated/torch.nn.GaussianNLLLoss.html#torch.nn.GaussianNLLLoss
    "torch.nn.GaussianNLLLoss") | 高斯负对数似然损失。 |'
- en: '| [`nn.KLDivLoss`](generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss "torch.nn.KLDivLoss")
    | The Kullback-Leibler divergence loss. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.KLDivLoss`](generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss "torch.nn.KLDivLoss")
    | Kullback-Leibler 散度损失。 |'
- en: '| [`nn.BCELoss`](generated/torch.nn.BCELoss.html#torch.nn.BCELoss "torch.nn.BCELoss")
    | Creates a criterion that measures the Binary Cross Entropy between the target
    and the input probabilities: |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.BCELoss`](generated/torch.nn.BCELoss.html#torch.nn.BCELoss "torch.nn.BCELoss")
    | 创建一个衡量目标和输入概率之间的二元交叉熵的标准： |'
- en: '| [`nn.BCEWithLogitsLoss`](generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss
    "torch.nn.BCEWithLogitsLoss") | This loss combines a Sigmoid layer and the BCELoss
    in one single class. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.BCEWithLogitsLoss`](generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss
    "torch.nn.BCEWithLogitsLoss") | 此损失将 Sigmoid 层和 BCELoss 结合在一个单一类中。 |'
- en: '| [`nn.MarginRankingLoss`](generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss
    "torch.nn.MarginRankingLoss") | Creates a criterion that measures the loss given
    inputs $x1$x1, $x2$x2, two 1D mini-batch or 0D Tensors, and a label 1D mini-batch
    or 0D Tensor $y$y (containing 1 or -1). |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MarginRankingLoss`](generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss
    "torch.nn.MarginRankingLoss") | 创建一个标准，衡量给定输入 $x1$、$x2$，两个1D mini-batch或0D张量，以及标签1D
    mini-batch或0D张量 $y$（包含1或-1）的损失。 |'
- en: '| [`nn.HingeEmbeddingLoss`](generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss
    "torch.nn.HingeEmbeddingLoss") | Measures the loss given an input tensor $x$x
    and a labels tensor $y$y (containing 1 or -1). |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.HingeEmbeddingLoss`](generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss
    "torch.nn.HingeEmbeddingLoss") | 给定输入张量 $x$ 和标签张量 $y$（包含1或-1），衡量损失。 |'
- en: '| [`nn.MultiLabelMarginLoss`](generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss
    "torch.nn.MultiLabelMarginLoss") | Creates a criterion that optimizes a multi-class
    multi-classification hinge loss (margin-based loss) between input $x$x (a 2D mini-batch
    Tensor) and output $y$y (which is a 2D Tensor of target class indices). |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MultiLabelMarginLoss`](generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss
    "torch.nn.MultiLabelMarginLoss") | 创建一个标准，优化输入 $x$（一个2D mini-batch张量）和输出 $y$（目标类别索引的2D张量）之间的多类多分类铰链损失（基于边缘的损失）。
    |'
- en: '| [`nn.HuberLoss`](generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss "torch.nn.HuberLoss")
    | Creates a criterion that uses a squared term if the absolute element-wise error
    falls below delta and a delta-scaled L1 term otherwise. |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.HuberLoss`](generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss "torch.nn.HuberLoss")
    | 创建一个标准，如果绝对逐元素误差低于 delta，则使用平方项，否则使用 delta-scaled L1 项。 |'
- en: '| [`nn.SmoothL1Loss`](generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss
    "torch.nn.SmoothL1Loss") | Creates a criterion that uses a squared term if the
    absolute element-wise error falls below beta and an L1 term otherwise. |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.SmoothL1Loss`](generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss
    "torch.nn.SmoothL1Loss") | 创建一个标准，如果绝对逐元素误差低于 beta，则使用平方项，否则使用L1项。 |'
- en: '| [`nn.SoftMarginLoss`](generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss
    "torch.nn.SoftMarginLoss") | Creates a criterion that optimizes a two-class classification
    logistic loss between input tensor $x$x and target tensor $y$y (containing 1 or
    -1). |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.SoftMarginLoss`](generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss
    "torch.nn.SoftMarginLoss") | 创建一个标准，优化输入张量$x$和目标张量$y$（包含1或-1）之间的两类分类逻辑损失。 |'
- en: '| [`nn.MultiLabelSoftMarginLoss`](generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss
    "torch.nn.MultiLabelSoftMarginLoss") | Creates a criterion that optimizes a multi-label
    one-versus-all loss based on max-entropy, between input $x$x and target $y$y of
    size $(N, C)$(N,C). |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MultiLabelSoftMarginLoss`](generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss
    "torch.nn.MultiLabelSoftMarginLoss") | 创建一个标准，基于最大熵，优化输入$x$和大小为$(N, C)$的目标$y$的多标签一对所有损失。
    |'
- en: '| [`nn.CosineEmbeddingLoss`](generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss
    "torch.nn.CosineEmbeddingLoss") | Creates a criterion that measures the loss given
    input tensors $x_1$x1​, $x_2$x2​ and a Tensor label $y$y with values 1 or -1.
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.CosineEmbeddingLoss`](generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss
    "torch.nn.CosineEmbeddingLoss") | 创建一个标准，根据输入张量$x_1$、$x_2$和一个值为1或-1的张量标签$y$来测量损失。
    |'
- en: '| [`nn.MultiMarginLoss`](generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss
    "torch.nn.MultiMarginLoss") | Creates a criterion that optimizes a multi-class
    classification hinge loss (margin-based loss) between input $x$x (a 2D mini-batch
    Tensor) and output $y$y (which is a 1D tensor of target class indices, $0 \leq
    y \leq \text{x.size}(1)-1$0≤y≤x.size(1)−1): |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.MultiMarginLoss`](generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss
    "torch.nn.MultiMarginLoss") | 创建一个标准，优化输入$x$(一个2D小批量张量)和输出$y$(一个目标类别索引的1D张量，$0
    \leq y \leq \text{x.size}(1)-1$)之间的多类分类铰链损失（基于边缘的损失）： |'
- en: '| [`nn.TripletMarginLoss`](generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss
    "torch.nn.TripletMarginLoss") | Creates a criterion that measures the triplet
    loss given an input tensors $x1$x1, $x2$x2, $x3$x3 and a margin with a value greater
    than $0$0. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.TripletMarginLoss`](generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss
    "torch.nn.TripletMarginLoss") | 创建一个标准，根据大于$0$的边距值，测量给定输入张量$x_1$、$x_2$、$x_3$的三元组损失。
    |'
- en: '| [`nn.TripletMarginWithDistanceLoss`](generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss
    "torch.nn.TripletMarginWithDistanceLoss") | Creates a criterion that measures
    the triplet loss given input tensors $a$a, $p$p, and $n$n (representing anchor,
    positive, and negative examples, respectively), and a nonnegative, real-valued
    function ("distance function") used to compute the relationship between the anchor
    and positive example ("positive distance") and the anchor and negative example
    ("negative distance"). |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.TripletMarginWithDistanceLoss`](generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss
    "torch.nn.TripletMarginWithDistanceLoss") | 创建一个标准，根据输入张量$a$、$p$、$n$（分别表示锚点、正例和负例）和用于计算锚点和正例之间关系（“正距离”）以及锚点和负例之间关系（“负距离”）的非负实值函数（“距离函数”）。
    |'
- en: '[Vision Layers](#id1)'
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[视觉层](#id1)'
- en: '| [`nn.PixelShuffle`](generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle
    "torch.nn.PixelShuffle") | Rearrange elements in a tensor according to an upscaling
    factor. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.PixelShuffle`](generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle
    "torch.nn.PixelShuffle") | 根据放大因子重新排列张量中的元素。 |'
- en: '| [`nn.PixelUnshuffle`](generated/torch.nn.PixelUnshuffle.html#torch.nn.PixelUnshuffle
    "torch.nn.PixelUnshuffle") | Reverse the PixelShuffle operation. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.PixelUnshuffle`](generated/torch.nn.PixelUnshuffle.html#torch.nn.PixelUnshuffle
    "torch.nn.PixelUnshuffle") | 反转PixelShuffle操作。 |'
- en: '| [`nn.Upsample`](generated/torch.nn.Upsample.html#torch.nn.Upsample "torch.nn.Upsample")
    | Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric)
    data. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Upsample`](generated/torch.nn.Upsample.html#torch.nn.Upsample "torch.nn.Upsample")
    | 上采样给定的多通道1D（时间）、2D（空间）或3D（体积）数据。 |'
- en: '| [`nn.UpsamplingNearest2d`](generated/torch.nn.UpsamplingNearest2d.html#torch.nn.UpsamplingNearest2d
    "torch.nn.UpsamplingNearest2d") | Applies a 2D nearest neighbor upsampling to
    an input signal composed of several input channels. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.UpsamplingNearest2d`](generated/torch.nn.UpsamplingNearest2d.html#torch.nn.UpsamplingNearest2d
    "torch.nn.UpsamplingNearest2d") | 对由多个输入通道组成的输入信号应用2D最近邻上采样。 |'
- en: '| [`nn.UpsamplingBilinear2d`](generated/torch.nn.UpsamplingBilinear2d.html#torch.nn.UpsamplingBilinear2d
    "torch.nn.UpsamplingBilinear2d") | Applies a 2D bilinear upsampling to an input
    signal composed of several input channels. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.UpsamplingBilinear2d`](generated/torch.nn.UpsamplingBilinear2d.html#torch.nn.UpsamplingBilinear2d
    "torch.nn.UpsamplingBilinear2d") | 对由多个输入通道组成的输入信号应用2D双线性上采样。 |'
- en: '[Shuffle Layers](#id1)'
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[洗牌层](#id1)'
- en: '| [`nn.ChannelShuffle`](generated/torch.nn.ChannelShuffle.html#torch.nn.ChannelShuffle
    "torch.nn.ChannelShuffle") | Divides and rearranges the channels in a tensor.
    |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.ChannelShuffle`](generated/torch.nn.ChannelShuffle.html#torch.nn.ChannelShuffle
    "torch.nn.ChannelShuffle") | 分割并重新排列张量中的通道。 |'
- en: '## [DataParallel Layers (multi-GPU, distributed)](#id1)[](#module-torch.nn.parallel
    "Permalink to this heading")'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '## [数据并行层（多GPU，分布式）](#id1)[](#module-torch.nn.parallel "跳转到此标题的永久链接")'
- en: '| [`nn.DataParallel`](generated/torch.nn.DataParallel.html#torch.nn.DataParallel
    "torch.nn.DataParallel") | Implements data parallelism at the module level. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.DataParallel`](generated/torch.nn.DataParallel.html#torch.nn.DataParallel
    "torch.nn.DataParallel") | 在模块级别实现数据并行。 |'
- en: '| [`nn.parallel.DistributedDataParallel`](generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") | Implement distributed data parallelism
    based on `torch.distributed` at module level. |  ## [Utilities](#id1)[](#module-torch.nn.utils
    "Permalink to this heading")'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '| [`nn.parallel.DistributedDataParallel`](generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") | 在模块级别基于`torch.distributed`实现分布式数据并行。
    |  ## [实用工具](#id1)[](#module-torch.nn.utils "跳转到此标题的永久链接")'
- en: 'From the `torch.nn.utils` module:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`torch.nn.utils`模块：
- en: Utility functions to clip parameter gradients.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 用于裁剪参数梯度的实用函数。
- en: '| [`clip_grad_norm_`](generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_
    "torch.nn.utils.clip_grad_norm_") | Clip the gradient norm of an iterable of parameters.
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| [`clip_grad_norm_`](generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_
    "torch.nn.utils.clip_grad_norm_") | 对参数的可迭代对象剪裁梯度范数。 |'
- en: '| [`clip_grad_norm`](generated/torch.nn.utils.clip_grad_norm.html#torch.nn.utils.clip_grad_norm
    "torch.nn.utils.clip_grad_norm") | Clip the gradient norm of an iterable of parameters.
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| [`clip_grad_norm`](generated/torch.nn.utils.clip_grad_norm.html#torch.nn.utils.clip_grad_norm
    "torch.nn.utils.clip_grad_norm") | 对参数的可迭代对象剪裁梯度范数。 |'
- en: '| [`clip_grad_value_`](generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_
    "torch.nn.utils.clip_grad_value_") | Clip the gradients of an iterable of parameters
    at specified value. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| [`clip_grad_value_`](generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_
    "torch.nn.utils.clip_grad_value_") | 将参数的梯度剪裁到指定值。 |'
- en: Utility functions to flatten and unflatten Module parameters to and from a single
    vector.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 将模块参数展平和展开为单个向量的实用函数。
- en: '| [`parameters_to_vector`](generated/torch.nn.utils.parameters_to_vector.html#torch.nn.utils.parameters_to_vector
    "torch.nn.utils.parameters_to_vector") | Flatten an iterable of parameters into
    a single vector. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| [`parameters_to_vector`](generated/torch.nn.utils.parameters_to_vector.html#torch.nn.utils.parameters_to_vector
    "torch.nn.utils.parameters_to_vector") | 将参数的可迭代对象展平为单个向量。 |'
- en: '| [`vector_to_parameters`](generated/torch.nn.utils.vector_to_parameters.html#torch.nn.utils.vector_to_parameters
    "torch.nn.utils.vector_to_parameters") | Copy slices of a vector into an iterable
    of parameters. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| [`vector_to_parameters`](generated/torch.nn.utils.vector_to_parameters.html#torch.nn.utils.vector_to_parameters
    "torch.nn.utils.vector_to_parameters") | 将向量的切片复制到参数的可迭代对象中。 |'
- en: Utility functions to fuse Modules with BatchNorm modules.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 用于融合带有BatchNorm模块的模块的实用函数。
- en: '| [`fuse_conv_bn_eval`](generated/torch.nn.utils.fuse_conv_bn_eval.html#torch.nn.utils.fuse_conv_bn_eval
    "torch.nn.utils.fuse_conv_bn_eval") | Fuse a convolutional module and a BatchNorm
    module into a single, new convolutional module. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| [`fuse_conv_bn_eval`](generated/torch.nn.utils.fuse_conv_bn_eval.html#torch.nn.utils.fuse_conv_bn_eval
    "torch.nn.utils.fuse_conv_bn_eval") | 将卷积模块和BatchNorm模块融合为单个新的卷积模块。 |'
- en: '| [`fuse_conv_bn_weights`](generated/torch.nn.utils.fuse_conv_bn_weights.html#torch.nn.utils.fuse_conv_bn_weights
    "torch.nn.utils.fuse_conv_bn_weights") | Fuse convolutional module parameters
    and BatchNorm module parameters into new convolutional module parameters. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| [`fuse_conv_bn_weights`](generated/torch.nn.utils.fuse_conv_bn_weights.html#torch.nn.utils.fuse_conv_bn_weights
    "torch.nn.utils.fuse_conv_bn_weights") | 将卷积模块参数和BatchNorm模块参数融合为新的卷积模块参数。 |'
- en: '| [`fuse_linear_bn_eval`](generated/torch.nn.utils.fuse_linear_bn_eval.html#torch.nn.utils.fuse_linear_bn_eval
    "torch.nn.utils.fuse_linear_bn_eval") | Fuse a linear module and a BatchNorm module
    into a single, new linear module. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| [`fuse_linear_bn_eval`](generated/torch.nn.utils.fuse_linear_bn_eval.html#torch.nn.utils.fuse_linear_bn_eval
    "torch.nn.utils.fuse_linear_bn_eval") | 将线性模块和BatchNorm模块融合为单个新的线性模块。 |'
- en: '| [`fuse_linear_bn_weights`](generated/torch.nn.utils.fuse_linear_bn_weights.html#torch.nn.utils.fuse_linear_bn_weights
    "torch.nn.utils.fuse_linear_bn_weights") | Fuse linear module parameters and BatchNorm
    module parameters into new linear module parameters. |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| [`fuse_linear_bn_weights`](generated/torch.nn.utils.fuse_linear_bn_weights.html#torch.nn.utils.fuse_linear_bn_weights
    "torch.nn.utils.fuse_linear_bn_weights") | 将线性模块参数和BatchNorm模块参数融合为新的线性模块参数。 |'
- en: Utility functions to convert Module parameter memory formats.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 用于转换模块参数内存格式的实用函数。
- en: '| [`convert_conv2d_weight_memory_format`](generated/torch.nn.utils.convert_conv2d_weight_memory_format.html#torch.nn.utils.convert_conv2d_weight_memory_format
    "torch.nn.utils.convert_conv2d_weight_memory_format") | Convert `memory_format`
    of `nn.Conv2d.weight` to `memory_format`. |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| [`convert_conv2d_weight_memory_format`](generated/torch.nn.utils.convert_conv2d_weight_memory_format.html#torch.nn.utils.convert_conv2d_weight_memory_format
    "torch.nn.utils.convert_conv2d_weight_memory_format") | 将`nn.Conv2d.weight`的`memory_format`转换为`memory_format`。
    |'
- en: Utility functions to apply and remove weight normalization from Module parameters.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 应用和移除模块参数中的权重归一化的实用函数。
- en: '| [`weight_norm`](#module-torch.nn.utils.weight_norm "torch.nn.utils.weight_norm")
    | Apply weight normalization to a parameter in the given module. |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| [`weight_norm`](#module-torch.nn.utils.weight_norm "torch.nn.utils.weight_norm")
    | 对给定模块中的参数应用权重归一化。 |'
- en: '| [`remove_weight_norm`](generated/torch.nn.utils.remove_weight_norm.html#torch.nn.utils.remove_weight_norm
    "torch.nn.utils.remove_weight_norm") | Remove the weight normalization reparameterization
    from a module. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| [`remove_weight_norm`](generated/torch.nn.utils.remove_weight_norm.html#torch.nn.utils.remove_weight_norm
    "torch.nn.utils.remove_weight_norm") | 从模块中移除权重归一化重新参数化。 |'
- en: '| [`spectral_norm`](#module-torch.nn.utils.spectral_norm "torch.nn.utils.spectral_norm")
    | Apply spectral normalization to a parameter in the given module. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| [`spectral_norm`](#module-torch.nn.utils.spectral_norm "torch.nn.utils.spectral_norm")
    | 对给定模块中的参数应用谱归一化。 |'
- en: '| [`remove_spectral_norm`](generated/torch.nn.utils.remove_spectral_norm.html#torch.nn.utils.remove_spectral_norm
    "torch.nn.utils.remove_spectral_norm") | Remove the spectral normalization reparameterization
    from a module. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| [`remove_spectral_norm`](generated/torch.nn.utils.remove_spectral_norm.html#torch.nn.utils.remove_spectral_norm
    "torch.nn.utils.remove_spectral_norm") | 从模块中移除谱归一化重新参数化。 |'
- en: Utility functions for initializing Module parameters.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 用于初始化模块参数的实用函数。
- en: '| [`skip_init`](generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init
    "torch.nn.utils.skip_init") | Given a module class object and args / kwargs, instantiate
    the module without initializing parameters / buffers. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| [`skip_init`](generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init
    "torch.nn.utils.skip_init") | 给定模块类对象和参数/关键字参数，实例化模块而不初始化参数/缓冲区。 |'
- en: Utility classes and functions for pruning Module parameters.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 用于修剪模块参数的实用类和函数。
- en: '| [`prune.BasePruningMethod`](generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod
    "torch.nn.utils.prune.BasePruningMethod") | Abstract base class for creation of
    new pruning techniques. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.BasePruningMethod`](generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod
    "torch.nn.utils.prune.BasePruningMethod") | 用于创建新修剪技术的抽象基类。 |'
- en: '| [`prune.PruningContainer`](generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer
    "torch.nn.utils.prune.PruningContainer") | Container holding a sequence of pruning
    methods for iterative pruning. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.PruningContainer`](generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer
    "torch.nn.utils.prune.PruningContainer") | 包含一系列迭代剪枝方法的容器。 |'
- en: '| [`prune.Identity`](generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity
    "torch.nn.utils.prune.Identity") | Utility pruning method that does not prune
    any units but generates the pruning parametrization with a mask of ones. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.Identity`](generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity
    "torch.nn.utils.prune.Identity") | 不剪枝任何单元，但生成具有全为1的掩码的剪枝参数化的实用剪枝方法。 |'
- en: '| [`prune.RandomUnstructured`](generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured
    "torch.nn.utils.prune.RandomUnstructured") | Prune (currently unpruned) units
    in a tensor at random. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.RandomUnstructured`](generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured
    "torch.nn.utils.prune.RandomUnstructured") | 随机剪枝张量中的（当前未剪枝）单元。 |'
- en: '| [`prune.L1Unstructured`](generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured
    "torch.nn.utils.prune.L1Unstructured") | Prune (currently unpruned) units in a
    tensor by zeroing out the ones with the lowest L1-norm. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.L1Unstructured`](generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured
    "torch.nn.utils.prune.L1Unstructured") | 通过将具有最低 L1-范数的单元置零来剪枝张量中的（当前未剪枝）单元。 |'
- en: '| [`prune.RandomStructured`](generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured
    "torch.nn.utils.prune.RandomStructured") | Prune entire (currently unpruned) channels
    in a tensor at random. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.RandomStructured`](generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured
    "torch.nn.utils.prune.RandomStructured") | 随机剪枝张量中的整个（当前未剪枝）通道。 |'
- en: '| [`prune.LnStructured`](generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured
    "torch.nn.utils.prune.LnStructured") | Prune entire (currently unpruned) channels
    in a tensor based on their L`n`-norm. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.LnStructured`](generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured
    "torch.nn.utils.prune.LnStructured") | 基于它们的 L`n`-范数剪枝张量中整个（当前未剪枝）通道。 |'
- en: '| [`prune.CustomFromMask`](generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask
    "torch.nn.utils.prune.CustomFromMask") |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.CustomFromMask`](generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask
    "torch.nn.utils.prune.CustomFromMask") |  |'
- en: '| [`prune.identity`](generated/torch.nn.utils.prune.identity.html#torch.nn.utils.prune.identity
    "torch.nn.utils.prune.identity") | Apply pruning reparametrization without pruning
    any units. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.identity`](generated/torch.nn.utils.prune.identity.html#torch.nn.utils.prune.identity
    "torch.nn.utils.prune.identity") | 应用剪枝重新参数化，而不剪枝任何单元。 |'
- en: '| [`prune.random_unstructured`](generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured
    "torch.nn.utils.prune.random_unstructured") | Prune tensor by removing random
    (currently unpruned) units. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.random_unstructured`](generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured
    "torch.nn.utils.prune.random_unstructured") | 通过移除随机（当前未剪枝）单元来剪枝张量。 |'
- en: '| [`prune.l1_unstructured`](generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured
    "torch.nn.utils.prune.l1_unstructured") | Prune tensor by removing units with
    the lowest L1-norm. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.l1_unstructured`](generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured
    "torch.nn.utils.prune.l1_unstructured") | 通过移除具有最低 L1-范数的单元来剪枝张量。 |'
- en: '| [`prune.random_structured`](generated/torch.nn.utils.prune.random_structured.html#torch.nn.utils.prune.random_structured
    "torch.nn.utils.prune.random_structured") | Prune tensor by removing random channels
    along the specified dimension. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.random_structured`](generated/torch.nn.utils.prune.random_structured.html#torch.nn.utils.prune.random_structured
    "torch.nn.utils.prune.random_structured") | 通过沿指定维度移除随机通道来剪枝张量。 |'
- en: '| [`prune.ln_structured`](generated/torch.nn.utils.prune.ln_structured.html#torch.nn.utils.prune.ln_structured
    "torch.nn.utils.prune.ln_structured") | Prune tensor by removing channels with
    the lowest L`n`-norm along the specified dimension. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.ln_structured`](generated/torch.nn.utils.prune.ln_structured.html#torch.nn.utils.prune.ln_structured
    "torch.nn.utils.prune.ln_structured") | 通过沿指定维度移除具有最低 L`n`-范数的通道来剪枝张量。 |'
- en: '| [`prune.global_unstructured`](generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured
    "torch.nn.utils.prune.global_unstructured") | Globally prunes tensors corresponding
    to all parameters in `parameters` by applying the specified `pruning_method`.
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.global_unstructured`](generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured
    "torch.nn.utils.prune.global_unstructured") | 通过应用指定的 `pruning_method` 全局剪枝与 `parameters`
    中所有参数对应的张量。 |'
- en: '| [`prune.custom_from_mask`](generated/torch.nn.utils.prune.custom_from_mask.html#torch.nn.utils.prune.custom_from_mask
    "torch.nn.utils.prune.custom_from_mask") | Prune tensor corresponding to parameter
    called `name` in `module` by applying the pre-computed mask in `mask`. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.custom_from_mask`](generated/torch.nn.utils.prune.custom_from_mask.html#torch.nn.utils.prune.custom_from_mask
    "torch.nn.utils.prune.custom_from_mask") | 通过应用 `mask` 中的预先计算的掩码来剪枝与 `module`
    中名为 `name` 的参数对应的张量。 |'
- en: '| [`prune.remove`](generated/torch.nn.utils.prune.remove.html#torch.nn.utils.prune.remove
    "torch.nn.utils.prune.remove") | Remove the pruning reparameterization from a
    module and the pruning method from the forward hook. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.remove`](generated/torch.nn.utils.prune.remove.html#torch.nn.utils.prune.remove
    "torch.nn.utils.prune.remove") | 从模块中移除剪枝重新参数化，并从前向钩子中移除剪枝方法。 |'
- en: '| [`prune.is_pruned`](generated/torch.nn.utils.prune.is_pruned.html#torch.nn.utils.prune.is_pruned
    "torch.nn.utils.prune.is_pruned") | Check if a module is pruned by looking for
    pruning pre-hooks. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| [`prune.is_pruned`](generated/torch.nn.utils.prune.is_pruned.html#torch.nn.utils.prune.is_pruned
    "torch.nn.utils.prune.is_pruned") | 通过查找剪枝预钩子来检查模块是否被剪枝。 |'
- en: Parametrizations implemented using the new parametrization functionality in
    `torch.nn.utils.parameterize.register_parametrization()`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `torch.nn.utils.parameterize.register_parametrization()` 中的新参数化功能实现的参数化。
- en: '| [`parametrizations.orthogonal`](generated/torch.nn.utils.parametrizations.orthogonal.html#torch.nn.utils.parametrizations.orthogonal
    "torch.nn.utils.parametrizations.orthogonal") | Apply an orthogonal or unitary
    parametrization to a matrix or a batch of matrices. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| [`parametrizations.orthogonal`](generated/torch.nn.utils.parametrizations.orthogonal.html#torch.nn.utils.parametrizations.orthogonal
    "torch.nn.utils.parametrizations.orthogonal") | 对矩阵或一批矩阵应用正交或酉参数化。 |'
- en: '| [`parametrizations.weight_norm`](generated/torch.nn.utils.parametrizations.weight_norm.html#torch.nn.utils.parametrizations.weight_norm
    "torch.nn.utils.parametrizations.weight_norm") | Apply weight normalization to
    a parameter in the given module. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| [`parametrizations.weight_norm`](generated/torch.nn.utils.parametrizations.weight_norm.html#torch.nn.utils.parametrizations.weight_norm
    "torch.nn.utils.parametrizations.weight_norm") | 将权重归一化应用于给定模块中的参数。 |'
- en: '| [`parametrizations.spectral_norm`](generated/torch.nn.utils.parametrizations.spectral_norm.html#torch.nn.utils.parametrizations.spectral_norm
    "torch.nn.utils.parametrizations.spectral_norm") | Apply spectral normalization
    to a parameter in the given module. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| [`parametrizations.spectral_norm`](generated/torch.nn.utils.parametrizations.spectral_norm.html#torch.nn.utils.parametrizations.spectral_norm
    "torch.nn.utils.parametrizations.spectral_norm") | 对给定模块中的参数应用谱归一化。 |'
- en: Utility functions to parametrize Tensors on existing Modules. Note that these
    functions can be used to parametrize a given Parameter or Buffer given a specific
    function that maps from an input space to the parametrized space. They are not
    parameterizations that would transform an object into a parameter. See the [Parametrizations
    tutorial](https://pytorch.org/tutorials/intermediate/parametrizations.html) for
    more information on how to implement your own parametrizations.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在现有模块上对张量进行参数化的实用函数。请注意，这些函数可用于对给定的参数或缓冲区进行参数化，给定特定函数从输入空间映射到参数化空间。它们不是将对象转换为参数的参数化。有关如何实现自己的参数化的更多信息，请参阅[参数化教程](https://pytorch.org/tutorials/intermediate/parametrizations.html)。
- en: '| [`parametrize.register_parametrization`](generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization
    "torch.nn.utils.parametrize.register_parametrization") | Register a parametrization
    to a tensor in a module. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| [`parametrize.register_parametrization`](generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization
    "torch.nn.utils.parametrize.register_parametrization") | 在模块中的张量上注册参数化。 |'
- en: '| [`parametrize.remove_parametrizations`](generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations
    "torch.nn.utils.parametrize.remove_parametrizations") | Remove the parametrizations
    on a tensor in a module. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| [`parametrize.remove_parametrizations`](generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations
    "torch.nn.utils.parametrize.remove_parametrizations") | 在模块中的张量上移除参数化。 |'
- en: '| [`parametrize.cached`](generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached
    "torch.nn.utils.parametrize.cached") | Context manager that enables the caching
    system within parametrizations registered with `register_parametrization()`. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| [`parametrize.cached`](generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached
    "torch.nn.utils.parametrize.cached") | 启用与`register_parametrization()`注册的参数化中的缓存系统的上下文管理器。
    |'
- en: '| [`parametrize.is_parametrized`](generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized
    "torch.nn.utils.parametrize.is_parametrized") | Determine if a module has a parametrization.
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| [`parametrize.is_parametrized`](generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized
    "torch.nn.utils.parametrize.is_parametrized") | 确定模块是否具有参数化。 |'
- en: '| [`parametrize.ParametrizationList`](generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList
    "torch.nn.utils.parametrize.ParametrizationList") | A sequential container that
    holds and manages the original parameters or buffers of a parametrized [`torch.nn.Module`](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module"). |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| [`parametrize.ParametrizationList`](generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList
    "torch.nn.utils.parametrize.ParametrizationList") | 一个顺序容器，保存和管理参数化[`torch.nn.Module`](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")的原始参数或缓冲区。 |'
- en: Utility functions to call a given Module in a stateless manner.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 以无状态方式调用给定模块的实用函数。
- en: '| [`stateless.functional_call`](generated/torch.nn.utils.stateless.functional_call.html#torch.nn.utils.stateless.functional_call
    "torch.nn.utils.stateless.functional_call") | Perform a functional call on the
    module by replacing the module parameters and buffers with the provided ones.
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| [`stateless.functional_call`](generated/torch.nn.utils.stateless.functional_call.html#torch.nn.utils.stateless.functional_call
    "torch.nn.utils.stateless.functional_call") | 通过用提供的参数替换模块的参数和缓冲区来对模块执行功能调用。 |'
- en: Utility functions in other modules
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 其他模块中的实用函数
- en: '| [`nn.utils.rnn.PackedSequence`](generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence
    "torch.nn.utils.rnn.PackedSequence") | Holds the data and list of `batch_sizes`
    of a packed sequence. |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.utils.rnn.PackedSequence`](generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence
    "torch.nn.utils.rnn.PackedSequence") | 包含打包序列的数据和`batch_sizes`列表。 |'
- en: '| [`nn.utils.rnn.pack_padded_sequence`](generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence
    "torch.nn.utils.rnn.pack_padded_sequence") | Packs a Tensor containing padded
    sequences of variable length. |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.utils.rnn.pack_padded_sequence`](generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence
    "torch.nn.utils.rnn.pack_padded_sequence") | 将包含变长填充序列的张量打包。 |'
- en: '| [`nn.utils.rnn.pad_packed_sequence`](generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence
    "torch.nn.utils.rnn.pad_packed_sequence") | Pad a packed batch of variable length
    sequences. |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.utils.rnn.pad_packed_sequence`](generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence
    "torch.nn.utils.rnn.pad_packed_sequence") | 填充变长序列的打包批次。 |'
- en: '| [`nn.utils.rnn.pad_sequence`](generated/torch.nn.utils.rnn.pad_sequence.html#torch.nn.utils.rnn.pad_sequence
    "torch.nn.utils.rnn.pad_sequence") | Pad a list of variable length Tensors with
    `padding_value`. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.utils.rnn.pad_sequence`](generated/torch.nn.utils.rnn.pad_sequence.html#torch.nn.utils.rnn.pad_sequence
    "torch.nn.utils.rnn.pad_sequence") | 使用`padding_value`填充变长张量列表。 |'
- en: '| [`nn.utils.rnn.pack_sequence`](generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence
    "torch.nn.utils.rnn.pack_sequence") | Packs a list of variable length Tensors.
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.utils.rnn.pack_sequence`](generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence
    "torch.nn.utils.rnn.pack_sequence") | 打包长度可变的张量列表。 |'
- en: '| [`nn.utils.rnn.unpack_sequence`](generated/torch.nn.utils.rnn.unpack_sequence.html#torch.nn.utils.rnn.unpack_sequence
    "torch.nn.utils.rnn.unpack_sequence") | Unpack PackedSequence into a list of variable
    length Tensors. |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.utils.rnn.unpack_sequence`](generated/torch.nn.utils.rnn.unpack_sequence.html#torch.nn.utils.rnn.unpack_sequence
    "torch.nn.utils.rnn.unpack_sequence") | 将PackedSequence解包成长度可变的张量列表。 |'
- en: '| [`nn.utils.rnn.unpad_sequence`](generated/torch.nn.utils.rnn.unpad_sequence.html#torch.nn.utils.rnn.unpad_sequence
    "torch.nn.utils.rnn.unpad_sequence") | Unpad padded Tensor into a list of variable
    length Tensors. |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.utils.rnn.unpad_sequence`](generated/torch.nn.utils.rnn.unpad_sequence.html#torch.nn.utils.rnn.unpad_sequence
    "torch.nn.utils.rnn.unpad_sequence") | 将填充的张量解除填充为长度可变的张量列表。 |'
- en: '| [`nn.Flatten`](generated/torch.nn.Flatten.html#torch.nn.Flatten "torch.nn.Flatten")
    | Flattens a contiguous range of dims into a tensor. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Flatten`](generated/torch.nn.Flatten.html#torch.nn.Flatten "torch.nn.Flatten")
    | 将连续的维度范围展平为张量。 |'
- en: '| [`nn.Unflatten`](generated/torch.nn.Unflatten.html#torch.nn.Unflatten "torch.nn.Unflatten")
    | Unflattens a tensor dim expanding it to a desired shape. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.Unflatten`](generated/torch.nn.Unflatten.html#torch.nn.Unflatten "torch.nn.Unflatten")
    | 将张量展平，将其扩展为所需的形状。 |'
- en: '[Quantized Functions](#id1)'
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[量化函数](#id1)'
- en: Quantization refers to techniques for performing computations and storing tensors
    at lower bitwidths than floating point precision. PyTorch supports both per tensor
    and per channel asymmetric linear quantization. To learn more how to use quantized
    functions in PyTorch, please refer to the [Quantization](quantization.html#quantization-doc)
    documentation.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是指在比浮点精度更低的比特宽度上执行计算和存储张量的技术。PyTorch支持每个张量和每个通道的非对称线性量化。要了解如何在PyTorch中使用量化函数，请参阅[量化](quantization.html#quantization-doc)文档。
- en: '[Lazy Modules Initialization](#id1)[](#lazy-modules-initialization "Permalink
    to this heading")'
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[延迟模块初始化](#id1)[](#lazy-modules-initialization "跳转到此标题")'
- en: '| [`nn.modules.lazy.LazyModuleMixin`](generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin
    "torch.nn.modules.lazy.LazyModuleMixin") | A mixin for modules that lazily initialize
    parameters, also known as "lazy modules". |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| [`nn.modules.lazy.LazyModuleMixin`](generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin
    "torch.nn.modules.lazy.LazyModuleMixin") | 用于延迟初始化参数的模块混合，也称为“延迟模块”。 |'
