- en: torch.nn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'These are the basic building blocks for graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: torch.nn
  prefs: []
  type: TYPE_NORMAL
- en: '[Containers](#containers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Convolution Layers](#convolution-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pooling layers](#pooling-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Padding Layers](#padding-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Non-linear Activations (weighted sum, nonlinearity)](#non-linear-activations-weighted-sum-nonlinearity)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Non-linear Activations (other)](#non-linear-activations-other)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Normalization Layers](#normalization-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Recurrent Layers](#recurrent-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transformer Layers](#transformer-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Layers](#linear-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dropout Layers](#dropout-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sparse Layers](#sparse-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Distance Functions](#distance-functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Loss Functions](#loss-functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vision Layers](#vision-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Shuffle Layers](#shuffle-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DataParallel Layers (multi-GPU, distributed)](#module-torch.nn.parallel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Utilities](#module-torch.nn.utils)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quantized Functions](#quantized-functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Lazy Modules Initialization](#lazy-modules-initialization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [`Parameter`](generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter
    "torch.nn.parameter.Parameter") | A kind of Tensor that is to be considered a
    module parameter. |'
  prefs: []
  type: TYPE_TB
- en: '| [`UninitializedParameter`](generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter
    "torch.nn.parameter.UninitializedParameter") | A parameter that is not initialized.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`UninitializedBuffer`](generated/torch.nn.parameter.UninitializedBuffer.html#torch.nn.parameter.UninitializedBuffer
    "torch.nn.parameter.UninitializedBuffer") | A buffer that is not initialized.
    |'
  prefs: []
  type: TYPE_TB
- en: '[Containers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`Module`](generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")
    | Base class for all neural network modules. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Sequential`](generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential")
    | A sequential container. |'
  prefs: []
  type: TYPE_TB
- en: '| [`ModuleList`](generated/torch.nn.ModuleList.html#torch.nn.ModuleList "torch.nn.ModuleList")
    | Holds submodules in a list. |'
  prefs: []
  type: TYPE_TB
- en: '| [`ModuleDict`](generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict "torch.nn.ModuleDict")
    | Holds submodules in a dictionary. |'
  prefs: []
  type: TYPE_TB
- en: '| [`ParameterList`](generated/torch.nn.ParameterList.html#torch.nn.ParameterList
    "torch.nn.ParameterList") | Holds parameters in a list. |'
  prefs: []
  type: TYPE_TB
- en: '| [`ParameterDict`](generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict
    "torch.nn.ParameterDict") | Holds parameters in a dictionary. |'
  prefs: []
  type: TYPE_TB
- en: Global Hooks For Module
  prefs: []
  type: TYPE_NORMAL
- en: '| [`register_module_forward_pre_hook`](generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook
    "torch.nn.modules.module.register_module_forward_pre_hook") | Register a forward
    pre-hook common to all modules. |'
  prefs: []
  type: TYPE_TB
- en: '| [`register_module_forward_hook`](generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook
    "torch.nn.modules.module.register_module_forward_hook") | Register a global forward
    hook for all the modules. |'
  prefs: []
  type: TYPE_TB
- en: '| [`register_module_backward_hook`](generated/torch.nn.modules.module.register_module_backward_hook.html#torch.nn.modules.module.register_module_backward_hook
    "torch.nn.modules.module.register_module_backward_hook") | Register a backward
    hook common to all the modules. |'
  prefs: []
  type: TYPE_TB
- en: '| [`register_module_full_backward_pre_hook`](generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook
    "torch.nn.modules.module.register_module_full_backward_pre_hook") | Register a
    backward pre-hook common to all the modules. |'
  prefs: []
  type: TYPE_TB
- en: '| [`register_module_full_backward_hook`](generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook
    "torch.nn.modules.module.register_module_full_backward_hook") | Register a backward
    hook common to all the modules. |'
  prefs: []
  type: TYPE_TB
- en: '| [`register_module_buffer_registration_hook`](generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch.nn.modules.module.register_module_buffer_registration_hook
    "torch.nn.modules.module.register_module_buffer_registration_hook") | Register
    a buffer registration hook common to all modules. |'
  prefs: []
  type: TYPE_TB
- en: '| [`register_module_module_registration_hook`](generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch.nn.modules.module.register_module_module_registration_hook
    "torch.nn.modules.module.register_module_module_registration_hook") | Register
    a module registration hook common to all modules. |'
  prefs: []
  type: TYPE_TB
- en: '| [`register_module_parameter_registration_hook`](generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch.nn.modules.module.register_module_parameter_registration_hook
    "torch.nn.modules.module.register_module_parameter_registration_hook") | Register
    a parameter registration hook common to all modules. |'
  prefs: []
  type: TYPE_TB
- en: '[Convolution Layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.Conv1d`](generated/torch.nn.Conv1d.html#torch.nn.Conv1d "torch.nn.Conv1d")
    | Applies a 1D convolution over an input signal composed of several input planes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Conv2d`](generated/torch.nn.Conv2d.html#torch.nn.Conv2d "torch.nn.Conv2d")
    | Applies a 2D convolution over an input signal composed of several input planes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Conv3d`](generated/torch.nn.Conv3d.html#torch.nn.Conv3d "torch.nn.Conv3d")
    | Applies a 3D convolution over an input signal composed of several input planes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ConvTranspose1d`](generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d
    "torch.nn.ConvTranspose1d") | Applies a 1D transposed convolution operator over
    an input image composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ConvTranspose2d`](generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d
    "torch.nn.ConvTranspose2d") | Applies a 2D transposed convolution operator over
    an input image composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ConvTranspose3d`](generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d
    "torch.nn.ConvTranspose3d") | Applies a 3D transposed convolution operator over
    an input image composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyConv1d`](generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d
    "torch.nn.LazyConv1d") | A [`torch.nn.Conv1d`](generated/torch.nn.Conv1d.html#torch.nn.Conv1d
    "torch.nn.Conv1d") module with lazy initialization of the `in_channels` argument.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyConv2d`](generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d
    "torch.nn.LazyConv2d") | A [`torch.nn.Conv2d`](generated/torch.nn.Conv2d.html#torch.nn.Conv2d
    "torch.nn.Conv2d") module with lazy initialization of the `in_channels` argument.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyConv3d`](generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d
    "torch.nn.LazyConv3d") | A [`torch.nn.Conv3d`](generated/torch.nn.Conv3d.html#torch.nn.Conv3d
    "torch.nn.Conv3d") module with lazy initialization of the `in_channels` argument.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyConvTranspose1d`](generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d
    "torch.nn.LazyConvTranspose1d") | A [`torch.nn.ConvTranspose1d`](generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d
    "torch.nn.ConvTranspose1d") module with lazy initialization of the `in_channels`
    argument. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyConvTranspose2d`](generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d
    "torch.nn.LazyConvTranspose2d") | A [`torch.nn.ConvTranspose2d`](generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d
    "torch.nn.ConvTranspose2d") module with lazy initialization of the `in_channels`
    argument. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyConvTranspose3d`](generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d
    "torch.nn.LazyConvTranspose3d") | A [`torch.nn.ConvTranspose3d`](generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d
    "torch.nn.ConvTranspose3d") module with lazy initialization of the `in_channels`
    argument. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Unfold`](generated/torch.nn.Unfold.html#torch.nn.Unfold "torch.nn.Unfold")
    | Extracts sliding local blocks from a batched input tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Fold`](generated/torch.nn.Fold.html#torch.nn.Fold "torch.nn.Fold") |
    Combines an array of sliding local blocks into a large containing tensor. |'
  prefs: []
  type: TYPE_TB
- en: '[Pooling layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.MaxPool1d`](generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d "torch.nn.MaxPool1d")
    | Applies a 1D max pooling over an input signal composed of several input planes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MaxPool2d`](generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d "torch.nn.MaxPool2d")
    | Applies a 2D max pooling over an input signal composed of several input planes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MaxPool3d`](generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d "torch.nn.MaxPool3d")
    | Applies a 3D max pooling over an input signal composed of several input planes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MaxUnpool1d`](generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d
    "torch.nn.MaxUnpool1d") | Computes a partial inverse of `MaxPool1d`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MaxUnpool2d`](generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d
    "torch.nn.MaxUnpool2d") | Computes a partial inverse of `MaxPool2d`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MaxUnpool3d`](generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d
    "torch.nn.MaxUnpool3d") | Computes a partial inverse of `MaxPool3d`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AvgPool1d`](generated/torch.nn.AvgPool1d.html#torch.nn.AvgPool1d "torch.nn.AvgPool1d")
    | Applies a 1D average pooling over an input signal composed of several input
    planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AvgPool2d`](generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d "torch.nn.AvgPool2d")
    | Applies a 2D average pooling over an input signal composed of several input
    planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AvgPool3d`](generated/torch.nn.AvgPool3d.html#torch.nn.AvgPool3d "torch.nn.AvgPool3d")
    | Applies a 3D average pooling over an input signal composed of several input
    planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.FractionalMaxPool2d`](generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d
    "torch.nn.FractionalMaxPool2d") | Applies a 2D fractional max pooling over an
    input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.FractionalMaxPool3d`](generated/torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d
    "torch.nn.FractionalMaxPool3d") | Applies a 3D fractional max pooling over an
    input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LPPool1d`](generated/torch.nn.LPPool1d.html#torch.nn.LPPool1d "torch.nn.LPPool1d")
    | Applies a 1D power-average pooling over an input signal composed of several
    input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LPPool2d`](generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d "torch.nn.LPPool2d")
    | Applies a 2D power-average pooling over an input signal composed of several
    input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AdaptiveMaxPool1d`](generated/torch.nn.AdaptiveMaxPool1d.html#torch.nn.AdaptiveMaxPool1d
    "torch.nn.AdaptiveMaxPool1d") | Applies a 1D adaptive max pooling over an input
    signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AdaptiveMaxPool2d`](generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d
    "torch.nn.AdaptiveMaxPool2d") | Applies a 2D adaptive max pooling over an input
    signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AdaptiveMaxPool3d`](generated/torch.nn.AdaptiveMaxPool3d.html#torch.nn.AdaptiveMaxPool3d
    "torch.nn.AdaptiveMaxPool3d") | Applies a 3D adaptive max pooling over an input
    signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AdaptiveAvgPool1d`](generated/torch.nn.AdaptiveAvgPool1d.html#torch.nn.AdaptiveAvgPool1d
    "torch.nn.AdaptiveAvgPool1d") | Applies a 1D adaptive average pooling over an
    input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AdaptiveAvgPool2d`](generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d
    "torch.nn.AdaptiveAvgPool2d") | Applies a 2D adaptive average pooling over an
    input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AdaptiveAvgPool3d`](generated/torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d
    "torch.nn.AdaptiveAvgPool3d") | Applies a 3D adaptive average pooling over an
    input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '[Padding Layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.ReflectionPad1d`](generated/torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d
    "torch.nn.ReflectionPad1d") | Pads the input tensor using the reflection of the
    input boundary. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ReflectionPad2d`](generated/torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d
    "torch.nn.ReflectionPad2d") | Pads the input tensor using the reflection of the
    input boundary. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ReflectionPad3d`](generated/torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d
    "torch.nn.ReflectionPad3d") | Pads the input tensor using the reflection of the
    input boundary. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ReplicationPad1d`](generated/torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d
    "torch.nn.ReplicationPad1d") | Pads the input tensor using replication of the
    input boundary. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ReplicationPad2d`](generated/torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d
    "torch.nn.ReplicationPad2d") | Pads the input tensor using replication of the
    input boundary. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ReplicationPad3d`](generated/torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d
    "torch.nn.ReplicationPad3d") | Pads the input tensor using replication of the
    input boundary. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ZeroPad1d`](generated/torch.nn.ZeroPad1d.html#torch.nn.ZeroPad1d "torch.nn.ZeroPad1d")
    | Pads the input tensor boundaries with zero. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ZeroPad2d`](generated/torch.nn.ZeroPad2d.html#torch.nn.ZeroPad2d "torch.nn.ZeroPad2d")
    | Pads the input tensor boundaries with zero. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ZeroPad3d`](generated/torch.nn.ZeroPad3d.html#torch.nn.ZeroPad3d "torch.nn.ZeroPad3d")
    | Pads the input tensor boundaries with zero. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ConstantPad1d`](generated/torch.nn.ConstantPad1d.html#torch.nn.ConstantPad1d
    "torch.nn.ConstantPad1d") | Pads the input tensor boundaries with a constant value.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ConstantPad2d`](generated/torch.nn.ConstantPad2d.html#torch.nn.ConstantPad2d
    "torch.nn.ConstantPad2d") | Pads the input tensor boundaries with a constant value.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ConstantPad3d`](generated/torch.nn.ConstantPad3d.html#torch.nn.ConstantPad3d
    "torch.nn.ConstantPad3d") | Pads the input tensor boundaries with a constant value.
    |'
  prefs: []
  type: TYPE_TB
- en: '[Non-linear Activations (weighted sum, nonlinearity)](#id1)[](#non-linear-activations-weighted-sum-nonlinearity
    "Permalink to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.ELU`](generated/torch.nn.ELU.html#torch.nn.ELU "torch.nn.ELU") | Applies
    the Exponential Linear Unit (ELU) function, element-wise, as described in the
    paper: [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289).
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Hardshrink`](generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink
    "torch.nn.Hardshrink") | Applies the Hard Shrinkage (Hardshrink) function element-wise.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Hardsigmoid`](generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid
    "torch.nn.Hardsigmoid") | Applies the Hardsigmoid function element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Hardtanh`](generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh "torch.nn.Hardtanh")
    | Applies the HardTanh function element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Hardswish`](generated/torch.nn.Hardswish.html#torch.nn.Hardswish "torch.nn.Hardswish")
    | Applies the Hardswish function, element-wise, as described in the paper: [Searching
    for MobileNetV3](https://arxiv.org/abs/1905.02244). |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LeakyReLU`](generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU "torch.nn.LeakyReLU")
    | Applies the element-wise function: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LogSigmoid`](generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid
    "torch.nn.LogSigmoid") | Applies the element-wise function: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MultiheadAttention`](generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention
    "torch.nn.MultiheadAttention") | Allows the model to jointly attend to information
    from different representation subspaces as described in the paper: [Attention
    Is All You Need](https://arxiv.org/abs/1706.03762). |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.PReLU`](generated/torch.nn.PReLU.html#torch.nn.PReLU "torch.nn.PReLU")
    | Applies the element-wise function: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ReLU`](generated/torch.nn.ReLU.html#torch.nn.ReLU "torch.nn.ReLU") |
    Applies the rectified linear unit function element-wise: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.ReLU6`](generated/torch.nn.ReLU6.html#torch.nn.ReLU6 "torch.nn.ReLU6")
    | Applies the element-wise function: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.RReLU`](generated/torch.nn.RReLU.html#torch.nn.RReLU "torch.nn.RReLU")
    | Applies the randomized leaky rectified linear unit function, element-wise, as
    described in the paper: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.SELU`](generated/torch.nn.SELU.html#torch.nn.SELU "torch.nn.SELU") |
    Applied element-wise, as: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.CELU`](generated/torch.nn.CELU.html#torch.nn.CELU "torch.nn.CELU") |
    Applies the element-wise function: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.GELU`](generated/torch.nn.GELU.html#torch.nn.GELU "torch.nn.GELU") |
    Applies the Gaussian Error Linear Units function: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Sigmoid`](generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid "torch.nn.Sigmoid")
    | Applies the element-wise function: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.SiLU`](generated/torch.nn.SiLU.html#torch.nn.SiLU "torch.nn.SiLU") |
    Applies the Sigmoid Linear Unit (SiLU) function, element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Mish`](generated/torch.nn.Mish.html#torch.nn.Mish "torch.nn.Mish") |
    Applies the Mish function, element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Softplus`](generated/torch.nn.Softplus.html#torch.nn.Softplus "torch.nn.Softplus")
    | Applies the Softplus function $\text{Softplus}(x)
    = \frac{1}{\beta} * \log(1 + \exp(\beta * x))$Softplus(x)=β1​∗log(1+exp(β∗x))
    element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Softshrink`](generated/torch.nn.Softshrink.html#torch.nn.Softshrink
    "torch.nn.Softshrink") | Applies the soft shrinkage function elementwise: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Softsign`](generated/torch.nn.Softsign.html#torch.nn.Softsign "torch.nn.Softsign")
    | Applies the element-wise function: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Tanh`](generated/torch.nn.Tanh.html#torch.nn.Tanh "torch.nn.Tanh") |
    Applies the Hyperbolic Tangent (Tanh) function element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Tanhshrink`](generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink
    "torch.nn.Tanhshrink") | Applies the element-wise function: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Threshold`](generated/torch.nn.Threshold.html#torch.nn.Threshold "torch.nn.Threshold")
    | Thresholds each element of the input Tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.GLU`](generated/torch.nn.GLU.html#torch.nn.GLU "torch.nn.GLU") | Applies
    the gated linear unit function ${GLU}(a, b)= a \otimes \sigma(b)$GLU(a,b)=a⊗σ(b)
    where $a$a
    is the first half of the input matrices and $b$b is the second
    half. |'
  prefs: []
  type: TYPE_TB
- en: '[Non-linear Activations (other)](#id1)[](#non-linear-activations-other "Permalink
    to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.Softmin`](generated/torch.nn.Softmin.html#torch.nn.Softmin "torch.nn.Softmin")
    | Applies the Softmin function to an n-dimensional input Tensor rescaling them
    so that the elements of the n-dimensional output Tensor lie in the range [0, 1]
    and sum to 1. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Softmax`](generated/torch.nn.Softmax.html#torch.nn.Softmax "torch.nn.Softmax")
    | Applies the Softmax function to an n-dimensional input Tensor rescaling them
    so that the elements of the n-dimensional output Tensor lie in the range [0,1]
    and sum to 1. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Softmax2d`](generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d "torch.nn.Softmax2d")
    | Applies SoftMax over features to each spatial location. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LogSoftmax`](generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax
    "torch.nn.LogSoftmax") | Applies the $\log(\text{Softmax}(x))$log(Softmax(x))
    function to an n-dimensional input Tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AdaptiveLogSoftmaxWithLoss`](generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss
    "torch.nn.AdaptiveLogSoftmaxWithLoss") | Efficient softmax approximation. |'
  prefs: []
  type: TYPE_TB
- en: '[Normalization Layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.BatchNorm1d`](generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d
    "torch.nn.BatchNorm1d") | Applies Batch Normalization over a 2D or 3D input as
    described in the paper [Batch Normalization: Accelerating Deep Network Training
    by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) . |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.BatchNorm2d`](generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d
    "torch.nn.BatchNorm2d") | Applies Batch Normalization over a 4D input (a mini-batch
    of 2D inputs with additional channel dimension) as described in the paper [Batch
    Normalization: Accelerating Deep Network Training by Reducing Internal Covariate
    Shift](https://arxiv.org/abs/1502.03167) . |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.BatchNorm3d`](generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d
    "torch.nn.BatchNorm3d") | Applies Batch Normalization over a 5D input (a mini-batch
    of 3D inputs with additional channel dimension) as described in the paper [Batch
    Normalization: Accelerating Deep Network Training by Reducing Internal Covariate
    Shift](https://arxiv.org/abs/1502.03167) . |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyBatchNorm1d`](generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d
    "torch.nn.LazyBatchNorm1d") | A [`torch.nn.BatchNorm1d`](generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d
    "torch.nn.BatchNorm1d") module with lazy initialization of the `num_features`
    argument of the `BatchNorm1d` that is inferred from the `input.size(1)`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyBatchNorm2d`](generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d
    "torch.nn.LazyBatchNorm2d") | A [`torch.nn.BatchNorm2d`](generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d
    "torch.nn.BatchNorm2d") module with lazy initialization of the `num_features`
    argument of the `BatchNorm2d` that is inferred from the `input.size(1)`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyBatchNorm3d`](generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d
    "torch.nn.LazyBatchNorm3d") | A [`torch.nn.BatchNorm3d`](generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d
    "torch.nn.BatchNorm3d") module with lazy initialization of the `num_features`
    argument of the `BatchNorm3d` that is inferred from the `input.size(1)`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.GroupNorm`](generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm "torch.nn.GroupNorm")
    | Applies Group Normalization over a mini-batch of inputs. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.SyncBatchNorm`](generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm
    "torch.nn.SyncBatchNorm") | Applies Batch Normalization over a N-Dimensional input
    (a mini-batch of [N-2]D inputs with additional channel dimension) as described
    in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift](https://arxiv.org/abs/1502.03167) . |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.InstanceNorm1d`](generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d
    "torch.nn.InstanceNorm1d") | Applies Instance Normalization. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.InstanceNorm2d`](generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d
    "torch.nn.InstanceNorm2d") | Applies Instance Normalization. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.InstanceNorm3d`](generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d
    "torch.nn.InstanceNorm3d") | Applies Instance Normalization. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyInstanceNorm1d`](generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d
    "torch.nn.LazyInstanceNorm1d") | A [`torch.nn.InstanceNorm1d`](generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d
    "torch.nn.InstanceNorm1d") module with lazy initialization of the `num_features`
    argument. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyInstanceNorm2d`](generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d
    "torch.nn.LazyInstanceNorm2d") | A [`torch.nn.InstanceNorm2d`](generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d
    "torch.nn.InstanceNorm2d") module with lazy initialization of the `num_features`
    argument. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyInstanceNorm3d`](generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d
    "torch.nn.LazyInstanceNorm3d") | A [`torch.nn.InstanceNorm3d`](generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d
    "torch.nn.InstanceNorm3d") module with lazy initialization of the `num_features`
    argument. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LayerNorm`](generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm "torch.nn.LayerNorm")
    | Applies Layer Normalization over a mini-batch of inputs. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LocalResponseNorm`](generated/torch.nn.LocalResponseNorm.html#torch.nn.LocalResponseNorm
    "torch.nn.LocalResponseNorm") | Applies local response normalization over an input
    signal. |'
  prefs: []
  type: TYPE_TB
- en: '[Recurrent Layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.RNNBase`](generated/torch.nn.RNNBase.html#torch.nn.RNNBase "torch.nn.RNNBase")
    | Base class for RNN modules (RNN, LSTM, GRU). |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.RNN`](generated/torch.nn.RNN.html#torch.nn.RNN "torch.nn.RNN") | Apply
    a multi-layer Elman RNN with $\tanh$tanh or $\text{ReLU}$ReLU non-linearity
    to an input sequence. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LSTM`](generated/torch.nn.LSTM.html#torch.nn.LSTM "torch.nn.LSTM") |
    Apply a multi-layer long short-term memory (LSTM) RNN to an input sequence. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.GRU`](generated/torch.nn.GRU.html#torch.nn.GRU "torch.nn.GRU") | Apply
    a multi-layer gated recurrent unit (GRU) RNN to an input sequence. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.RNNCell`](generated/torch.nn.RNNCell.html#torch.nn.RNNCell "torch.nn.RNNCell")
    | An Elman RNN cell with tanh or ReLU non-linearity. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LSTMCell`](generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell "torch.nn.LSTMCell")
    | A long short-term memory (LSTM) cell. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.GRUCell`](generated/torch.nn.GRUCell.html#torch.nn.GRUCell "torch.nn.GRUCell")
    | A gated recurrent unit (GRU) cell. |'
  prefs: []
  type: TYPE_TB
- en: '[Transformer Layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.Transformer`](generated/torch.nn.Transformer.html#torch.nn.Transformer
    "torch.nn.Transformer") | A transformer model. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.TransformerEncoder`](generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder
    "torch.nn.TransformerEncoder") | TransformerEncoder is a stack of N encoder layers.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.TransformerDecoder`](generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder
    "torch.nn.TransformerDecoder") | TransformerDecoder is a stack of N decoder layers.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.TransformerEncoderLayer`](generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer
    "torch.nn.TransformerEncoderLayer") | TransformerEncoderLayer is made up of self-attn
    and feedforward network. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.TransformerDecoderLayer`](generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer
    "torch.nn.TransformerDecoderLayer") | TransformerDecoderLayer is made up of self-attn,
    multi-head-attn and feedforward network. |'
  prefs: []
  type: TYPE_TB
- en: '[Linear Layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.Identity`](generated/torch.nn.Identity.html#torch.nn.Identity "torch.nn.Identity")
    | A placeholder identity operator that is argument-insensitive. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Linear`](generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")
    | Applies a linear transformation to the incoming data: $y = xA^T + b$y=xAT+b.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Bilinear`](generated/torch.nn.Bilinear.html#torch.nn.Bilinear "torch.nn.Bilinear")
    | Applies a bilinear transformation to the incoming data: $y = x_1^T A x_2 + b$y=x1T​Ax2​+b.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.LazyLinear`](generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear
    "torch.nn.LazyLinear") | A [`torch.nn.Linear`](generated/torch.nn.Linear.html#torch.nn.Linear
    "torch.nn.Linear") module where in_features is inferred. |'
  prefs: []
  type: TYPE_TB
- en: '[Dropout Layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.Dropout`](generated/torch.nn.Dropout.html#torch.nn.Dropout "torch.nn.Dropout")
    | During training, randomly zeroes some of the elements of the input tensor with
    probability `p`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Dropout1d`](generated/torch.nn.Dropout1d.html#torch.nn.Dropout1d "torch.nn.Dropout1d")
    | Randomly zero out entire channels. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Dropout2d`](generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d "torch.nn.Dropout2d")
    | Randomly zero out entire channels. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Dropout3d`](generated/torch.nn.Dropout3d.html#torch.nn.Dropout3d "torch.nn.Dropout3d")
    | Randomly zero out entire channels. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.AlphaDropout`](generated/torch.nn.AlphaDropout.html#torch.nn.AlphaDropout
    "torch.nn.AlphaDropout") | Applies Alpha Dropout over the input. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.FeatureAlphaDropout`](generated/torch.nn.FeatureAlphaDropout.html#torch.nn.FeatureAlphaDropout
    "torch.nn.FeatureAlphaDropout") | Randomly masks out entire channels. |'
  prefs: []
  type: TYPE_TB
- en: '[Sparse Layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.Embedding`](generated/torch.nn.Embedding.html#torch.nn.Embedding "torch.nn.Embedding")
    | A simple lookup table that stores embeddings of a fixed dictionary and size.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.EmbeddingBag`](generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag
    "torch.nn.EmbeddingBag") | Compute sums or means of ''bags'' of embeddings, without
    instantiating the intermediate embeddings. |'
  prefs: []
  type: TYPE_TB
- en: '[Distance Functions](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.CosineSimilarity`](generated/torch.nn.CosineSimilarity.html#torch.nn.CosineSimilarity
    "torch.nn.CosineSimilarity") | Returns cosine similarity between $x_1$x1​ and $x_2$x2​, computed
    along dim. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.PairwiseDistance`](generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance
    "torch.nn.PairwiseDistance") | Computes the pairwise distance between input vectors,
    or between columns of input matrices. |'
  prefs: []
  type: TYPE_TB
- en: '[Loss Functions](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.L1Loss`](generated/torch.nn.L1Loss.html#torch.nn.L1Loss "torch.nn.L1Loss")
    | Creates a criterion that measures the mean absolute error (MAE) between each
    element in the input $x$x
    and target $y$y.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MSELoss`](generated/torch.nn.MSELoss.html#torch.nn.MSELoss "torch.nn.MSELoss")
    | Creates a criterion that measures the mean squared error (squared L2 norm) between
    each element in the input $x$x and target $y$y. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.CrossEntropyLoss`](generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss
    "torch.nn.CrossEntropyLoss") | This criterion computes the cross entropy loss
    between input logits and target. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.CTCLoss`](generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss "torch.nn.CTCLoss")
    | The Connectionist Temporal Classification loss. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.NLLLoss`](generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss "torch.nn.NLLLoss")
    | The negative log likelihood loss. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.PoissonNLLLoss`](generated/torch.nn.PoissonNLLLoss.html#torch.nn.PoissonNLLLoss
    "torch.nn.PoissonNLLLoss") | Negative log likelihood loss with Poisson distribution
    of target. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.GaussianNLLLoss`](generated/torch.nn.GaussianNLLLoss.html#torch.nn.GaussianNLLLoss
    "torch.nn.GaussianNLLLoss") | Gaussian negative log likelihood loss. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.KLDivLoss`](generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss "torch.nn.KLDivLoss")
    | The Kullback-Leibler divergence loss. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.BCELoss`](generated/torch.nn.BCELoss.html#torch.nn.BCELoss "torch.nn.BCELoss")
    | Creates a criterion that measures the Binary Cross Entropy between the target
    and the input probabilities: |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.BCEWithLogitsLoss`](generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss
    "torch.nn.BCEWithLogitsLoss") | This loss combines a Sigmoid layer and the BCELoss
    in one single class. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MarginRankingLoss`](generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss
    "torch.nn.MarginRankingLoss") | Creates a criterion that measures the loss given
    inputs $x1$x1,
    $x2$x2,
    two 1D mini-batch or 0D Tensors, and a label 1D mini-batch or 0D Tensor $y$y (containing 1
    or -1). |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.HingeEmbeddingLoss`](generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss
    "torch.nn.HingeEmbeddingLoss") | Measures the loss given an input tensor $x$x and a labels tensor
    $y$y
    (containing 1 or -1). |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MultiLabelMarginLoss`](generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss
    "torch.nn.MultiLabelMarginLoss") | Creates a criterion that optimizes a multi-class
    multi-classification hinge loss (margin-based loss) between input $x$x (a 2D mini-batch
    Tensor) and output $y$y
    (which is a 2D Tensor of target class indices). |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.HuberLoss`](generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss "torch.nn.HuberLoss")
    | Creates a criterion that uses a squared term if the absolute element-wise error
    falls below delta and a delta-scaled L1 term otherwise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.SmoothL1Loss`](generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss
    "torch.nn.SmoothL1Loss") | Creates a criterion that uses a squared term if the
    absolute element-wise error falls below beta and an L1 term otherwise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.SoftMarginLoss`](generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss
    "torch.nn.SoftMarginLoss") | Creates a criterion that optimizes a two-class classification
    logistic loss between input tensor $x$x and target tensor
    $y$y
    (containing 1 or -1). |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MultiLabelSoftMarginLoss`](generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss
    "torch.nn.MultiLabelSoftMarginLoss") | Creates a criterion that optimizes a multi-label
    one-versus-all loss based on max-entropy, between input $x$x and target $y$y of size $(N, C)$(N,C). |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.CosineEmbeddingLoss`](generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss
    "torch.nn.CosineEmbeddingLoss") | Creates a criterion that measures the loss given
    input tensors $x_1$x1​, $x_2$x2​ and a Tensor
    label $y$y
    with values 1 or -1. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.MultiMarginLoss`](generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss
    "torch.nn.MultiMarginLoss") | Creates a criterion that optimizes a multi-class
    classification hinge loss (margin-based loss) between input $x$x (a 2D mini-batch
    Tensor) and output $y$y
    (which is a 1D tensor of target class indices, $0 \leq y \leq \text{x.size}(1)-1$0≤y≤x.size(1)−1):
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.TripletMarginLoss`](generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss
    "torch.nn.TripletMarginLoss") | Creates a criterion that measures the triplet
    loss given an input tensors $x1$x1, $x2$x2, $x3$x3 and a margin
    with a value greater than $0$0. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.TripletMarginWithDistanceLoss`](generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss
    "torch.nn.TripletMarginWithDistanceLoss") | Creates a criterion that measures
    the triplet loss given input tensors $a$a, $p$p, and $n$n (representing
    anchor, positive, and negative examples, respectively), and a nonnegative, real-valued
    function ("distance function") used to compute the relationship between the anchor
    and positive example ("positive distance") and the anchor and negative example
    ("negative distance"). |'
  prefs: []
  type: TYPE_TB
- en: '[Vision Layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.PixelShuffle`](generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle
    "torch.nn.PixelShuffle") | Rearrange elements in a tensor according to an upscaling
    factor. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.PixelUnshuffle`](generated/torch.nn.PixelUnshuffle.html#torch.nn.PixelUnshuffle
    "torch.nn.PixelUnshuffle") | Reverse the PixelShuffle operation. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Upsample`](generated/torch.nn.Upsample.html#torch.nn.Upsample "torch.nn.Upsample")
    | Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric)
    data. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.UpsamplingNearest2d`](generated/torch.nn.UpsamplingNearest2d.html#torch.nn.UpsamplingNearest2d
    "torch.nn.UpsamplingNearest2d") | Applies a 2D nearest neighbor upsampling to
    an input signal composed of several input channels. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.UpsamplingBilinear2d`](generated/torch.nn.UpsamplingBilinear2d.html#torch.nn.UpsamplingBilinear2d
    "torch.nn.UpsamplingBilinear2d") | Applies a 2D bilinear upsampling to an input
    signal composed of several input channels. |'
  prefs: []
  type: TYPE_TB
- en: '[Shuffle Layers](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.ChannelShuffle`](generated/torch.nn.ChannelShuffle.html#torch.nn.ChannelShuffle
    "torch.nn.ChannelShuffle") | Divides and rearranges the channels in a tensor.
    |'
  prefs: []
  type: TYPE_TB
- en: '## [DataParallel Layers (multi-GPU, distributed)](#id1)[](#module-torch.nn.parallel
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`nn.DataParallel`](generated/torch.nn.DataParallel.html#torch.nn.DataParallel
    "torch.nn.DataParallel") | Implements data parallelism at the module level. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.parallel.DistributedDataParallel`](generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") | Implement distributed data parallelism
    based on `torch.distributed` at module level. |  ## [Utilities](#id1)[](#module-torch.nn.utils
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the `torch.nn.utils` module:'
  prefs: []
  type: TYPE_NORMAL
- en: Utility functions to clip parameter gradients.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`clip_grad_norm_`](generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_
    "torch.nn.utils.clip_grad_norm_") | Clip the gradient norm of an iterable of parameters.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`clip_grad_norm`](generated/torch.nn.utils.clip_grad_norm.html#torch.nn.utils.clip_grad_norm
    "torch.nn.utils.clip_grad_norm") | Clip the gradient norm of an iterable of parameters.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`clip_grad_value_`](generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_
    "torch.nn.utils.clip_grad_value_") | Clip the gradients of an iterable of parameters
    at specified value. |'
  prefs: []
  type: TYPE_TB
- en: Utility functions to flatten and unflatten Module parameters to and from a single
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`parameters_to_vector`](generated/torch.nn.utils.parameters_to_vector.html#torch.nn.utils.parameters_to_vector
    "torch.nn.utils.parameters_to_vector") | Flatten an iterable of parameters into
    a single vector. |'
  prefs: []
  type: TYPE_TB
- en: '| [`vector_to_parameters`](generated/torch.nn.utils.vector_to_parameters.html#torch.nn.utils.vector_to_parameters
    "torch.nn.utils.vector_to_parameters") | Copy slices of a vector into an iterable
    of parameters. |'
  prefs: []
  type: TYPE_TB
- en: Utility functions to fuse Modules with BatchNorm modules.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`fuse_conv_bn_eval`](generated/torch.nn.utils.fuse_conv_bn_eval.html#torch.nn.utils.fuse_conv_bn_eval
    "torch.nn.utils.fuse_conv_bn_eval") | Fuse a convolutional module and a BatchNorm
    module into a single, new convolutional module. |'
  prefs: []
  type: TYPE_TB
- en: '| [`fuse_conv_bn_weights`](generated/torch.nn.utils.fuse_conv_bn_weights.html#torch.nn.utils.fuse_conv_bn_weights
    "torch.nn.utils.fuse_conv_bn_weights") | Fuse convolutional module parameters
    and BatchNorm module parameters into new convolutional module parameters. |'
  prefs: []
  type: TYPE_TB
- en: '| [`fuse_linear_bn_eval`](generated/torch.nn.utils.fuse_linear_bn_eval.html#torch.nn.utils.fuse_linear_bn_eval
    "torch.nn.utils.fuse_linear_bn_eval") | Fuse a linear module and a BatchNorm module
    into a single, new linear module. |'
  prefs: []
  type: TYPE_TB
- en: '| [`fuse_linear_bn_weights`](generated/torch.nn.utils.fuse_linear_bn_weights.html#torch.nn.utils.fuse_linear_bn_weights
    "torch.nn.utils.fuse_linear_bn_weights") | Fuse linear module parameters and BatchNorm
    module parameters into new linear module parameters. |'
  prefs: []
  type: TYPE_TB
- en: Utility functions to convert Module parameter memory formats.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`convert_conv2d_weight_memory_format`](generated/torch.nn.utils.convert_conv2d_weight_memory_format.html#torch.nn.utils.convert_conv2d_weight_memory_format
    "torch.nn.utils.convert_conv2d_weight_memory_format") | Convert `memory_format`
    of `nn.Conv2d.weight` to `memory_format`. |'
  prefs: []
  type: TYPE_TB
- en: Utility functions to apply and remove weight normalization from Module parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`weight_norm`](#module-torch.nn.utils.weight_norm "torch.nn.utils.weight_norm")
    | Apply weight normalization to a parameter in the given module. |'
  prefs: []
  type: TYPE_TB
- en: '| [`remove_weight_norm`](generated/torch.nn.utils.remove_weight_norm.html#torch.nn.utils.remove_weight_norm
    "torch.nn.utils.remove_weight_norm") | Remove the weight normalization reparameterization
    from a module. |'
  prefs: []
  type: TYPE_TB
- en: '| [`spectral_norm`](#module-torch.nn.utils.spectral_norm "torch.nn.utils.spectral_norm")
    | Apply spectral normalization to a parameter in the given module. |'
  prefs: []
  type: TYPE_TB
- en: '| [`remove_spectral_norm`](generated/torch.nn.utils.remove_spectral_norm.html#torch.nn.utils.remove_spectral_norm
    "torch.nn.utils.remove_spectral_norm") | Remove the spectral normalization reparameterization
    from a module. |'
  prefs: []
  type: TYPE_TB
- en: Utility functions for initializing Module parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`skip_init`](generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init
    "torch.nn.utils.skip_init") | Given a module class object and args / kwargs, instantiate
    the module without initializing parameters / buffers. |'
  prefs: []
  type: TYPE_TB
- en: Utility classes and functions for pruning Module parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`prune.BasePruningMethod`](generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod
    "torch.nn.utils.prune.BasePruningMethod") | Abstract base class for creation of
    new pruning techniques. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.PruningContainer`](generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer
    "torch.nn.utils.prune.PruningContainer") | Container holding a sequence of pruning
    methods for iterative pruning. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.Identity`](generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity
    "torch.nn.utils.prune.Identity") | Utility pruning method that does not prune
    any units but generates the pruning parametrization with a mask of ones. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.RandomUnstructured`](generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured
    "torch.nn.utils.prune.RandomUnstructured") | Prune (currently unpruned) units
    in a tensor at random. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.L1Unstructured`](generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured
    "torch.nn.utils.prune.L1Unstructured") | Prune (currently unpruned) units in a
    tensor by zeroing out the ones with the lowest L1-norm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.RandomStructured`](generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured
    "torch.nn.utils.prune.RandomStructured") | Prune entire (currently unpruned) channels
    in a tensor at random. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.LnStructured`](generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured
    "torch.nn.utils.prune.LnStructured") | Prune entire (currently unpruned) channels
    in a tensor based on their L`n`-norm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.CustomFromMask`](generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask
    "torch.nn.utils.prune.CustomFromMask") |  |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.identity`](generated/torch.nn.utils.prune.identity.html#torch.nn.utils.prune.identity
    "torch.nn.utils.prune.identity") | Apply pruning reparametrization without pruning
    any units. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.random_unstructured`](generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured
    "torch.nn.utils.prune.random_unstructured") | Prune tensor by removing random
    (currently unpruned) units. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.l1_unstructured`](generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured
    "torch.nn.utils.prune.l1_unstructured") | Prune tensor by removing units with
    the lowest L1-norm. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.random_structured`](generated/torch.nn.utils.prune.random_structured.html#torch.nn.utils.prune.random_structured
    "torch.nn.utils.prune.random_structured") | Prune tensor by removing random channels
    along the specified dimension. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.ln_structured`](generated/torch.nn.utils.prune.ln_structured.html#torch.nn.utils.prune.ln_structured
    "torch.nn.utils.prune.ln_structured") | Prune tensor by removing channels with
    the lowest L`n`-norm along the specified dimension. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.global_unstructured`](generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured
    "torch.nn.utils.prune.global_unstructured") | Globally prunes tensors corresponding
    to all parameters in `parameters` by applying the specified `pruning_method`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.custom_from_mask`](generated/torch.nn.utils.prune.custom_from_mask.html#torch.nn.utils.prune.custom_from_mask
    "torch.nn.utils.prune.custom_from_mask") | Prune tensor corresponding to parameter
    called `name` in `module` by applying the pre-computed mask in `mask`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.remove`](generated/torch.nn.utils.prune.remove.html#torch.nn.utils.prune.remove
    "torch.nn.utils.prune.remove") | Remove the pruning reparameterization from a
    module and the pruning method from the forward hook. |'
  prefs: []
  type: TYPE_TB
- en: '| [`prune.is_pruned`](generated/torch.nn.utils.prune.is_pruned.html#torch.nn.utils.prune.is_pruned
    "torch.nn.utils.prune.is_pruned") | Check if a module is pruned by looking for
    pruning pre-hooks. |'
  prefs: []
  type: TYPE_TB
- en: Parametrizations implemented using the new parametrization functionality in
    `torch.nn.utils.parameterize.register_parametrization()`.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`parametrizations.orthogonal`](generated/torch.nn.utils.parametrizations.orthogonal.html#torch.nn.utils.parametrizations.orthogonal
    "torch.nn.utils.parametrizations.orthogonal") | Apply an orthogonal or unitary
    parametrization to a matrix or a batch of matrices. |'
  prefs: []
  type: TYPE_TB
- en: '| [`parametrizations.weight_norm`](generated/torch.nn.utils.parametrizations.weight_norm.html#torch.nn.utils.parametrizations.weight_norm
    "torch.nn.utils.parametrizations.weight_norm") | Apply weight normalization to
    a parameter in the given module. |'
  prefs: []
  type: TYPE_TB
- en: '| [`parametrizations.spectral_norm`](generated/torch.nn.utils.parametrizations.spectral_norm.html#torch.nn.utils.parametrizations.spectral_norm
    "torch.nn.utils.parametrizations.spectral_norm") | Apply spectral normalization
    to a parameter in the given module. |'
  prefs: []
  type: TYPE_TB
- en: Utility functions to parametrize Tensors on existing Modules. Note that these
    functions can be used to parametrize a given Parameter or Buffer given a specific
    function that maps from an input space to the parametrized space. They are not
    parameterizations that would transform an object into a parameter. See the [Parametrizations
    tutorial](https://pytorch.org/tutorials/intermediate/parametrizations.html) for
    more information on how to implement your own parametrizations.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`parametrize.register_parametrization`](generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization
    "torch.nn.utils.parametrize.register_parametrization") | Register a parametrization
    to a tensor in a module. |'
  prefs: []
  type: TYPE_TB
- en: '| [`parametrize.remove_parametrizations`](generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations
    "torch.nn.utils.parametrize.remove_parametrizations") | Remove the parametrizations
    on a tensor in a module. |'
  prefs: []
  type: TYPE_TB
- en: '| [`parametrize.cached`](generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached
    "torch.nn.utils.parametrize.cached") | Context manager that enables the caching
    system within parametrizations registered with `register_parametrization()`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`parametrize.is_parametrized`](generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized
    "torch.nn.utils.parametrize.is_parametrized") | Determine if a module has a parametrization.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`parametrize.ParametrizationList`](generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList
    "torch.nn.utils.parametrize.ParametrizationList") | A sequential container that
    holds and manages the original parameters or buffers of a parametrized [`torch.nn.Module`](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module"). |'
  prefs: []
  type: TYPE_TB
- en: Utility functions to call a given Module in a stateless manner.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`stateless.functional_call`](generated/torch.nn.utils.stateless.functional_call.html#torch.nn.utils.stateless.functional_call
    "torch.nn.utils.stateless.functional_call") | Perform a functional call on the
    module by replacing the module parameters and buffers with the provided ones.
    |'
  prefs: []
  type: TYPE_TB
- en: Utility functions in other modules
  prefs: []
  type: TYPE_NORMAL
- en: '| [`nn.utils.rnn.PackedSequence`](generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence
    "torch.nn.utils.rnn.PackedSequence") | Holds the data and list of `batch_sizes`
    of a packed sequence. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.utils.rnn.pack_padded_sequence`](generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence
    "torch.nn.utils.rnn.pack_padded_sequence") | Packs a Tensor containing padded
    sequences of variable length. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.utils.rnn.pad_packed_sequence`](generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence
    "torch.nn.utils.rnn.pad_packed_sequence") | Pad a packed batch of variable length
    sequences. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.utils.rnn.pad_sequence`](generated/torch.nn.utils.rnn.pad_sequence.html#torch.nn.utils.rnn.pad_sequence
    "torch.nn.utils.rnn.pad_sequence") | Pad a list of variable length Tensors with
    `padding_value`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.utils.rnn.pack_sequence`](generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence
    "torch.nn.utils.rnn.pack_sequence") | Packs a list of variable length Tensors.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.utils.rnn.unpack_sequence`](generated/torch.nn.utils.rnn.unpack_sequence.html#torch.nn.utils.rnn.unpack_sequence
    "torch.nn.utils.rnn.unpack_sequence") | Unpack PackedSequence into a list of variable
    length Tensors. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.utils.rnn.unpad_sequence`](generated/torch.nn.utils.rnn.unpad_sequence.html#torch.nn.utils.rnn.unpad_sequence
    "torch.nn.utils.rnn.unpad_sequence") | Unpad padded Tensor into a list of variable
    length Tensors. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Flatten`](generated/torch.nn.Flatten.html#torch.nn.Flatten "torch.nn.Flatten")
    | Flattens a contiguous range of dims into a tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nn.Unflatten`](generated/torch.nn.Unflatten.html#torch.nn.Unflatten "torch.nn.Unflatten")
    | Unflattens a tensor dim expanding it to a desired shape. |'
  prefs: []
  type: TYPE_TB
- en: '[Quantized Functions](#id1)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantization refers to techniques for performing computations and storing tensors
    at lower bitwidths than floating point precision. PyTorch supports both per tensor
    and per channel asymmetric linear quantization. To learn more how to use quantized
    functions in PyTorch, please refer to the [Quantization](quantization.html#quantization-doc)
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Lazy Modules Initialization](#id1)[](#lazy-modules-initialization "Permalink
    to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nn.modules.lazy.LazyModuleMixin`](generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin
    "torch.nn.modules.lazy.LazyModuleMixin") | A mixin for modules that lazily initialize
    parameters, also known as "lazy modules". |'
  prefs: []
  type: TYPE_TB
