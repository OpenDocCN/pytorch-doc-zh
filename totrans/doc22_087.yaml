- en: torch.utils.data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/data.html](https://pytorch.org/docs/stable/data.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At the heart of PyTorch data loading utility is the [`torch.utils.data.DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") class. It represents a Python iterable over a dataset,
    with support for
  prefs: []
  type: TYPE_NORMAL
- en: '[map-style and iterable-style datasets](#dataset-types),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[customizing data loading order](#data-loading-order-and-sampler),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[automatic batching](#loading-batched-and-non-batched-data),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[single- and multi-process data loading](#single-and-multi-process-data-loading),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[automatic memory pinning](#memory-pinning).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These options are configured by the constructor arguments of a [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader"), which has signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The sections below describe in details the effects and usages of these options.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important argument of [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") constructor is [`dataset`](utils.html#module-torch.utils.data.dataset
    "torch.utils.data.dataset"), which indicates a dataset object to load data from.
    PyTorch supports two different types of datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[map-style datasets](#map-style-datasets),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[iterable-style datasets](#iterable-style-datasets).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Map-style datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A map-style dataset is one that implements the `__getitem__()` and `__len__()`
    protocols, and represents a map from (possibly non-integral) indices/keys to data
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: For example, such a dataset, when accessed with `dataset[idx]`, could read the
    `idx`-th image and its corresponding label from a folder on the disk.
  prefs: []
  type: TYPE_NORMAL
- en: See [`Dataset`](#torch.utils.data.Dataset "torch.utils.data.Dataset") for more
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Iterable-style datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An iterable-style dataset is an instance of a subclass of [`IterableDataset`](#torch.utils.data.IterableDataset
    "torch.utils.data.IterableDataset") that implements the `__iter__()` protocol,
    and represents an iterable over data samples. This type of datasets is particularly
    suitable for cases where random reads are expensive or even improbable, and where
    the batch size depends on the fetched data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, such a dataset, when called `iter(dataset)`, could return a stream
    of data reading from a database, a remote server, or even logs generated in real
    time.
  prefs: []
  type: TYPE_NORMAL
- en: See [`IterableDataset`](#torch.utils.data.IterableDataset "torch.utils.data.IterableDataset")
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When using a [`IterableDataset`](#torch.utils.data.IterableDataset "torch.utils.data.IterableDataset")
    with [multi-process data loading](#multi-process-data-loading). The same dataset
    object is replicated on each worker process, and thus the replicas must be configured
    differently to avoid duplicated data. See [`IterableDataset`](#torch.utils.data.IterableDataset
    "torch.utils.data.IterableDataset") documentations for how to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: Data Loading Order and [`Sampler`](#torch.utils.data.Sampler "torch.utils.data.Sampler")[](#data-loading-order-and-sampler
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For [iterable-style datasets](#iterable-style-datasets), data loading order
    is entirely controlled by the user-defined iterable. This allows easier implementations
    of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at
    each time).
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this section concerns the case with [map-style datasets](#map-style-datasets).
    [`torch.utils.data.Sampler`](#torch.utils.data.Sampler "torch.utils.data.Sampler")
    classes are used to specify the sequence of indices/keys used in data loading.
    They represent iterable objects over the indices to datasets. E.g., in the common
    case with stochastic gradient decent (SGD), a [`Sampler`](#torch.utils.data.Sampler
    "torch.utils.data.Sampler") could randomly permute a list of indices and yield
    each one at a time, or yield a small number of them for mini-batch SGD.
  prefs: []
  type: TYPE_NORMAL
- en: A sequential or shuffled sampler will be automatically constructed based on
    the `shuffle` argument to a [`DataLoader`](#torch.utils.data.DataLoader "torch.utils.data.DataLoader").
    Alternatively, users may use the [`sampler`](utils.html#module-torch.utils.data.sampler
    "torch.utils.data.sampler") argument to specify a custom [`Sampler`](#torch.utils.data.Sampler
    "torch.utils.data.Sampler") object that at each time yields the next index/key
    to fetch.
  prefs: []
  type: TYPE_NORMAL
- en: A custom [`Sampler`](#torch.utils.data.Sampler "torch.utils.data.Sampler") that
    yields a list of batch indices at a time can be passed as the `batch_sampler`
    argument. Automatic batching can also be enabled via `batch_size` and `drop_last`
    arguments. See [the next section](#loading-batched-and-non-batched-data) for more
    details on this.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Neither [`sampler`](utils.html#module-torch.utils.data.sampler "torch.utils.data.sampler")
    nor `batch_sampler` is compatible with iterable-style datasets, since such datasets
    have no notion of a key or an index.
  prefs: []
  type: TYPE_NORMAL
- en: Loading Batched and Non-Batched Data[](#loading-batched-and-non-batched-data
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`DataLoader`](#torch.utils.data.DataLoader "torch.utils.data.DataLoader")
    supports automatically collating individual fetched data samples into batches
    via arguments `batch_size`, `drop_last`, `batch_sampler`, and `collate_fn` (which
    has a default function).'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic batching (default)[](#automatic-batching-default "Permalink to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the most common case, and corresponds to fetching a minibatch of data
    and collating them into batched samples, i.e., containing Tensors with one dimension
    being the batch dimension (usually the first).
  prefs: []
  type: TYPE_NORMAL
- en: When `batch_size` (default `1`) is not `None`, the data loader yields batched
    samples instead of individual samples. `batch_size` and `drop_last` arguments
    are used to specify how the data loader obtains batches of dataset keys. For map-style
    datasets, users can alternatively specify `batch_sampler`, which yields a list
    of keys at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `batch_size` and `drop_last` arguments essentially are used to construct
    a `batch_sampler` from [`sampler`](utils.html#module-torch.utils.data.sampler
    "torch.utils.data.sampler"). For map-style datasets, the [`sampler`](utils.html#module-torch.utils.data.sampler
    "torch.utils.data.sampler") is either provided by user or constructed based on
    the `shuffle` argument. For iterable-style datasets, the [`sampler`](utils.html#module-torch.utils.data.sampler
    "torch.utils.data.sampler") is a dummy infinite one. See [this section](#data-loading-order-and-sampler)
    on more details on samplers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When fetching from [iterable-style datasets](#iterable-style-datasets) with
    [multi-processing](#multi-process-data-loading), the `drop_last` argument drops
    the last non-full batch of each worker’s dataset replica.
  prefs: []
  type: TYPE_NORMAL
- en: After fetching a list of samples using the indices from sampler, the function
    passed as the `collate_fn` argument is used to collate lists of samples into batches.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, loading from a map-style dataset is roughly equivalent with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'and loading from an iterable-style dataset is roughly equivalent with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A custom `collate_fn` can be used to customize collation, e.g., padding sequential
    data to max length of a batch. See [this section](#dataloader-collate-fn) on more
    about `collate_fn`.
  prefs: []
  type: TYPE_NORMAL
- en: Disable automatic batching[](#disable-automatic-batching "Permalink to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In certain cases, users may want to handle batching manually in dataset code,
    or simply load individual samples. For example, it could be cheaper to directly
    load batched data (e.g., bulk reads from a database or reading continuous chunks
    of memory), or the batch size is data dependent, or the program is designed to
    work on individual samples. Under these scenarios, it’s likely better to not use
    automatic batching (where `collate_fn` is used to collate the samples), but let
    the data loader directly return each member of the [`dataset`](utils.html#module-torch.utils.data.dataset
    "torch.utils.data.dataset") object.
  prefs: []
  type: TYPE_NORMAL
- en: When both `batch_size` and `batch_sampler` are `None` (default value for `batch_sampler`
    is already `None`), automatic batching is disabled. Each sample obtained from
    the [`dataset`](utils.html#module-torch.utils.data.dataset "torch.utils.data.dataset")
    is processed with the function passed as the `collate_fn` argument.
  prefs: []
  type: TYPE_NORMAL
- en: '**When automatic batching is disabled**, the default `collate_fn` simply converts
    NumPy arrays into PyTorch Tensors, and keeps everything else untouched.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, loading from a map-style dataset is roughly equivalent with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'and loading from an iterable-style dataset is roughly equivalent with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: See [this section](#dataloader-collate-fn) on more about `collate_fn`.
  prefs: []
  type: TYPE_NORMAL
- en: '### Working with `collate_fn`[](#working-with-collate-fn "Permalink to this
    heading")'
  prefs: []
  type: TYPE_NORMAL
- en: The use of `collate_fn` is slightly different when automatic batching is enabled
    or disabled.
  prefs: []
  type: TYPE_NORMAL
- en: '**When automatic batching is disabled**, `collate_fn` is called with each individual
    data sample, and the output is yielded from the data loader iterator. In this
    case, the default `collate_fn` simply converts NumPy arrays in PyTorch tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: '**When automatic batching is enabled**, `collate_fn` is called with a list
    of data samples at each time. It is expected to collate the input samples into
    a batch for yielding from the data loader iterator. The rest of this section describes
    the behavior of the default `collate_fn` ([`default_collate()`](#torch.utils.data.default_collate
    "torch.utils.data.default_collate")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if each data sample consists of a 3-channel image and an integral
    class label, i.e., each element of the dataset returns a tuple `(image, class_index)`,
    the default `collate_fn` collates a list of such tuples into a single tuple of
    a batched image tensor and a batched class label Tensor. In particular, the default
    `collate_fn` has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: It always prepends a new dimension as the batch dimension.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It automatically converts NumPy arrays and Python numerical values into PyTorch
    Tensors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It preserves the data structure, e.g., if each sample is a dictionary, it outputs
    a dictionary with the same set of keys but batched Tensors as values (or lists
    if the values can not be converted into Tensors). Same for `list` s, `tuple` s,
    `namedtuple` s, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users may use customized `collate_fn` to achieve custom batching, e.g., collating
    along a dimension other than the first, padding sequences of various lengths,
    or adding support for custom data types.
  prefs: []
  type: TYPE_NORMAL
- en: If you run into a situation where the outputs of [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") have dimensions or type that is different from
    your expectation, you may want to check your `collate_fn`.
  prefs: []
  type: TYPE_NORMAL
- en: Single- and Multi-process Data Loading[](#single-and-multi-process-data-loading
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A [`DataLoader`](#torch.utils.data.DataLoader "torch.utils.data.DataLoader")
    uses single-process data loading by default.
  prefs: []
  type: TYPE_NORMAL
- en: Within a Python process, the [Global Interpreter Lock (GIL)](https://wiki.python.org/moin/GlobalInterpreterLock)
    prevents true fully parallelizing Python code across threads. To avoid blocking
    computation code with data loading, PyTorch provides an easy switch to perform
    multi-process data loading by simply setting the argument `num_workers` to a positive
    integer.
  prefs: []
  type: TYPE_NORMAL
- en: Single-process data loading (default)[](#single-process-data-loading-default
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this mode, data fetching is done in the same process a [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") is initialized. Therefore, data loading may block
    computing. However, this mode may be preferred when resource(s) used for sharing
    data among processes (e.g., shared memory, file descriptors) is limited, or when
    the entire dataset is small and can be loaded entirely in memory. Additionally,
    single-process loading often shows more readable error traces and thus is useful
    for debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-process data loading[](#multi-process-data-loading "Permalink to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Setting the argument `num_workers` as a positive integer will turn on multi-process
    data loading with the specified number of loader worker processes.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'After several iterations, the loader worker processes will consume the same
    amount of CPU memory as the parent process for all Python objects in the parent
    process which are accessed from the worker processes. This can be problematic
    if the Dataset contains a lot of data (e.g., you are loading a very large list
    of filenames at Dataset construction time) and/or you are using a lot of workers
    (overall memory usage is `number of workers * size of parent process`). The simplest
    workaround is to replace Python objects with non-refcounted representations such
    as Pandas, Numpy or PyArrow objects. Check out [issue #13246](https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662)
    for more details on why this occurs and example code for how to workaround these
    problems.'
  prefs: []
  type: TYPE_NORMAL
- en: In this mode, each time an iterator of a [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") is created (e.g., when you call `enumerate(dataloader)`),
    `num_workers` worker processes are created. At this point, the [`dataset`](utils.html#module-torch.utils.data.dataset
    "torch.utils.data.dataset"), `collate_fn`, and `worker_init_fn` are passed to
    each worker, where they are used to initialize, and fetch data. This means that
    dataset access together with its internal IO, transforms (including `collate_fn`)
    runs in the worker process.
  prefs: []
  type: TYPE_NORMAL
- en: '[`torch.utils.data.get_worker_info()`](#torch.utils.data.get_worker_info "torch.utils.data.get_worker_info")
    returns various useful information in a worker process (including the worker id,
    dataset replica, initial seed, etc.), and returns `None` in main process. Users
    may use this function in dataset code and/or `worker_init_fn` to individually
    configure each dataset replica, and to determine whether the code is running in
    a worker process. For example, this can be particularly helpful in sharding the
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: For map-style datasets, the main process generates the indices using [`sampler`](utils.html#module-torch.utils.data.sampler
    "torch.utils.data.sampler") and sends them to the workers. So any shuffle randomization
    is done in the main process which guides loading by assigning indices to load.
  prefs: []
  type: TYPE_NORMAL
- en: For iterable-style datasets, since each worker process gets a replica of the
    [`dataset`](utils.html#module-torch.utils.data.dataset "torch.utils.data.dataset")
    object, naive multi-process loading will often result in duplicated data. Using
    [`torch.utils.data.get_worker_info()`](#torch.utils.data.get_worker_info "torch.utils.data.get_worker_info")
    and/or `worker_init_fn`, users may configure each replica independently. (See
    [`IterableDataset`](#torch.utils.data.IterableDataset "torch.utils.data.IterableDataset")
    documentations for how to achieve this. ) For similar reasons, in multi-process
    loading, the `drop_last` argument drops the last non-full batch of each worker’s
    iterable-style dataset replica.
  prefs: []
  type: TYPE_NORMAL
- en: Workers are shut down once the end of the iteration is reached, or when the
    iterator becomes garbage collected.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: It is generally not recommended to return CUDA tensors in multi-process loading
    because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing
    (see [CUDA in multiprocessing](notes/multiprocessing.html#multiprocessing-cuda-note)).
    Instead, we recommend using [automatic memory pinning](#memory-pinning) (i.e.,
    setting `pin_memory=True`), which enables fast data transfer to CUDA-enabled GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Platform-specific behaviors[](#platform-specific-behaviors "Permalink to this
    heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since workers rely on Python [`multiprocessing`](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing
    "(in Python v3.12)"), worker launch behavior is different on Windows compared
    to Unix.
  prefs: []
  type: TYPE_NORMAL
- en: On Unix, `fork()` is the default [`multiprocessing`](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing
    "(in Python v3.12)") start method. Using `fork()`, child workers typically can
    access the [`dataset`](utils.html#module-torch.utils.data.dataset "torch.utils.data.dataset")
    and Python argument functions directly through the cloned address space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On Windows or MacOS, `spawn()` is the default [`multiprocessing`](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing
    "(in Python v3.12)") start method. Using `spawn()`, another interpreter is launched
    which runs your main script, followed by the internal worker function that receives
    the [`dataset`](utils.html#module-torch.utils.data.dataset "torch.utils.data.dataset"),
    `collate_fn` and other arguments through [`pickle`](https://docs.python.org/3/library/pickle.html#module-pickle
    "(in Python v3.12)") serialization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This separate serialization means that you should take two steps to ensure
    you are compatible with Windows while using multi-process data loading:'
  prefs: []
  type: TYPE_NORMAL
- en: Wrap most of you main script’s code within `if __name__ == '__main__':` block,
    to make sure it doesn’t run again (most likely generating error) when each worker
    process is launched. You can place your dataset and [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") instance creation logic here, as it doesn’t need
    to be re-executed in workers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure that any custom `collate_fn`, `worker_init_fn` or [`dataset`](utils.html#module-torch.utils.data.dataset
    "torch.utils.data.dataset") code is declared as top level definitions, outside
    of the `__main__` check. This ensures that they are available in worker processes.
    (this is needed since functions are pickled as references only, not `bytecode`.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### Randomness in multi-process data loading[](#randomness-in-multi-process-data-loading
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: By default, each worker will have its PyTorch seed set to `base_seed + worker_id`,
    where `base_seed` is a long generated by main process using its RNG (thereby,
    consuming a RNG state mandatorily) or a specified `generator`. However, seeds
    for other libraries may be duplicated upon initializing workers, causing each
    worker to return identical random numbers. (See [this section](notes/faq.html#dataloader-workers-random-seed)
    in FAQ.).
  prefs: []
  type: TYPE_NORMAL
- en: In `worker_init_fn`, you may access the PyTorch seed set for each worker with
    either [`torch.utils.data.get_worker_info().seed`](#torch.utils.data.get_worker_info
    "torch.utils.data.get_worker_info") or [`torch.initial_seed()`](generated/torch.initial_seed.html#torch.initial_seed
    "torch.initial_seed"), and use it to seed other libraries before data loading.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Pinning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Host to GPU copies are much faster when they originate from pinned (page-locked)
    memory. See [Use pinned memory buffers](notes/cuda.html#cuda-memory-pinning) for
    more details on when and how to use pinned memory generally.
  prefs: []
  type: TYPE_NORMAL
- en: For data loading, passing `pin_memory=True` to a [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") will automatically put the fetched data Tensors
    in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: The default memory pinning logic only recognizes Tensors and maps and iterables
    containing Tensors. By default, if the pinning logic sees a batch that is a custom
    type (which will occur if you have a `collate_fn` that returns a custom batch
    type), or if each element of your batch is a custom type, the pinning logic will
    not recognize them, and it will return that batch (or those elements) without
    pinning the memory. To enable memory pinning for custom batch or data type(s),
    define a `pin_memory()` method on your custom type(s).
  prefs: []
  type: TYPE_NORMAL
- en: See the example below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Data loader combines a dataset and a sampler, and provides an iterable over
    the given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The [`DataLoader`](#torch.utils.data.DataLoader "torch.utils.data.DataLoader")
    supports both map-style and iterable-style datasets with single- or multi-process
    loading, customizing loading order and optional automatic batching (collation)
    and memory pinning.
  prefs: []
  type: TYPE_NORMAL
- en: See [`torch.utils.data`](#module-torch.utils.data "torch.utils.data") documentation
    page for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**dataset** ([*Dataset*](#torch.utils.data.Dataset "torch.utils.data.Dataset"))
    – dataset from which to load the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* *optional*) – how many samples per batch to load (default:
    `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**shuffle** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – set to `True` to have the data reshuffled
    at every epoch (default: `False`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sampler** ([*Sampler*](#torch.utils.data.Sampler "torch.utils.data.Sampler")
    *or* *Iterable**,* *optional*) – defines the strategy to draw samples from the
    dataset. Can be any `Iterable` with `__len__` implemented. If specified, `shuffle`
    must not be specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_sampler** ([*Sampler*](#torch.utils.data.Sampler "torch.utils.data.Sampler")
    *or* *Iterable**,* *optional*) – like [`sampler`](utils.html#module-torch.utils.data.sampler
    "torch.utils.data.sampler"), but returns a batch of indices at a time. Mutually
    exclusive with `batch_size`, `shuffle`, [`sampler`](utils.html#module-torch.utils.data.sampler
    "torch.utils.data.sampler"), and `drop_last`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_workers** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* *optional*) – how many subprocesses to use for data loading.
    `0` means that the data will be loaded in the main process. (default: `0`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**collate_fn** (*Callable**,* *optional*) – merges a list of samples to form
    a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pin_memory** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If `True`, the data loader will copy Tensors
    into device/CUDA pinned memory before returning them. If your data elements are
    a custom type, or your `collate_fn` returns a batch that is a custom type, see
    the example below.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**drop_last** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – set to `True` to drop the last incomplete
    batch, if the dataset size is not divisible by the batch size. If `False` and
    the size of dataset is not divisible by the batch size, then the last batch will
    be smaller. (default: `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**timeout** (*numeric**,* *optional*) – if positive, the timeout value for
    collecting a batch from workers. Should always be non-negative. (default: `0`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**worker_init_fn** (*Callable**,* *optional*) – If not `None`, this will be
    called on each worker subprocess with the worker id (an int in `[0, num_workers
    - 1]`) as input, after seeding and before data loading. (default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**multiprocessing_context** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)") *or* *multiprocessing.context.BaseContext**,* *optional*)
    – If `None`, the default [multiprocessing context](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods)
    of your operating system will be used. (default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*torch.Generator*](generated/torch.Generator.html#torch.Generator
    "torch.Generator")*,* *optional*) – If not `None`, this RNG will be used by RandomSampler
    to generate random indexes and multiprocessing to generate `base_seed` for workers.
    (default: `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefetch_factor** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* *optional**,* *keyword-only arg*) – Number of batches
    loaded in advance by each worker. `2` means there will be a total of 2 * num_workers
    batches prefetched across all workers. (default value depends on the set value
    for num_workers. If value of num_workers=0 default is `None`. Otherwise, if value
    of `num_workers > 0` default is `2`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**persistent_workers** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If `True`, the data loader will not shut
    down the worker processes after a dataset has been consumed once. This allows
    to maintain the workers Dataset instances alive. (default: `False`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pin_memory_device** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* *optional*) – the device to `pin_memory` to if `pin_memory`
    is `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: If the `spawn` start method is used, `worker_init_fn` cannot be an unpicklable
    object, e.g., a lambda function. See [Multiprocessing best practices](notes/multiprocessing.html#multiprocessing-best-practices)
    on more details related to multiprocessing in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`len(dataloader)` heuristic is based on the length of the sampler used. When
    [`dataset`](utils.html#module-torch.utils.data.dataset "torch.utils.data.dataset")
    is an [`IterableDataset`](#torch.utils.data.IterableDataset "torch.utils.data.IterableDataset"),
    it instead returns an estimate based on `len(dataset) / batch_size`, with proper
    rounding depending on `drop_last`, regardless of multi-process loading configurations.
    This represents the best guess PyTorch can make because PyTorch trusts user [`dataset`](utils.html#module-torch.utils.data.dataset
    "torch.utils.data.dataset") code in correctly handling multi-process loading to
    avoid duplicate data.'
  prefs: []
  type: TYPE_NORMAL
- en: However, if sharding results in multiple workers having incomplete last batches,
    this estimate can still be inaccurate, because (1) an otherwise complete batch
    can be broken into multiple ones and (2) more than one batch worth of samples
    can be dropped when `drop_last` is set. Unfortunately, PyTorch can not detect
    such cases in general.
  prefs: []
  type: TYPE_NORMAL
- en: See [Dataset Types](#dataset-types) for more details on these two types of datasets
    and how [`IterableDataset`](#torch.utils.data.IterableDataset "torch.utils.data.IterableDataset")
    interacts with [Multi-process data loading](#multi-process-data-loading).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: See [Reproducibility](notes/randomness.html#reproducibility), and [My data loader
    workers return identical random numbers](notes/faq.html#dataloader-workers-random-seed),
    and [Randomness in multi-process data loading](#data-loading-randomness) notes
    for random seed related questions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: An abstract class representing a [`Dataset`](#torch.utils.data.Dataset "torch.utils.data.Dataset").
  prefs: []
  type: TYPE_NORMAL
- en: All datasets that represent a map from keys to data samples should subclass
    it. All subclasses should overwrite `__getitem__()`, supporting fetching a data
    sample for a given key. Subclasses could also optionally overwrite `__len__()`,
    which is expected to return the size of the dataset by many [`Sampler`](#torch.utils.data.Sampler
    "torch.utils.data.Sampler") implementations and the default options of [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader"). Subclasses could also optionally implement `__getitems__()`,
    for speedup batched samples loading. This method accepts list of indices of samples
    of batch and returns list of samples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '[`DataLoader`](#torch.utils.data.DataLoader "torch.utils.data.DataLoader")
    by default constructs an index sampler that yields integral indices. To make it
    work with a map-style dataset with non-integral indices/keys, a custom sampler
    must be provided.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: An iterable Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: All datasets that represent an iterable of data samples should subclass it.
    Such form of datasets is particularly useful when data come from a stream.
  prefs: []
  type: TYPE_NORMAL
- en: All subclasses should overwrite `__iter__()`, which would return an iterator
    of samples in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: When a subclass is used with [`DataLoader`](#torch.utils.data.DataLoader "torch.utils.data.DataLoader"),
    each item in the dataset will be yielded from the [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") iterator. When `num_workers > 0`, each worker process
    will have a different copy of the dataset object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the workers.
    [`get_worker_info()`](#torch.utils.data.get_worker_info "torch.utils.data.get_worker_info"),
    when called in a worker process, returns information about the worker. It can
    be used in either the dataset’s `__iter__()` method or the [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") ‘s `worker_init_fn` option to modify each copy’s
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: splitting workload across all workers in `__iter__()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Example 2: splitting workload across all workers using `worker_init_fn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Dataset wrapping tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Each sample will be retrieved by indexing tensors along the first dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '***tensors** ([*Tensor*](tensors.html#torch.Tensor "torch.Tensor")) – tensors
    that have the same size of the first dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Dataset as a stacking of multiple datasets.
  prefs: []
  type: TYPE_NORMAL
- en: This class is useful to assemble different parts of complex input data, given
    as datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '***args** ([*Dataset*](#torch.utils.data.Dataset "torch.utils.data.Dataset"))
    – Datasets for stacking returned as tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****kwargs** ([*Dataset*](#torch.utils.data.Dataset "torch.utils.data.Dataset"))
    – Datasets for stacking returned as dict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Dataset as a concatenation of multiple datasets.
  prefs: []
  type: TYPE_NORMAL
- en: This class is useful to assemble different existing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**datasets** (*sequence*) – List of datasets to be concatenated'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Dataset for chaining multiple [`IterableDataset`](#torch.utils.data.IterableDataset
    "torch.utils.data.IterableDataset") s.
  prefs: []
  type: TYPE_NORMAL
- en: This class is useful to assemble different existing dataset streams. The chaining
    operation is done on-the-fly, so concatenating large-scale datasets with this
    class will be efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**datasets** (*iterable* *of* [*IterableDataset*](#torch.utils.data.IterableDataset
    "torch.utils.data.IterableDataset")) – datasets to be chained together'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Subset of a dataset at specified indices.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**dataset** ([*Dataset*](#torch.utils.data.Dataset "torch.utils.data.Dataset"))
    – The whole Dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**indices** (*sequence*) – Indices in the whole set selected for subset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: General collate function that handles collection type of element within each
    batch.
  prefs: []
  type: TYPE_NORMAL
- en: The function also opens function registry to deal with specific element types.
    default_collate_fn_map provides default collate functions for tensors, numpy arrays,
    numbers and strings.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**batch** – a single batch to be collated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**collate_fn_map** ([*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")*[*[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict
    "(in Python v3.12)")*[*[*Union*](https://docs.python.org/3/library/typing.html#typing.Union
    "(in Python v3.12)")*[*[*Type*](https://docs.python.org/3/library/typing.html#typing.Type
    "(in Python v3.12)")*,* [*Tuple*](https://docs.python.org/3/library/typing.html#typing.Tuple
    "(in Python v3.12)")*[*[*Type*](https://docs.python.org/3/library/typing.html#typing.Type
    "(in Python v3.12)")*,* *...**]**]**,* [*Callable*](https://docs.python.org/3/library/typing.html#typing.Callable
    "(in Python v3.12)")*]**]*) – Optional dictionary mapping from element type to
    the corresponding collate function. If the element type isn’t present in this
    dictionary, this function will go through each key of the dictionary in the insertion
    order to invoke the corresponding collate function if the element type is a subclass
    of the key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Each collate function requires a positional argument for batch and a keyword
    argument for the dictionary of collate functions as collate_fn_map.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Take in a batch of data and put the elements within the batch into a tensor
    with an additional outer dimension - batch size.
  prefs: []
  type: TYPE_NORMAL
- en: The exact output type can be a [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor"),
    a Sequence of [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor"), a Collection
    of [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor"), or left unchanged,
    depending on the input type. This is used as the default function for collation
    when batch_size or batch_sampler is defined in [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader").
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the general input type (based on the type of the element within the
    batch) to output type mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor") -> [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor") (with an added outer dimension batch size)'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: NumPy Arrays -> [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor")
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: float -> [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor")
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: int -> [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor")
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: str -> str (unchanged)
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: bytes -> bytes (unchanged)
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Mapping[K, V_i] -> Mapping[K, default_collate([V_1, V_2, …])]
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: NamedTuple[V1_i, V2_i, …] -> NamedTuple[default_collate([V1_1, V1_2, …]), default_collate([V2_1,
    V2_2, …]), …]
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Sequence[V1_i, V2_i, …] -> Sequence[default_collate([V1_1, V1_2, …]), default_collate([V2_1,
    V2_2, …]), …]
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**batch** – a single batch to be collated'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Convert each NumPy array element into a [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor").
  prefs: []
  type: TYPE_NORMAL
- en: If the input is a Sequence, Collection, or Mapping, it tries to convert each
    element inside to a [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor").
    If the input is not an NumPy array, it is left unchanged. This is used as the
    default function for collation when both batch_sampler and batch_size are NOT
    defined in [`DataLoader`](#torch.utils.data.DataLoader "torch.utils.data.DataLoader").
  prefs: []
  type: TYPE_NORMAL
- en: The general input type to output type mapping is similar to that of [`default_collate()`](#torch.utils.data.default_collate
    "torch.utils.data.default_collate"). See the description there for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**data** – a single data point to be converted'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Returns the information about the current [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") iterator worker process.
  prefs: []
  type: TYPE_NORMAL
- en: 'When called in a worker, this returns an object guaranteed to have the following
    attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id`: the current worker id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers`: the total number of workers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed`: the random seed set for the current worker. This value is determined
    by main process RNG and the worker id. See [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader")’s documentation for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`dataset`](utils.html#module-torch.utils.data.dataset "torch.utils.data.dataset"):
    the copy of the dataset object in **this** process. Note that this will be a different
    object in a different process than the one in the main process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When called in the main process, this returns `None`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When used in a `worker_init_fn` passed over to [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader"), this method can be useful to set up each worker
    process differently, for instance, using `worker_id` to configure the `dataset`
    object to only read a specific fraction of a sharded dataset, or use `seed` to
    seed other libraries used in dataset code.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Optional*](https://docs.python.org/3/library/typing.html#typing.Optional
    "(in Python v3.12)")[*WorkerInfo*]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Randomly split a dataset into non-overlapping new datasets of given lengths.
  prefs: []
  type: TYPE_NORMAL
- en: If a list of fractions that sum up to 1 is given, the lengths will be computed
    automatically as floor(frac * len(dataset)) for each fraction provided.
  prefs: []
  type: TYPE_NORMAL
- en: After computing the lengths, if there are any remainders, 1 count will be distributed
    in round-robin fashion to the lengths until there are no remainders left.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optionally fix the generator for reproducible results, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**dataset** ([*Dataset*](#torch.utils.data.Dataset "torch.utils.data.Dataset"))
    – Dataset to be split'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lengths** (*sequence*) – lengths or fractions of splits to be produced'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Generator*](generated/torch.Generator.html#torch.Generator
    "torch.Generator")) – Generator used for the random permutation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*List*](https://docs.python.org/3/library/typing.html#typing.List "(in Python
    v3.12)")[[*Subset*](#torch.utils.data.Subset "torch.utils.data.dataset.Subset")[*T*]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Base class for all Samplers.
  prefs: []
  type: TYPE_NORMAL
- en: Every Sampler subclass has to provide an `__iter__()` method, providing a way
    to iterate over indices or lists of indices (batches) of dataset elements, and
    a `__len__()` method that returns the length of the returned iterators.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**data_source** ([*Dataset*](#torch.utils.data.Dataset "torch.utils.data.Dataset"))
    – This argument is not used and will be removed in 2.2.0. You may still have custom
    implementation that utilizes it.'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `__len__()` method isn’t strictly required by [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader"), but is expected in any calculation involving the
    length of a [`DataLoader`](#torch.utils.data.DataLoader "torch.utils.data.DataLoader").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Samples elements sequentially, always in the same order.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**data_source** ([*Dataset*](#torch.utils.data.Dataset "torch.utils.data.Dataset"))
    – dataset to sample from'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Samples elements randomly. If without replacement, then sample from a shuffled
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: If with replacement, then user can specify `num_samples` to draw.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**data_source** ([*Dataset*](#torch.utils.data.Dataset "torch.utils.data.Dataset"))
    – dataset to sample from'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**replacement** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – samples are drawn on-demand with replacement if `True`,
    default=``False``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_samples** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – number of samples to draw, default=`len(dataset)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Generator*](generated/torch.Generator.html#torch.Generator
    "torch.Generator")) – Generator used in sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Samples elements randomly from a given list of indices, without replacement.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**indices** (*sequence*) – a sequence of indices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Generator*](generated/torch.Generator.html#torch.Generator
    "torch.Generator")) – Generator used in sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Samples elements from `[0,..,len(weights)-1]` with given probabilities (weights).
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**weights** (*sequence*) – a sequence of weights, not necessary summing up
    to one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_samples** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – number of samples to draw'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**replacement** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – if `True`, samples are drawn with replacement. If not,
    they are drawn without replacement, which means that when a sample index is drawn
    for a row, it cannot be drawn again for that row.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** ([*Generator*](generated/torch.Generator.html#torch.Generator
    "torch.Generator")) – Generator used in sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Wraps another sampler to yield a mini-batch of indices.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**sampler** ([*Sampler*](#torch.utils.data.Sampler "torch.utils.data.Sampler")
    *or* *Iterable*) – Base sampler. Can be any iterable object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")) – Size of mini-batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**drop_last** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True`, the sampler will drop the last batch if its
    size would be less than `batch_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Sampler that restricts data loading to a subset of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: It is especially useful in conjunction with [`torch.nn.parallel.DistributedDataParallel`](generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel"). In such a case, each process can
    pass a `DistributedSampler` instance as a [`DataLoader`](#torch.utils.data.DataLoader
    "torch.utils.data.DataLoader") sampler, and load a subset of the original dataset
    that is exclusive to it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Dataset is assumed to be of constant size and that any instance of it always
    returns the same elements in the same order.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**dataset** – Dataset used for sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_replicas** ([*int*](https://docs.python.org/3/library/functions.html#int
    "(in Python v3.12)")*,* *optional*) – Number of processes participating in distributed
    training. By default, `world_size` is retrieved from the current distributed group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rank** ([*int*](https://docs.python.org/3/library/functions.html#int "(in
    Python v3.12)")*,* *optional*) – Rank of the current process within `num_replicas`.
    By default, `rank` is retrieved from the current distributed group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**shuffle** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If `True` (default), sampler will shuffle
    the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**seed** ([*int*](https://docs.python.org/3/library/functions.html#int "(in
    Python v3.12)")*,* *optional*) – random seed used to shuffle the sampler if `shuffle=True`.
    This number should be identical across all processes in the distributed group.
    Default: `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**drop_last** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – if `True`, then the sampler will drop the
    tail of the data to make it evenly divisible across the number of replicas. If
    `False`, the sampler will add extra indices to make the data evenly divisible
    across the replicas. Default: `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: In distributed mode, calling the `set_epoch()` method at the beginning of each
    epoch **before** creating the `DataLoader` iterator is necessary to make shuffling
    work properly across multiple epochs. Otherwise, the same ordering will be always
    used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
