- en: 'Pendulum: Writing your environment and transforms with TorchRL'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/advanced/pendulum.html](https://pytorch.org/tutorials/advanced/pendulum.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-advanced-pendulum-py) to download the full example
    code
  prefs: []
  type: TYPE_NORMAL
- en: '**Author**: [Vincent Moens](https://github.com/vmoens)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an environment (a simulator or an interface to a physical control system)
    is an integrative part of reinforcement learning and control engineering.
  prefs: []
  type: TYPE_NORMAL
- en: TorchRL provides a set of tools to do this in multiple contexts. This tutorial
    demonstrates how to use PyTorch and TorchRL code a pendulum simulator from the
    ground up. It is freely inspired by the Pendulum-v1 implementation from [OpenAI-Gym/Farama-Gymnasium
    control library](https://github.com/Farama-Foundation/Gymnasium).
  prefs: []
  type: TYPE_NORMAL
- en: '![Pendulum](../Images/6be04eb745758aea3a2462c7d6adf03a.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple Pendulum
  prefs: []
  type: TYPE_NORMAL
- en: 'Key learnings:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to design an environment in TorchRL: - Writing specs (input, observation
    and reward); - Implementing behavior: seeding, reset and step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming your environment inputs and outputs, and writing your own transforms;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") to carry arbitrary data structures through the
    `codebase`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the process, we will touch three crucial components of TorchRL:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[environments](https://pytorch.org/rl/reference/envs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[transforms](https://pytorch.org/rl/reference/envs.html#transforms)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[models (policy and value function)](https://pytorch.org/rl/reference/modules.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To give a sense of what can be achieved with TorchRL’s environments, we will
    be designing a *stateless* environment. While stateful environments keep track
    of the latest physical state encountered and rely on this to simulate the state-to-state
    transition, stateless environments expect the current state to be provided to
    them at each step, along with the action undertaken. TorchRL supports both types
    of environments, but stateless environments are more generic and hence cover a
    broader range of features of the environment API in TorchRL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modeling stateless environments gives users full control over the input and
    outputs of the simulator: one can reset an experiment at any stage or actively
    modify the dynamics from the outside. However, it assumes that we have some control
    over a task, which may not always be the case: solving a problem where we cannot
    control the current state is more challenging but has a much wider set of applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of stateless environments is that they can enable batched
    execution of transition simulations. If the backend and the implementation allow
    it, an algebraic operation can be executed seamlessly on scalars, vectors, or
    tensors. This tutorial gives such examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'This tutorial will be structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first get acquainted with the environment properties: its shape (`batch_size`),
    its methods (mainly [`step()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id4
    "(in torchrl vmain (0.4.0 ))"), [`reset()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id1
    "(in torchrl vmain (0.4.0 ))") and [`set_seed()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id3
    "(in torchrl vmain (0.4.0 ))")) and finally its specs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After having coded our simulator, we will demonstrate how it can be used during
    training with transforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will explore new avenues that follow from the TorchRL’s API, including:
    the possibility of transforming inputs, the vectorized execution of the simulation
    and the possibility of backpropagation through the simulation graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will train a simple policy to solve the system we implemented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are four things you must take care of when designing a new environment
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`EnvBase._reset()`, which codes for the resetting of the simulator at a (potentially
    random) initial state;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EnvBase._step()` which codes for the state transition dynamic;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EnvBase._set_seed`()` which implements the seeding mechanism;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the environment specs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us first describe the problem at hand: we would like to model a simple
    pendulum over which we can control the torque applied on its fixed point. Our
    goal is to place the pendulum in upward position (angular position at 0 by convention)
    and having it standing still in that position. To design our dynamic system, we
    need to define two equations: the motion equation following an action (the torque
    applied) and the reward equation that will constitute our objective function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the motion equation, we will update the angular velocity following:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\dot{\theta}_{t+1} = \dot{\theta}_t + (3 * g / (2 * L) * \sin(\theta_t) +
    3 / (m * L^2) * u) * dt\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\dot{\theta}\) is the angular velocity in rad/sec, \(g\) is the gravitational
    force, \(L\) is the pendulum length, \(m\) is its mass, \(\theta\) is its angular
    position and \(u\) is the torque. The angular position is then updated according
    to
  prefs: []
  type: TYPE_NORMAL
- en: \[\theta_{t+1} = \theta_{t} + \dot{\theta}_{t+1} dt\]
  prefs: []
  type: TYPE_NORMAL
- en: We define our reward as
  prefs: []
  type: TYPE_NORMAL
- en: \[r = -(\theta^2 + 0.1 * \dot{\theta}^2 + 0.001 * u^2)\]
  prefs: []
  type: TYPE_NORMAL
- en: which will be maximized when the angle is close to 0 (pendulum in upward position),
    the angular velocity is close to 0 (no motion) and the torque is 0 too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coding the effect of an action: `_step()`'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The step method is the first thing to consider, as it will encode the simulation
    that is of interest to us. In TorchRL, the [`EnvBase`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase
    "(in torchrl vmain (0.4.0 ))") class has a `EnvBase.step()` method that receives
    a [`tensordict.TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") instance with an `"action"` entry indicating
    what action is to be taken.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate the reading and writing from that `tensordict` and to make sure
    that the keys are consistent with what’s expected from the library, the simulation
    part has been delegated to a private abstract method `_step()` which reads input
    data from a `tensordict`, and writes a *new* `tensordict` with the output data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `_step()` method should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the input keys (such as `"action"`) and execute the simulation based on
    these;
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Retrieve observations, done state and reward;
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Write the set of observation values along with the reward and done state at
    the corresponding entries in a new `TensorDict`.
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the [`step()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id4
    "(in torchrl vmain (0.4.0 ))") method will merge the output of [`step()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id4
    "(in torchrl vmain (0.4.0 ))") in the input `tensordict` to enforce input/output
    consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, for stateful environments, this will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the root `tensordict` has not changed, the only modification is
    the appearance of a new `"next"` entry that contains the new information.
  prefs: []
  type: TYPE_NORMAL
- en: In the Pendulum example, our `_step()` method will read the relevant entries
    from the input `tensordict` and compute the position and velocity of the pendulum
    after the force encoded by the `"action"` key has been applied onto it. We compute
    the new angular position of the pendulum `"new_th"` as the result of the previous
    position `"th"` plus the new velocity `"new_thdot"` over a time interval `dt`.
  prefs: []
  type: TYPE_NORMAL
- en: Since our goal is to turn the pendulum up and maintain it still in that position,
    our `cost` (negative reward) function is lower for positions close to the target
    and low speeds. Indeed, we want to discourage positions that are far from being
    “upward” and/or speeds that are far from 0.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, `EnvBase._step()` is encoded as a static method since our environment
    is stateless. In stateful settings, the `self` argument is needed as the state
    needs to be read from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Resetting the simulator: `_reset()`'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second method we need to care about is the `_reset()` method. Like `_step()`,
    it should write the observation entries and possibly a done state in the `tensordict`
    it outputs (if the done state is omitted, it will be filled as `False` by the
    parent method [`reset()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id1
    "(in torchrl vmain (0.4.0 ))")). In some contexts, it is required that the `_reset`
    method receives a command from the function that called it (for example, in multi-agent
    settings we may want to indicate which agents need to be reset). This is why the
    `_reset()` method also expects a `tensordict` as input, albeit it may perfectly
    be empty or `None`.
  prefs: []
  type: TYPE_NORMAL
- en: The parent `EnvBase.reset()` does some simple checks like the `EnvBase.step()`
    does, such as making sure that a `"done"` state is returned in the output `tensordict`
    and that the shapes match what is expected from the specs.
  prefs: []
  type: TYPE_NORMAL
- en: For us, the only important thing to consider is whether `EnvBase._reset()` contains
    all the expected observations. Once more, since we are working with a stateless
    environment, we pass the configuration of the pendulum in a nested `tensordict`
    named `"params"`.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we do not pass a done state as this is not mandatory for `_reset()`
    and our environment is non-terminating, so we always expect it to be `False`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Environment metadata: `env.*_spec`'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The specs define the input and output domain of the environment. It is important
    that the specs accurately define the tensors that will be received at runtime,
    as they are often used to carry information about environments in multiprocessing
    and distributed settings. They can also be used to instantiate lazily defined
    neural networks and test scripts without actually querying the environment (which
    can be costly with real-world physical systems for instance).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four specs that we must code in our environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '`EnvBase.observation_spec`: This will be a [`CompositeSpec`](https://pytorch.org/rl/reference/generated/torchrl.data.CompositeSpec.html#torchrl.data.CompositeSpec
    "(in torchrl vmain (0.4.0 ))") instance where each key is an observation (a `CompositeSpec`
    can be viewed as a dictionary of specs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EnvBase.action_spec`: It can be any type of spec, but it is required that
    it corresponds to the `"action"` entry in the input `tensordict`;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EnvBase.reward_spec`: provides information about the reward space;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EnvBase.done_spec`: provides information about the space of the done flag.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TorchRL specs are organized in two general containers: `input_spec` which contains
    the specs of the information that the step function reads (divided between `action_spec`
    containing the action and `state_spec` containing all the rest), and `output_spec`
    which encodes the specs that the step outputs (`observation_spec`, `reward_spec`
    and `done_spec`). In general, you should not interact directly with `output_spec`
    and `input_spec` but only with their content: `observation_spec`, `reward_spec`,
    `done_spec`, `action_spec` and `state_spec`. The reason if that the specs are
    organized in a non-trivial way within `output_spec` and `input_spec` and neither
    of these should be directly modified.'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the `observation_spec` and related properties are convenient
    shortcuts to the content of the output and input spec containers.
  prefs: []
  type: TYPE_NORMAL
- en: TorchRL offers multiple [`TensorSpec`](https://pytorch.org/rl/reference/generated/torchrl.data.TensorSpec.html#torchrl.data.TensorSpec
    "(in torchrl vmain (0.4.0 ))") [subclasses](https://pytorch.org/rl/reference/data.html#tensorspec)
    to encode the environment’s input and output characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Specs shape
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The environment specs leading dimensions must match the environment batch-size.
    This is done to enforce that every component of an environment (including its
    transforms) have an accurate representation of the expected input and output shapes.
    This is something that should be accurately coded in stateful settings.
  prefs: []
  type: TYPE_NORMAL
- en: For non batch-locked environments, such as the one in our example (see below),
    this is irrelevant as the environment batch size will most likely be empty.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Reproducible experiments: seeding'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Seeding an environment is a common operation when initializing an experiment.
    The only goal of `EnvBase._set_seed()` is to set the seed of the contained simulator.
    If possible, this operation should not call `reset()` or interact with the environment
    execution. The parent `EnvBase.set_seed()` method incorporates a mechanism that
    allows seeding multiple environments with a different pseudo-random and reproducible
    seed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Wrapping things together: the [`EnvBase`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase
    "(in torchrl vmain (0.4.0 ))") class'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can finally put together the pieces and design our environment class. The
    specs initialization needs to be performed during the environment construction,
    so we must take care of calling the `_make_spec()` method within `PendulumEnv.__init__()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We add a static method `PendulumEnv.gen_params()` which deterministically generates
    a set of hyperparameters to be used during execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We define the environment as non-`batch_locked` by turning the `homonymous`
    attribute to `False`. This means that we will **not** enforce the input `tensordict`
    to have a `batch-size` that matches the one of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: The following code will just put together the pieces we have coded above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Testing our environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TorchRL provides a simple function [`check_env_specs()`](https://pytorch.org/rl/reference/generated/torchrl.envs.utils.check_env_specs.html#torchrl.envs.utils.check_env_specs
    "(in torchrl vmain (0.4.0 ))") to check that a (transformed) environment has an
    input/output structure that matches the one dictated by its specs. Let us try
    it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can have a look at our specs to have a visual representation of the environment
    signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can execute a couple of commands too to check that the output structure matches
    what is expected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can run the `env.rand_step()` to generate an action randomly from the `action_spec`
    domain. A `tensordict` containing the hyperparameters and the current state **must**
    be passed since our environment is stateless. In stateful contexts, `env.rand_step()`
    works perfectly too.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Transforming an environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Writing environment transforms for stateless simulators is slightly more complicated
    than for stateful ones: transforming an output entry that needs to be read at
    the following iteration requires to apply the inverse transform before calling
    `meth.step()` at the next step. This is an ideal scenario to showcase all the
    features of TorchRL’s transforms!'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the following transformed environment we `unsqueeze` the entries
    `["th", "thdot"]` to be able to stack them along the last dimension. We also pass
    them as `in_keys_inv` to squeeze them back to their original shape once they are
    passed as input in the next iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Writing custom transforms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TorchRL’s transforms may not cover all the operations one wants to execute
    after an environment has been executed. Writing a transform does not require much
    effort. As for the environment design, there are two steps in writing a transform:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting the dynamics right (forward and inverse);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapting the environment specs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A transform can be used in two settings: on its own, it can be used as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module
    "(in PyTorch v2.2)"). It can also be used appended to a [`TransformedEnv`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv
    "(in torchrl vmain (0.4.0 ))"). The structure of the class allows to customize
    the behavior in the different contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [`Transform`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.Transform.html#torchrl.envs.transforms.Transform
    "(in torchrl vmain (0.4.0 ))") skeleton can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There are three entry points (`forward()`, `_step()` and `inv()`) which all
    receive [`tensordict.TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") instances. The first two will eventually go
    through the keys indicated by `in_keys` and call `_apply_transform()` to each
    of these. The results will be written in the entries pointed by `Transform.out_keys`
    if provided (if not the `in_keys` will be updated with the transformed values).
    If inverse transforms need to be executed, a similar data flow will be executed
    but with the `Transform.inv()` and `Transform._inv_apply_transform()` methods
    and across the `in_keys_inv` and `out_keys_inv` list of keys. The following figure
    summarized this flow for environments and replay buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Transform API
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In some cases, a transform will not work on a subset of keys in a unitary manner,
    but will execute some operation on the parent environment or work with the entire
    input `tensordict`. In those cases, the `_call()` and `forward()` methods should
    be re-written, and the `_apply_transform()` method can be skipped.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us code new transforms that will compute the `sine` and `cosine` values
    of the position angle, as these values are more useful to us to learn a policy
    than the raw angle value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Concatenates the observations onto an “observation” entry. `del_keys=False`
    ensures that we keep these values for the next iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Once more, let us check that our environment specs match what is received:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Executing a rollout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Executing a rollout is a succession of simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: reset the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'while some condition is not met:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: compute an action given a policy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: execute a step given this action
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: collect the data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: make a `MDP` step
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: gather the data and return
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These operations have been conveniently wrapped in the [`rollout()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id2
    "(in torchrl vmain (0.4.0 ))") method, from which we provide a simplified version
    here below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Batching computations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last unexplored end of our tutorial is the ability that we have to batch
    computations in TorchRL. Because our environment does not make any assumptions
    regarding the input data shape, we can seamlessly execute it over batches of data.
    Even better: for non-batch-locked environments such as our Pendulum, we can change
    the batch size on the fly without recreating the environment. To do this, we just
    generate parameters with the desired shape.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing a rollout with a batch of data requires us to reset the environment
    out of the rollout function, since we need to define the batch_size dynamically
    and this is not supported by [`rollout()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id2
    "(in torchrl vmain (0.4.0 ))"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Training a simple policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we will train a simple policy using the reward as a differentiable
    objective, such as a negative loss. We will take advantage of the fact that our
    dynamic system is fully differentiable to backpropagate through the trajectory
    return and adjust the weights of our policy to maximize this value directly. Of
    course, in many settings many of the assumptions we make do not hold, such as
    differentiable system and full access to the underlying mechanics.
  prefs: []
  type: TYPE_NORMAL
- en: Still, this is a very simple example that showcases how a training loop can
    be coded with a custom environment in TorchRL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us first write the policy network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'and our optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Training loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will successively:'
  prefs: []
  type: TYPE_NORMAL
- en: generate a trajectory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sum the rewards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: backpropagate through the graph defined by these operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clip the gradient norm and make an optimization step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: repeat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the training loop, we should have a final reward close to 0 which
    demonstrates that the pendulum is upward and still as desired.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![returns, last reward](../Images/9f96ad35f80739a0aa9e5611f81b2cd0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this tutorial, we have learned how to code a stateless environment from
    scratch. We touched the subjects of:'
  prefs: []
  type: TYPE_NORMAL
- en: The four essential components that need to be taken care of when coding an environment
    (`step`, `reset`, seeding and building specs). We saw how these methods and classes
    interact with the [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") class;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to test that an environment is properly coded using [`check_env_specs()`](https://pytorch.org/rl/reference/generated/torchrl.envs.utils.check_env_specs.html#torchrl.envs.utils.check_env_specs
    "(in torchrl vmain (0.4.0 ))");
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to append transforms in the context of stateless environments and how to
    write custom transformations;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train a policy on a fully differentiable simulator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 2 minutes 30.147 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: pendulum.py`](../_downloads/0c4dc681209d8c964aae9de5e477d280/pendulum.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: pendulum.ipynb`](../_downloads/8016e5cfa285bd92b9684c45552fffcc/pendulum.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
