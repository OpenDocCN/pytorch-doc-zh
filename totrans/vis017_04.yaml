- en: Models and pre-trained weights¶
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The `torchvision.models` subpackage contains definitions of models for addressing
    different tasks, including: image classification, pixelwise semantic segmentation,
    object detection, instance segmentation, person keypoint detection, video classification,
    and optical flow.'
  prefs: []
  type: TYPE_NORMAL
- en: General information on pre-trained weights[¶](#general-information-on-pre-trained-weights
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TorchVision offers pre-trained weights for every provided architecture, using
    the PyTorch [`torch.hub`](https://pytorch.org/docs/stable/hub.html#module-torch.hub
    "(in PyTorch v2.2)"). Instancing a pre-trained model will download its weights
    to a cache directory. This directory can be set using the TORCH_HOME environment
    variable. See [`torch.hub.load_state_dict_from_url()`](https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url
    "(in PyTorch v2.2)") for details.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The pre-trained models provided in this library may have their own licenses
    or terms and conditions derived from the dataset used for training. It is your
    responsibility to determine whether you have permission to use the models for
    your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Backward compatibility is guaranteed for loading a serialized `state_dict` to
    the model created using old PyTorch version. On the contrary, loading entire saved
    models or serialized `ScriptModules` (serialized using older versions of PyTorch)
    may not preserve the historic behaviour. Refer to the following [documentation](https://pytorch.org/docs/stable/notes/serialization.html#id6)
  prefs: []
  type: TYPE_NORMAL
- en: Initializing pre-trained models[¶](#initializing-pre-trained-models "Permalink
    to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As of v0.13, TorchVision offers a new [Multi-weight support API](https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/)
    for loading different weights to the existing model builder methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Migrating to the new API is very straightforward. The following method calls
    between the 2 APIs are all equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `pretrained` parameter is now deprecated, using it will emit warnings
    and will be removed on v0.15.
  prefs: []
  type: TYPE_NORMAL
- en: Using the pre-trained models[¶](#using-the-pre-trained-models "Permalink to
    this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before using the pre-trained models, one must preprocess the image (resize with
    right resolution/interpolation, apply inference transforms, rescale the values
    etc). There is no standard way to do this as it depends on how a given model was
    trained. It can vary across model families, variants or even weight versions.
    Using the correct preprocessing method is critical and failing to do so may lead
    to decreased accuracy or incorrect outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the necessary information for the inference transforms of each pre-trained
    model is provided on its weights documentation. To simplify inference, TorchVision
    bundles the necessary preprocessing transforms into each model weight. These are
    accessible via the `weight.transforms` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Some models use modules which have different training and evaluation behavior,
    such as batch normalization. To switch between these modes, use `model.train()`
    or `model.eval()` as appropriate. See [`train()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train
    "(in PyTorch v2.2)") or [`eval()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval
    "(in PyTorch v2.2)") for details.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Listing and retrieving available models[¶](#listing-and-retrieving-available-models
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As of v0.14, TorchVision offers a new mechanism which allows listing and retrieving
    models and weights by their names. Here are a few examples on how to use them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the available public functions to retrieve models and their corresponding
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`get_model`](generated/torchvision.models.get_model.html#torchvision.models.get_model
    "torchvision.models.get_model")(name, **config) | Gets the model name and configuration
    and returns an instantiated model. |'
  prefs: []
  type: TYPE_TB
- en: '| [`get_model_weights`](generated/torchvision.models.get_model_weights.html#torchvision.models.get_model_weights
    "torchvision.models.get_model_weights")(name) | Returns the weights enum class
    associated to the given model. |'
  prefs: []
  type: TYPE_TB
- en: '| [`get_weight`](generated/torchvision.models.get_weight.html#torchvision.models.get_weight
    "torchvision.models.get_weight")(name) | Gets the weights enum value by its full
    name. |'
  prefs: []
  type: TYPE_TB
- en: '| [`list_models`](generated/torchvision.models.list_models.html#torchvision.models.list_models
    "torchvision.models.list_models")([module, include, exclude]) | Returns a list
    with the names of registered models. |'
  prefs: []
  type: TYPE_TB
- en: Using models from Hub[¶](#using-models-from-hub "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most pre-trained models can be accessed directly via PyTorch Hub without having
    TorchVision installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also retrieve all the available weights of a specific model via PyTorch
    Hub by doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The only exception to the above are the detection models included on `torchvision.models.detection`.
    These models require TorchVision to be installed because they depend on custom
    C++ operators.
  prefs: []
  type: TYPE_NORMAL
- en: Classification[¶](#classification "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following classification models are available, with or without pre-trained
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[AlexNet](models/alexnet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ConvNeXt](models/convnext.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DenseNet](models/densenet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[EfficientNet](models/efficientnet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[EfficientNetV2](models/efficientnetv2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GoogLeNet](models/googlenet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Inception V3](models/inception.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MaxVit](models/maxvit.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MNASNet](models/mnasnet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MobileNet V2](models/mobilenetv2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MobileNet V3](models/mobilenetv3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RegNet](models/regnet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ResNet](models/resnet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ResNeXt](models/resnext.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ShuffleNet V2](models/shufflenetv2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SqueezeNet](models/squeezenet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SwinTransformer](models/swin_transformer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[VGG](models/vgg.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[VisionTransformer](models/vision_transformer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Wide ResNet](models/wide_resnet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of how to use the pre-trained image classification models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The classes of the pre-trained model outputs can be found at `weights.meta["categories"]`.
  prefs: []
  type: TYPE_NORMAL
- en: Table of all available classification weights[¶](#table-of-all-available-classification-weights
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Accuracies are reported on ImageNet-1K using single crops:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Weight** | **Acc@1** | **Acc@5** | **Params** | **GFLOPS** | **Recipe**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`AlexNet_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.alexnet.html#torchvision.models.AlexNet_Weights
    "torchvision.models.AlexNet_Weights") | 56.522 | 79.066 | 61.1M | 0.71 | [link](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ConvNeXt_Base_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.convnext_base.html#torchvision.models.ConvNeXt_Base_Weights
    "torchvision.models.ConvNeXt_Base_Weights") | 84.062 | 96.87 | 88.6M | 15.36 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#convnext)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ConvNeXt_Large_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.convnext_large.html#torchvision.models.ConvNeXt_Large_Weights
    "torchvision.models.ConvNeXt_Large_Weights") | 84.414 | 96.976 | 197.8M | 34.36
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#convnext)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ConvNeXt_Small_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.convnext_small.html#torchvision.models.ConvNeXt_Small_Weights
    "torchvision.models.ConvNeXt_Small_Weights") | 83.616 | 96.65 | 50.2M | 8.68 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#convnext)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ConvNeXt_Tiny_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.convnext_tiny.html#torchvision.models.ConvNeXt_Tiny_Weights
    "torchvision.models.ConvNeXt_Tiny_Weights") | 82.52 | 96.146 | 28.6M | 4.46 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#convnext)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`DenseNet121_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.densenet121.html#torchvision.models.DenseNet121_Weights
    "torchvision.models.DenseNet121_Weights") | 74.434 | 91.972 | 8.0M | 2.83 | [link](https://github.com/pytorch/vision/pull/116)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`DenseNet161_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.densenet161.html#torchvision.models.DenseNet161_Weights
    "torchvision.models.DenseNet161_Weights") | 77.138 | 93.56 | 28.7M | 7.73 | [link](https://github.com/pytorch/vision/pull/116)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`DenseNet169_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.densenet169.html#torchvision.models.DenseNet169_Weights
    "torchvision.models.DenseNet169_Weights") | 75.6 | 92.806 | 14.1M | 3.36 | [link](https://github.com/pytorch/vision/pull/116)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`DenseNet201_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.densenet201.html#torchvision.models.DenseNet201_Weights
    "torchvision.models.DenseNet201_Weights") | 76.896 | 93.37 | 20.0M | 4.29 | [link](https://github.com/pytorch/vision/pull/116)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_B0_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.EfficientNet_B0_Weights
    "torchvision.models.EfficientNet_B0_Weights") | 77.692 | 93.532 | 5.3M | 0.39
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_B1_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b1.html#torchvision.models.EfficientNet_B1_Weights
    "torchvision.models.EfficientNet_B1_Weights") | 78.642 | 94.186 | 7.8M | 0.69
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_B1_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.efficientnet_b1.html#torchvision.models.EfficientNet_B1_Weights
    "torchvision.models.EfficientNet_B1_Weights") | 79.838 | 94.934 | 7.8M | 0.69
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-lr-wd-crop-tuning)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_B2_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights
    "torchvision.models.EfficientNet_B2_Weights") | 80.608 | 95.31 | 9.1M | 1.09 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_B3_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b3.html#torchvision.models.EfficientNet_B3_Weights
    "torchvision.models.EfficientNet_B3_Weights") | 82.008 | 96.054 | 12.2M | 1.83
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_B4_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b4.html#torchvision.models.EfficientNet_B4_Weights
    "torchvision.models.EfficientNet_B4_Weights") | 83.384 | 96.594 | 19.3M | 4.39
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_B5_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b5.html#torchvision.models.EfficientNet_B5_Weights
    "torchvision.models.EfficientNet_B5_Weights") | 83.444 | 96.628 | 30.4M | 10.27
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_B6_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b6.html#torchvision.models.EfficientNet_B6_Weights
    "torchvision.models.EfficientNet_B6_Weights") | 84.008 | 96.916 | 43.0M | 19.07
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_B7_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_b7.html#torchvision.models.EfficientNet_B7_Weights
    "torchvision.models.EfficientNet_B7_Weights") | 84.122 | 96.908 | 66.3M | 37.75
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_V2_L_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_v2_l.html#torchvision.models.EfficientNet_V2_L_Weights
    "torchvision.models.EfficientNet_V2_L_Weights") | 85.808 | 97.788 | 118.5M | 56.08
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_V2_M_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_v2_m.html#torchvision.models.EfficientNet_V2_M_Weights
    "torchvision.models.EfficientNet_V2_M_Weights") | 85.112 | 97.156 | 54.1M | 24.58
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`EfficientNet_V2_S_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.efficientnet_v2_s.html#torchvision.models.EfficientNet_V2_S_Weights
    "torchvision.models.EfficientNet_V2_S_Weights") | 84.228 | 96.878 | 21.5M | 8.37
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`GoogLeNet_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.googlenet.html#torchvision.models.GoogLeNet_Weights
    "torchvision.models.GoogLeNet_Weights") | 69.778 | 89.53 | 6.6M | 1.5 | [link](https://github.com/pytorch/vision/tree/main/references/classification#googlenet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Inception_V3_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.inception_v3.html#torchvision.models.Inception_V3_Weights
    "torchvision.models.Inception_V3_Weights") | 77.294 | 93.45 | 27.2M | 5.71 | [link](https://github.com/pytorch/vision/tree/main/references/classification#inception-v3)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MNASNet0_5_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mnasnet0_5.html#torchvision.models.MNASNet0_5_Weights
    "torchvision.models.MNASNet0_5_Weights") | 67.734 | 87.49 | 2.2M | 0.1 | [link](https://github.com/1e100/mnasnet_trainer)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MNASNet0_75_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mnasnet0_75.html#torchvision.models.MNASNet0_75_Weights
    "torchvision.models.MNASNet0_75_Weights") | 71.18 | 90.496 | 3.2M | 0.21 | [link](https://github.com/pytorch/vision/pull/6019)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MNASNet1_0_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mnasnet1_0.html#torchvision.models.MNASNet1_0_Weights
    "torchvision.models.MNASNet1_0_Weights") | 73.456 | 91.51 | 4.4M | 0.31 | [link](https://github.com/1e100/mnasnet_trainer)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MNASNet1_3_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mnasnet1_3.html#torchvision.models.MNASNet1_3_Weights
    "torchvision.models.MNASNet1_3_Weights") | 76.506 | 93.522 | 6.3M | 0.53 | [link](https://github.com/pytorch/vision/pull/6019)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MaxVit_T_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.maxvit_t.html#torchvision.models.MaxVit_T_Weights
    "torchvision.models.MaxVit_T_Weights") | 83.7 | 96.722 | 30.9M | 5.56 | [link](https://github.com/pytorch/vision/tree/main/references/classification#maxvit)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MobileNet_V2_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.MobileNet_V2_Weights
    "torchvision.models.MobileNet_V2_Weights") | 71.878 | 90.286 | 3.5M | 0.3 | [link](https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv2)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MobileNet_V2_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.MobileNet_V2_Weights
    "torchvision.models.MobileNet_V2_Weights") | 72.154 | 90.822 | 3.5M | 0.3 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MobileNet_V3_Large_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.MobileNet_V3_Large_Weights
    "torchvision.models.MobileNet_V3_Large_Weights") | 74.042 | 91.34 | 5.5M | 0.22
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv3-large--small)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MobileNet_V3_Large_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.MobileNet_V3_Large_Weights
    "torchvision.models.MobileNet_V3_Large_Weights") | 75.274 | 92.566 | 5.5M | 0.22
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MobileNet_V3_Small_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.mobilenet_v3_small.html#torchvision.models.MobileNet_V3_Small_Weights
    "torchvision.models.MobileNet_V3_Small_Weights") | 67.668 | 87.402 | 2.5M | 0.06
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv3-large--small)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_16GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_16gf.html#torchvision.models.RegNet_X_16GF_Weights
    "torchvision.models.RegNet_X_16GF_Weights") | 80.058 | 94.944 | 54.3M | 15.94
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#medium-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_16GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_16gf.html#torchvision.models.RegNet_X_16GF_Weights
    "torchvision.models.RegNet_X_16GF_Weights") | 82.716 | 96.196 | 54.3M | 15.94
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_1_6GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_1_6gf.html#torchvision.models.RegNet_X_1_6GF_Weights
    "torchvision.models.RegNet_X_1_6GF_Weights") | 77.04 | 93.44 | 9.2M | 1.6 | [link](https://github.com/pytorch/vision/tree/main/references/classification#small-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_1_6GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_1_6gf.html#torchvision.models.RegNet_X_1_6GF_Weights
    "torchvision.models.RegNet_X_1_6GF_Weights") | 79.668 | 94.922 | 9.2M | 1.6 |
    [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_32GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_32gf.html#torchvision.models.RegNet_X_32GF_Weights
    "torchvision.models.RegNet_X_32GF_Weights") | 80.622 | 95.248 | 107.8M | 31.74
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#large-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_32GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_32gf.html#torchvision.models.RegNet_X_32GF_Weights
    "torchvision.models.RegNet_X_32GF_Weights") | 83.014 | 96.288 | 107.8M | 31.74
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_3_2GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_3_2gf.html#torchvision.models.RegNet_X_3_2GF_Weights
    "torchvision.models.RegNet_X_3_2GF_Weights") | 78.364 | 93.992 | 15.3M | 3.18
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#medium-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_3_2GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_3_2gf.html#torchvision.models.RegNet_X_3_2GF_Weights
    "torchvision.models.RegNet_X_3_2GF_Weights") | 81.196 | 95.43 | 15.3M | 3.18 |
    [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_400MF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_400mf.html#torchvision.models.RegNet_X_400MF_Weights
    "torchvision.models.RegNet_X_400MF_Weights") | 72.834 | 90.95 | 5.5M | 0.41 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#small-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_400MF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_400mf.html#torchvision.models.RegNet_X_400MF_Weights
    "torchvision.models.RegNet_X_400MF_Weights") | 74.864 | 92.322 | 5.5M | 0.41 |
    [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_800MF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_800mf.html#torchvision.models.RegNet_X_800MF_Weights
    "torchvision.models.RegNet_X_800MF_Weights") | 75.212 | 92.348 | 7.3M | 0.8 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#small-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_800MF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_800mf.html#torchvision.models.RegNet_X_800MF_Weights
    "torchvision.models.RegNet_X_800MF_Weights") | 77.522 | 93.826 | 7.3M | 0.8 |
    [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_8GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_x_8gf.html#torchvision.models.RegNet_X_8GF_Weights
    "torchvision.models.RegNet_X_8GF_Weights") | 79.344 | 94.686 | 39.6M | 8 | [link](https://github.com/pytorch/vision/tree/main/references/classification#medium-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_X_8GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_x_8gf.html#torchvision.models.RegNet_X_8GF_Weights
    "torchvision.models.RegNet_X_8GF_Weights") | 81.682 | 95.678 | 39.6M | 8 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.regnet_y_128gf.html#torchvision.models.RegNet_Y_128GF_Weights
    "torchvision.models.RegNet_Y_128GF_Weights") | 88.228 | 98.682 | 644.8M | 374.57
    | [link](https://github.com/facebookresearch/SWAG) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.regnet_y_128gf.html#torchvision.models.RegNet_Y_128GF_Weights
    "torchvision.models.RegNet_Y_128GF_Weights") | 86.068 | 97.844 | 644.8M | 127.52
    | [link](https://github.com/pytorch/vision/pull/5793) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_16GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights
    "torchvision.models.RegNet_Y_16GF_Weights") | 80.424 | 95.24 | 83.6M | 15.91 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#large-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_16GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights
    "torchvision.models.RegNet_Y_16GF_Weights") | 82.886 | 96.328 | 83.6M | 15.91
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights
    "torchvision.models.RegNet_Y_16GF_Weights") | 86.012 | 98.054 | 83.6M | 46.73
    | [link](https://github.com/facebookresearch/SWAG) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights
    "torchvision.models.RegNet_Y_16GF_Weights") | 83.976 | 97.244 | 83.6M | 15.91
    | [link](https://github.com/pytorch/vision/pull/5793) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_1_6GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_1_6gf.html#torchvision.models.RegNet_Y_1_6GF_Weights
    "torchvision.models.RegNet_Y_1_6GF_Weights") | 77.95 | 93.966 | 11.2M | 1.61 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#small-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_1_6GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_1_6gf.html#torchvision.models.RegNet_Y_1_6GF_Weights
    "torchvision.models.RegNet_Y_1_6GF_Weights") | 80.876 | 95.444 | 11.2M | 1.61
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_32GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights
    "torchvision.models.RegNet_Y_32GF_Weights") | 80.878 | 95.34 | 145.0M | 32.28
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#large-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_32GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights
    "torchvision.models.RegNet_Y_32GF_Weights") | 83.368 | 96.498 | 145.0M | 32.28
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights
    "torchvision.models.RegNet_Y_32GF_Weights") | 86.838 | 98.362 | 145.0M | 94.83
    | [link](https://github.com/facebookresearch/SWAG) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights
    "torchvision.models.RegNet_Y_32GF_Weights") | 84.622 | 97.48 | 145.0M | 32.28
    | [link](https://github.com/pytorch/vision/pull/5793) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_3_2GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_3_2gf.html#torchvision.models.RegNet_Y_3_2GF_Weights
    "torchvision.models.RegNet_Y_3_2GF_Weights") | 78.948 | 94.576 | 19.4M | 3.18
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#medium-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_3_2GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_3_2gf.html#torchvision.models.RegNet_Y_3_2GF_Weights
    "torchvision.models.RegNet_Y_3_2GF_Weights") | 81.982 | 95.972 | 19.4M | 3.18
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_400MF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_400mf.html#torchvision.models.RegNet_Y_400MF_Weights
    "torchvision.models.RegNet_Y_400MF_Weights") | 74.046 | 91.716 | 4.3M | 0.4 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#small-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_400MF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_400mf.html#torchvision.models.RegNet_Y_400MF_Weights
    "torchvision.models.RegNet_Y_400MF_Weights") | 75.804 | 92.742 | 4.3M | 0.4 |
    [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_800MF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_800mf.html#torchvision.models.RegNet_Y_800MF_Weights
    "torchvision.models.RegNet_Y_800MF_Weights") | 76.42 | 93.136 | 6.4M | 0.83 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#small-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_800MF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_800mf.html#torchvision.models.RegNet_Y_800MF_Weights
    "torchvision.models.RegNet_Y_800MF_Weights") | 78.828 | 94.502 | 6.4M | 0.83 |
    [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_8GF_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.regnet_y_8gf.html#torchvision.models.RegNet_Y_8GF_Weights
    "torchvision.models.RegNet_Y_8GF_Weights") | 80.032 | 95.048 | 39.4M | 8.47 |
    [link](https://github.com/pytorch/vision/tree/main/references/classification#medium-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RegNet_Y_8GF_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.regnet_y_8gf.html#torchvision.models.RegNet_Y_8GF_Weights
    "torchvision.models.RegNet_Y_8GF_Weights") | 82.828 | 96.33 | 39.4M | 8.47 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNeXt101_32X8D_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnext101_32x8d.html#torchvision.models.ResNeXt101_32X8D_Weights
    "torchvision.models.ResNeXt101_32X8D_Weights") | 79.312 | 94.526 | 88.8M | 16.41
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#resnext)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNeXt101_32X8D_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.resnext101_32x8d.html#torchvision.models.ResNeXt101_32X8D_Weights
    "torchvision.models.ResNeXt101_32X8D_Weights") | 82.834 | 96.228 | 88.8M | 16.41
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNeXt101_64X4D_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnext101_64x4d.html#torchvision.models.ResNeXt101_64X4D_Weights
    "torchvision.models.ResNeXt101_64X4D_Weights") | 83.246 | 96.454 | 83.5M | 15.46
    | [link](https://github.com/pytorch/vision/pull/5935) |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNeXt50_32X4D_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnext50_32x4d.html#torchvision.models.ResNeXt50_32X4D_Weights
    "torchvision.models.ResNeXt50_32X4D_Weights") | 77.618 | 93.698 | 25.0M | 4.23
    | [link](https://github.com/pytorch/vision/tree/main/references/classification#resnext)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNeXt50_32X4D_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.resnext50_32x4d.html#torchvision.models.ResNeXt50_32X4D_Weights
    "torchvision.models.ResNeXt50_32X4D_Weights") | 81.198 | 95.34 | 25.0M | 4.23
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet101_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnet101.html#torchvision.models.ResNet101_Weights
    "torchvision.models.ResNet101_Weights") | 77.374 | 93.546 | 44.5M | 7.8 | [link](https://github.com/pytorch/vision/tree/main/references/classification#resnet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet101_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.resnet101.html#torchvision.models.ResNet101_Weights
    "torchvision.models.ResNet101_Weights") | 81.886 | 95.78 | 44.5M | 7.8 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet152_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnet152.html#torchvision.models.ResNet152_Weights
    "torchvision.models.ResNet152_Weights") | 78.312 | 94.046 | 60.2M | 11.51 | [link](https://github.com/pytorch/vision/tree/main/references/classification#resnet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet152_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.resnet152.html#torchvision.models.ResNet152_Weights
    "torchvision.models.ResNet152_Weights") | 82.284 | 96.002 | 60.2M | 11.51 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet18_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnet18.html#torchvision.models.ResNet18_Weights
    "torchvision.models.ResNet18_Weights") | 69.758 | 89.078 | 11.7M | 1.81 | [link](https://github.com/pytorch/vision/tree/main/references/classification#resnet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet34_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnet34.html#torchvision.models.ResNet34_Weights
    "torchvision.models.ResNet34_Weights") | 73.314 | 91.42 | 21.8M | 3.66 | [link](https://github.com/pytorch/vision/tree/main/references/classification#resnet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet50_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights
    "torchvision.models.ResNet50_Weights") | 76.13 | 92.862 | 25.6M | 4.09 | [link](https://github.com/pytorch/vision/tree/main/references/classification#resnet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet50_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights
    "torchvision.models.ResNet50_Weights") | 80.858 | 95.434 | 25.6M | 4.09 | [link](https://github.com/pytorch/vision/issues/3995#issuecomment-1013906621)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.shufflenet_v2_x0_5.html#torchvision.models.ShuffleNet_V2_X0_5_Weights
    "torchvision.models.ShuffleNet_V2_X0_5_Weights") | 60.552 | 81.746 | 1.4M | 0.04
    | [link](https://github.com/ericsun99/Shufflenet-v2-Pytorch) |'
  prefs: []
  type: TYPE_TB
- en: '| [`ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.shufflenet_v2_x1_0.html#torchvision.models.ShuffleNet_V2_X1_0_Weights
    "torchvision.models.ShuffleNet_V2_X1_0_Weights") | 69.362 | 88.316 | 2.3M | 0.14
    | [link](https://github.com/ericsun99/Shufflenet-v2-Pytorch) |'
  prefs: []
  type: TYPE_TB
- en: '| [`ShuffleNet_V2_X1_5_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.shufflenet_v2_x1_5.html#torchvision.models.ShuffleNet_V2_X1_5_Weights
    "torchvision.models.ShuffleNet_V2_X1_5_Weights") | 72.996 | 91.086 | 3.5M | 0.3
    | [link](https://github.com/pytorch/vision/pull/5906) |'
  prefs: []
  type: TYPE_TB
- en: '| [`ShuffleNet_V2_X2_0_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.shufflenet_v2_x2_0.html#torchvision.models.ShuffleNet_V2_X2_0_Weights
    "torchvision.models.ShuffleNet_V2_X2_0_Weights") | 76.23 | 93.006 | 7.4M | 0.58
    | [link](https://github.com/pytorch/vision/pull/5906) |'
  prefs: []
  type: TYPE_TB
- en: '| [`SqueezeNet1_0_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.squeezenet1_0.html#torchvision.models.SqueezeNet1_0_Weights
    "torchvision.models.SqueezeNet1_0_Weights") | 58.092 | 80.42 | 1.2M | 0.82 | [link](https://github.com/pytorch/vision/pull/49#issuecomment-277560717)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`SqueezeNet1_1_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.squeezenet1_1.html#torchvision.models.SqueezeNet1_1_Weights
    "torchvision.models.SqueezeNet1_1_Weights") | 58.178 | 80.624 | 1.2M | 0.35 |
    [link](https://github.com/pytorch/vision/pull/49#issuecomment-277560717) |'
  prefs: []
  type: TYPE_TB
- en: '| [`Swin_B_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_b.html#torchvision.models.Swin_B_Weights
    "torchvision.models.Swin_B_Weights") | 83.582 | 96.64 | 87.8M | 15.43 | [link](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Swin_S_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_s.html#torchvision.models.Swin_S_Weights
    "torchvision.models.Swin_S_Weights") | 83.196 | 96.36 | 49.6M | 8.74 | [link](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Swin_T_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_t.html#torchvision.models.Swin_T_Weights
    "torchvision.models.Swin_T_Weights") | 81.474 | 95.776 | 28.3M | 4.49 | [link](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Swin_V2_B_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_v2_b.html#torchvision.models.Swin_V2_B_Weights
    "torchvision.models.Swin_V2_B_Weights") | 84.112 | 96.864 | 87.9M | 20.32 | [link](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Swin_V2_S_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_v2_s.html#torchvision.models.Swin_V2_S_Weights
    "torchvision.models.Swin_V2_S_Weights") | 83.712 | 96.816 | 49.7M | 11.55 | [link](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Swin_V2_T_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.swin_v2_t.html#torchvision.models.Swin_V2_T_Weights
    "torchvision.models.Swin_V2_T_Weights") | 82.072 | 96.132 | 28.4M | 5.94 | [link](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`VGG11_BN_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg11_bn.html#torchvision.models.VGG11_BN_Weights
    "torchvision.models.VGG11_BN_Weights") | 70.37 | 89.81 | 132.9M | 7.61 | [link](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`VGG11_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg11.html#torchvision.models.VGG11_Weights
    "torchvision.models.VGG11_Weights") | 69.02 | 88.628 | 132.9M | 7.61 | [link](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`VGG13_BN_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg13_bn.html#torchvision.models.VGG13_BN_Weights
    "torchvision.models.VGG13_BN_Weights") | 71.586 | 90.374 | 133.1M | 11.31 | [link](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`VGG13_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg13.html#torchvision.models.VGG13_Weights
    "torchvision.models.VGG13_Weights") | 69.928 | 89.246 | 133.0M | 11.31 | [link](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`VGG16_BN_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg16_bn.html#torchvision.models.VGG16_BN_Weights
    "torchvision.models.VGG16_BN_Weights") | 73.36 | 91.516 | 138.4M | 15.47 | [link](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`VGG16_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights
    "torchvision.models.VGG16_Weights") | 71.592 | 90.382 | 138.4M | 15.47 | [link](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`VGG16_Weights.IMAGENET1K_FEATURES`](models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights
    "torchvision.models.VGG16_Weights") | nan | nan | 138.4M | 15.47 | [link](https://github.com/amdegroot/ssd.pytorch#training-ssd)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`VGG19_BN_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg19_bn.html#torchvision.models.VGG19_BN_Weights
    "torchvision.models.VGG19_BN_Weights") | 74.218 | 91.842 | 143.7M | 19.63 | [link](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`VGG19_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vgg19.html#torchvision.models.VGG19_Weights
    "torchvision.models.VGG19_Weights") | 72.376 | 90.876 | 143.7M | 19.63 | [link](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ViT_B_16_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights
    "torchvision.models.ViT_B_16_Weights") | 81.072 | 95.318 | 86.6M | 17.56 | [link](https://github.com/pytorch/vision/tree/main/references/classification#vit_b_16)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights
    "torchvision.models.ViT_B_16_Weights") | 85.304 | 97.65 | 86.9M | 55.48 | [link](https://github.com/facebookresearch/SWAG)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights
    "torchvision.models.ViT_B_16_Weights") | 81.886 | 96.18 | 86.6M | 17.56 | [link](https://github.com/pytorch/vision/pull/5793)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ViT_B_32_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vit_b_32.html#torchvision.models.ViT_B_32_Weights
    "torchvision.models.ViT_B_32_Weights") | 75.912 | 92.466 | 88.2M | 4.41 | [link](https://github.com/pytorch/vision/tree/main/references/classification#vit_b_32)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.vit_h_14.html#torchvision.models.ViT_H_14_Weights
    "torchvision.models.ViT_H_14_Weights") | 88.552 | 98.694 | 633.5M | 1016.72 |
    [link](https://github.com/facebookresearch/SWAG) |'
  prefs: []
  type: TYPE_TB
- en: '| [`ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.vit_h_14.html#torchvision.models.ViT_H_14_Weights
    "torchvision.models.ViT_H_14_Weights") | 85.708 | 97.73 | 632.0M | 167.29 | [link](https://github.com/pytorch/vision/pull/5793)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ViT_L_16_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights
    "torchvision.models.ViT_L_16_Weights") | 79.662 | 94.638 | 304.3M | 61.55 | [link](https://github.com/pytorch/vision/tree/main/references/classification#vit_l_16)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ViT_L_16_Weights.IMAGENET1K_SWAG_E2E_V1`](models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights
    "torchvision.models.ViT_L_16_Weights") | 88.064 | 98.512 | 305.2M | 361.99 | [link](https://github.com/facebookresearch/SWAG)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1`](models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights
    "torchvision.models.ViT_L_16_Weights") | 85.146 | 97.422 | 304.3M | 61.55 | [link](https://github.com/pytorch/vision/pull/5793)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ViT_L_32_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.vit_l_32.html#torchvision.models.ViT_L_32_Weights
    "torchvision.models.ViT_L_32_Weights") | 76.972 | 93.07 | 306.5M | 15.38 | [link](https://github.com/pytorch/vision/tree/main/references/classification#vit_l_32)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Wide_ResNet101_2_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.wide_resnet101_2.html#torchvision.models.Wide_ResNet101_2_Weights
    "torchvision.models.Wide_ResNet101_2_Weights") | 78.848 | 94.284 | 126.9M | 22.75
    | [link](https://github.com/pytorch/vision/pull/912#issue-445437439) |'
  prefs: []
  type: TYPE_TB
- en: '| [`Wide_ResNet101_2_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.wide_resnet101_2.html#torchvision.models.Wide_ResNet101_2_Weights
    "torchvision.models.Wide_ResNet101_2_Weights") | 82.51 | 96.02 | 126.9M | 22.75
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |'
  prefs: []
  type: TYPE_TB
- en: '| [`Wide_ResNet50_2_Weights.IMAGENET1K_V1`](models/generated/torchvision.models.wide_resnet50_2.html#torchvision.models.Wide_ResNet50_2_Weights
    "torchvision.models.Wide_ResNet50_2_Weights") | 78.468 | 94.086 | 68.9M | 11.4
    | [link](https://github.com/pytorch/vision/pull/912#issue-445437439) |'
  prefs: []
  type: TYPE_TB
- en: '| [`Wide_ResNet50_2_Weights.IMAGENET1K_V2`](models/generated/torchvision.models.wide_resnet50_2.html#torchvision.models.Wide_ResNet50_2_Weights
    "torchvision.models.Wide_ResNet50_2_Weights") | 81.602 | 95.758 | 68.9M | 11.4
    | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres)
    |'
  prefs: []
  type: TYPE_TB
- en: Quantized models[¶](#quantized-models "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following architectures provide support for INT8 quantized models, with
    or without pre-trained weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Quantized GoogLeNet](models/googlenet_quant.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quantized InceptionV3](models/inception_quant.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quantized MobileNet V2](models/mobilenetv2_quant.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quantized MobileNet V3](models/mobilenetv3_quant.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quantized ResNet](models/resnet_quant.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quantized ResNeXt](models/resnext_quant.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quantized ShuffleNet V2](models/shufflenetv2_quant.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of how to use the pre-trained quantized image classification
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The classes of the pre-trained model outputs can be found at `weights.meta["categories"]`.
  prefs: []
  type: TYPE_NORMAL
- en: Table of all available quantized classification weights[¶](#table-of-all-available-quantized-classification-weights
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Accuracies are reported on ImageNet-1K using single crops:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Weight** | **Acc@1** | **Acc@5** | **Params** | **GIPS** | **Recipe** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`GoogLeNet_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.googlenet.html#torchvision.models.quantization.GoogLeNet_QuantizedWeights
    "torchvision.models.quantization.GoogLeNet_QuantizedWeights") | 69.826 | 89.404
    | 6.6M | 1.5 | [link](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Inception_V3_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.inception_v3.html#torchvision.models.quantization.Inception_V3_QuantizedWeights
    "torchvision.models.quantization.Inception_V3_QuantizedWeights") | 77.176 | 93.354
    | 27.2M | 5.71 | [link](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MobileNet_V2_QuantizedWeights.IMAGENET1K_QNNPACK_V1`](models/generated/torchvision.models.quantization.mobilenet_v2.html#torchvision.models.quantization.MobileNet_V2_QuantizedWeights
    "torchvision.models.quantization.MobileNet_V2_QuantizedWeights") | 71.658 | 90.15
    | 3.5M | 0.3 | [link](https://github.com/pytorch/vision/tree/main/references/classification#qat-mobilenetv2)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MobileNet_V3_Large_QuantizedWeights.IMAGENET1K_QNNPACK_V1`](models/generated/torchvision.models.quantization.mobilenet_v3_large.html#torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights
    "torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights") | 73.004
    | 90.858 | 5.5M | 0.22 | [link](https://github.com/pytorch/vision/tree/main/references/classification#qat-mobilenetv3)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNeXt101_32X8D_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.resnext101_32x8d.html#torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights
    "torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights") | 78.986
    | 94.48 | 88.8M | 16.41 | [link](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNeXt101_32X8D_QuantizedWeights.IMAGENET1K_FBGEMM_V2`](models/generated/torchvision.models.quantization.resnext101_32x8d.html#torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights
    "torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights") | 82.574
    | 96.132 | 88.8M | 16.41 | [link](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNeXt101_64X4D_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.resnext101_64x4d.html#torchvision.models.quantization.ResNeXt101_64X4D_QuantizedWeights
    "torchvision.models.quantization.ResNeXt101_64X4D_QuantizedWeights") | 82.898
    | 96.326 | 83.5M | 15.46 | [link](https://github.com/pytorch/vision/pull/5935)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet18_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.resnet18.html#torchvision.models.quantization.ResNet18_QuantizedWeights
    "torchvision.models.quantization.ResNet18_QuantizedWeights") | 69.494 | 88.882
    | 11.7M | 1.81 | [link](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet50_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.resnet50.html#torchvision.models.quantization.ResNet50_QuantizedWeights
    "torchvision.models.quantization.ResNet50_QuantizedWeights") | 75.92 | 92.814
    | 25.6M | 4.09 | [link](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ResNet50_QuantizedWeights.IMAGENET1K_FBGEMM_V2`](models/generated/torchvision.models.quantization.resnet50.html#torchvision.models.quantization.ResNet50_QuantizedWeights
    "torchvision.models.quantization.ResNet50_QuantizedWeights") | 80.282 | 94.976
    | 25.6M | 4.09 | [link](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ShuffleNet_V2_X0_5_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.shufflenet_v2_x0_5.html#torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights
    "torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights") | 57.972
    | 79.78 | 1.4M | 0.04 | [link](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ShuffleNet_V2_X1_0_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.shufflenet_v2_x1_0.html#torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights
    "torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights") | 68.36
    | 87.582 | 2.3M | 0.14 | [link](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`ShuffleNet_V2_X1_5_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.shufflenet_v2_x1_5.html#torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights
    "torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights") | 72.052
    | 90.7 | 3.5M | 0.3 | [link](https://github.com/pytorch/vision/pull/5906) |'
  prefs: []
  type: TYPE_TB
- en: '| [`ShuffleNet_V2_X2_0_QuantizedWeights.IMAGENET1K_FBGEMM_V1`](models/generated/torchvision.models.quantization.shufflenet_v2_x2_0.html#torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights
    "torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights") | 75.354
    | 92.488 | 7.4M | 0.58 | [link](https://github.com/pytorch/vision/pull/5906) |'
  prefs: []
  type: TYPE_TB
- en: Semantic Segmentation[¶](#semantic-segmentation "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The segmentation module is in Beta stage, and backward compatibility is not
    guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following semantic segmentation models are available, with or without pre-trained
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepLabV3](models/deeplabv3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FCN](models/fcn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LRASPP](models/lraspp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of how to use the pre-trained semantic segmentation models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The classes of the pre-trained model outputs can be found at `weights.meta["categories"]`.
    The output format of the models is illustrated in [Semantic segmentation models](auto_examples/others/plot_visualization_utils.html#semantic-seg-output).
  prefs: []
  type: TYPE_NORMAL
- en: Table of all available semantic segmentation weights[¶](#table-of-all-available-semantic-segmentation-weights
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All models are evaluated a subset of COCO val2017, on the 20 categories that
    are present in the Pascal VOC dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Weight** | **Mean IoU** | **pixelwise Acc** | **Params** | **GFLOPS** |
    **Recipe** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.deeplabv3_mobilenet_v3_large.html#torchvision.models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights
    "torchvision.models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights") | 60.3
    | 91.2 | 11.0M | 10.45 | [link](https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_mobilenet_v3_large)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.deeplabv3_resnet101.html#torchvision.models.segmentation.DeepLabV3_ResNet101_Weights
    "torchvision.models.segmentation.DeepLabV3_ResNet101_Weights") | 67.4 | 92.4 |
    61.0M | 258.74 | [link](https://github.com/pytorch/vision/tree/main/references/segmentation#fcn_resnet101)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.deeplabv3_resnet50.html#torchvision.models.segmentation.DeepLabV3_ResNet50_Weights
    "torchvision.models.segmentation.DeepLabV3_ResNet50_Weights") | 66.4 | 92.4 |
    42.0M | 178.72 | [link](https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_resnet50)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`FCN_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.fcn_resnet101.html#torchvision.models.segmentation.FCN_ResNet101_Weights
    "torchvision.models.segmentation.FCN_ResNet101_Weights") | 63.7 | 91.9 | 54.3M
    | 232.74 | [link](https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_resnet101)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.fcn_resnet50.html#torchvision.models.segmentation.FCN_ResNet50_Weights
    "torchvision.models.segmentation.FCN_ResNet50_Weights") | 60.5 | 91.4 | 35.3M
    | 152.72 | [link](https://github.com/pytorch/vision/tree/main/references/segmentation#fcn_resnet50)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1`](models/generated/torchvision.models.segmentation.lraspp_mobilenet_v3_large.html#torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights
    "torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights") | 57.9 |
    91.2 | 3.2M | 2.09 | [link](https://github.com/pytorch/vision/tree/main/references/segmentation#lraspp_mobilenet_v3_large)
    |'
  prefs: []
  type: TYPE_TB
- en: '## Object Detection, Instance Segmentation and Person Keypoint Detection[¶](#object-detection-instance-segmentation-and-person-keypoint-detection
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: The pre-trained models for detection, instance segmentation and keypoint detection
    are initialized with the classification models in torchvision. The models expect
    a list of `Tensor[C, H, W]`. Check the constructor of the models for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The detection module is in Beta stage, and backward compatibility is not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: Object Detection[¶](#object-detection "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following object detection models are available, with or without pre-trained
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Faster R-CNN](models/faster_rcnn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FCOS](models/fcos.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RetinaNet](models/retinanet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SSD](models/ssd.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SSDlite](models/ssdlite.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of how to use the pre-trained object detection models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The classes of the pre-trained model outputs can be found at `weights.meta["categories"]`.
    For details on how to plot the bounding boxes of the models, you may refer to
    [Instance segmentation models](auto_examples/others/plot_visualization_utils.html#instance-seg-output).
  prefs: []
  type: TYPE_NORMAL
- en: Table of all available Object detection weights[¶](#table-of-all-available-object-detection-weights
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Box MAPs are reported on COCO val2017:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Weight** | **Box MAP** | **Params** | **GFLOPS** | **Recipe** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`FCOS_ResNet50_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.fcos_resnet50_fpn.html#torchvision.models.detection.FCOS_ResNet50_FPN_Weights
    "torchvision.models.detection.FCOS_ResNet50_FPN_Weights") | 39.2 | 32.3M | 128.21
    | [link](https://github.com/pytorch/vision/tree/main/references/detection#fcos-resnet-50-fpn)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn.html#torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights
    "torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights")
    | 22.8 | 19.4M | 0.72 | [link](https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-mobilenetv3-large-320-fpn)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn.html#torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights
    "torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights") | 32.8
    | 19.4M | 4.49 | [link](https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-mobilenetv3-large-fpn)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1`](models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn_v2.html#torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights
    "torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights") | 46.7 | 43.7M
    | 280.37 | [link](https://github.com/pytorch/vision/pull/5763) |'
  prefs: []
  type: TYPE_TB
- en: '| [`FasterRCNN_ResNet50_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights
    "torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights") | 37 | 41.8M |
    134.38 | [link](https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-resnet-50-fpn)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1`](models/generated/torchvision.models.detection.retinanet_resnet50_fpn_v2.html#torchvision.models.detection.RetinaNet_ResNet50_FPN_V2_Weights
    "torchvision.models.detection.RetinaNet_ResNet50_FPN_V2_Weights") | 41.5 | 38.2M
    | 152.24 | [link](https://github.com/pytorch/vision/pull/5756) |'
  prefs: []
  type: TYPE_TB
- en: '| [`RetinaNet_ResNet50_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.retinanet_resnet50_fpn.html#torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights
    "torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights") | 36.4 | 34.0M
    | 151.54 | [link](https://github.com/pytorch/vision/tree/main/references/detection#retinanet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`SSD300_VGG16_Weights.COCO_V1`](models/generated/torchvision.models.detection.ssd300_vgg16.html#torchvision.models.detection.SSD300_VGG16_Weights
    "torchvision.models.detection.SSD300_VGG16_Weights") | 25.1 | 35.6M | 34.86 |
    [link](https://github.com/pytorch/vision/tree/main/references/detection#ssd300-vgg16)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`SSDLite320_MobileNet_V3_Large_Weights.COCO_V1`](models/generated/torchvision.models.detection.ssdlite320_mobilenet_v3_large.html#torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights
    "torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights") | 21.3 |
    3.4M | 0.58 | [link](https://github.com/pytorch/vision/tree/main/references/detection#ssdlite320-mobilenetv3-large)
    |'
  prefs: []
  type: TYPE_TB
- en: Instance Segmentation[¶](#instance-segmentation "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following instance segmentation models are available, with or without pre-trained
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Mask R-CNN](models/mask_rcnn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For details on how to plot the masks of the models, you may refer to [Instance
    segmentation models](auto_examples/others/plot_visualization_utils.html#instance-seg-output).
  prefs: []
  type: TYPE_NORMAL
- en: Table of all available Instance segmentation weights[¶](#table-of-all-available-instance-segmentation-weights
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Box and Mask MAPs are reported on COCO val2017:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Weight** | **Box MAP** | **Mask MAP** | **Params** | **GFLOPS** | **Recipe**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1`](models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn_v2.html#torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights
    "torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights") | 47.4 | 41.8
    | 46.4M | 333.58 | [link](https://github.com/pytorch/vision/pull/5773) |'
  prefs: []
  type: TYPE_TB
- en: '| [`MaskRCNN_ResNet50_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html#torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights
    "torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights") | 37.9 | 34.6 |
    44.4M | 134.38 | [link](https://github.com/pytorch/vision/tree/main/references/detection#mask-r-cnn)
    |'
  prefs: []
  type: TYPE_TB
- en: Keypoint Detection[¶](#keypoint-detection "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following person keypoint detection models are available, with or without
    pre-trained weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Keypoint R-CNN](models/keypoint_rcnn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classes of the pre-trained model outputs can be found at `weights.meta["keypoint_names"]`.
    For details on how to plot the bounding boxes of the models, you may refer to
    [Visualizing keypoints](auto_examples/others/plot_visualization_utils.html#keypoint-output).
  prefs: []
  type: TYPE_NORMAL
- en: Table of all available Keypoint detection weights[¶](#table-of-all-available-keypoint-detection-weights
    "Permalink to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Box and Keypoint MAPs are reported on COCO val2017:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Weight** | **Box MAP** | **Keypoint MAP** | **Params** | **GFLOPS** | **Recipe**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`KeypointRCNN_ResNet50_FPN_Weights.COCO_LEGACY`](models/generated/torchvision.models.detection.keypointrcnn_resnet50_fpn.html#torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights
    "torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights") | 50.6 | 61.1
    | 59.1M | 133.92 | [link](https://github.com/pytorch/vision/issues/1606) |'
  prefs: []
  type: TYPE_TB
- en: '| [`KeypointRCNN_ResNet50_FPN_Weights.COCO_V1`](models/generated/torchvision.models.detection.keypointrcnn_resnet50_fpn.html#torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights
    "torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights") | 54.6 | 65
    | 59.1M | 137.42 | [link](https://github.com/pytorch/vision/tree/main/references/detection#keypoint-r-cnn)
    |'
  prefs: []
  type: TYPE_TB
- en: Video Classification[¶](#video-classification "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The video module is in Beta stage, and backward compatibility is not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following video classification models are available, with or without pre-trained
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Video MViT](models/video_mvit.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Video ResNet](models/video_resnet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Video S3D](models/video_s3d.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Video SwinTransformer](models/video_swin_transformer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of how to use the pre-trained video classification models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The classes of the pre-trained model outputs can be found at `weights.meta["categories"]`.
  prefs: []
  type: TYPE_NORMAL
- en: Table of all available video classification weights[¶](#table-of-all-available-video-classification-weights
    "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Accuracies are reported on Kinetics-400 using single crops for clip length
    16:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Weight** | **Acc@1** | **Acc@5** | **Params** | **GFLOPS** | **Recipe**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`MC3_18_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.mc3_18.html#torchvision.models.video.MC3_18_Weights
    "torchvision.models.video.MC3_18_Weights") | 63.96 | 84.13 | 11.7M | 43.34 | [link](https://github.com/pytorch/vision/tree/main/references/video_classification)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MViT_V1_B_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.mvit_v1_b.html#torchvision.models.video.MViT_V1_B_Weights
    "torchvision.models.video.MViT_V1_B_Weights") | 78.477 | 93.582 | 36.6M | 70.6
    | [link](https://github.com/facebookresearch/pytorchvideo/blob/main/docs/source/model_zoo.md)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`MViT_V2_S_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.mvit_v2_s.html#torchvision.models.video.MViT_V2_S_Weights
    "torchvision.models.video.MViT_V2_S_Weights") | 80.757 | 94.665 | 34.5M | 64.22
    | [link](https://github.com/facebookresearch/SlowFast/blob/main/MODEL_ZOO.md)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`R2Plus1D_18_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.r2plus1d_18.html#torchvision.models.video.R2Plus1D_18_Weights
    "torchvision.models.video.R2Plus1D_18_Weights") | 67.463 | 86.175 | 31.5M | 40.52
    | [link](https://github.com/pytorch/vision/tree/main/references/video_classification)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`R3D_18_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.r3d_18.html#torchvision.models.video.R3D_18_Weights
    "torchvision.models.video.R3D_18_Weights") | 63.2 | 83.479 | 33.4M | 40.7 | [link](https://github.com/pytorch/vision/tree/main/references/video_classification)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`S3D_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.s3d.html#torchvision.models.video.S3D_Weights
    "torchvision.models.video.S3D_Weights") | 68.368 | 88.05 | 8.3M | 17.98 | [link](https://github.com/pytorch/vision/tree/main/references/video_classification#s3d)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Swin3D_B_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.swin3d_b.html#torchvision.models.video.Swin3D_B_Weights
    "torchvision.models.video.Swin3D_B_Weights") | 79.427 | 94.386 | 88.0M | 140.67
    | [link](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Swin3D_B_Weights.KINETICS400_IMAGENET22K_V1`](models/generated/torchvision.models.video.swin3d_b.html#torchvision.models.video.Swin3D_B_Weights
    "torchvision.models.video.Swin3D_B_Weights") | 81.643 | 95.574 | 88.0M | 140.67
    | [link](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Swin3D_S_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.swin3d_s.html#torchvision.models.video.Swin3D_S_Weights
    "torchvision.models.video.Swin3D_S_Weights") | 79.521 | 94.158 | 49.8M | 82.84
    | [link](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Swin3D_T_Weights.KINETICS400_V1`](models/generated/torchvision.models.video.swin3d_t.html#torchvision.models.video.Swin3D_T_Weights
    "torchvision.models.video.Swin3D_T_Weights") | 77.715 | 93.519 | 28.2M | 43.88
    | [link](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400)
    |'
  prefs: []
  type: TYPE_TB
- en: Optical Flow[¶](#optical-flow "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following Optical Flow models are available, with or without pre-trained
  prefs: []
  type: TYPE_NORMAL
- en: '[RAFT](models/raft.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
