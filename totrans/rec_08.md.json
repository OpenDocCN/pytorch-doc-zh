["```py\nclass torchrec.inference.model_packager.PredictFactoryPackager\u00b6\n```", "```py\nclassmethod save_predict_factory(predict_factory: ~typing.Type[~torchrec.inference.modules.PredictFactory], configs: ~typing.Dict[str, ~typing.Any], output: ~typing.Union[str, ~pathlib.Path, ~typing.BinaryIO], extra_files: ~typing.Dict[str, ~typing.Union[str, bytes]], loader_code: str = '\\nimport %PACKAGE%\\n\\nMODULE_FACTORY=%PACKAGE%.%CLASS%\\n', package_importer: ~typing.Union[~torch.package.importer.Importer, ~typing.List[~torch.package.importer.Importer]] = <torch.package.importer._SysImporter object>) \u2192 None\u00b6\n```", "```py\nabstract classmethod set_extern_modules()\u00b6\n```", "```py\nabstract classmethod set_mocked_modules()\u00b6\n```", "```py\ntorchrec.inference.model_packager.load_config_text(name: str) \u2192 str\u00b6\n```", "```py\ntorchrec.inference.model_packager.load_pickle_config(name: str, clazz: Type[T]) \u2192 T\u00b6\n```", "```py\nclass torchrec.inference.modules.BatchingMetadata(type: str, device: str, pinned: List[str])\u00b6\n```", "```py\ndevice: str\u00b6\n```", "```py\npinned: List[str]\u00b6\n```", "```py\ntype: str\u00b6\n```", "```py\nclass torchrec.inference.modules.PredictFactory\u00b6\n```", "```py\nabstract batching_metadata() \u2192 Dict[str, BatchingMetadata]\u00b6\n```", "```py\nbatching_metadata_json() \u2192 str\u00b6\n```", "```py\nabstract create_predict_module() \u2192 Module\u00b6\n```", "```py\nmodel_inputs_data() \u2192 Dict[str, Any]\u00b6\n```", "```py\nqualname_metadata() \u2192 Dict[str, QualNameMetadata]\u00b6\n```", "```py\nqualname_metadata_json() \u2192 str\u00b6\n```", "```py\nabstract result_metadata() \u2192 str\u00b6\n```", "```py\nabstract run_weights_dependent_transformations(predict_module: Module) \u2192 Module\u00b6\n```", "```py\nabstract run_weights_independent_tranformations(predict_module: Module) \u2192 Module\u00b6\n```", "```py\nclass torchrec.inference.modules.PredictModule(module: Module)\u00b6\n```", "```py\nmodule = PredictModule(torch.device(\"cuda\", torch.cuda.current_device())) \n```", "```py\nforward(batch: Dict[str, Tensor]) \u2192 Any\u00b6\n```", "```py\nabstract predict_forward(batch: Dict[str, Tensor]) \u2192 Any\u00b6\n```", "```py\nproperty predict_module: Module\u00b6\n```", "```py\nstate_dict(destination: Optional[Dict[str, Any]] = None, prefix: str = '', keep_vars: bool = False) \u2192 Dict[str, Any]\u00b6\n```", "```py\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> module.state_dict().keys()\n['bias', 'weight'] \n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.inference.modules.QualNameMetadata(need_preproc: bool)\u00b6\n```", "```py\nneed_preproc: bool\u00b6\n```", "```py\ntorchrec.inference.modules.quantize_dense(predict_module: PredictModule, dtype: dtype, additional_embedding_module_type: List[Type[Module]] = []) \u2192 Module\u00b6\n```", "```py\ntorchrec.inference.modules.quantize_embeddings(module: Module, dtype: dtype, inplace: bool, additional_qconfig_spec_keys: Optional[List[Type[Module]]] = None, additional_mapping: Optional[Dict[Type[Module], Type[Module]]] = None, output_dtype: dtype = torch.float32, per_table_weight_dtype: Optional[Dict[str, dtype]] = None) \u2192 Module\u00b6\n```", "```py\ntorchrec.inference.modules.quantize_feature(module: Module, inputs: Tuple[Tensor, ...]) \u2192 Tuple[Tensor, ...]\u00b6\n```", "```py\ntorchrec.inference.modules.trim_torch_package_prefix_from_typename(typename: str) \u2192 str\u00b6\n```"]