- en: The Fundamentals of Autograd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html](https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-beginner-introyt-autogradyt-tutorial-py) to
    download the full example code
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction](introyt1_tutorial.html) || [Tensors](tensors_deeper_tutorial.html)
    || **Autograd** || [Building Models](modelsyt_tutorial.html) || [TensorBoard Support](tensorboardyt_tutorial.html)
    || [Training Models](trainingyt.html) || [Model Understanding](captumyt.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Follow along with the video below or on [youtube](https://www.youtube.com/watch?v=M0fX15_-xrY).
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/embed/M0fX15_-xrY](https://www.youtube.com/embed/M0fX15_-xrY)'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch’s *Autograd* feature is part of what make PyTorch flexible and fast
    for building machine learning projects. It allows for the rapid and easy computation
    of multiple partial derivatives (also referred to as *gradients)* over a complex
    computation. This operation is central to backpropagation-based neural network
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: The power of autograd comes from the fact that it traces your computation dynamically
    *at runtime,* meaning that if your model has decision branches, or loops whose
    lengths are not known until runtime, the computation will still be traced correctly,
    and you’ll get correct gradients to drive learning. This, combined with the fact
    that your models are built in Python, offers far more flexibility than frameworks
    that rely on static analysis of a more rigidly-structured model for computing
    gradients.
  prefs: []
  type: TYPE_NORMAL
- en: What Do We Need Autograd For?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A machine learning model is a *function*, with inputs and outputs. For this
    discussion, we’ll treat the inputs as an *i*-dimensional vector \(\vec{x}\), with
    elements \(x_{i}\). We can then express the model, *M*, as a vector-valued function
    of the input: \(\vec{y} = \vec{M}(\vec{x})\). (We treat the value of M’s output
    as a vector because in general, a model may have any number of outputs.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we’ll mostly be discussing autograd in the context of training, our output
    of interest will be the model’s loss. The *loss function* L(\(\vec{y}\)) = L(\(\vec{M}\)(\(\vec{x}\)))
    is a single-valued scalar function of the model’s output. This function expresses
    how far off our model’s prediction was from a particular input’s *ideal* output.
    *Note: After this point, we will often omit the vector sign where it should be
    contextually clear - e.g.,* \(y\) instead of \(\vec y\).'
  prefs: []
  type: TYPE_NORMAL
- en: In training a model, we want to minimize the loss. In the idealized case of
    a perfect model, that means adjusting its learning weights - that is, the adjustable
    parameters of the function - such that loss is zero for all inputs. In the real
    world, it means an iterative process of nudging the learning weights until we
    see that we get a tolerable loss for a wide variety of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we decide how far and in which direction to nudge the weights? We want
    to *minimize* the loss, which means making its first derivative with respect to
    the input equal to 0: \(\frac{\partial L}{\partial x} = 0\).'
  prefs: []
  type: TYPE_NORMAL
- en: Recall, though, that the loss is not *directly* derived from the input, but
    a function of the model’s output (which is a function of the input directly),
    \(\frac{\partial L}{\partial x}\) = \(\frac{\partial {L({\vec y})}}{\partial x}\).
    By the chain rule of differential calculus, we have \(\frac{\partial {L({\vec
    y})}}{\partial x}\) = \(\frac{\partial L}{\partial y}\frac{\partial y}{\partial
    x}\) = \(\frac{\partial L}{\partial y}\frac{\partial M(x)}{\partial x}\).
  prefs: []
  type: TYPE_NORMAL
- en: \(\frac{\partial M(x)}{\partial x}\) is where things get complex. The partial
    derivatives of the model’s outputs with respect to its inputs, if we were to expand
    the expression using the chain rule again, would involve many local partial derivatives
    over every multiplied learning weight, every activation function, and every other
    mathematical transformation in the model. The full expression for each such partial
    derivative is the sum of the products of the local gradient of *every possible
    path* through the computation graph that ends with the variable whose gradient
    we are trying to measure.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the gradients over the learning weights are of interest to us
    - they tell us *what direction to change each weight* to get the loss function
    closer to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the number of such local derivatives (each corresponding to a separate
    path through the model’s computation graph) will tend to go up exponentially with
    the depth of a neural network, so does the complexity in computing them. This
    is where autograd comes in: It tracks the history of every computation. Every
    computed tensor in your PyTorch model carries a history of its input tensors and
    the function used to create it. Combined with the fact that PyTorch functions
    meant to act on tensors each have a built-in implementation for computing their
    own derivatives, this greatly speeds the computation of the local derivatives
    needed for learning.'
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: That was a lot of theory - but what does it look like to use autograd in practice?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with a straightforward example. First, we’ll do some imports to
    let us graph our results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll create an input tensor full of evenly spaced values on the interval
    \([0, 2{\pi}]\), and specify `requires_grad=True`. (Like most functions that create
    tensors, `torch.linspace()` accepts an optional `requires_grad` option.) Setting
    this flag means that in every computation that follows, autograd will be accumulating
    the history of the computation in the output tensors of that computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll perform a computation, and plot its output in terms of its inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![autogradyt tutorial](../Images/c0fd39cae39bc44746dc67d7e9a22ff1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s have a closer look at the tensor `b`. When we print it, we see an indicator
    that it is tracking its computation history:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This `grad_fn` gives us a hint that when we execute the backpropagation step
    and compute gradients, we’ll need to compute the derivative of \(\sin(x)\) for
    all this tensor’s inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s perform some more computations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Finally, let’s compute a single-element output. When you call `.backward()`
    on a tensor with no arguments, it expects the calling tensor to contain only a
    single element, as is the case when computing a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Each `grad_fn` stored with our tensors allows you to walk the computation all
    the way back to its inputs with its `next_functions` property. We can see below
    that drilling down on this property on `d` shows us the gradient functions for
    all the prior tensors. Note that `a.grad_fn` is reported as `None`, indicating
    that this was an input to the function with no history of its own.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With all this machinery in place, how do we get derivatives out? You call the
    `backward()` method on the output, and check the input’s `grad` property to inspect
    the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![autogradyt tutorial](../Images/240722184f25ec9362a34b6c16336c3a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall the computation steps we took to get here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Adding a constant, as we did to compute `d`, does not change the derivative.
    That leaves \(c = 2 * b = 2 * \sin(a)\), the derivative of which should be \(2
    * \cos(a)\). Looking at the graph above, that’s just what we see.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that only *leaf nodes* of the computation have their gradients computed.
    If you tried, for example, `print(c.grad)` you’d get back `None`. In this simple
    example, only the input is a leaf node, so only it has gradients computed.
  prefs: []
  type: TYPE_NORMAL
- en: Autograd in Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve had a brief look at how autograd works, but how does it look when it’s
    used for its intended purpose? Let’s define a small model and examine how it changes
    after a single training batch. First, define a few constants, our model, and some
    stand-ins for inputs and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: One thing you might notice is that we never specify `requires_grad=True` for
    the model’s layers. Within a subclass of `torch.nn.Module`, it’s assumed that
    we want to track gradients on the layers’ weights for learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the layers of the model, we can examine the values of the weights,
    and verify that no gradients have been computed yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see how this changes when we run through one training batch. For a loss
    function, we’ll just use the square of the Euclidean distance between our `prediction`
    and the `ideal_output`, and we’ll use a basic stochastic gradient descent optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s call `loss.backward()` and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the gradients have been computed for each learning weight, but
    the weights remain unchanged, because we haven’t run the optimizer yet. The optimizer
    is responsible for updating model weights based on the computed gradients.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You should see that `layer2`’s weights have changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'One important thing about the process: After calling `optimizer.step()`, you
    need to call `optimizer.zero_grad()`, or else every time you run `loss.backward()`,
    the gradients on the learning weights will accumulate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: After running the cell above, you should see that after running `loss.backward()`
    multiple times, the magnitudes of most of the gradients will be much larger. Failing
    to zero the gradients before running your next training batch will cause the gradients
    to blow up in this manner, causing incorrect and unpredictable learning results.
  prefs: []
  type: TYPE_NORMAL
- en: Turning Autograd Off and On
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are situations where you will need fine-grained control over whether autograd
    is enabled. There are multiple ways to do this, depending on the situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest is to change the `requires_grad` flag on a tensor directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In the cell above, we see that `b1` has a `grad_fn` (i.e., a traced computation
    history), which is what we expect, since it was derived from a tensor, `a`, that
    had autograd turned on. When we turn off autograd explicitly with `a.requires_grad
    = False`, computation history is no longer tracked, as we see when we compute
    `b2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you only need autograd turned off temporarily, a better way is to use the
    `torch.no_grad()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`torch.no_grad()` can also be used as a function or method decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: There’s a corresponding context manager, `torch.enable_grad()`, for turning
    autograd on when it isn’t already. It may also be used as a decorator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you may have a tensor that requires gradient tracking, but you want
    a copy that does not. For this we have the `Tensor` object’s `detach()` method
    - it creates a copy of the tensor that is *detached* from the computation history:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We did this above when we wanted to graph some of our tensors. This is because
    `matplotlib` expects a NumPy array as input, and the implicit conversion from
    a PyTorch tensor to a NumPy array is not enabled for tensors with requires_grad=True.
    Making a detached copy lets us move forward.
  prefs: []
  type: TYPE_NORMAL
- en: Autograd and In-place Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In every example in this notebook so far, we’ve used variables to capture the
    intermediate values of a computation. Autograd needs these intermediate values
    to perform gradient computations. *For this reason, you must be careful about
    using in-place operations when using autograd.* Doing so can destroy information
    you need to compute derivatives in the `backward()` call. PyTorch will even stop
    you if you attempt an in-place operation on leaf variable that requires autograd,
    as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The following code cell throws a runtime error. This is expected.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Autograd Profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Autograd tracks every step of your computation in detail. Such a computation
    history, combined with timing information, would make a handy profiler - and autograd
    has that feature baked in. Here’s a quick example usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The profiler can also label individual sub-blocks of code, break out the data
    by input tensor shape, and export data as a Chrome tracing tools file. For full
    details of the API, see the [documentation](https://pytorch.org/docs/stable/autograd.html#profiler).
  prefs: []
  type: TYPE_NORMAL
- en: 'Advanced Topic: More Autograd Detail and the High-Level API'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have a function with an n-dimensional input and m-dimensional output,
    \(\vec{y}=f(\vec{x})\), the complete gradient is a matrix of the derivative of
    every output with respect to every input, called the *Jacobian:*
  prefs: []
  type: TYPE_NORMAL
- en: \[J = \left(\begin{array}{ccc} \frac{\partial y_{1}}{\partial x_{1}} & \cdots
    & \frac{\partial y_{1}}{\partial x_{n}}\\ \vdots & \ddots & \vdots\\ \frac{\partial
    y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}} \end{array}\right)\]
  prefs: []
  type: TYPE_NORMAL
- en: If you have a second function, \(l=g\left(\vec{y}\right)\) that takes m-dimensional
    input (that is, the same dimensionality as the output above), and returns a scalar
    output, you can express its gradients with respect to \(\vec{y}\) as a column
    vector, \(v=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} & \cdots
    & \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}\) - which is really
    just a one-column Jacobian.
  prefs: []
  type: TYPE_NORMAL
- en: More concretely, imagine the first function as your PyTorch model (with potentially
    many inputs and many outputs) and the second function as a loss function (with
    the model’s output as input, and the loss value as the scalar output).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we multiply the first function’s Jacobian by the gradient of the second
    function, and apply the chain rule, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: \[J^{T}\cdot v=\left(\begin{array}{ccc} \frac{\partial y_{1}}{\partial x_{1}}
    & \cdots & \frac{\partial y_{m}}{\partial x_{1}}\\ \vdots & \ddots & \vdots\\
    \frac{\partial y_{1}}{\partial x_{n}} & \cdots & \frac{\partial y_{m}}{\partial
    x_{n}} \end{array}\right)\left(\begin{array}{c} \frac{\partial l}{\partial y_{1}}\\
    \vdots\\ \frac{\partial l}{\partial y_{m}} \end{array}\right)=\left(\begin{array}{c}
    \frac{\partial l}{\partial x_{1}}\\ \vdots\\ \frac{\partial l}{\partial x_{n}}
    \end{array}\right)\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: You could also use the equivalent operation \(v^{T}\cdot J\), and get
    back a row vector.'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting column vector is the *gradient of the second function with respect
    to the inputs of the first* - or in the case of our model and loss function, the
    gradient of the loss with respect to the model inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**``torch.autograd`` is an engine for computing these products.** This is how
    we accumulate the gradients over the learning weights during the backward pass.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, the `backward()` call can *also* take an optional vector input.
    This vector represents a set of gradients over the tensor, which are multiplied
    by the Jacobian of the autograd-traced tensor that precedes it. Let’s try a specific
    example with a small vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'If we tried to call `y.backward()` now, we’d get a runtime error and a message
    that gradients can only be *implicitly* computed for scalar outputs. For a multi-dimensional
    output, autograd expects us to provide gradients for those three outputs that
    it can multiply into the Jacobian:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: (Note that the output gradients are all related to powers of two - which we’d
    expect from a repeated doubling operation.)
  prefs: []
  type: TYPE_NORMAL
- en: The High-Level API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is an API on autograd that gives you direct access to important differential
    matrix and vector operations. In particular, it allows you to calculate the Jacobian
    and the *Hessian* matrices of a particular function for particular inputs. (The
    Hessian is like the Jacobian, but expresses all partial *second* derivatives.)
    It also provides methods for taking vector products with these matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take the Jacobian of a simple function, evaluated for a 2 single-element
    inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: If you look closely, the first output should equal \(2e^x\) (since the derivative
    of \(e^x\) is \(e^x\)), and the second value should be 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can, of course, do this with higher-order tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `torch.autograd.functional.hessian()` method works identically (assuming
    your function is twice differentiable), but returns a matrix of all second derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a function to directly compute the vector-Jacobian product, if
    you provide the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The `torch.autograd.functional.jvp()` method performs the same matrix multiplication
    as `vjp()` with the operands reversed. The `vhp()` and `hvp()` methods do the
    same for a vector-Hessian product.
  prefs: []
  type: TYPE_NORMAL
- en: For more information, including performance notes on the [docs for the functional
    API](https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api)
  prefs: []
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 0 minutes 0.706 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: autogradyt_tutorial.py`](../../_downloads/1a94e27be9b0e79da5acafc1f68a7143/autogradyt_tutorial.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: autogradyt_tutorial.ipynb`](../../_downloads/ed9d4f94afb79f7dada6742a06c486a5/autogradyt_tutorial.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
