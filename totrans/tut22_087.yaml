- en: Custom C++ and CUDA Extensions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义C++和CUDA扩展
- en: 原文：[https://pytorch.org/tutorials/advanced/cpp_extension.html](https://pytorch.org/tutorials/advanced/cpp_extension.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/advanced/cpp_extension.html](https://pytorch.org/tutorials/advanced/cpp_extension.html)
- en: '**Author**: [Peter Goldsborough](https://www.goldsborough.me/)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：[Peter Goldsborough](https://www.goldsborough.me/)'
- en: PyTorch provides a plethora of operations related to neural networks, arbitrary
    tensor algebra, data wrangling and other purposes. However, you may still find
    yourself in need of a more customized operation. For example, you might want to
    use a novel activation function you found in a paper, or implement an operation
    you developed as part of your research.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了大量与神经网络、任意张量代数、数据处理和其他目的相关的操作。然而，您可能仍然需要更定制化的操作。例如，您可能想使用在论文中找到的新型激活函数，或者实现您作为研究的一部分开发的操作。
- en: The easiest way of integrating such a custom operation in PyTorch is to write
    it in Python by extending `Function` and `Module` as outlined [here](https://pytorch.org/docs/master/notes/extending.html).
    This gives you the full power of automatic differentiation (spares you from writing
    derivative functions) as well as the usual expressiveness of Python. However,
    there may be times when your operation is better implemented in C++. For example,
    your code may need to be *really* fast because it is called very frequently in
    your model or is very expensive even for few calls. Another plausible reason is
    that it depends on or interacts with other C or C++ libraries. To address such
    cases, PyTorch provides a very easy way of writing custom *C++ extensions*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中集成这种自定义操作的最简单方法是通过扩展`Function`和`Module`来用Python编写它，如[此处](https://pytorch.org/docs/master/notes/extending.html)所述。这为您提供了自动微分的全部功能（免去了编写导数函数的麻烦），以及Python的通常表达能力。然而，有时候您的操作最好在C++中实现。例如，您的代码可能需要*非常*快，因为它在模型中被频繁调用，或者即使是少数调用也非常昂贵。另一个可能的原因是它依赖于或与其他C或C++库交互。为了解决这些情况，PyTorch提供了一种非常简单的编写自定义*C++扩展*的方法。
- en: C++ extensions are a mechanism we have developed to allow users (you) to create
    PyTorch operators defined *out-of-source*, i.e. separate from the PyTorch backend.
    This approach is *different* from the way native PyTorch operations are implemented.
    C++ extensions are intended to spare you much of the boilerplate associated with
    integrating an operation with PyTorch’s backend while providing you with a high
    degree of flexibility for your PyTorch-based projects. Nevertheless, once you
    have defined your operation as a C++ extension, turning it into a native PyTorch
    function is largely a matter of code organization, which you can tackle after
    the fact if you decide to contribute your operation upstream.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: C++扩展是我们开发的一种机制，允许用户（您）创建PyTorch操作符定义为*源外*，即与PyTorch后端分开。这种方法与实现本机PyTorch操作的方式*不同*。C++扩展旨在为您提供高度灵活性，以便在PyTorch项目中节省与将操作与PyTorch后端集成相关的大量样板代码。然而，一旦您将操作定义为C++扩展，将其转换为本机PyTorch函数在很大程度上是代码组织的问题，如果您决定向上游贡献您的操作，可以在事后处理。
- en: Motivation and Example
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机和示例
- en: The rest of this note will walk through a practical example of writing and using
    a C++ (and CUDA) extension. If you are being chased or someone will fire you if
    you don’t get that op done by the end of the day, you can skip this section and
    head straight to the implementation details in the next section.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分将演示如何编写和使用C++（和CUDA）扩展的实际示例。如果您被追赶，或者如果您不在今天结束之前完成该操作，将会被解雇，您可以跳过本节，直接前往下一节中的实现细节。
- en: Let’s say you’ve come up with a new kind of recurrent unit that you found to
    have superior properties compared to the state of the art. This recurrent unit
    is similar to an LSTM, but differs in that it lacks a *forget gate* and uses an
    *Exponential Linear Unit* (ELU) as its internal activation function. Because this
    unit never forgets, we’ll call it *LLTM*, or *Long-Long-Term-Memory* unit.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想出了一种新型的循环单元，发现它具有比现有技术更优越的性能。这种循环单元类似于LSTM，但不同之处在于它没有*遗忘门*，而是使用*指数线性单元*（ELU）作为其内部激活函数。因为这个单元永远不会忘记，我们将其称为*LLTM*，或*长长期记忆*单元。
- en: 'The two ways in which LLTMs differ from vanilla LSTMs are significant enough
    that we can’t configure PyTorch’s `LSTMCell` for our purposes, so we’ll have to
    create a custom cell. The first and easiest approach for this – and likely in
    all cases a good first step – is to implement our desired functionality in plain
    PyTorch with Python. For this, we need to subclass [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module
    "(in PyTorch v2.2)") and implement the forward pass of the LLTM. This would look
    something like this:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLTM与普通LSTM不同的两种方式是显著的，以至于我们无法配置PyTorch的`LSTMCell`以满足我们的需求，因此我们必须创建一个自定义单元。这种情况下的第一种最简单的方法
    - 也可能是所有情况下的一个很好的第一步 - 是在纯PyTorch中用Python实现我们想要的功能。为此，我们需要继承[`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module
    "(在PyTorch v2.2中)")并实现LLTM的前向传播。这看起来可能是这样的：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'which we could then use as expected:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按预期使用：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Naturally, if at all possible and plausible, you should use this approach to
    extend PyTorch. Since PyTorch has highly optimized implementations of its operations
    for CPU *and* GPU, powered by libraries such as [NVIDIA cuDNN](https://developer.nvidia.com/cudnn),
    [Intel MKL](https://software.intel.com/en-us/mkl) or [NNPACK](https://github.com/Maratyszcza/NNPACK),
    PyTorch code like above will often be fast enough. However, we can also see why,
    under certain circumstances, there is room for further performance improvements.
    The most obvious reason is that PyTorch has no knowledge of the *algorithm* you
    are implementing. It knows only of the individual operations you use to compose
    your algorithm. As such, PyTorch must execute your operations individually, one
    after the other. Since each individual call to the implementation (or *kernel*)
    of an operation, which may involve the launch of a CUDA kernel, has a certain
    amount of overhead, this overhead may become significant across many function
    calls. Furthermore, the Python interpreter that is running our code can itself
    slow down our program.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果可能和合理的话，您应该使用这种方法来扩展 PyTorch。由于 PyTorch 高度优化了其针对 CPU *和* GPU 的操作实现，由诸如
    [NVIDIA cuDNN](https://developer.nvidia.com/cudnn)、[Intel MKL](https://software.intel.com/en-us/mkl)
    或 [NNPACK](https://github.com/Maratyszcza/NNPACK) 等库支持，因此像上面的 PyTorch 代码通常已经足够快。然而，我们也可以看到，在某些情况下，还有进一步提高性能的空间。最明显的原因是
    PyTorch 对您正在实现的 *算法* 一无所知。它只知道您用来组成算法的各个操作。因此，PyTorch 必须逐个执行您的操作。由于对每个操作的实现（或
    *内核*）的每个单独调用，可能涉及启动 CUDA 内核，都有一定的开销，这种开销在许多函数调用中可能变得显著。此外，运行我们代码的 Python 解释器本身也可能减慢我们程序的运行速度。
- en: A definite method of speeding things up is therefore to rewrite parts in C++
    (or CUDA) and *fuse* particular groups of operations. Fusing means combining the
    implementations of many functions into a single function, which profits from fewer
    kernel launches as well as other optimizations we can perform with increased visibility
    of the global flow of data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，加快速度的一种明确方法是将部分代码重写为 C++（或 CUDA），并 *融合* 特定组的操作。融合意味着将许多函数的实现合并到一个函数中，从中获益于更少的内核启动以及我们可以通过增加数据全局流动的可见性执行的其他优化。
- en: Let’s see how we can use C++ extensions to implement a *fused* version of the
    LLTM. We’ll begin by writing it in plain C++, using the [ATen](https://github.com/zdevito/ATen)
    library that powers much of PyTorch’s backend, and see how easily it lets us translate
    our Python code. We’ll then speed things up even more by moving parts of the model
    to CUDA kernel to benefit from the massive parallelism GPUs provide.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用 C++ 扩展来实现 LLTM 的 *融合* 版本。我们将首先用普通的 C++ 编写它，使用 [ATen](https://github.com/zdevito/ATen)
    库来支持 PyTorch 后端的大部分功能，并看看它是如何轻松地让我们转换我们的 Python 代码的。然后，我们将通过将模型的部分移动到 CUDA 内核来进一步加快速度，以便从
    GPU 提供的大规模并行性中获益。
- en: Writing a C++ Extension
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写 C++ 扩展
- en: 'C++ extensions come in two flavors: They can be built “ahead of time” with
    `setuptools`, or “just in time” via [`torch.utils.cpp_extension.load()`](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load
    "(in PyTorch v2.2)"). We’ll begin with the first approach and discuss the latter
    later.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: C++ 扩展有两种类型：可以使用 `setuptools` “预先构建”，也可以通过 [`torch.utils.cpp_extension.load()`](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load)
    “即时构建”。我们将从第一种方法开始，并稍后讨论后者。
- en: Building with `setuptools`
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `setuptools` 构建
- en: 'For the “ahead of time” flavor, we build our C++ extension by writing a `setup.py`
    script that uses setuptools to compile our C++ code. For the LLTM, it looks as
    simple as this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“预先构建”类型，我们通过编写一个 `setup.py` 脚本来构建我们的 C++ 扩展，该脚本使用 setuptools 来编译我们的 C++ 代码。对于
    LLTM，它看起来就像这样简单：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this code, `CppExtension` is a convenience wrapper around `setuptools.Extension`
    that passes the correct include paths and sets the language of the extension to
    C++. The equivalent vanilla `setuptools` code would simply be:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，`CppExtension` 是围绕 `setuptools.Extension` 的一个便利包装器，它传递了正确的包含路径并将扩展的语言设置为
    C++。等效的原始 `setuptools` 代码将简单地是：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`BuildExtension` performs a number of required configuration steps and checks
    and also manages mixed compilation in the case of mixed C++/CUDA extensions. And
    that’s all we really need to know about building C++ extensions for now! Let’s
    now take a look at the implementation of our C++ extension, which goes into `lltm.cpp`.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`BuildExtension` 执行了许多必需的配置步骤和检查，还管理了混合编译，以处理混合的 C++/CUDA 扩展。这就是我们现在需要了解有关构建
    C++ 扩展的全部内容！现在让我们来看看我们的 C++ 扩展的实现，它位于 `lltm.cpp` 中。'
- en: Writing the C++ Op
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写 C++ Op
- en: 'Let’s start implementing the LLTM in C++! One function we’ll need for the backward
    pass is the derivative of the sigmoid. This is a small enough piece of code to
    discuss the overall environment that is available to us when writing C++ extensions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始在 C++ 中实现 LLTM！我们在反向传播中需要的一个函数是 sigmoid 的导数。这是一个足够小的代码片段，可以讨论一下在编写 C++
    扩展时可用的整体环境：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`<torch/extension.h>` is the one-stop header to include all the necessary PyTorch
    bits to write C++ extensions. It includes:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`<torch/extension.h>` 是一个一站式头文件，包含了编写 C++ 扩展所需的所有必要 PyTorch 组件。它包括：'
- en: The ATen library, which is our primary API for tensor computation,
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ATen 库是我们进行张量计算的主要 API，
- en: '[pybind11](https://github.com/pybind/pybind11), which is how we create Python
    bindings for our C++ code,'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[pybind11](https://github.com/pybind/pybind11) 是我们为 C++ 代码创建 Python 绑定的方式，'
- en: Headers that manage the details of interaction between ATen and pybind11.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理 ATen 和 pybind11 之间交互细节的头文件。
- en: The implementation of `d_sigmoid()` shows how to use the ATen API. PyTorch’s
    tensor and variable interface is generated automatically from the ATen library,
    so we can more or less translate our Python implementation 1:1 into C++. Our primary
    datatype for all computations will be `torch::Tensor`. Its full API can be inspected
    [here](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html). Notice also that
    we can include `<iostream>` or *any other C or C++ header* – we have the full
    power of C++11 at our disposal.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`d_sigmoid()`的实现展示了如何使用ATen API。PyTorch的张量和变量接口是从ATen库自动生成的，因此我们可以将我们的Python实现几乎一对一地转换成C++。我们所有计算的主要数据类型将是`torch::Tensor`。其完整API可以在[这里](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html)查看。还要注意，我们可以包含`<iostream>`或*任何其他C或C++头文件*
    - 我们可以充分利用C++11的全部功能。'
- en: 'Note that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h
    on Windows. To workaround the issue, move python binding logic to pure C++ file.
    Example use:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，CUDA-11.5 nvcc在Windows上解析torch/extension.h时会遇到内部编译器错误。为了解决此问题，将Python绑定逻辑移至纯C++文件。示例用法：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Instead of:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Currently open issue for nvcc bug [here](https://github.com/pytorch/pytorch/issues/69460).
    Complete workaround code example [here](https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 目前存在的nvcc bug问题请参考[这里](https://github.com/pytorch/pytorch/issues/69460)。完整的解决方案代码示例请参考[这里](https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48)。
- en: Forward Pass
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前向传递
- en: 'Next we can port our entire forward pass to C++:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们可以将整个前向传递移植到C++中：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Backward Pass
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反向传递
- en: 'The C++ extension API currently does not provide a way of automatically generating
    a backwards function for us. As such, we have to also implement the backward pass
    of our LLTM, which computes the derivative of the loss with respect to each input
    of the forward pass. Ultimately, we will plop both the forward and backward function
    into a [`torch.autograd.Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function
    "(in PyTorch v2.2)") to create a nice Python binding. The backward function is
    slightly more involved, so we’ll not dig deeper into the code (if you are interested,
    [Alex Graves’ thesis](https://www.cs.toronto.edu/~graves/phd.pdf) is a good read
    for more information on this):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: C++扩展API目前没有提供一种自动生成反向函数的方法。因此，我们还必须实现LLTM的反向传递，它计算损失相对于前向传递的每个输入的导数。最终，我们将前向和反向函数一起放入[`torch.autograd.Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function
    "(在PyTorch v2.2中)")中，以创建一个很好的Python绑定。反向函数稍微复杂一些，因此我们不会深入研究代码（如果您感兴趣，[Alex Graves的论文](https://www.cs.toronto.edu/~graves/phd.pdf)是一个更多信息的好读物）：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Binding to Python
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绑定到Python
- en: Once you have your operation written in C++ and ATen, you can use pybind11 to
    bind your C++ functions or classes into Python in a very simple manner. Questions
    or issues you have about this part of PyTorch C++ extensions will largely be addressed
    by [pybind11 documentation](https://pybind11.readthedocs.io/en/stable/).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您用C++和ATen编写了操作，您可以使用pybind11以非常简单的方式将您的C++函数或类绑定到Python中。关于PyTorch C++扩展的这部分问题或问题将主要由[pybind11文档](https://pybind11.readthedocs.io/en/stable/)解决。
- en: 'For our extensions, the necessary binding code spans only four lines:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的扩展，必要的绑定代码仅涉及四行：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: One bit to note here is the macro `TORCH_EXTENSION_NAME`. The torch extension
    build will define it as the name you give your extension in the `setup.py` script.
    In this case, the value of `TORCH_EXTENSION_NAME` would be “lltm_cpp”. This is
    to avoid having to maintain the name of the extension in two places (the build
    script and your C++ code), as a mismatch between the two can lead to nasty and
    hard to track issues.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要注意的一点是宏`TORCH_EXTENSION_NAME`。torch扩展构建将其定义为您在`setup.py`脚本中给出的扩展名称。在这种情况下，`TORCH_EXTENSION_NAME`的值将是“lltm_cpp”。这是为了避免在两个地方（构建脚本和您的C++代码）维护扩展名，因为两者之间的不匹配可能导致难以跟踪的问题。
- en: Using Your Extension
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用您的扩展
- en: 'We are now set to import our extension in PyTorch. At this point, your directory
    structure could look something like this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好在PyTorch中导入我们的扩展。此时，您的目录结构可能如下所示：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, run `python setup.py install` to build and install your extension. This
    should look something like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行`python setup.py install`来构建和安装您的扩展。这应该看起来像这样：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A small note on compilers: Due to ABI versioning issues, the compiler you use
    to build your C++ extension must be *ABI-compatible* with the compiler PyTorch
    was built with. In practice, this means that you must use GCC version 4.9 and
    above on Linux. For Ubuntu 16.04 and other more-recent Linux distributions, this
    should be the default compiler already. On MacOS, you must use clang (which does
    not have any ABI versioning issues). In the worst case, you can build PyTorch
    from source with your compiler and then build the extension with that same compiler.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 关于编译器的一点说明：由于ABI版本问题，用于构建C++扩展的编译器必须与PyTorch构建时使用的编译器*ABI兼容*。实际上，这意味着您必须在Linux上使用GCC版本4.9及以上。对于Ubuntu
    16.04和其他更近期的Linux发行版，这应该已经是默认编译器了。在MacOS上，您必须使用clang（它没有任何ABI版本问题）。在最坏的情况下，您可以使用您的编译器从源代码构建PyTorch，然后使用相同的编译器构建扩展。
- en: 'Once your extension is built, you can simply import it in Python, using the
    name you specified in your `setup.py` script. Just be sure to `import torch` first,
    as this will resolve some symbols that the dynamic linker must see:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 构建完您的扩展后，您可以在Python中简单地导入它，使用您在`setup.py`脚本中指定的名称。只需确保首先`import torch`，因为这将解析动态链接器必须看到的一些符号：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If we call `help()` on the function or module, we can see that its signature
    matches our C++ code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在函数或模块上调用`help()`，我们可以看到其签名与我们的C++代码匹配：
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Since we are now able to call our C++ functions from Python, we can wrap them
    with [`torch.autograd.Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function
    "(in PyTorch v2.2)") and [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module
    "(in PyTorch v2.2)") to make them first class citizens of PyTorch:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在能够从Python调用我们的C++函数，我们可以将它们包装在[`torch.autograd.Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)和[`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)中，使它们成为PyTorch的一等公民：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Performance Comparison
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能比较
- en: 'Now that we are able to use and call our C++ code from PyTorch, we can run
    a small benchmark to see how much performance we gained from rewriting our op
    in C++. We’ll run the LLTM forwards and backwards a few times and measure the
    duration:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够从PyTorch使用和调用我们的C++代码，我们可以运行一个小型基准测试，看看我们从将操作重写为C++中获得了多少性能提升。我们将运行LLTM的前向和反向几次，并测量持续时间：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we run this code with the original LLTM we wrote in pure Python at the start
    of this post, we get the following numbers (on my machine):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用在本文开头纯Python编写的原始LLTM运行此代码，我们将得到以下数字（在我的机器上）：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'and with our new C++ version:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以及我们的新C++版本：
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can already see a significant speedup for the forward function (more than
    30%). For the backward function, a speedup is visible, albeit not a major one.
    The backward pass I wrote above was not particularly optimized and could definitely
    be improved. Also, PyTorch’s automatic differentiation engine can automatically
    parallelize computation graphs, may use a more efficient flow of operations overall,
    and is also implemented in C++, so it’s expected to be fast. Nevertheless, this
    is a good start.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到前向函数的显着加速（超过30%）。对于反向函数，虽然可以看到加速，但并不是很大。我上面写的反向传播并没有特别优化，肯定可以改进。此外，PyTorch的自动微分引擎可以自动并行化计算图，可能会使用更高效的操作流程，并且也是用C++实现的，因此预计速度会很快。尽管如此，这是一个很好的开始。
- en: Performance on GPU Devices
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPU设备上的性能
- en: 'A wonderful fact about PyTorch’s *ATen* backend is that it abstracts the computing
    device you are running on. This means the same code we wrote for CPU can *also*
    run on GPU, and individual operations will correspondingly dispatch to GPU-optimized
    implementations. For certain operations like matrix multiply (like `mm` or `addmm`),
    this is a big win. Let’s take a look at how much performance we gain from running
    our C++ code with CUDA tensors. No changes to our implementation are required,
    we simply need to put our tensors in GPU memory from Python, with either adding
    `device=cuda_device` argument at creation time or using `.to(cuda_device)` after
    creation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 关于PyTorch的*ATen*后端的一个奇妙事实是，它抽象了您正在运行的计算设备。这意味着我们为CPU编写的相同代码也可以在GPU上运行，并且各个操作将相应地分派到针对GPU优化的实现。对于某些操作，如矩阵乘法（如`mm`或`addmm`），这是一个巨大的优势。让我们看看通过在CUDA张量上运行我们的C++代码可以获得多少性能提升。我们不需要对实现进行任何更改，只需在Python中将张量放入GPU内存，要么在创建时添加`device=cuda_device`参数，要么在创建后使用`.to(cuda_device)`：
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once more comparing our plain PyTorch code with our C++ version, now both running
    on CUDA devices, we again see performance gains. For Python/PyTorch:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 再次比较我们的纯PyTorch代码与我们的C++版本，现在两者都在CUDA设备上运行，我们再次看到性能提升。对于Python/PyTorch：
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And C++/ATen:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以及C++/ATen：
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: That’s a great overall speedup compared to non-CUDA code. However, we can pull
    even more performance out of our C++ code by writing custom CUDA kernels, which
    we’ll dive into soon. Before that, let’s discuss another way of building your
    C++ extensions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与非CUDA代码相比的整体加速效果很好。然而，我们可以通过编写自定义CUDA核心来进一步提高C++代码的性能，我们将很快深入讨论这一点。在此之前，让我们讨论另一种构建C++扩展的方法。
- en: JIT Compiling Extensions
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JIT编译扩展
- en: 'Previously, I mentioned there were two ways of building C++ extensions: using
    `setuptools` or just in time (JIT). Having covered the former, let’s elaborate
    on the latter. The JIT compilation mechanism provides you with a way of compiling
    and loading your extensions on the fly by calling a simple function in PyTorch’s
    API called [`torch.utils.cpp_extension.load()`](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load
    "(in PyTorch v2.2)"). For the LLTM, this would look as simple as this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我提到构建C++扩展有两种方法：使用`setuptools`或即时编译（JIT）。在介绍了前者之后，让我们详细说明后者。JIT编译机制为您提供了一种通过调用PyTorch
    API中的一个简单函数[`torch.utils.cpp_extension.load()`](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load)来即时编译和加载扩展的方法。对于LLTM，这看起来就像这样简单：
- en: '[PRE21]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here, we provide the function with the same information as for `setuptools`.
    In the background, this will do the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们为函数提供与`setuptools`相同的信息。在后台，这将执行以下操作：
- en: Create a temporary directory `/tmp/torch_extensions/lltm`,
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个临时目录`/tmp/torch_extensions/lltm`，
- en: Emit a [Ninja](https://ninja-build.org/) build file into that temporary directory,
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在临时目录中生成一个[Ninja](https://ninja-build.org/)构建文件，
- en: Compile your source files into a shared library,
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的源文件编译成共享库，
- en: Import this shared library as a Python module.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此共享库导入为Python模块。
- en: 'In fact, if you pass `verbose=True` to `cpp_extension.load()`, you will be
    informed about the process:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果将`verbose=True`传递给`cpp_extension.load()`，您将了解到整个过程：
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The resulting Python module will be exactly the same as produced by setuptools,
    but removes the requirement of having to maintain a separate `setup.py` build
    file. If your setup is more complicated and you do need the full power of `setuptools`,
    you *can* write your own `setup.py` – but in many cases this JIT technique will
    do just fine. The first time you run through this line, it will take some time,
    as the extension is compiling in the background. Since we use the Ninja build
    system to build your sources, re-compilation is incremental and thus re-loading
    the extension when you run your Python module a second time is fast and has low
    overhead if you didn’t change the extension’s source files.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的Python模块将与setuptools生成的完全相同，但消除了必须维护单独的`setup.py`构建文件的要求。如果您的设置更复杂，并且确实需要`setuptools`的全部功能，您*可以*编写自己的`setup.py`
    - 但在许多情况下，这种JIT技术就足够了。第一次运行这行代码时，会花费一些时间，因为扩展正在后台编译。由于我们使用Ninja构建系统来构建您的源代码，因此重新编译是增量的，因此在第二次运行Python模块时重新加载扩展是快速的，如果您没有更改扩展的源文件，则开销很低。
- en: Writing a Mixed C++/CUDA extension
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写混合C++/CUDA扩展
- en: To really take our implementation to the next level, we can hand-write parts
    of our forward and backward passes with custom CUDA kernels. For the LLTM, this
    has the prospect of being particularly effective, as there are a large number
    of pointwise operations in sequence, that can all be fused and parallelized in
    a single CUDA kernel. Let’s see how we could write such a CUDA kernel and integrate
    it with PyTorch using this extension mechanism.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的实现提升到下一个级别，我们可以手写部分前向和后向传递的自定义CUDA核心。对于LLTM来说，这有可能特别有效，因为有大量的逐点操作序列，可以在单个CUDA核心中融合并并行化。让我们看看如何编写这样一个CUDA核心，并使用这个扩展机制将其集成到PyTorch中。
- en: The general strategy for writing a CUDA extension is to first write a C++ file
    which defines the functions that will be called from Python, and binds those functions
    to Python with pybind11\. Furthermore, this file will also *declare* functions
    that are defined in CUDA (`.cu`) files. The C++ functions will then do some checks
    and ultimately forward its calls to the CUDA functions. In the CUDA files, we
    write our actual CUDA kernels. The `cpp_extension` package will then take care
    of compiling the C++ sources with a C++ compiler like `gcc` and the CUDA sources
    with NVIDIA’s `nvcc` compiler. This ensures that each compiler takes care of files
    it knows best to compile. Ultimately, they will be linked into one shared library
    that is available to us from Python code.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 编写CUDA扩展的一般策略是首先编写一个C++文件，定义将从Python调用的函数，并使用pybind11将这些函数绑定到Python。此外，这个文件还将*声明*在CUDA（`.cu`）文件中定义的函数。然后，C++函数将进行一些检查，并最终将其调用转发到CUDA函数。在CUDA文件中，我们编写我们的实际CUDA核心。`cpp_extension`包将负责使用类似`gcc`的C++编译器编译C++源代码，使用NVIDIA的`nvcc`编译器编译CUDA源代码。这确保每个编译器负责编译它最擅长的文件。最终，它们将被链接成一个共享库，可以在Python代码中使用。
- en: 'We’ll start with the C++ file, which we’ll call `lltm_cuda.cpp`, for example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从C++文件开始，我们将称之为`lltm_cuda.cpp`，例如：
- en: '[PRE23]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you can see, it is largely boilerplate, checks and forwarding to functions
    that we’ll define in the CUDA file. We’ll name this file `lltm_cuda_kernel.cu`
    (note the `.cu` extension!). NVCC can reasonably compile C++11, thus we still
    have ATen and the C++ standard library available to us (but not `torch.h`). Note
    that `setuptools` cannot handle files with the same name but different extensions,
    so if you use the `setup.py` method instead of the JIT method, you must give your
    CUDA file a different name than your C++ file (for the JIT method, `lltm.cpp`
    and `lltm.cu` would work fine). Let’s take a small peek at what this file will
    look like:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这主要是样板代码、检查和转发到我们将在CUDA文件中定义的函数。我们将命名这个文件为`lltm_cuda_kernel.cu`（注意`.cu`扩展名！）。NVCC可以合理地编译C++11，因此我们仍然可以使用ATen和C++标准库（但不能使用`torch.h`）。请注意，`setuptools`无法处理具有相同名称但不同扩展名的文件，因此如果您使用`setup.py`方法而不是JIT方法，您必须为CUDA文件和C++文件分配不同的名称（对于JIT方法，`lltm.cpp`和`lltm.cu`将正常工作）。让我们来看一下这个文件将是什么样子：
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here we see the headers I just described, as well as the fact that we are using
    CUDA-specific declarations like `__device__` and `__forceinline__` and functions
    like `exp`. Let’s continue with a few more helper functions that we’ll need:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了我刚刚描述的头文件，以及我们正在使用CUDA特定声明，如`__device__`和`__forceinline__`，以及`exp`等函数。让我们继续写一些我们需要的辅助函数：
- en: '[PRE25]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To now actually implement a function, we’ll again need two things: one function
    that performs operations we don’t wish to explicitly write by hand and calls into
    CUDA kernels, and then the actual CUDA kernel for the parts we want to speed up.
    For the forward pass, the first function should look like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在实际实现一个函数，我们将再次需要两件事：一个执行我们不希望手动编写的操作并调用CUDA核心的函数，然后是我们想要加速的部分的实际CUDA核心。对于前向传递，第一个函数应该如下所示：
- en: '[PRE26]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The main point of interest here is the `AT_DISPATCH_FLOATING_TYPES` macro and
    the kernel launch (indicated by the `<<<...>>>`). While ATen abstracts away the
    device and datatype of the tensors we deal with, a tensor will, at runtime, still
    be backed by memory of a concrete type on a concrete device. As such, we need
    a way of determining at runtime what type a tensor is and then selectively call
    functions with the corresponding correct type signature. Done manually, this would
    (conceptually) look something like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要关注点是`AT_DISPATCH_FLOATING_TYPES`宏和内核启动（由`<<<...>>>`指示）。虽然ATen抽象了我们处理的张量的设备和数据类型，但在运行时，张量仍然由具体类型和具体设备的内存支持。因此，我们需要一种在运行时确定张量类型并有选择地调用具有相应正确类型签名的函数的方法。手动完成，这将（概念上）看起来像这样：
- en: '[PRE27]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The purpose of `AT_DISPATCH_FLOATING_TYPES` is to take care of this dispatch
    for us. It takes a type (`gates.type()` in our case), a name (for error messages)
    and a lambda function. Inside this lambda function, the type alias `scalar_t`
    is available and is defined as the type that the tensor actually is at runtime
    in that context. As such, if we have a template function (which our CUDA kernel
    will be), we can instantiate it with this `scalar_t` alias, and the correct function
    will be called. In this case, we also want to retrieve the data pointers of the
    tensors as pointers of that `scalar_t` type. If you wanted to dispatch over all
    types and not just floating point types (`Float` and `Double`), you can use `AT_DISPATCH_ALL_TYPES`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`AT_DISPATCH_FLOATING_TYPES`的目的是为我们处理这个分发。它接受一个类型（在我们的情况下是`gates.type()`），一个名称（用于错误消息）和一个lambda函数。在这个lambda函数内部，类型别名`scalar_t`可用，并在该上下文中定义为张量在运行时实际上是的类型。因此，如果我们有一个模板函数（我们的CUDA内核将是这样的），我们可以用这个`scalar_t`别名实例化它，正确的函数将被调用。在这种情况下，我们还想以`scalar_t`类型的指针形式检索张量的数据指针。如果您想要分发所有类型而不仅仅是浮点类型（`Float`和`Double`），您可以使用`AT_DISPATCH_ALL_TYPES`。'
- en: Note that we perform some operations with plain ATen. These operations will
    still run on the GPU, but using ATen’s default implementations. This makes sense
    because ATen will use highly optimized routines for things like matrix multiplies
    (e.g. `addmm`) or convolutions which would be much harder to implement and improve
    ourselves.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用普通的ATen执行一些操作。这些操作仍将在GPU上运行，但使用ATen的默认实现。这是有道理的，因为ATen将使用高度优化的例程来执行矩阵乘法（例如`addmm`）或卷积等操作，这些操作对我们自己来说要难得多。
- en: As for the kernel launch itself, we are here specifying that each CUDA block
    will have 1024 threads, and that the entire GPU grid is split into as many blocks
    of `1 x 1024` threads as are required to fill our matrices with one thread per
    component. For example, if our state size was 2048 and our batch size 4, we’d
    launch a total of `4 x 2 = 8` blocks with each 1024 threads. If you’ve never heard
    of CUDA “blocks” or “grids” before, an [introductory read about CUDA](https://devblogs.nvidia.com/even-easier-introduction-cuda)
    may help.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 至于内核启动本身，我们在这里指定每个CUDA块将有1024个线程，并且整个GPU网格被分割为尽可能多的`1 x 1024`线程的块，以填充我们的矩阵，每个组件一个线程。例如，如果我们的状态大小为2048，批处理大小为4，我们将启动总共`4
    x 2 = 8`个块，每个块有1024个线程。如果您以前从未听说过CUDA的“块”或“网格”，那么[CUDA的入门阅读](https://devblogs.nvidia.com/even-easier-introduction-cuda)可能会有所帮助。
- en: 'The actual CUDA kernel is fairly simple (if you’ve ever programmed GPUs before):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的CUDA内核相当简单（如果您以前编程过GPU的话）：
- en: '[PRE28]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: What’s primarily interesting here is that we are able to compute all of these
    pointwise operations entirely in parallel for each individual component in our
    gate matrices. If you imagine having to do this with a giant `for` loop over a
    million elements in serial, you can see why this would be much faster.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里主要有趣的是，我们能够为门控矩阵中的每个单独组件完全并行计算所有这些逐点操作。如果想象要在串行中对一百万个元素进行巨大的`for`循环，您就会明白为什么这样会更快。
- en: Using accessors
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用访问器
- en: You can see in the CUDA kernel that we work directly on pointers with the right
    type. Indeed, working directly with high level type agnostic tensors inside cuda
    kernels would be very inefficient.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到在CUDA内核中，我们直接使用正确类型的指针进行操作。事实上，在cuda内核中直接使用高级类型不可知的张量将非常低效。
- en: 'However, this comes at a cost of ease of use and readability, especially for
    highly dimensional data. In our example, we know for example that the contiguous
    `gates` tensor has 3 dimensions:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这样做会带来易用性和可读性的代价，特别是对于高维数据。在我们的示例中，我们知道连续的`gates`张量有3个维度：
- en: batch, size of `batch_size` and stride of `3*state_size`
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批处理，`batch_size`的大小和`3*state_size`的步幅
- en: row, size of `3` and stride of `state_size`
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行，`3`的大小和`state_size`的步幅
- en: index, size of `state_size` and stride of `1`
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 索引，`state_size`的大小和步幅为`1`
- en: How can we access the element `gates[n][row][column]` inside the kernel then?
    It turns out that you need the strides to access your element with some simple
    arithmetic.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何在内核中访问元素`gates[n][row][column]`呢？事实证明，您需要步幅来使用一些简单的算术来访问您的元素。
- en: '[PRE29]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In addition to being verbose, this expression needs stride to be explicitly
    known, and thus passed to the kernel function within its arguments. You can see
    that in the case of kernel functions accepting multiple tensors with different
    sizes you will end up with a very long list of arguments.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了冗长外，这个表达式需要明确知道步幅，并在其参数中传递给内核函数。您可以看到，在接受具有不同大小的多个张量的内核函数的情况下，您最终将得到一个非常长的参数列表。
- en: 'Fortunately for us, ATen provides accessors that are created with a single
    dynamic check that a Tensor is the type and number of dimensions. Accessors then
    expose an API for accessing the Tensor elements efficiently without having to
    convert to a single pointer:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，幸运的是，ATen提供了通过单个动态检查创建的访问器，以确保张量是指定类型和维度的。然后，访问器公开了一个API，用于有效地访问张量元素，而无需转换为单个指针：
- en: '[PRE30]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Accessor objects have a relatively high level interface, with `.size()` and
    `.stride()` methods and multi-dimensional indexing. The `.accessor<>` interface
    is designed to access data efficiently on cpu tensor. The equivalent for cuda
    tensors are `packed_accessor64<>` and `packed_accessor32<>`, which produce Packed
    Accessors with either 64-bit or 32-bit integer indexing.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Accessor对象具有相对较高级的接口，具有`.size()`和`.stride()`方法以及多维索引。`.accessor<>`接口旨在有效地访问cpu张量上的数据。cuda张量的等效物是`packed_accessor64<>`和`packed_accessor32<>`，它们产生具有64位或32位整数索引的Packed
    Accessors。
- en: The fundamental difference with Accessor is that a Packed Accessor copies size
    and stride data inside of its structure instead of pointing to it. It allows us
    to pass it to a CUDA kernel function and use its interface inside it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Accessor与Packed Accessor的根本区别在于Packed Accessor将大小和步幅数据复制到其结构内部，而不是指向它。这使我们能够将其传递给CUDA内核函数并在其中使用其接口。
- en: We can design a function that takes Packed Accessors instead of pointers.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以设计一个函数，它接受Packed Accessors而不是指针。
- en: '[PRE31]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Let’s decompose the template used here. the first two arguments `scalar_t` and
    `2` are the same as regular Accessor. The argument `torch::RestrictPtrTraits`
    indicates that the `__restrict__` keyword must be used. Note also that we’ve used
    the `PackedAccessor32` variant which store the sizes and strides in an `int32_t`.
    This is important as using the 64-bit variant (`PackedAccessor64`) can make the
    kernel slower.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这里使用的模板。前两个参数`scalar_t`和`2`与常规Accessor相同。参数`torch::RestrictPtrTraits`表示必须使用`__restrict__`关键字。还要注意，我们使用了存储大小和步幅的`int32_t`的`PackedAccessor32`变体。这很重要，因为使用64位变体（`PackedAccessor64`）可能会使内核变慢。
- en: The function declaration becomes
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 函数声明变为
- en: '[PRE32]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The implementation is much more readable! This function is then called by creating
    Packed Accessors with the `.packed_accessor32<>` method within the host function.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 实现更加可读！然后通过在主机函数中使用`.packed_accessor32<>`方法创建Packed Accessors来调用此函数。
- en: '[PRE33]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The backwards pass follows much the same pattern and I won’t elaborate further
    on it:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播遵循了大致相同的模式，我不会进一步详细说明：
- en: '[PRE34]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Integrating a C++/CUDA Operation with PyTorch
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将C++/CUDA操作集成到PyTorch中
- en: 'Integration of our CUDA-enabled op with PyTorch is again very straightforward.
    If you want to write a `setup.py` script, it could look like this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 再次非常简单地将我们的CUDA启用的操作集成到PyTorch中。如果您想编写一个`setup.py`脚本，它可能如下所示：
- en: '[PRE35]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Instead of `CppExtension()`, we now use `CUDAExtension()`. We can just specify
    the `.cu` file along with the `.cpp` files – the library takes care of all the
    hassle this entails for you. The JIT mechanism is even simpler:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用`CUDAExtension()`而不是`CppExtension()`。我们只需指定`.cu`文件以及`.cpp`文件 - 库会为您处理所有这些麻烦。JIT机制甚至更简单：
- en: '[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Performance Comparison
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能比较
- en: 'Our hope was that parallelizing and fusing the pointwise operations of our
    code with CUDA would improve the performance of our LLTM. Let’s see if that holds
    true. We can run the code I listed earlier to run a benchmark. Our fastest version
    earlier was the CUDA-based C++ code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过将代码的逐点操作并行化和融合到CUDA中，可以提高LLTM的性能。让我们看看这是否成立。我们可以运行我之前列出的代码来运行基准测试。我们之前最快的版本是基于CUDA的C++代码：
- en: '[PRE37]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'And now with our custom CUDA kernel:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用我们自定义的CUDA内核：
- en: '[PRE38]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: More performance increases!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 更多性能提升！
- en: Conclusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: You should now be equipped with a good overview of PyTorch’s C++ extension mechanism
    as well as a motivation for using them. You can find the code examples displayed
    in this note [here](https://github.com/pytorch/extension-cpp). If you have questions,
    please use [the forums](https://discuss.pytorch.org). Also be sure to check our
    [FAQ](https://pytorch.org/cppdocs/notes/faq.html) in case you run into any issues.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该已经掌握了PyTorch的C++扩展机制的概述以及使用它们的动机。您可以在此笔记中找到显示的代码示例[这里](https://github.com/pytorch/extension-cpp)。如果您有问题，请使用[论坛](https://discuss.pytorch.org)。还要确保查看我们的[FAQ](https://pytorch.org/cppdocs/notes/faq.html)，以防遇到任何问题。
