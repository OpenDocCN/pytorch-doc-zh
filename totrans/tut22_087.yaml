- en: Custom C++ and CUDA Extensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/advanced/cpp_extension.html](https://pytorch.org/tutorials/advanced/cpp_extension.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Author**: [Peter Goldsborough](https://www.goldsborough.me/)'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch provides a plethora of operations related to neural networks, arbitrary
    tensor algebra, data wrangling and other purposes. However, you may still find
    yourself in need of a more customized operation. For example, you might want to
    use a novel activation function you found in a paper, or implement an operation
    you developed as part of your research.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way of integrating such a custom operation in PyTorch is to write
    it in Python by extending `Function` and `Module` as outlined [here](https://pytorch.org/docs/master/notes/extending.html).
    This gives you the full power of automatic differentiation (spares you from writing
    derivative functions) as well as the usual expressiveness of Python. However,
    there may be times when your operation is better implemented in C++. For example,
    your code may need to be *really* fast because it is called very frequently in
    your model or is very expensive even for few calls. Another plausible reason is
    that it depends on or interacts with other C or C++ libraries. To address such
    cases, PyTorch provides a very easy way of writing custom *C++ extensions*.
  prefs: []
  type: TYPE_NORMAL
- en: C++ extensions are a mechanism we have developed to allow users (you) to create
    PyTorch operators defined *out-of-source*, i.e. separate from the PyTorch backend.
    This approach is *different* from the way native PyTorch operations are implemented.
    C++ extensions are intended to spare you much of the boilerplate associated with
    integrating an operation with PyTorch’s backend while providing you with a high
    degree of flexibility for your PyTorch-based projects. Nevertheless, once you
    have defined your operation as a C++ extension, turning it into a native PyTorch
    function is largely a matter of code organization, which you can tackle after
    the fact if you decide to contribute your operation upstream.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation and Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rest of this note will walk through a practical example of writing and using
    a C++ (and CUDA) extension. If you are being chased or someone will fire you if
    you don’t get that op done by the end of the day, you can skip this section and
    head straight to the implementation details in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you’ve come up with a new kind of recurrent unit that you found to
    have superior properties compared to the state of the art. This recurrent unit
    is similar to an LSTM, but differs in that it lacks a *forget gate* and uses an
    *Exponential Linear Unit* (ELU) as its internal activation function. Because this
    unit never forgets, we’ll call it *LLTM*, or *Long-Long-Term-Memory* unit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two ways in which LLTMs differ from vanilla LSTMs are significant enough
    that we can’t configure PyTorch’s `LSTMCell` for our purposes, so we’ll have to
    create a custom cell. The first and easiest approach for this – and likely in
    all cases a good first step – is to implement our desired functionality in plain
    PyTorch with Python. For this, we need to subclass [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module
    "(in PyTorch v2.2)") and implement the forward pass of the LLTM. This would look
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'which we could then use as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Naturally, if at all possible and plausible, you should use this approach to
    extend PyTorch. Since PyTorch has highly optimized implementations of its operations
    for CPU *and* GPU, powered by libraries such as [NVIDIA cuDNN](https://developer.nvidia.com/cudnn),
    [Intel MKL](https://software.intel.com/en-us/mkl) or [NNPACK](https://github.com/Maratyszcza/NNPACK),
    PyTorch code like above will often be fast enough. However, we can also see why,
    under certain circumstances, there is room for further performance improvements.
    The most obvious reason is that PyTorch has no knowledge of the *algorithm* you
    are implementing. It knows only of the individual operations you use to compose
    your algorithm. As such, PyTorch must execute your operations individually, one
    after the other. Since each individual call to the implementation (or *kernel*)
    of an operation, which may involve the launch of a CUDA kernel, has a certain
    amount of overhead, this overhead may become significant across many function
    calls. Furthermore, the Python interpreter that is running our code can itself
    slow down our program.
  prefs: []
  type: TYPE_NORMAL
- en: A definite method of speeding things up is therefore to rewrite parts in C++
    (or CUDA) and *fuse* particular groups of operations. Fusing means combining the
    implementations of many functions into a single function, which profits from fewer
    kernel launches as well as other optimizations we can perform with increased visibility
    of the global flow of data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can use C++ extensions to implement a *fused* version of the
    LLTM. We’ll begin by writing it in plain C++, using the [ATen](https://github.com/zdevito/ATen)
    library that powers much of PyTorch’s backend, and see how easily it lets us translate
    our Python code. We’ll then speed things up even more by moving parts of the model
    to CUDA kernel to benefit from the massive parallelism GPUs provide.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a C++ Extension
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'C++ extensions come in two flavors: They can be built “ahead of time” with
    `setuptools`, or “just in time” via [`torch.utils.cpp_extension.load()`](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load
    "(in PyTorch v2.2)"). We’ll begin with the first approach and discuss the latter
    later.'
  prefs: []
  type: TYPE_NORMAL
- en: Building with `setuptools`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the “ahead of time” flavor, we build our C++ extension by writing a `setup.py`
    script that uses setuptools to compile our C++ code. For the LLTM, it looks as
    simple as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, `CppExtension` is a convenience wrapper around `setuptools.Extension`
    that passes the correct include paths and sets the language of the extension to
    C++. The equivalent vanilla `setuptools` code would simply be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`BuildExtension` performs a number of required configuration steps and checks
    and also manages mixed compilation in the case of mixed C++/CUDA extensions. And
    that’s all we really need to know about building C++ extensions for now! Let’s
    now take a look at the implementation of our C++ extension, which goes into `lltm.cpp`.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing the C++ Op
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start implementing the LLTM in C++! One function we’ll need for the backward
    pass is the derivative of the sigmoid. This is a small enough piece of code to
    discuss the overall environment that is available to us when writing C++ extensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`<torch/extension.h>` is the one-stop header to include all the necessary PyTorch
    bits to write C++ extensions. It includes:'
  prefs: []
  type: TYPE_NORMAL
- en: The ATen library, which is our primary API for tensor computation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[pybind11](https://github.com/pybind/pybind11), which is how we create Python
    bindings for our C++ code,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Headers that manage the details of interaction between ATen and pybind11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation of `d_sigmoid()` shows how to use the ATen API. PyTorch’s
    tensor and variable interface is generated automatically from the ATen library,
    so we can more or less translate our Python implementation 1:1 into C++. Our primary
    datatype for all computations will be `torch::Tensor`. Its full API can be inspected
    [here](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html). Notice also that
    we can include `<iostream>` or *any other C or C++ header* – we have the full
    power of C++11 at our disposal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h
    on Windows. To workaround the issue, move python binding logic to pure C++ file.
    Example use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Currently open issue for nvcc bug [here](https://github.com/pytorch/pytorch/issues/69460).
    Complete workaround code example [here](https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48).
  prefs: []
  type: TYPE_NORMAL
- en: Forward Pass
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next we can port our entire forward pass to C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Backward Pass
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The C++ extension API currently does not provide a way of automatically generating
    a backwards function for us. As such, we have to also implement the backward pass
    of our LLTM, which computes the derivative of the loss with respect to each input
    of the forward pass. Ultimately, we will plop both the forward and backward function
    into a [`torch.autograd.Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function
    "(in PyTorch v2.2)") to create a nice Python binding. The backward function is
    slightly more involved, so we’ll not dig deeper into the code (if you are interested,
    [Alex Graves’ thesis](https://www.cs.toronto.edu/~graves/phd.pdf) is a good read
    for more information on this):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Binding to Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have your operation written in C++ and ATen, you can use pybind11 to
    bind your C++ functions or classes into Python in a very simple manner. Questions
    or issues you have about this part of PyTorch C++ extensions will largely be addressed
    by [pybind11 documentation](https://pybind11.readthedocs.io/en/stable/).
  prefs: []
  type: TYPE_NORMAL
- en: 'For our extensions, the necessary binding code spans only four lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: One bit to note here is the macro `TORCH_EXTENSION_NAME`. The torch extension
    build will define it as the name you give your extension in the `setup.py` script.
    In this case, the value of `TORCH_EXTENSION_NAME` would be “lltm_cpp”. This is
    to avoid having to maintain the name of the extension in two places (the build
    script and your C++ code), as a mismatch between the two can lead to nasty and
    hard to track issues.
  prefs: []
  type: TYPE_NORMAL
- en: Using Your Extension
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are now set to import our extension in PyTorch. At this point, your directory
    structure could look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run `python setup.py install` to build and install your extension. This
    should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A small note on compilers: Due to ABI versioning issues, the compiler you use
    to build your C++ extension must be *ABI-compatible* with the compiler PyTorch
    was built with. In practice, this means that you must use GCC version 4.9 and
    above on Linux. For Ubuntu 16.04 and other more-recent Linux distributions, this
    should be the default compiler already. On MacOS, you must use clang (which does
    not have any ABI versioning issues). In the worst case, you can build PyTorch
    from source with your compiler and then build the extension with that same compiler.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your extension is built, you can simply import it in Python, using the
    name you specified in your `setup.py` script. Just be sure to `import torch` first,
    as this will resolve some symbols that the dynamic linker must see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If we call `help()` on the function or module, we can see that its signature
    matches our C++ code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are now able to call our C++ functions from Python, we can wrap them
    with [`torch.autograd.Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function
    "(in PyTorch v2.2)") and [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module
    "(in PyTorch v2.2)") to make them first class citizens of PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Performance Comparison
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now that we are able to use and call our C++ code from PyTorch, we can run
    a small benchmark to see how much performance we gained from rewriting our op
    in C++. We’ll run the LLTM forwards and backwards a few times and measure the
    duration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this code with the original LLTM we wrote in pure Python at the start
    of this post, we get the following numbers (on my machine):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'and with our new C++ version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can already see a significant speedup for the forward function (more than
    30%). For the backward function, a speedup is visible, albeit not a major one.
    The backward pass I wrote above was not particularly optimized and could definitely
    be improved. Also, PyTorch’s automatic differentiation engine can automatically
    parallelize computation graphs, may use a more efficient flow of operations overall,
    and is also implemented in C++, so it’s expected to be fast. Nevertheless, this
    is a good start.
  prefs: []
  type: TYPE_NORMAL
- en: Performance on GPU Devices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A wonderful fact about PyTorch’s *ATen* backend is that it abstracts the computing
    device you are running on. This means the same code we wrote for CPU can *also*
    run on GPU, and individual operations will correspondingly dispatch to GPU-optimized
    implementations. For certain operations like matrix multiply (like `mm` or `addmm`),
    this is a big win. Let’s take a look at how much performance we gain from running
    our C++ code with CUDA tensors. No changes to our implementation are required,
    we simply need to put our tensors in GPU memory from Python, with either adding
    `device=cuda_device` argument at creation time or using `.to(cuda_device)` after
    creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once more comparing our plain PyTorch code with our C++ version, now both running
    on CUDA devices, we again see performance gains. For Python/PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And C++/ATen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: That’s a great overall speedup compared to non-CUDA code. However, we can pull
    even more performance out of our C++ code by writing custom CUDA kernels, which
    we’ll dive into soon. Before that, let’s discuss another way of building your
    C++ extensions.
  prefs: []
  type: TYPE_NORMAL
- en: JIT Compiling Extensions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previously, I mentioned there were two ways of building C++ extensions: using
    `setuptools` or just in time (JIT). Having covered the former, let’s elaborate
    on the latter. The JIT compilation mechanism provides you with a way of compiling
    and loading your extensions on the fly by calling a simple function in PyTorch’s
    API called [`torch.utils.cpp_extension.load()`](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load
    "(in PyTorch v2.2)"). For the LLTM, this would look as simple as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we provide the function with the same information as for `setuptools`.
    In the background, this will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a temporary directory `/tmp/torch_extensions/lltm`,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Emit a [Ninja](https://ninja-build.org/) build file into that temporary directory,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile your source files into a shared library,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import this shared library as a Python module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In fact, if you pass `verbose=True` to `cpp_extension.load()`, you will be
    informed about the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The resulting Python module will be exactly the same as produced by setuptools,
    but removes the requirement of having to maintain a separate `setup.py` build
    file. If your setup is more complicated and you do need the full power of `setuptools`,
    you *can* write your own `setup.py` – but in many cases this JIT technique will
    do just fine. The first time you run through this line, it will take some time,
    as the extension is compiling in the background. Since we use the Ninja build
    system to build your sources, re-compilation is incremental and thus re-loading
    the extension when you run your Python module a second time is fast and has low
    overhead if you didn’t change the extension’s source files.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a Mixed C++/CUDA extension
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To really take our implementation to the next level, we can hand-write parts
    of our forward and backward passes with custom CUDA kernels. For the LLTM, this
    has the prospect of being particularly effective, as there are a large number
    of pointwise operations in sequence, that can all be fused and parallelized in
    a single CUDA kernel. Let’s see how we could write such a CUDA kernel and integrate
    it with PyTorch using this extension mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: The general strategy for writing a CUDA extension is to first write a C++ file
    which defines the functions that will be called from Python, and binds those functions
    to Python with pybind11\. Furthermore, this file will also *declare* functions
    that are defined in CUDA (`.cu`) files. The C++ functions will then do some checks
    and ultimately forward its calls to the CUDA functions. In the CUDA files, we
    write our actual CUDA kernels. The `cpp_extension` package will then take care
    of compiling the C++ sources with a C++ compiler like `gcc` and the CUDA sources
    with NVIDIA’s `nvcc` compiler. This ensures that each compiler takes care of files
    it knows best to compile. Ultimately, they will be linked into one shared library
    that is available to us from Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with the C++ file, which we’ll call `lltm_cuda.cpp`, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, it is largely boilerplate, checks and forwarding to functions
    that we’ll define in the CUDA file. We’ll name this file `lltm_cuda_kernel.cu`
    (note the `.cu` extension!). NVCC can reasonably compile C++11, thus we still
    have ATen and the C++ standard library available to us (but not `torch.h`). Note
    that `setuptools` cannot handle files with the same name but different extensions,
    so if you use the `setup.py` method instead of the JIT method, you must give your
    CUDA file a different name than your C++ file (for the JIT method, `lltm.cpp`
    and `lltm.cu` would work fine). Let’s take a small peek at what this file will
    look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we see the headers I just described, as well as the fact that we are using
    CUDA-specific declarations like `__device__` and `__forceinline__` and functions
    like `exp`. Let’s continue with a few more helper functions that we’ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To now actually implement a function, we’ll again need two things: one function
    that performs operations we don’t wish to explicitly write by hand and calls into
    CUDA kernels, and then the actual CUDA kernel for the parts we want to speed up.
    For the forward pass, the first function should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The main point of interest here is the `AT_DISPATCH_FLOATING_TYPES` macro and
    the kernel launch (indicated by the `<<<...>>>`). While ATen abstracts away the
    device and datatype of the tensors we deal with, a tensor will, at runtime, still
    be backed by memory of a concrete type on a concrete device. As such, we need
    a way of determining at runtime what type a tensor is and then selectively call
    functions with the corresponding correct type signature. Done manually, this would
    (conceptually) look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The purpose of `AT_DISPATCH_FLOATING_TYPES` is to take care of this dispatch
    for us. It takes a type (`gates.type()` in our case), a name (for error messages)
    and a lambda function. Inside this lambda function, the type alias `scalar_t`
    is available and is defined as the type that the tensor actually is at runtime
    in that context. As such, if we have a template function (which our CUDA kernel
    will be), we can instantiate it with this `scalar_t` alias, and the correct function
    will be called. In this case, we also want to retrieve the data pointers of the
    tensors as pointers of that `scalar_t` type. If you wanted to dispatch over all
    types and not just floating point types (`Float` and `Double`), you can use `AT_DISPATCH_ALL_TYPES`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we perform some operations with plain ATen. These operations will
    still run on the GPU, but using ATen’s default implementations. This makes sense
    because ATen will use highly optimized routines for things like matrix multiplies
    (e.g. `addmm`) or convolutions which would be much harder to implement and improve
    ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: As for the kernel launch itself, we are here specifying that each CUDA block
    will have 1024 threads, and that the entire GPU grid is split into as many blocks
    of `1 x 1024` threads as are required to fill our matrices with one thread per
    component. For example, if our state size was 2048 and our batch size 4, we’d
    launch a total of `4 x 2 = 8` blocks with each 1024 threads. If you’ve never heard
    of CUDA “blocks” or “grids” before, an [introductory read about CUDA](https://devblogs.nvidia.com/even-easier-introduction-cuda)
    may help.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual CUDA kernel is fairly simple (if you’ve ever programmed GPUs before):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: What’s primarily interesting here is that we are able to compute all of these
    pointwise operations entirely in parallel for each individual component in our
    gate matrices. If you imagine having to do this with a giant `for` loop over a
    million elements in serial, you can see why this would be much faster.
  prefs: []
  type: TYPE_NORMAL
- en: Using accessors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can see in the CUDA kernel that we work directly on pointers with the right
    type. Indeed, working directly with high level type agnostic tensors inside cuda
    kernels would be very inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this comes at a cost of ease of use and readability, especially for
    highly dimensional data. In our example, we know for example that the contiguous
    `gates` tensor has 3 dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: batch, size of `batch_size` and stride of `3*state_size`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: row, size of `3` and stride of `state_size`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: index, size of `state_size` and stride of `1`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we access the element `gates[n][row][column]` inside the kernel then?
    It turns out that you need the strides to access your element with some simple
    arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In addition to being verbose, this expression needs stride to be explicitly
    known, and thus passed to the kernel function within its arguments. You can see
    that in the case of kernel functions accepting multiple tensors with different
    sizes you will end up with a very long list of arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately for us, ATen provides accessors that are created with a single
    dynamic check that a Tensor is the type and number of dimensions. Accessors then
    expose an API for accessing the Tensor elements efficiently without having to
    convert to a single pointer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Accessor objects have a relatively high level interface, with `.size()` and
    `.stride()` methods and multi-dimensional indexing. The `.accessor<>` interface
    is designed to access data efficiently on cpu tensor. The equivalent for cuda
    tensors are `packed_accessor64<>` and `packed_accessor32<>`, which produce Packed
    Accessors with either 64-bit or 32-bit integer indexing.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental difference with Accessor is that a Packed Accessor copies size
    and stride data inside of its structure instead of pointing to it. It allows us
    to pass it to a CUDA kernel function and use its interface inside it.
  prefs: []
  type: TYPE_NORMAL
- en: We can design a function that takes Packed Accessors instead of pointers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Let’s decompose the template used here. the first two arguments `scalar_t` and
    `2` are the same as regular Accessor. The argument `torch::RestrictPtrTraits`
    indicates that the `__restrict__` keyword must be used. Note also that we’ve used
    the `PackedAccessor32` variant which store the sizes and strides in an `int32_t`.
    This is important as using the 64-bit variant (`PackedAccessor64`) can make the
    kernel slower.
  prefs: []
  type: TYPE_NORMAL
- en: The function declaration becomes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The implementation is much more readable! This function is then called by creating
    Packed Accessors with the `.packed_accessor32<>` method within the host function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The backwards pass follows much the same pattern and I won’t elaborate further
    on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Integrating a C++/CUDA Operation with PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Integration of our CUDA-enabled op with PyTorch is again very straightforward.
    If you want to write a `setup.py` script, it could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of `CppExtension()`, we now use `CUDAExtension()`. We can just specify
    the `.cu` file along with the `.cpp` files – the library takes care of all the
    hassle this entails for you. The JIT mechanism is even simpler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Performance Comparison
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our hope was that parallelizing and fusing the pointwise operations of our
    code with CUDA would improve the performance of our LLTM. Let’s see if that holds
    true. We can run the code I listed earlier to run a benchmark. Our fastest version
    earlier was the CUDA-based C++ code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'And now with our custom CUDA kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: More performance increases!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You should now be equipped with a good overview of PyTorch’s C++ extension mechanism
    as well as a motivation for using them. You can find the code examples displayed
    in this note [here](https://github.com/pytorch/extension-cpp). If you have questions,
    please use [the forums](https://discuss.pytorch.org). Also be sure to check our
    [FAQ](https://pytorch.org/cppdocs/notes/faq.html) in case you run into any issues.
  prefs: []
  type: TYPE_NORMAL
