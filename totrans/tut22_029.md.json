["```py\nwget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data\ncd data && unzip PennFudanPed.zip \n```", "```py\nPennFudanPed/\n  PedMasks/\n    FudanPed00001_mask.png\n    FudanPed00002_mask.png\n    FudanPed00003_mask.png\n    FudanPed00004_mask.png\n    ...\n  PNGImages/\n    FudanPed00001.png\n    FudanPed00002.png\n    FudanPed00003.png\n    FudanPed00004.png \n```", "```py\nimport matplotlib.pyplot as plt\nfrom torchvision.io import [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image \"torchvision.io.read_image\")\n\n[image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image \"torchvision.io.read_image\")(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\n[mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image \"torchvision.io.read_image\")(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n\nplt.figure(figsize=(16, 8))\nplt.subplot(121)\nplt.title(\"Image\")\nplt.imshow([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").permute(1, 2, 0))\nplt.subplot(122)\nplt.title(\"Mask\")\nplt.imshow([mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").permute(1, 2, 0)) \n```", "```py\n<matplotlib.image.AxesImage object at 0x7f489920ffd0> \n```", "```py\nimport os\nimport torch\n\nfrom torchvision.io import [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image \"torchvision.io.read_image\")\nfrom torchvision.ops.boxes import [masks_to_boxes](https://pytorch.org/vision/stable/generated/torchvision.ops.masks_to_boxes.html#torchvision.ops.masks_to_boxes \"torchvision.ops.masks_to_boxes\")\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\n\nclass PennFudanDataset([torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset \"torch.utils.data.Dataset\")):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.[masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.[masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")[idx])\n        img = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image \"torchvision.io.read_image\")(img_path)\n        [mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image \"torchvision.io.read_image\")(mask_path)\n        # instances are encoded as different colors\n        obj_ids = [torch.unique](https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique \"torch.unique\")([mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n        num_objs = len(obj_ids)\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        [masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = ([mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") == obj_ids[:, None, None]).to(dtype=[torch.uint8](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n\n        # get bounding box coordinates for each mask\n        boxes = [masks_to_boxes](https://pytorch.org/vision/stable/generated/torchvision.ops.masks_to_boxes.html#torchvision.ops.masks_to_boxes \"torchvision.ops.masks_to_boxes\")([masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\n        # there is only one class\n        labels = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones \"torch.ones\")((num_objs,), dtype=[torch.int64](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros \"torch.zeros\")((num_objs,), dtype=[torch.int64](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n\n        # Wrap sample and targets into torchvision tv_tensors:\n        img = [tv_tensors.Image](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Image.html#torchvision.tv_tensors.Image \"torchvision.tv_tensors.Image\")(img)\n\n        target = {}\n        target[\"boxes\"] = [tv_tensors.BoundingBoxes](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.BoundingBoxes.html#torchvision.tv_tensors.BoundingBoxes \"torchvision.tv_tensors.BoundingBoxes\")(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"masks\"] = [tv_tensors.Mask](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Mask.html#torchvision.tv_tensors.Mask \"torchvision.tv_tensors.Mask\")([masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs) \n```", "```py\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import [FastRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")\n\n# load a model pre-trained on COCO\nmodel = [torchvision.models.detection.fasterrcnn_resnet50_fpn](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn \"torchvision.models.detection.fasterrcnn_resnet50_fpn\")(weights=\"DEFAULT\")\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = [FastRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(in_features, num_classes) \n```", "```py\nDownloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n\n  0%|          | 0.00/160M [00:00<?, ?B/s]\n  8%|7         | 12.1M/160M [00:00<00:01, 127MB/s]\n 16%|#6        | 25.8M/160M [00:00<00:01, 137MB/s]\n 25%|##4       | 39.6M/160M [00:00<00:00, 140MB/s]\n 34%|###3      | 53.6M/160M [00:00<00:00, 143MB/s]\n 42%|####2     | 67.5M/160M [00:00<00:00, 144MB/s]\n 51%|#####     | 81.4M/160M [00:00<00:00, 145MB/s]\n 60%|#####9    | 95.4M/160M [00:00<00:00, 145MB/s]\n 68%|######8   | 109M/160M [00:00<00:00, 145MB/s]\n 77%|#######7  | 123M/160M [00:00<00:00, 146MB/s]\n 86%|########5 | 137M/160M [00:01<00:00, 146MB/s]\n 95%|#########4| 151M/160M [00:01<00:00, 146MB/s]\n100%|##########| 160M/160M [00:01<00:00, 144MB/s] \n```", "```py\nimport torchvision\nfrom torchvision.models.detection import [FasterRCNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")\nfrom torchvision.models.detection.rpn import [AnchorGenerator](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")\n\n# load a pre-trained model for classification and return\n# only the features\n[backbone](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\") = [torchvision.models.mobilenet_v2](https://pytorch.org/vision/stable/models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.mobilenet_v2 \"torchvision.models.mobilenet_v2\")(weights=\"DEFAULT\").features\n# ``FasterRCNN`` needs to know the number of\n# output channels in a backbone. For mobilenet_v2, it's 1280\n# so we need to add it here\n[backbone](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\").out_channels = 1280\n\n# let's make the RPN generate 5 x 3 anchors per spatial\n# location, with 5 different sizes and 3 different aspect\n# ratios. We have a Tuple[Tuple[int]] because each feature\n# map could potentially have different sizes and\n# aspect ratios\nanchor_generator = [AnchorGenerator](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(\n    sizes=((32, 64, 128, 256, 512),),\n    aspect_ratios=((0.5, 1.0, 2.0),)\n)\n\n# let's define what are the feature maps that we will\n# use to perform the region of interest cropping, as well as\n# the size of the crop after rescaling.\n# if your backbone returns a Tensor, featmap_names is expected to\n# be [0]. More generally, the backbone should return an\n# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n# feature maps to use.\n[roi_pooler](https://pytorch.org/vision/stable/generated/torchvision.ops.MultiScaleRoIAlign.html#torchvision.ops.MultiScaleRoIAlign \"torchvision.ops.MultiScaleRoIAlign\") = [torchvision.ops.MultiScaleRoIAlign](https://pytorch.org/vision/stable/generated/torchvision.ops.MultiScaleRoIAlign.html#torchvision.ops.MultiScaleRoIAlign \"torchvision.ops.MultiScaleRoIAlign\")(\n    featmap_names=['0'],\n    output_size=7,\n    sampling_ratio=2\n)\n\n# put the pieces together inside a Faster-RCNN model\nmodel = [FasterRCNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(\n    [backbone](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\"),\n    num_classes=2,\n    rpn_anchor_generator=anchor_generator,\n    box_roi_pool=[roi_pooler](https://pytorch.org/vision/stable/generated/torchvision.ops.MultiScaleRoIAlign.html#torchvision.ops.MultiScaleRoIAlign \"torchvision.ops.MultiScaleRoIAlign\")\n) \n```", "```py\nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n\n  0%|          | 0.00/13.6M [00:00<?, ?B/s]\n 92%|#########1| 12.5M/13.6M [00:00<00:00, 131MB/s]\n100%|##########| 13.6M/13.6M [00:00<00:00, 131MB/s] \n```", "```py\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import [FastRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")\nfrom torchvision.models.detection.mask_rcnn import [MaskRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")\n\ndef get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = [torchvision.models.detection.maskrcnn_resnet50_fpn](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html#torchvision.models.detection.maskrcnn_resnet50_fpn \"torchvision.models.detection.maskrcnn_resnet50_fpn\")(weights=\"DEFAULT\")\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = [FastRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = [MaskRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n        in_features_mask,\n        hidden_layer,\n        num_classes\n    )\n\n    return model \n```", "```py\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\") \n```", "```py\n0 \n```", "```py\nfrom torchvision.transforms import v2 as T\n\ndef get_transform(train):\n    transforms = []\n    if train:\n        transforms.append([T.RandomHorizontalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomHorizontalFlip.html#torchvision.transforms.v2.RandomHorizontalFlip \"torchvision.transforms.v2.RandomHorizontalFlip\")(0.5))\n    transforms.append([T.ToDtype](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToDtype.html#torchvision.transforms.v2.ToDtype \"torchvision.transforms.v2.ToDtype\")([torch.float](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"), scale=True))\n    transforms.append([T.ToPureTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToPureTensor.html#torchvision.transforms.v2.ToPureTensor \"torchvision.transforms.v2.ToPureTensor\")())\n    return [T.Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose \"torchvision.transforms.v2.Compose\")(transforms) \n```", "```py\nimport utils\n\nmodel = [torchvision.models.detection.fasterrcnn_resnet50_fpn](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn \"torchvision.models.detection.fasterrcnn_resnet50_fpn\")(weights=\"DEFAULT\")\n[dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\") = [PennFudanDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset \"torch.utils.data.Dataset\")('data/PennFudanPed', get_transform(train=True))\n[data_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")(\n    [dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\"),\n    batch_size=2,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=utils.collate_fn\n)\n\n# For Training\nimages, targets = next(iter([data_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")))\nimages = list([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") for [image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]\noutput = model(images, targets)  # Returns losses and detections\nprint(output)\n\n# For inference\n[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")()\n[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [[torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(3, 300, 400), [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(3, 500, 400)]\npredictions = model([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))  # Returns predictions\nprint(predictions[0]) \n```", "```py\n{'loss_classifier': tensor(0.0689, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0268, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0055, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0036, grad_fn=<DivBackward0>)}\n{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)} \n```", "```py\nfrom engine import train_one_epoch, evaluate\n\n# train on the GPU or on the CPU, if a GPU is not available\n[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")('cuda') if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available \"torch.cuda.is_available\")() else [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n# use our dataset and defined transformations\n[dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\") = [PennFudanDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset \"torch.utils.data.Dataset\")('data/PennFudanPed', get_transform(train=True))\n[dataset_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\") = [PennFudanDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset \"torch.utils.data.Dataset\")('data/PennFudanPed', get_transform(train=False))\n\n# split the dataset in train and test set\nindices = [torch.randperm](https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm \"torch.randperm\")(len([dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\"))).tolist()\n[dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\") = [torch.utils.data.Subset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\")([dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\"), indices[:-50])\n[dataset_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\") = [torch.utils.data.Subset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\")([dataset_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\"), indices[-50:])\n\n# define training and validation data loaders\n[data_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")(\n    [dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\"),\n    batch_size=2,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=utils.collate_fn\n)\n\n[data_loader_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")(\n    [dataset_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset \"torch.utils.data.Subset\"),\n    batch_size=1,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=utils.collate_fn\n)\n\n# get the model using our helper function\nmodel = get_model_instance_segmentation(num_classes)\n\n# move model to the right device\n[model.to](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to \"torch.nn.Module.to\")([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\n# construct an optimizer\nparams = [p for p in [model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")() if p.requires_grad]\n[optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\") = [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\")(\n    params,\n    lr=0.005,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\n# and a learning rate scheduler\n[lr_scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\") = [torch.optim.lr_scheduler.StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\")(\n    [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\"),\n    step_size=3,\n    gamma=0.1\n)\n\n# let's train it just for 2 epochs\nnum_epochs = 2\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\"), [data_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"), epoch, print_freq=10)\n    # update the learning rate\n    [lr_scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\").step()\n    # evaluate on the test dataset\n    evaluate(model, [data_loader_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\nprint(\"That's it!\") \n```", "```py\nDownloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n\n  0%|          | 0.00/170M [00:00<?, ?B/s]\n  8%|7         | 12.7M/170M [00:00<00:01, 134MB/s]\n 16%|#5        | 26.8M/170M [00:00<00:01, 142MB/s]\n 24%|##3       | 40.7M/170M [00:00<00:00, 144MB/s]\n 32%|###2      | 54.8M/170M [00:00<00:00, 145MB/s]\n 41%|####      | 68.9M/170M [00:00<00:00, 146MB/s]\n 49%|####8     | 83.0M/170M [00:00<00:00, 147MB/s]\n 57%|#####7    | 97.1M/170M [00:00<00:00, 147MB/s]\n 65%|######5   | 111M/170M [00:00<00:00, 147MB/s]\n 74%|#######3  | 125M/170M [00:00<00:00, 148MB/s]\n 82%|########2 | 140M/170M [00:01<00:00, 148MB/s]\n 90%|######### | 154M/170M [00:01<00:00, 148MB/s]\n 99%|#########8| 168M/170M [00:01<00:00, 148MB/s]\n100%|##########| 170M/170M [00:01<00:00, 147MB/s]\nEpoch: [0]  [ 0/60]  eta: 0:02:32  lr: 0.000090  loss: 3.8792 (3.8792)  loss_classifier: 0.4863 (0.4863)  loss_box_reg: 0.2543 (0.2543)  loss_mask: 3.1288 (3.1288)  loss_objectness: 0.0043 (0.0043)  loss_rpn_box_reg: 0.0055 (0.0055)  time: 2.5479  data: 0.2985  max mem: 2783\nEpoch: [0]  [10/60]  eta: 0:00:52  lr: 0.000936  loss: 1.7038 (2.3420)  loss_classifier: 0.3913 (0.3626)  loss_box_reg: 0.2683 (0.2687)  loss_mask: 1.1038 (1.6881)  loss_objectness: 0.0204 (0.0184)  loss_rpn_box_reg: 0.0049 (0.0043)  time: 1.0576  data: 0.0315  max mem: 3158\nEpoch: [0]  [20/60]  eta: 0:00:39  lr: 0.001783  loss: 0.9972 (1.5790)  loss_classifier: 0.2425 (0.2735)  loss_box_reg: 0.2683 (0.2756)  loss_mask: 0.3489 (1.0043)  loss_objectness: 0.0127 (0.0184)  loss_rpn_box_reg: 0.0051 (0.0072)  time: 0.9143  data: 0.0057  max mem: 3158\nEpoch: [0]  [30/60]  eta: 0:00:28  lr: 0.002629  loss: 0.5966 (1.2415)  loss_classifier: 0.0979 (0.2102)  loss_box_reg: 0.2580 (0.2584)  loss_mask: 0.2155 (0.7493)  loss_objectness: 0.0119 (0.0165)  loss_rpn_box_reg: 0.0057 (0.0071)  time: 0.9036  data: 0.0065  max mem: 3158\nEpoch: [0]  [40/60]  eta: 0:00:18  lr: 0.003476  loss: 0.5234 (1.0541)  loss_classifier: 0.0737 (0.1749)  loss_box_reg: 0.2241 (0.2505)  loss_mask: 0.1796 (0.6080)  loss_objectness: 0.0055 (0.0135)  loss_rpn_box_reg: 0.0047 (0.0071)  time: 0.8759  data: 0.0064  max mem: 3158\nEpoch: [0]  [50/60]  eta: 0:00:09  lr: 0.004323  loss: 0.3642 (0.9195)  loss_classifier: 0.0435 (0.1485)  loss_box_reg: 0.1648 (0.2312)  loss_mask: 0.1585 (0.5217)  loss_objectness: 0.0025 (0.0113)  loss_rpn_box_reg: 0.0047 (0.0069)  time: 0.8693  data: 0.0065  max mem: 3158\nEpoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.3504 (0.8381)  loss_classifier: 0.0379 (0.1339)  loss_box_reg: 0.1343 (0.2178)  loss_mask: 0.1585 (0.4690)  loss_objectness: 0.0011 (0.0102)  loss_rpn_box_reg: 0.0048 (0.0071)  time: 0.8884  data: 0.0066  max mem: 3158\nEpoch: [0] Total time: 0:00:55 (0.9230 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:23  model_time: 0.2550 (0.2550)  evaluator_time: 0.0066 (0.0066)  time: 0.4734  data: 0.2107  max mem: 3158\nTest:  [49/50]  eta: 0:00:00  model_time: 0.1697 (0.1848)  evaluator_time: 0.0057 (0.0078)  time: 0.1933  data: 0.0034  max mem: 3158\nTest: Total time: 0:00:10 (0.2022 s / it)\nAveraged stats: model_time: 0.1697 (0.1848)  evaluator_time: 0.0057 (0.0078)\nAccumulating evaluation results...\nDONE (t=0.02s).\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.686\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.974\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.802\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.322\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.611\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.708\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.314\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.738\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.739\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.727\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.750\nIoU metric: segm\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.697\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.871\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.339\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.332\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.719\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.314\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.736\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.737\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.709\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.744\nEpoch: [1]  [ 0/60]  eta: 0:01:12  lr: 0.005000  loss: 0.3167 (0.3167)  loss_classifier: 0.0377 (0.0377)  loss_box_reg: 0.1232 (0.1232)  loss_mask: 0.1439 (0.1439)  loss_objectness: 0.0022 (0.0022)  loss_rpn_box_reg: 0.0097 (0.0097)  time: 1.2113  data: 0.2601  max mem: 3158\nEpoch: [1]  [10/60]  eta: 0:00:45  lr: 0.005000  loss: 0.3185 (0.3209)  loss_classifier: 0.0377 (0.0376)  loss_box_reg: 0.1053 (0.1058)  loss_mask: 0.1563 (0.1684)  loss_objectness: 0.0012 (0.0017)  loss_rpn_box_reg: 0.0064 (0.0073)  time: 0.9182  data: 0.0290  max mem: 3158\nEpoch: [1]  [20/60]  eta: 0:00:36  lr: 0.005000  loss: 0.2989 (0.2902)  loss_classifier: 0.0338 (0.0358)  loss_box_reg: 0.0875 (0.0952)  loss_mask: 0.1456 (0.1517)  loss_objectness: 0.0009 (0.0017)  loss_rpn_box_reg: 0.0050 (0.0058)  time: 0.8946  data: 0.0062  max mem: 3158\nEpoch: [1]  [30/60]  eta: 0:00:27  lr: 0.005000  loss: 0.2568 (0.2833)  loss_classifier: 0.0301 (0.0360)  loss_box_reg: 0.0836 (0.0912)  loss_mask: 0.1351 (0.1482)  loss_objectness: 0.0008 (0.0018)  loss_rpn_box_reg: 0.0031 (0.0061)  time: 0.8904  data: 0.0065  max mem: 3158\nEpoch: [1]  [40/60]  eta: 0:00:17  lr: 0.005000  loss: 0.2630 (0.2794)  loss_classifier: 0.0335 (0.0363)  loss_box_reg: 0.0804 (0.0855)  loss_mask: 0.1381 (0.1497)  loss_objectness: 0.0020 (0.0022)  loss_rpn_box_reg: 0.0030 (0.0056)  time: 0.8667  data: 0.0065  max mem: 3158\nEpoch: [1]  [50/60]  eta: 0:00:08  lr: 0.005000  loss: 0.2729 (0.2829)  loss_classifier: 0.0365 (0.0375)  loss_box_reg: 0.0685 (0.0860)  loss_mask: 0.1604 (0.1515)  loss_objectness: 0.0022 (0.0022)  loss_rpn_box_reg: 0.0031 (0.0056)  time: 0.8834  data: 0.0064  max mem: 3158\nEpoch: [1]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2930 (0.2816)  loss_classifier: 0.0486 (0.0381)  loss_box_reg: 0.0809 (0.0847)  loss_mask: 0.1466 (0.1511)  loss_objectness: 0.0012 (0.0021)  loss_rpn_box_reg: 0.0042 (0.0056)  time: 0.8855  data: 0.0064  max mem: 3158\nEpoch: [1] Total time: 0:00:53 (0.8890 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:23  model_time: 0.2422 (0.2422)  evaluator_time: 0.0061 (0.0061)  time: 0.4774  data: 0.2283  max mem: 3158\nTest:  [49/50]  eta: 0:00:00  model_time: 0.1712 (0.1832)  evaluator_time: 0.0051 (0.0066)  time: 0.1911  data: 0.0036  max mem: 3158\nTest: Total time: 0:00:10 (0.2001 s / it)\nAveraged stats: model_time: 0.1712 (0.1832)  evaluator_time: 0.0051 (0.0066)\nAccumulating evaluation results...\nDONE (t=0.01s).\nAccumulating evaluation results...\nDONE (t=0.01s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.791\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.981\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.961\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.368\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.673\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.809\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.361\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.826\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.826\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.500\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.800\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.838\nIoU metric: segm\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.745\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.984\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.902\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.334\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.504\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.769\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.341\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.782\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.782\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.500\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.709\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.797\nThat's it! \n```", "```py\nimport matplotlib.pyplot as plt\n\nfrom torchvision.utils import [draw_bounding_boxes](https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html#torchvision.utils.draw_bounding_boxes \"torchvision.utils.draw_bounding_boxes\"), [draw_segmentation_masks](https://pytorch.org/vision/stable/generated/torchvision.utils.draw_segmentation_masks.html#torchvision.utils.draw_segmentation_masks \"torchvision.utils.draw_segmentation_masks\")\n\n[image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image \"torchvision.io.read_image\")(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\n[eval_transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose \"torchvision.transforms.v2.Compose\") = get_transform(train=False)\n\n[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")()\nwith [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n    [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [eval_transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose \"torchvision.transforms.v2.Compose\")([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    # convert RGBA -> RGB and move to device\n    [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")[:3, ...].to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n    predictions = model([[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), ])\n    pred = predictions[0]\n\n[image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = (255.0 * ([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") - [image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").min()) / ([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").max() - [image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").min())).to([torch.uint8](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n[image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")[:3, ...]\npred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n[pred_boxes](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = pred[\"boxes\"].long()\n[output_image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [draw_bounding_boxes](https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html#torchvision.utils.draw_bounding_boxes \"torchvision.utils.draw_bounding_boxes\")([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [pred_boxes](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), pred_labels, colors=\"red\")\n\n[masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = (pred[\"masks\"] > 0.7).squeeze(1)\n[output_image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [draw_segmentation_masks](https://pytorch.org/vision/stable/generated/torchvision.utils.draw_segmentation_masks.html#torchvision.utils.draw_segmentation_masks \"torchvision.utils.draw_segmentation_masks\")([output_image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), alpha=0.5, colors=\"blue\")\n\nplt.figure(figsize=(12, 12))\nplt.imshow([output_image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").permute(1, 2, 0)) \n```", "```py\n<matplotlib.image.AxesImage object at 0x7f48881f2830> \n```"]