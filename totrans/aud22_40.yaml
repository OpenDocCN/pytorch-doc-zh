- en: Forced Alignment with Wav2Vec2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html](https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-tutorials-forced-alignment-tutorial-py) to download
    the full example code
  prefs: []
  type: TYPE_NORMAL
- en: '**Author**: [Moto Hira](mailto:moto%40meta.com)'
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial shows how to align transcript to speech with `torchaudio`, using
    CTC segmentation algorithm described in [CTC-Segmentation of Large Corpora for
    German End-to-end Speech Recognition](https://arxiv.org/abs/2007.09127).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial was originally written to illustrate a usecase for Wav2Vec2 pretrained
    model.
  prefs: []
  type: TYPE_NORMAL
- en: TorchAudio now has a set of APIs designed for forced alignment. The [CTC forced
    alignment API tutorial](./ctc_forced_alignment_api_tutorial.html) illustrates
    the usage of [`torchaudio.functional.forced_align()`](../generated/torchaudio.functional.forced_align.html#torchaudio.functional.forced_align
    "torchaudio.functional.forced_align"), which is the core API.
  prefs: []
  type: TYPE_NORMAL
- en: If you are looking to align your corpus, we recommend to use [`torchaudio.pipelines.Wav2Vec2FABundle`](../generated/torchaudio.pipelines.Wav2Vec2FABundle.html#torchaudio.pipelines.Wav2Vec2FABundle
    "torchaudio.pipelines.Wav2Vec2FABundle"), which combines [`forced_align()`](../generated/torchaudio.functional.forced_align.html#torchaudio.functional.forced_align
    "torchaudio.functional.forced_align") and other support functions with pre-trained
    model specifically trained for forced-alignment. Please refer to the [Forced alignment
    for multilingual data](forced_alignment_for_multilingual_data_tutorial.html) which
    illustrates its usage.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Overview[](#overview "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of alignment looks like the following.
  prefs: []
  type: TYPE_NORMAL
- en: Estimate the frame-wise label probability from audio waveform
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the trellis matrix which represents the probability of labels aligned
    at time step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the most likely path from the trellis matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example, we use `torchaudio`’s `Wav2Vec2` model for acoustic feature
    extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation[](#preparation "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First we import the necessary packages, and fetch data that we work on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Generate frame-wise label probability[](#generate-frame-wise-label-probability
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to generate the label class porbability of each audio frame.
    We can use a Wav2Vec2 model that is trained for ASR. Here we use [`torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H()`](../generated/torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H.html#torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H
    "torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H").
  prefs: []
  type: TYPE_NORMAL
- en: '`torchaudio` provides easy access to pretrained models with associated labels.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections, we will compute the probability in log-domain to
    avoid numerical instability. For this purpose, we normalize the `emission` with
    `torch.log_softmax()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Visualization[](#visualization "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Frame-wise class probability](../Images/efb61f2d411fc5066755dd6b78a9a867.png)'
  prefs: []
  type: TYPE_IMG
- en: Generate alignment probability (trellis)[](#generate-alignment-probability-trellis
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the emission matrix, next we generate the trellis which represents the
    probability of transcript labels occur at each time frame.
  prefs: []
  type: TYPE_NORMAL
- en: Trellis is 2D matrix with time axis and label axis. The label axis represents
    the transcript that we are aligning. In the following, we use \(t\) to denote
    the index in time axis and \(j\) to denote the index in label axis. \(c_j\) represents
    the label at label index \(j\).
  prefs: []
  type: TYPE_NORMAL
- en: To generate, the probability of time step \(t+1\), we look at the trellis from
    time step \(t\) and emission at time step \(t+1\). There are two path to reach
    to time step \(t+1\) with label \(c_{j+1}\). The first one is the case where the
    label was \(c_{j+1}\) at \(t\) and there was no label change from \(t\) to \(t+1\).
    The other case is where the label was \(c_j\) at \(t\) and it transitioned to
    the next label \(c_{j+1}\) at \(t+1\).
  prefs: []
  type: TYPE_NORMAL
- en: The follwoing diagram illustrates this transition.
  prefs: []
  type: TYPE_NORMAL
- en: '![https://download.pytorch.org/torchaudio/tutorial-assets/ctc-forward.png](../Images/cb0c89f6f8c29828d4d4d04ded7193b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Since we are looking for the most likely transitions, we take the more likely
    path for the value of \(k_{(t+1, j+1)}\), that is
  prefs: []
  type: TYPE_NORMAL
- en: \(k_{(t+1, j+1)} = max( k_{(t, j)} p(t+1, c_{j+1}), k_{(t, j+1)} p(t+1, repeat)
    )\)
  prefs: []
  type: TYPE_NORMAL
- en: where \(k\) represents is trellis matrix, and \(p(t, c_j)\) represents the probability
    of label \(c_j\) at time step \(t\). \(repeat\) represents the blank token from
    CTC formulation. (For the detail of CTC algorithm, please refer to the *Sequence
    Modeling with CTC* [[distill.pub](https://distill.pub/2017/ctc/)])
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Visualization[](#id1 "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![forced alignment tutorial](../Images/9cba4b626edb17a6e4b5838fd55a4e90.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above visualization, we can see that there is a trace of high probability
    crossing the matrix diagonally.
  prefs: []
  type: TYPE_NORMAL
- en: Find the most likely path (backtracking)[](#find-the-most-likely-path-backtracking
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the trellis is generated, we will traverse it following the elements with
    high probability.
  prefs: []
  type: TYPE_NORMAL
- en: We will start from the last label index with the time step of highest probability,
    then, we traverse back in time, picking stay (\(c_j \rightarrow c_j\)) or transition
    (\(c_j \rightarrow c_{j+1}\)), based on the post-transition probability \(k_{t,
    j} p(t+1, c_{j+1})\) or \(k_{t, j+1} p(t+1, repeat)\).
  prefs: []
  type: TYPE_NORMAL
- en: Transition is done once the label reaches the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: The trellis matrix is used for path-finding, but for the final probability of
    each segment, we take the frame-wise probability from emission matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Visualization[](#id2 "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![The path found by backtracking](../Images/bfe239f26439c642dad7b47fc213e358.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking good.
  prefs: []
  type: TYPE_NORMAL
- en: Segment the path[](#segment-the-path "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now this path contains repetations for the same labels, so let’s merge them
    to make it close to the original transcript.
  prefs: []
  type: TYPE_NORMAL
- en: When merging the multiple path points, we simply take the average probability
    for the merged segments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Visualization[](#id3 "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Path, label and probability for each label, Label probability with and without
    repetation](../Images/f07166f8b26588977594bdaa39644315.png)'
  prefs: []
  type: TYPE_IMG
- en: Looks good.
  prefs: []
  type: TYPE_NORMAL
- en: Merge the segments into words[](#merge-the-segments-into-words "Permalink to
    this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s merge the words. The Wav2Vec2 model uses `'|'` as the word boundary,
    so we merge the segments before each occurance of `'|'`.
  prefs: []
  type: TYPE_NORMAL
- en: Then, finally, we segment the original audio into segmented audio and listen
    to them to see if the segmentation is correct.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Visualization[](#id4 "Permalink to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![forced alignment tutorial](../Images/5f304131dabeba702068f67a1e4db351.png)'
  prefs: []
  type: TYPE_IMG
- en: Audio Samples[](#audio-samples "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 
  prefs: []
  type: TYPE_NORMAL
- en: Your browser does not support the audio element.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion[](#conclusion "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we looked how to use torchaudio’s Wav2Vec2 model to perform
    CTC segmentation for forced alignment.
  prefs: []
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 0 minutes 1.734 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: forced_alignment_tutorial.py`](../_downloads/fa57890a830bd47c0baa254781b3a8e1/forced_alignment_tutorial.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: forced_alignment_tutorial.ipynb`](../_downloads/160356f33d521341c47ec6b1406a3c2e/forced_alignment_tutorial.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
