- en: Pruning Tutorial
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/intermediate/pruning_tutorial.html](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-intermediate-pruning-tutorial-py) to download
    the full example code
  prefs: []
  type: TYPE_NORMAL
- en: '**Author**: [Michela Paganini](https://github.com/mickypaganini)'
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art deep learning techniques rely on over-parametrized models that
    are hard to deploy. On the contrary, biological neural networks are known to use
    efficient sparse connectivity. Identifying optimal techniques to compress models
    by reducing the number of parameters in them is important in order to reduce memory,
    battery, and hardware consumption without sacrificing accuracy. This in turn allows
    you to deploy lightweight models on device, and guarantee privacy with private
    on-device computation. On the research front, pruning is used to investigate the
    differences in learning dynamics between over-parametrized and under-parametrized
    networks, to study the role of lucky sparse subnetworks and initializations (“[lottery
    tickets](https://arxiv.org/abs/1803.03635)”) as a destructive neural architecture
    search technique, and more.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, you will learn how to use `torch.nn.utils.prune` to sparsify
    your neural networks, and how to extend it to implement your own custom pruning
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`"torch>=1.4.0a0+8e8a5e0"`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Create a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we use the [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)
    architecture from LeCun et al., 1998.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Inspect a Module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s inspect the (unpruned) `conv1` layer in our LeNet model. It will contain
    two parameters `weight` and `bias`, and no buffers, for now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Pruning a Module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To prune a module (in this example, the `conv1` layer of our LeNet architecture),
    first select a pruning technique among those available in `torch.nn.utils.prune`
    (or [implement](#extending-torch-nn-utils-pruning-with-custom-pruning-functions)
    your own by subclassing `BasePruningMethod`). Then, specify the module and the
    name of the parameter to prune within that module. Finally, using the adequate
    keyword arguments required by the selected pruning technique, specify the pruning
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will prune at random 30% of the connections in the parameter
    named `weight` in the `conv1` layer. The module is passed as the first argument
    to the function; `name` identifies the parameter within that module using its
    string identifier; and `amount` indicates either the percentage of connections
    to prune (if it is a float between 0\. and 1.), or the absolute number of connections
    to prune (if it is a non-negative integer).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Pruning acts by removing `weight` from the parameters and replacing it with
    a new parameter called `weight_orig` (i.e. appending `"_orig"` to the initial
    parameter `name`). `weight_orig` stores the unpruned version of the tensor. The
    `bias` was not pruned, so it will remain intact.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The pruning mask generated by the pruning technique selected above is saved
    as a module buffer named `weight_mask` (i.e. appending `"_mask"` to the initial
    parameter `name`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For the forward pass to work without modification, the `weight` attribute needs
    to exist. The pruning techniques implemented in `torch.nn.utils.prune` compute
    the pruned version of the weight (by combining the mask with the original parameter)
    and store them in the attribute `weight`. Note, this is no longer a parameter
    of the `module`, it is now simply an attribute.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Finally, pruning is applied prior to each forward pass using PyTorch’s `forward_pre_hooks`.
    Specifically, when the `module` is pruned, as we have done here, it will acquire
    a `forward_pre_hook` for each parameter associated with it that gets pruned. In
    this case, since we have so far only pruned the original parameter named `weight`,
    only one hook will be present.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For completeness, we can now prune the `bias` too, to see how the parameters,
    buffers, hooks, and attributes of the `module` change. Just for the sake of trying
    out another pruning technique, here we prune the 3 smallest entries in the bias
    by L1 norm, as implemented in the `l1_unstructured` pruning function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We now expect the named parameters to include both `weight_orig` (from before)
    and `bias_orig`. The buffers will include `weight_mask` and `bias_mask`. The pruned
    versions of the two tensors will exist as module attributes, and the module will
    now have two `forward_pre_hooks`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Iterative Pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The same parameter in a module can be pruned multiple times, with the effect
    of the various pruning calls being equal to the combination of the various masks
    applied in series. The combination of a new mask with the old mask is handled
    by the `PruningContainer`’s `compute_mask` method.
  prefs: []
  type: TYPE_NORMAL
- en: Say, for example, that we now want to further prune `module.weight`, this time
    using structured pruning along the 0th axis of the tensor (the 0th axis corresponds
    to the output channels of the convolutional layer and has dimensionality 6 for
    `conv1`), based on the channels’ L2 norm. This can be achieved using the `ln_structured`
    function, with `n=2` and `dim=0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The corresponding hook will now be of type `torch.nn.utils.prune.PruningContainer`,
    and will store the history of pruning applied to the `weight` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Serializing a pruned model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All relevant tensors, including the mask buffers and the original parameters
    used to compute the pruned tensors are stored in the model’s `state_dict` and
    can therefore be easily serialized and saved, if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Remove pruning re-parametrization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make the pruning permanent, remove the re-parametrization in terms of `weight_orig`
    and `weight_mask`, and remove the `forward_pre_hook`, we can use the `remove`
    functionality from `torch.nn.utils.prune`. Note that this doesn’t undo the pruning,
    as if it never happened. It simply makes it permanent, instead, by reassigning
    the parameter `weight` to the model parameters, in its pruned version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior to removing the re-parametrization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'After removing the re-parametrization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Pruning multiple parameters in a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By specifying the desired pruning technique and parameters, we can easily prune
    multiple tensors in a network, perhaps according to their type, as we will see
    in this example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Global pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we only looked at what is usually referred to as “local” pruning, i.e.
    the practice of pruning tensors in a model one by one, by comparing the statistics
    (weight magnitude, activation, gradient, etc.) of each entry exclusively to the
    other entries in that tensor. However, a common and perhaps more powerful technique
    is to prune the model all at once, by removing (for example) the lowest 20% of
    connections across the whole model, instead of removing the lowest 20% of connections
    in each layer. This is likely to result in different pruning percentages per layer.
    Let’s see how to do that using `global_unstructured` from `torch.nn.utils.prune`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now we can check the sparsity induced in every pruned parameter, which will
    not be equal to 20% in each layer. However, the global sparsity will be (approximately)
    20%.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Extending `torch.nn.utils.prune` with custom pruning functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement your own pruning function, you can extend the `nn.utils.prune`
    module by subclassing the `BasePruningMethod` base class, the same way all other
    pruning methods do. The base class implements the following methods for you: `__call__`,
    `apply_mask`, `apply`, `prune`, and `remove`. Beyond some special cases, you shouldn’t
    have to reimplement these methods for your new pruning technique. You will, however,
    have to implement `__init__` (the constructor), and `compute_mask` (the instructions
    on how to compute the mask for the given tensor according to the logic of your
    pruning technique). In addition, you will have to specify which type of pruning
    this technique implements (supported options are `global`, `structured`, and `unstructured`).
    This is needed to determine how to combine masks in the case in which pruning
    is applied iteratively. In other words, when pruning a prepruned parameter, the
    current pruning technique is expected to act on the unpruned portion of the parameter.
    Specifying the `PRUNING_TYPE` will enable the `PruningContainer` (which handles
    the iterative application of pruning masks) to correctly identify the slice of
    the parameter to prune.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume, for example, that you want to implement a pruning technique that
    prunes every other entry in a tensor (or – if the tensor has previously been pruned
    – in the remaining unpruned portion of the tensor). This will be of `PRUNING_TYPE='unstructured'`
    because it acts on individual connections in a layer and not on entire units/channels
    (`'structured'`), or across different parameters (`'global'`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Now, to apply this to a parameter in an `nn.Module`, you should also provide
    a simple function that instantiates the method and applies it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try it out!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '**Total running time of the script:** ( 0 minutes 0.373 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: pruning_tutorial.py`](../_downloads/ef3541eb2ef78e22efa65b3d6f4ba737/pruning_tutorial.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: pruning_tutorial.ipynb`](../_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
