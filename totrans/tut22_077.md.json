["```py\nimport torch\nimport torch.autograd.forward_ad as fwAD\n\n[primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n\ndef fn(x, [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n    return x ** 2 + [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") ** 2\n\n# All forward AD computation must be performed in the context of\n# a ``dual_level`` context. All dual tensors created in such a context\n# will have their tangents destroyed upon exit. This is to ensure that\n# if the output or intermediate results of this computation are reused\n# in a future forward AD computation, their tangents (which are associated\n# with this computation) won't be confused with tangents from the later\n# computation.\nwith [fwAD.dual_level](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level \"torch.autograd.forward_ad.dual_level\")():\n    # To create a dual tensor we associate a tensor, which we call the\n    # primal with another tensor of the same size, which we call the tangent.\n    # If the layout of the tangent is different from that of the primal,\n    # The values of the tangent are copied into a new tensor with the same\n    # metadata as the primal. Otherwise, the tangent itself is used as-is.\n    #\n    # It is also important to note that the dual tensor created by\n    # ``make_dual`` is a view of the primal.\n    [dual_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [fwAD.make_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual \"torch.autograd.forward_ad.make_dual\")([primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    assert [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual \"torch.autograd.forward_ad.unpack_dual\")([dual_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") is [tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\n\n    # To demonstrate the case where the copy of the tangent happens,\n    # we pass in a tangent with a layout different from that of the primal\n    [dual_input_alt](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [fwAD.make_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual \"torch.autograd.forward_ad.make_dual\")([primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [tangent.T](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    assert [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual \"torch.autograd.forward_ad.unpack_dual\")([dual_input_alt](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") is not [tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\n\n    # Tensors that do not have an associated tangent are automatically\n    # considered to have a zero-filled tangent of the same shape.\n    [plain_tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n    [dual_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = fn([dual_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [plain_tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\n    # Unpacking the dual returns a ``namedtuple`` with ``primal`` and ``tangent``\n    # as attributes\n    [jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual \"torch.autograd.forward_ad.unpack_dual\")([dual_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\n\nassert [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual \"torch.autograd.forward_ad.unpack_dual\")([dual_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") is None \n```", "```py\nimport torch.nn as nn\n\n[model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\") = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(5, 5)\ninput = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(16, 5)\n\nparams = {name: [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") for name, [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") in [model.named_parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters \"torch.nn.Module.named_parameters\")()}\ntangents = {name: [torch.rand_like](https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like \"torch.rand_like\")([p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\")) for name, [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") in params.items()}\n\nwith [fwAD.dual_level](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level \"torch.autograd.forward_ad.dual_level\")():\n    for name, [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") in params.items():\n        delattr([model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), name)\n        setattr([model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), name, [fwAD.make_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual \"torch.autograd.forward_ad.make_dual\")([p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\"), tangents[name]))\n\n    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(input)\n    [jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual \"torch.autograd.forward_ad.unpack_dual\")([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") \n```", "```py\nfrom torch.func import [functional_call](https://pytorch.org/docs/stable/generated/torch.func.functional_call.html#torch.func.functional_call \"torch.func.functional_call\")\n\n# We need a fresh module because the functional call requires the\n# the model to have parameters registered.\n[model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\") = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(5, 5)\n\ndual_params = {}\nwith [fwAD.dual_level](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level \"torch.autograd.forward_ad.dual_level\")():\n    for name, [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") in params.items():\n        # Using the same ``tangents`` from the above section\n        dual_params[name] = [fwAD.make_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual \"torch.autograd.forward_ad.make_dual\")([p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\"), tangents[name])\n    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [functional_call](https://pytorch.org/docs/stable/generated/torch.func.functional_call.html#torch.func.functional_call \"torch.func.functional_call\")([model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), dual_params, input)\n    [jvp2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual \"torch.autograd.forward_ad.unpack_dual\")([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\n\n# Check our results\nassert [torch.allclose](https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose \"torch.allclose\")([jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [jvp2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) \n```", "```py\nclass Fn([torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function \"torch.autograd.Function\")):\n    @staticmethod\n    def forward(ctx, foo):\n        result = [torch.exp](https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp \"torch.exp\")(foo)\n        # Tensors stored in ``ctx`` can be used in the subsequent forward grad\n        # computation.\n        ctx.result = result\n        return result\n\n    @staticmethod\n    def jvp(ctx, gI):\n        gO = gI * ctx.result\n        # If the tensor stored in`` ctx`` will not also be used in the backward pass,\n        # one can manually free it using ``del``\n        del ctx.result\n        return gO\n\nfn = Fn.apply\n\n[primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10, dtype=[torch.double](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"), requires_grad=True)\n[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n\nwith [fwAD.dual_level](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level \"torch.autograd.forward_ad.dual_level\")():\n    [dual_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [fwAD.make_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual \"torch.autograd.forward_ad.make_dual\")([primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    [dual_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = fn([dual_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    [jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual \"torch.autograd.forward_ad.unpack_dual\")([dual_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")\n\n# It is important to use ``autograd.gradcheck`` to verify that your\n# custom autograd Function computes the gradients correctly. By default,\n# ``gradcheck`` only checks the backward-mode (reverse-mode) AD gradients. Specify\n# ``check_forward_ad=True`` to also check forward grads. If you did not\n# implement the backward formula for your function, you can also tell ``gradcheck``\n# to skip the tests that require backward-mode AD by specifying\n# ``check_backward_ad=False``, ``check_undefined_grad=False``, and\n# ``check_batched_grad=False``.\n[torch.autograd.gradcheck](https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.gradcheck \"torch.autograd.gradcheck\")(Fn.apply, ([primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),), check_forward_ad=True,\n                         check_backward_ad=False, check_undefined_grad=False,\n                         check_batched_grad=False) \n```", "```py\nTrue \n```", "```py\nimport functorch as ft\n\n[primal0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n[tangent0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n[primal1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n[tangent1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n\ndef fn(x, [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n    return x ** 2 + [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") ** 2\n\n# Here is a basic example to compute the JVP of the above function.\n# The ``jvp(func, primals, tangents)`` returns ``func(*primals)`` as well as the\n# computed Jacobian-vector product (JVP). Each primal must be associated with a tangent of the same shape.\n[primal_out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [tangent_out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = ft.[jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")(fn, ([primal0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [primal1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")), ([tangent0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [tangent1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")))\n\n# ``functorch.jvp`` requires every primal to be associated with a tangent.\n# If we only want to associate certain inputs to `fn` with tangents,\n# then we'll need to create a new function that captures inputs without tangents:\n[primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(10, 10)\n\nimport functools\nnew_fn = functools.partial(fn, [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")=[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n[primal_out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [tangent_out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = ft.[jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")(new_fn, ([primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),), ([tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),)) \n```", "```py\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/deprecated.py:77: UserWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, functorch.jvp is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3\\. Please use torch.func.jvp instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html \n```", "```py\n[model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\") = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(5, 5)\ninput = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(16, 5)\ntangents = tuple([[torch.rand_like](https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like \"torch.rand_like\")([p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\")) for [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") in [model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")()])\n\n# Given a ``torch.nn.Module``, ``ft.make_functional_with_buffers`` extracts the state\n# (``params`` and buffers) and returns a functional version of the model that\n# can be invoked like a function.\n# That is, the returned ``func`` can be invoked like\n# ``func(params, buffers, input)``.\n# ``ft.make_functional_with_buffers`` is analogous to the ``nn.Modules`` stateless API\n# that you saw previously and we're working on consolidating the two.\nfunc, params, buffers = ft.make_functional_with_buffers([model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"))\n\n# Because ``jvp`` requires every input to be associated with a tangent, we need to\n# create a new function that, when given the parameters, produces the output\ndef func_params_only(params):\n    return func(params, buffers, input)\n\n[model_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [jvp_out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = ft.[jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")(func_params_only, (params,), (tangents,)) \n```", "```py\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/deprecated.py:104: UserWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional_with_buffers is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3\\. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/deprecated.py:77: UserWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, functorch.jvp is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3\\. Please use torch.func.jvp instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html \n```"]