- en: CUDA semantics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/notes/cuda.html](https://pytorch.org/docs/stable/notes/cuda.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[`torch.cuda`](../cuda.html#module-torch.cuda "torch.cuda") is used to set
    up and run CUDA operations. It keeps track of the currently selected GPU, and
    all CUDA tensors you allocate will by default be created on that device. The selected
    device can be changed with a [`torch.cuda.device`](../generated/torch.cuda.device.html#torch.cuda.device
    "torch.cuda.device") context manager.'
  prefs: []
  type: TYPE_NORMAL
- en: However, once a tensor is allocated, you can do operations on it irrespective
    of the selected device, and the results will be always placed on the same device
    as the tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-GPU operations are not allowed by default, with the exception of [`copy_()`](../generated/torch.Tensor.copy_.html#torch.Tensor.copy_
    "torch.Tensor.copy_") and other methods with copy-like functionality such as [`to()`](../generated/torch.Tensor.to.html#torch.Tensor.to
    "torch.Tensor.to") and [`cuda()`](../generated/torch.Tensor.cuda.html#torch.Tensor.cuda
    "torch.Tensor.cuda"). Unless you enable peer-to-peer memory access, any attempts
    to launch ops on tensors spread across different devices will raise an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below you can find a small example showcasing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '## TensorFloat-32 (TF32) on Ampere (and later) devices[](#tensorfloat-32-tf32-on-ampere-and-later-devices
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: Starting in PyTorch 1.7, there is a new flag called allow_tf32. This flag defaults
    to True in PyTorch 1.7 to PyTorch 1.11, and False in PyTorch 1.12 and later. This
    flag controls whether PyTorch is allowed to use the TensorFloat32 (TF32) tensor
    cores, available on NVIDIA GPUs since Ampere, internally to compute matmul (matrix
    multiplies and batched matrix multiplies) and convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: TF32 tensor cores are designed to achieve better performance on matmul and convolutions
    on torch.float32 tensors by rounding input data to have 10 bits of mantissa, and
    accumulating results with FP32 precision, maintaining FP32 dynamic range.
  prefs: []
  type: TYPE_NORMAL
- en: 'matmuls and convolutions are controlled separately, and their corresponding
    flags can be accessed at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The precision of matmuls can also be set more broadly (limited not just to CUDA)
    via `set_float_32_matmul_precision()`. Note that besides matmuls and convolutions
    themselves, functions and nn modules that internally uses matmuls or convolutions
    are also affected. These include nn.Linear, nn.Conv*, cdist, tensordot, affine
    grid and grid sample, adaptive log softmax, GRU and LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an idea of the precision and speed, see the example code and benchmark
    data (on A100) below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'From the above example, we can see that with TF32 enabled, the speed is ~7x
    faster on A100, and that relative error compared to double precision is approximately
    2 orders of magnitude larger. Note that the exact ratio of TF32 to single precision
    speed depends on the hardware generation, as properties such as the ratio of memory
    bandwidth to compute as well as the ratio of TF32 to FP32 matmul throughput may
    vary from generation to generation or model to model. If full FP32 precision is
    needed, users can disable TF32 by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To toggle the TF32 flags off in C++, you can do
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For more information about TF32, see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[TensorFloat-32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CUDA 11](https://devblogs.nvidia.com/cuda-11-features-revealed/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ampere architecture](https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/)  ##
    Reduced Precision Reduction in FP16 GEMMs[](#reduced-precision-reduction-in-fp16-gemms
    "Permalink to this heading")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fp16 GEMMs are potentially done with some intermediate reduced precision reductions
    (e.g., in fp16 rather than fp32). These selective reductions in precision can
    allow for higher performance on certain workloads (particularly those with a large
    k dimension) and GPU architectures at the cost of numerical precision and potential
    for overflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some example benchmark data on V100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If full precision reductions are needed, users can disable reduced precision
    reductions in fp16 GEMMs with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To toggle the reduced precision reduction flags in C++, one can do
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]  ## Reduced Precision Reduction in BF16 GEMMs[](#reduced-precision-reduction-in-bf16-gemms
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: A similar flag (as above) exists for BFloat16 GEMMs. Note that this switch is
    set to True by default for BF16, if you observe numerical instability in your
    workload, you may wish to set it to False.
  prefs: []
  type: TYPE_NORMAL
- en: 'If reduced precision reductions are not desired, users can disable reduced
    precision reductions in bf16 GEMMs with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To toggle the reduced precision reduction flags in C++, one can do
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Asynchronous execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, GPU operations are asynchronous. When you call a function that uses
    the GPU, the operations are *enqueued* to the particular device, but not necessarily
    executed until later. This allows us to execute more computations in parallel,
    including operations on CPU or other GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the effect of asynchronous computation is invisible to the caller,
    because (1) each device executes operations in the order they are queued, and
    (2) PyTorch automatically performs necessary synchronization when copying data
    between CPU and GPU or between two GPUs. Hence, computation will proceed as if
    every operation was executed synchronously.
  prefs: []
  type: TYPE_NORMAL
- en: You can force synchronous computation by setting environment variable `CUDA_LAUNCH_BLOCKING=1`.
    This can be handy when an error occurs on the GPU. (With asynchronous execution,
    such an error isn’t reported until after the operation is actually executed, so
    the stack trace does not show where it was requested.)
  prefs: []
  type: TYPE_NORMAL
- en: 'A consequence of the asynchronous computation is that time measurements without
    synchronizations are not accurate. To get precise measurements, one should either
    call [`torch.cuda.synchronize()`](../generated/torch.cuda.synchronize.html#torch.cuda.synchronize
    "torch.cuda.synchronize") before measuring, or use [`torch.cuda.Event`](../generated/torch.cuda.Event.html#torch.cuda.Event
    "torch.cuda.Event") to record times as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As an exception, several functions such as [`to()`](../generated/torch.Tensor.to.html#torch.Tensor.to
    "torch.Tensor.to") and [`copy_()`](../generated/torch.Tensor.copy_.html#torch.Tensor.copy_
    "torch.Tensor.copy_") admit an explicit `non_blocking` argument, which lets the
    caller bypass synchronization when it is unnecessary. Another exception is CUDA
    streams, explained below.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA streams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A [CUDA stream](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams)
    is a linear sequence of execution that belongs to a specific device. You normally
    do not need to create one explicitly: by default, each device uses its own “default”
    stream.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operations inside each stream are serialized in the order they are created,
    but operations from different streams can execute concurrently in any relative
    order, unless explicit synchronization functions (such as [`synchronize()`](../generated/torch.cuda.synchronize.html#torch.cuda.synchronize
    "torch.cuda.synchronize") or [`wait_stream()`](../generated/torch.cuda.Stream.html#torch.cuda.Stream.wait_stream
    "torch.cuda.Stream.wait_stream")) are used. For example, the following code is
    incorrect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When the “current stream” is the default stream, PyTorch automatically performs
    necessary synchronization when data is moved around, as explained above. However,
    when using non-default streams, it is the user’s responsibility to ensure proper
    synchronization. The fixed version of this example is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are two new additions. The [`torch.cuda.Stream.wait_stream()`](../generated/torch.cuda.Stream.html#torch.cuda.Stream.wait_stream
    "torch.cuda.Stream.wait_stream") call ensures that the `normal_()` execution has
    finished before we start running `sum(A)` on a side stream. The [`torch.Tensor.record_stream()`](../generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream
    "torch.Tensor.record_stream") (see for more details) ensures that we do not deallocate
    A before `sum(A)` has completed. You can also manually wait on the stream at some
    later point in time with `torch.cuda.default_stream(cuda).wait_stream(s)` (note
    that it is pointless to wait immediately, since that will prevent the stream execution
    from running in parallel with other work on the default stream.) See the documentation
    for [`torch.Tensor.record_stream()`](../generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream
    "torch.Tensor.record_stream") on more details on when to use one or another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this synchronization is necessary even when there is no read dependency,
    e.g., as seen in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Despite the computation on `s` not reading the contents of `A` and no other
    uses of `A`, it is still necessary to synchronize, because `A` may correspond
    to memory reallocated by the CUDA caching allocator, with pending operations from
    the old (deallocated) memory.
  prefs: []
  type: TYPE_NORMAL
- en: '### Stream semantics of backward passes[](#stream-semantics-of-backward-passes
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: Each backward CUDA op runs on the same stream that was used for its corresponding
    forward op. If your forward pass runs independent ops in parallel on different
    streams, this helps the backward pass exploit that same parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: The stream semantics of a backward call with respect to surrounding ops are
    the same as for any other call. The backward pass inserts internal syncs to ensure
    this even when backward ops run on multiple streams as described in the previous
    paragraph. More concretely, when calling [`autograd.backward`](../generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward"), [`autograd.grad`](../generated/torch.autograd.grad.html#torch.autograd.grad
    "torch.autograd.grad"), or [`tensor.backward`](../generated/torch.Tensor.backward.html#torch.Tensor.backward
    "torch.Tensor.backward"), and optionally supplying CUDA tensor(s) as the initial
    gradient(s) (e.g., [`autograd.backward(..., grad_tensors=initial_grads)`](../generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward"), [`autograd.grad(..., grad_outputs=initial_grads)`](../generated/torch.autograd.grad.html#torch.autograd.grad
    "torch.autograd.grad"), or [`tensor.backward(..., gradient=initial_grad)`](../generated/torch.Tensor.backward.html#torch.Tensor.backward
    "torch.Tensor.backward")), the acts of
  prefs: []
  type: TYPE_NORMAL
- en: optionally populating initial gradient(s),
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: invoking the backward pass, and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: using the gradients
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'have the same stream-semantics relationship as any group of ops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'BC note: Using grads on the default stream[](#bc-note-using-grads-on-the-default-stream
    "Permalink to this heading")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In prior versions of PyTorch (1.9 and earlier), the autograd engine always
    synced the default stream with all backward ops, so the following pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'was safe as long as `use grads` happened on the default stream. In present
    PyTorch, that pattern is no longer safe. If `backward()` and `use grads` are in
    different stream contexts, you must sync the streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'even if `use grads` is on the default stream.  ## Memory management[](#memory-management
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch uses a caching memory allocator to speed up memory allocations. This
    allows fast memory deallocation without device synchronizations. However, the
    unused memory managed by the allocator will still show as if used in `nvidia-smi`.
    You can use [`memory_allocated()`](../generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated
    "torch.cuda.memory_allocated") and [`max_memory_allocated()`](../generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated
    "torch.cuda.max_memory_allocated") to monitor memory occupied by tensors, and
    use [`memory_reserved()`](../generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved
    "torch.cuda.memory_reserved") and [`max_memory_reserved()`](../generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved
    "torch.cuda.max_memory_reserved") to monitor the total amount of memory managed
    by the caching allocator. Calling [`empty_cache()`](../generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache
    "torch.cuda.empty_cache") releases all **unused** cached memory from PyTorch so
    that those can be used by other GPU applications. However, the occupied GPU memory
    by tensors will not be freed so it can not increase the amount of GPU memory available
    for PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how CUDA memory is being used over time, [Understanding
    CUDA Memory Usage](../torch_cuda_memory.html#torch-cuda-memory) describes tools
    for capturing and visualizing traces of memory use.
  prefs: []
  type: TYPE_NORMAL
- en: For more advanced users, we offer more comprehensive memory benchmarking via
    [`memory_stats()`](../generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats
    "torch.cuda.memory_stats"). We also offer the capability to capture a complete
    snapshot of the memory allocator state via [`memory_snapshot()`](../generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot
    "torch.cuda.memory_snapshot"), which can help you understand the underlying allocation
    patterns produced by your code.
  prefs: []
  type: TYPE_NORMAL
- en: '### Environment variables'
  prefs: []
  type: TYPE_NORMAL
- en: Use of a caching allocator can interfere with memory checking tools such as
    `cuda-memcheck`. To debug memory errors using `cuda-memcheck`, set `PYTORCH_NO_CUDA_MEMORY_CACHING=1`
    in your environment to disable caching.
  prefs: []
  type: TYPE_NORMAL
- en: 'The behavior of the caching allocator can be controlled via the environment
    variable `PYTORCH_CUDA_ALLOC_CONF`. The format is `PYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>...`
    Available options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`backend` allows selecting the underlying allocator implementation. Currently,
    valid options are `native`, which uses PyTorch’s native implementation, and `cudaMallocAsync`,
    which uses [CUDA’s built-in asynchronous allocator](https://developer.nvidia.com/blog/using-cuda-stream-ordered-memory-allocator-part-1/).
    `cudaMallocAsync` requires CUDA 11.4 or newer. The default is `native`. `backend`
    applies to all devices used by the process, and can’t be specified on a per-device
    basis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_split_size_mb` prevents the native allocator from splitting blocks larger
    than this size (in MB). This can reduce fragmentation and may allow some borderline
    workloads to complete without running out of memory. Performance cost can range
    from ‘zero’ to ‘substantial’ depending on allocation patterns. Default value is
    unlimited, i.e. all blocks can be split. The [`memory_stats()`](../generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats
    "torch.cuda.memory_stats") and [`memory_summary()`](../generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary
    "torch.cuda.memory_summary") methods are useful for tuning. This option should
    be used as a last resort for a workload that is aborting due to ‘out of memory’
    and showing a large amount of inactive split blocks. `max_split_size_mb` is only
    meaningful with `backend:native`. With `backend:cudaMallocAsync`, `max_split_size_mb`
    is ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roundup_power2_divisions` helps with rounding the requested allocation size
    to nearest power-2 division and making better use of the blocks. In the native
    CUDACachingAllocator, the sizes are rounded up in multiple of blocks size of 512,
    so this works fine for smaller sizes. However, this can be inefficient for large
    near-by allocations as each will go to different size of blocks and re-use of
    those blocks are minimized. This might create lots of unused blocks and will waste
    GPU memory capacity. This option enables the rounding of allocation size to nearest
    power-2 division. For example, if we need to round-up size of 1200 and if number
    of divisions is 4, the size 1200 lies between 1024 and 2048 and if we do 4 divisions
    between them, the values are 1024, 1280, 1536, and 1792\. So, allocation size
    of 1200 will be rounded to 1280 as the nearest ceiling of power-2 division. Specify
    a single value to apply for all allocation sizes or specify an array of key value
    pairs to set power-2 division individually for each power of two interval. For
    example to set 1 division for all allocations under 256MB, 2 division for allocations
    between 256MB and 512MB, 4 divisions for allocations between 512MB and 1GB and
    8 divisions for any larger allocations, set the knob value to: [256:1,512:2,1024:4,>:8].
    `roundup_power2_divisions` is only meaningful with `backend:native`. With `backend:cudaMallocAsync`,
    `roundup_power2_divisions` is ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`garbage_collection_threshold` helps actively reclaiming unused GPU memory
    to avoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks),
    which can be unfavorable to latency-critical GPU applications (e.g., servers).
    Upon setting this threshold (e.g., 0.8), the allocator will start reclaiming GPU
    memory blocks if the GPU memory capacity usage exceeds the threshold (i.e., 80%
    of the total memory allocated to the GPU application). The algorithm prefers to
    free old & unused blocks first to avoid freeing blocks that are actively being
    reused. The threshold value should be between greater than 0.0 and less than 1.0.
    `garbage_collection_threshold` is only meaningful with `backend:native`. With
    `backend:cudaMallocAsync`, `garbage_collection_threshold` is ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`expandable_segments` (experimental, default: False) If set to True, this setting
    instructs the allocator to create CUDA allocations that can later be expanded
    to better handle cases where a job changing allocation sizes frequently, such
    as having a changing batch size. Normally for large (>2MB) allocations, the allocator
    calls cudaMalloc to get allocations that are the same size as what the user requests.
    In the future, parts of these allocations can be reused for other requests if
    they are free. This works well when the program makes many requests of exactly
    the same size or of sizes that even multiples of that size. Many deep learning
    models follow this behavior. However, one common exception is when the batch size
    changes slightly from one iteration to the next, e.g. in batched inference. When
    the program runs initially with batch size N, it will make allocations appropriate
    for that size. If in the future, it runs at size N - 1, the existing allocations
    will still be big enough. However, if it runs at size N + 1, then it will have
    to make new allocations that are slightly larger. Not all the tensors are the
    same size. Some might be (N + 1)*A and others (N + 1)*A*B where A and B are some
    non-batch dimensions in the model. Because the allocator reuses existing allocations
    when they are big enough, some number of (N + 1)*A allocations will actually fit
    in the already existing N*B*A segments, though not perfectly. As the model runs
    it will partially fill up all of these segments leaving unusable free slices of
    memory at the end of these segments. The allocator at some point will need to
    cudaMalloc a new (N + 1)*A*B segment. If there is not enough memory, there is
    now no way to recover the slices of memory that are free at the end of existing
    segments. With models 50+ layers deep, this pattern might repeat 50+ times creating
    many slivers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: expandable_segments allows the allocator to create a segment initially and then
    expand its size later when more memory is needed. Instead of making one segment
    per allocation, it tries to make one segment (per stream) that grows as necessary.
    Now when the N + 1 case runs, the allocations will tile nicely into the one large
    segment until it fills up. Then more memory is requested and appended to the end
    of the segment. This process does not create as many slivers of unusable memory,
    so it is more likely to succeed at finding this memory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pinned_use_cuda_host_register option is a boolean flag that determines whether
    to use the CUDA API’s cudaHostRegister function for allocating pinned memory instead
    of the default cudaHostAlloc. When set to True, the memory is allocated using
    regular malloc and then pages are mapped to the memory before calling cudaHostRegister.
    This pre-mapping of pages helps reduce the lock time during the execution of cudaHostRegister.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pinned_num_register_threads option is only valid when pinned_use_cuda_host_register
    is set to True. By default, one thread is used to map the pages. This option allows
    using more threads to parallelize the page mapping operations to reduce the overall
    allocation time of pinned memory. A good value for this option is 8 based on benchmarking
    results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Some stats reported by the [CUDA memory management API](../cuda.html#cuda-memory-management-api)
    are specific to `backend:native`, and are not meaningful with `backend:cudaMallocAsync`.
    See each function’s docstring for details.  ## Using custom memory allocators
    for CUDA'
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to define allocators as simple functions in C/C++ and compile
    them as a shared library, the code below shows a basic allocator that just traces
    all the memory operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This can be used in python through the [`torch.cuda.memory.CUDAPluggableAllocator`](../generated/torch.cuda.CUDAPluggableAllocator.html#torch.cuda.CUDAPluggableAllocator
    "torch.cuda.memory.CUDAPluggableAllocator"). The user is responsible for supplying
    the path to the .so file and the name of the alloc/free functions that match the
    signatures specified above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: cuBLAS workspaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For each combination of cuBLAS handle and CUDA stream, a cuBLAS workspace will
    be allocated if that handle and stream combination executes a cuBLAS kernel that
    requires a workspace. In order to avoid repeatedly allocating workspaces, these
    workspaces are not deallocated unless `torch._C._cuda_clearCublasWorkspaces()`
    is called. The workspace size per allocation can be specified via the environment
    variable `CUBLAS_WORKSPACE_CONFIG` with the format `:[SIZE]:[COUNT]`. As an example,
    the default workspace size per allocation is `CUBLAS_WORKSPACE_CONFIG=:4096:2:16:8`
    which specifies a total size of `2 * 4096 + 8 * 16 KiB`. To force cuBLAS to avoid
    using workspaces, set `CUBLAS_WORKSPACE_CONFIG=:0:0`.
  prefs: []
  type: TYPE_NORMAL
- en: '## cuFFT plan cache'
  prefs: []
  type: TYPE_NORMAL
- en: For each CUDA device, an LRU cache of cuFFT plans is used to speed up repeatedly
    running FFT methods (e.g., [`torch.fft.fft()`](../generated/torch.fft.fft.html#torch.fft.fft
    "torch.fft.fft")) on CUDA tensors of same geometry with same configuration. Because
    some cuFFT plans may allocate GPU memory, these caches have a maximum capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may control and query the properties of the cache of current device with
    the following APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.backends.cuda.cufft_plan_cache.max_size` gives the capacity of the cache
    (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions). Setting
    this value directly modifies the capacity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.backends.cuda.cufft_plan_cache.size` gives the number of plans currently
    residing in the cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.backends.cuda.cufft_plan_cache.clear()` clears the cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To control and query plan caches of a non-default device, you can index the
    `torch.backends.cuda.cufft_plan_cache` object with either a [`torch.device`](../tensor_attributes.html#torch.device
    "torch.device") object or a device index, and access one of the above attributes.
    E.g., to set the capacity of the cache for device `1`, one can write `torch.backends.cuda.cufft_plan_cache[1].max_size
    = 10`.  ## Just-in-Time Compilation[](#just-in-time-compilation "Permalink to
    this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch just-in-time compiles some operations, like torch.special.zeta, when
    performed on CUDA tensors. This compilation can be time consuming (up to a few
    seconds depending on your hardware and software) and may occur multiple times
    for a single operator since many PyTorch operators actually select from a variety
    of kernels, each of which must be compiled once, depending on their input. This
    compilation occurs once per process, or just once if a kernel cache is used.
  prefs: []
  type: TYPE_NORMAL
- en: By default, PyTorch creates a kernel cache in $XDG_CACHE_HOME/torch/kernels
    if XDG_CACHE_HOME is defined and $HOME/.cache/torch/kernels if it’s not (except
    on Windows, where the kernel cache is not yet supported). The caching behavior
    can be directly controlled with two environment variables. If USE_PYTORCH_KERNEL_CACHE
    is set to 0 then no cache will be used, and if PYTORCH_KERNEL_CACHE_PATH is set
    then that path will be used as a kernel cache instead of the default location.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Device-agnostic code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the structure of PyTorch, you may need to explicitly write device-agnostic
    (CPU or GPU) code; an example may be creating a new tensor as the initial hidden
    state of a recurrent neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to determine whether the GPU should be used or not. A common
    pattern is to use Python’s `argparse` module to read in user arguments, and have
    a flag that can be used to disable CUDA, in combination with [`is_available()`](../generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available"). In the following, `args.device` results in a [`torch.device`](../tensor_attributes.html#torch.device
    "torch.device") object that can be used to move tensors to CPU or CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When assessing the availability of CUDA in a given environment ([`is_available()`](../generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available")), PyTorch’s default behavior is to call the CUDA Runtime
    API method [cudaGetDeviceCount](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18808e54893cfcaafefeab31a73cc55f).
    Because this call in turn initializes the CUDA Driver API (via [cuInit](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__INITIALIZE.html#group__CUDA__INITIALIZE_1g0a2f1517e1bd8502c7194c3a8c134bc3))
    if it is not already initialized, subsequent forks of a process that has run [`is_available()`](../generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available") will fail with a CUDA initialization error.
  prefs: []
  type: TYPE_NORMAL
- en: One can set `PYTORCH_NVML_BASED_CUDA_CHECK=1` in your environment before importing
    PyTorch modules that execute [`is_available()`](../generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available") (or before executing it directly) in order to direct
    [`is_available()`](../generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available") to attempt an NVML-based assessment ([nvmlDeviceGetCount_v2](https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_1ga93623b195bff04bbe3490ca33c8a42d)).
    If the NVML-based assessment is successful (i.e. NVML discovery/initialization
    does not fail), [`is_available()`](../generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available") calls will not poison subsequent forks.
  prefs: []
  type: TYPE_NORMAL
- en: If NVML discovery/initialization fails, [`is_available()`](../generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available") will fallback to the standard CUDA Runtime API assessment
    and the aforementioned fork constraint will apply.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the above NVML-based CUDA availability assessment provides a weaker
    guarantee than the default CUDA Runtime API approach (which requires CUDA initialization
    to succeed). In some circumstances, the NVML-based check may succeed while later
    CUDA initialization fails.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have `args.device`, we can use it to create a Tensor on the desired
    device.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be used in a number of cases to produce device agnostic code. Below
    is an example when using a dataloader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: When working with multiple GPUs on a system, you can use the `CUDA_VISIBLE_DEVICES`
    environment flag to manage which GPUs are available to PyTorch. As mentioned above,
    to manually control which GPU a tensor is created on, the best practice is to
    use a [`torch.cuda.device`](../generated/torch.cuda.device.html#torch.cuda.device
    "torch.cuda.device") context manager.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If you have a tensor and would like to create a new tensor of the same type
    on the same device, then you can use a `torch.Tensor.new_*` method (see [`torch.Tensor`](../tensors.html#torch.Tensor
    "torch.Tensor")). Whilst the previously mentioned `torch.*` factory functions
    ([Creation Ops](../torch.html#tensor-creation-ops)) depend on the current GPU
    context and the attributes arguments you pass in, `torch.Tensor.new_*` methods
    preserve the device and other attributes of the tensor.
  prefs: []
  type: TYPE_NORMAL
- en: This is the recommended practice when creating modules in which new tensors
    need to be created internally during the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If you want to create a tensor of the same type and size of another tensor,
    and fill it with either ones or zeros, [`ones_like()`](../generated/torch.ones_like.html#torch.ones_like
    "torch.ones_like") or [`zeros_like()`](../generated/torch.zeros_like.html#torch.zeros_like
    "torch.zeros_like") are provided as convenient helper functions (which also preserve
    [`torch.device`](../tensor_attributes.html#torch.device "torch.device") and [`torch.dtype`](../tensor_attributes.html#torch.dtype
    "torch.dtype") of a Tensor).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '### Use pinned memory buffers[](#use-pinned-memory-buffers "Permalink to this
    heading")'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This is an advanced tip. If you overuse pinned memory, it can cause serious
    problems when running low on RAM, and you should be aware that pinning is often
    an expensive operation.
  prefs: []
  type: TYPE_NORMAL
- en: Host to GPU copies are much faster when they originate from pinned (page-locked)
    memory. CPU tensors and storages expose a [`pin_memory()`](../generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory
    "torch.Tensor.pin_memory") method, that returns a copy of the object, with data
    put in a pinned region.
  prefs: []
  type: TYPE_NORMAL
- en: Also, once you pin a tensor or storage, you can use asynchronous GPU copies.
    Just pass an additional `non_blocking=True` argument to a [`to()`](../generated/torch.Tensor.to.html#torch.Tensor.to
    "torch.Tensor.to") or a [`cuda()`](../generated/torch.Tensor.cuda.html#torch.Tensor.cuda
    "torch.Tensor.cuda") call. This can be used to overlap data transfers with computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can make the [`DataLoader`](../data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader")
    return batches placed in pinned memory by passing `pin_memory=True` to its constructor.  ###
    Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel[](#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: Most use cases involving batched inputs and multiple GPUs should default to
    using [`DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") to utilize more than one GPU.
  prefs: []
  type: TYPE_NORMAL
- en: There are significant caveats to using CUDA models with [`multiprocessing`](../multiprocessing.html#module-torch.multiprocessing
    "torch.multiprocessing"); unless care is taken to meet the data handling requirements
    exactly, it is likely that your program will have incorrect or undefined behavior.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to use [`DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel"), instead of [`DataParallel`](../generated/torch.nn.DataParallel.html#torch.nn.DataParallel
    "torch.nn.DataParallel") to do multi-GPU training, even if there is only a single
    node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between [`DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") and [`DataParallel`](../generated/torch.nn.DataParallel.html#torch.nn.DataParallel
    "torch.nn.DataParallel") is: [`DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") uses multiprocessing where a process
    is created for each GPU, while [`DataParallel`](../generated/torch.nn.DataParallel.html#torch.nn.DataParallel
    "torch.nn.DataParallel") uses multithreading. By using multiprocessing, each GPU
    has its dedicated process, this avoids the performance overhead caused by GIL
    of Python interpreter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use [`DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel"), you could use torch.distributed.launch
    utility to launch your program, see [Third-party backends](../distributed.html#distributed-launch).  ##
    CUDA Graphs'
  prefs: []
  type: TYPE_NORMAL
- en: A CUDA graph is a record of the work (mostly kernels and their arguments) that
    a CUDA stream and its dependent streams perform. For general principles and details
    on the underlying CUDA API, see [Getting Started with CUDA Graphs](https://developer.nvidia.com/blog/cuda-graphs/)
    and the [Graphs section](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs)
    of the CUDA C Programming Guide.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch supports the construction of CUDA graphs using [stream capture](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#creating-a-graph-using-stream-capture),
    which puts a CUDA stream in *capture mode*. CUDA work issued to a capturing stream
    doesn’t actually run on the GPU. Instead, the work is recorded in a graph.
  prefs: []
  type: TYPE_NORMAL
- en: After capture, the graph can be *launched* to run the GPU work as many times
    as needed. Each replay runs the same kernels with the same arguments. For pointer
    arguments this means the same memory addresses are used. By filling input memory
    with new data (e.g., from a new batch) before each replay, you can rerun the same
    work on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Why CUDA Graphs?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Replaying a graph sacrifices the dynamic flexibility of typical eager execution
    in exchange for **greatly reduced CPU overhead**. A graph’s arguments and kernels
    are fixed, so a graph replay skips all layers of argument setup and kernel dispatch,
    including Python, C++, and CUDA driver overheads. Under the hood, a replay submits
    the entire graph’s work to the GPU with a single call to [cudaGraphLaunch](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH_1g1accfe1da0c605a577c22d9751a09597).
    Kernels in a replay also execute slightly faster on the GPU, but eliding CPU overhead
    is the main benefit.
  prefs: []
  type: TYPE_NORMAL
- en: You should try CUDA graphs if all or part of your network is graph-safe (usually
    this means static shapes and static control flow, but see the other [constraints](#capture-constraints))
    and you suspect its runtime is at least somewhat CPU-limited.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This API is in beta and may change in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch exposes graphs via a raw [`torch.cuda.CUDAGraph`](../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph
    "torch.cuda.CUDAGraph") class and two convenience wrappers, [`torch.cuda.graph`](../generated/torch.cuda.graph.html#torch.cuda.graph
    "torch.cuda.graph") and [`torch.cuda.make_graphed_callables`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables").
  prefs: []
  type: TYPE_NORMAL
- en: '[`torch.cuda.graph`](../generated/torch.cuda.graph.html#torch.cuda.graph "torch.cuda.graph")
    is a simple, versatile context manager that captures CUDA work in its context.
    Before capture, warm up the workload to be captured by running a few eager iterations.
    Warmup must occur on a side stream. Because the graph reads from and writes to
    the same memory addresses in every replay, you must maintain long-lived references
    to tensors that hold input and output data during capture. To run the graph on
    new input data, copy new data to the capture’s input tensor(s), replay the graph,
    then read the new output from the capture’s output tensor(s). Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: See [Whole-network capture](#whole-network-capture), [Usage with torch.cuda.amp](#graphs-with-amp),
    and [Usage with multiple streams](#multistream-capture) for realistic and advanced
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '[`make_graphed_callables`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") is more sophisticated. [`make_graphed_callables`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") accepts Python functions and [`torch.nn.Module`](../generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")s. For each passed function or Module, it creates separate graphs
    of the forward-pass and backward-pass work. See [Partial-network capture](#partial-network-capture).'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Constraints'
  prefs: []
  type: TYPE_NORMAL
- en: A set of ops is *capturable* if it doesn’t violate any of the following constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Constraints apply to all work in a [`torch.cuda.graph`](../generated/torch.cuda.graph.html#torch.cuda.graph
    "torch.cuda.graph") context and all work in the forward and backward passes of
    any callable you pass to [`torch.cuda.make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables").
  prefs: []
  type: TYPE_NORMAL
- en: 'Violating any of these will likely cause a runtime error:'
  prefs: []
  type: TYPE_NORMAL
- en: Capture must occur on a non-default stream. (This is only a concern if you use
    the raw [`CUDAGraph.capture_begin`](../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_begin
    "torch.cuda.CUDAGraph.capture_begin") and [`CUDAGraph.capture_end`](../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_end
    "torch.cuda.CUDAGraph.capture_end") calls. [`graph`](../generated/torch.cuda.graph.html#torch.cuda.graph
    "torch.cuda.graph") and [`make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") set a side stream for you.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ops that synchronize the CPU with the GPU (e.g., `.item()` calls) are prohibited.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA RNG ops are allowed, but must use default generators. For example, explicitly
    constructing a new [`torch.Generator`](../generated/torch.Generator.html#torch.Generator
    "torch.Generator") instance and passing it as the `generator` argument to an RNG
    function is prohibited.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Violating any of these will likely cause silent numerical errors or undefined
    behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: Within a process, only one capture may be underway at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No non-captured CUDA work may run in this process (on any thread) while capture
    is underway.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU work is not captured. If the captured ops include CPU work, that work will
    be elided during replay.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every replay reads from and writes to the same (virtual) memory addresses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic control flow (based on CPU or GPU data) is prohibited.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic shapes are prohibited. The graph assumes every tensor in the captured
    op sequence has the same size and layout in every replay.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple streams in a capture is allowed, but there are [restrictions](#multistream-capture).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-constraints
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once captured, the graph may be replayed on any stream.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### Whole-network capture'
  prefs: []
  type: TYPE_NORMAL
- en: 'If your entire network is capturable, you can capture and replay an entire
    iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]  ### Partial-network capture[](#partial-network-capture "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: If some of your network is unsafe to capture (e.g., due to dynamic control flow,
    dynamic shapes, CPU syncs, or essential CPU-side logic), you can run the unsafe
    part(s) eagerly and use [`torch.cuda.make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") to graph only the capture-safe part(s).
  prefs: []
  type: TYPE_NORMAL
- en: By default, callables returned by [`make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") are autograd-aware, and can be used in the
    training loop as direct replacements for the functions or [`nn.Module`](../generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")s you passed.
  prefs: []
  type: TYPE_NORMAL
- en: '[`make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") internally creates [`CUDAGraph`](../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph
    "torch.cuda.CUDAGraph") objects, runs warmup iterations, and maintains static
    inputs and outputs as needed. Therefore (unlike with [`torch.cuda.graph`](../generated/torch.cuda.graph.html#torch.cuda.graph
    "torch.cuda.graph")) you don’t need to handle those manually.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, data-dependent dynamic control flow means the network
    isn’t capturable end-to-end, but [`make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") lets us capture and run graph-safe sections
    as graphs regardless:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]  ### Usage with torch.cuda.amp[](#usage-with-torch-cuda-amp "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'For typical optimizers, [`GradScaler.step`](../amp.html#torch.cuda.amp.GradScaler.step
    "torch.cuda.amp.GradScaler.step") syncs the CPU with the GPU, which is prohibited
    during capture. To avoid errors, either use [partial-network capture](#partial-network-capture),
    or (if forward, loss, and backward are capture-safe) capture forward, loss, and
    backward but not the optimizer step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]  ### Usage with multiple streams[](#usage-with-multiple-streams "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Capture mode automatically propagates to any streams that sync with a capturing
    stream. Within capture, you may expose parallelism by issuing calls to different
    streams, but the overall stream dependency DAG must branch out from the initial
    capturing stream after capture begins and rejoin the initial stream before capture
    ends:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid confusion for power users looking at replays in nsight systems or
    nvprof: Unlike eager execution, the graph interprets a nontrivial stream DAG in
    capture as a hint, not a command. During replay, the graph may reorganize independent
    ops onto different streams or enqueue them in a different order (while respecting
    your original DAG’s overall dependencies).'
  prefs: []
  type: TYPE_NORMAL
- en: Usage with DistributedDataParallel[](#usage-with-distributeddataparallel "Permalink
    to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NCCL < 2.9.6
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NCCL versions earlier than 2.9.6 don’t allow collectives to be captured. You
    must use [partial-network capture](#partial-network-capture), which defers allreduces
    to happen outside graphed sections of backward.
  prefs: []
  type: TYPE_NORMAL
- en: Call [`make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") on graphable network sections *before* wrapping
    the network with DDP.
  prefs: []
  type: TYPE_NORMAL
- en: NCCL >= 2.9.6
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NCCL versions 2.9.6 or later allow collectives in the graph. Approaches that
    capture an [entire backward pass](#whole-network-capture) are a viable option,
    but need three setup steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Disable DDP’s internal async error handling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before full-backward capture, DDP must be constructed in a side-stream context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Your warmup must run at least 11 DDP-enabled eager iterations before capture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '### Graph memory management[](#graph-memory-management "Permalink to this
    heading")'
  prefs: []
  type: TYPE_NORMAL
- en: A captured graph acts on the same virtual addresses every time it replays. If
    PyTorch frees the memory, a later replay can hit an illegal memory access. If
    PyTorch reassigns the memory to new tensors, the replay can corrupt the values
    seen by those tensors. Therefore, the virtual addresses used by the graph must
    be reserved for the graph across replays. The PyTorch caching allocator achieves
    this by detecting when capture is underway and satisfying the capture’s allocations
    from a graph-private memory pool. The private pool stays alive until its [`CUDAGraph`](../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph
    "torch.cuda.CUDAGraph") object and all tensors created during capture go out of
    scope.
  prefs: []
  type: TYPE_NORMAL
- en: Private pools are maintained automatically. By default, the allocator creates
    a separate private pool for each capture. If you capture multiple graphs, this
    conservative approach ensures graph replays never corrupt each other’s values,
    but sometimes needlessly wastes memory.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing memory across captures[](#sharing-memory-across-captures "Permalink
    to this heading")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To economize the memory stashed in private pools, [`torch.cuda.graph`](../generated/torch.cuda.graph.html#torch.cuda.graph
    "torch.cuda.graph") and [`torch.cuda.make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") optionally allow different captures to share
    the same private pool. It’s safe for a set of graphs to share a private pool if
    you know they’ll always be replayed in the same order they were captured, and
    never be replayed concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: '[`torch.cuda.graph`](../generated/torch.cuda.graph.html#torch.cuda.graph "torch.cuda.graph")’s
    `pool` argument is a hint to use a particular private pool, and can be used to
    share memory across graphs as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: With [`torch.cuda.make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables"), if you want to graph several callables and
    you know they’ll always run in the same order (and never concurrently) pass them
    as a tuple in the same order they’ll run in the live workload, and [`make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") will capture their graphs using a shared
    private pool.
  prefs: []
  type: TYPE_NORMAL
- en: If, in the live workload, your callables will run in an order that occasionally
    changes, or if they’ll run concurrently, passing them as a tuple to a single invocation
    of [`make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") is not allowed. Instead, you must call [`make_graphed_callables()`](../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") separately for each one.
  prefs: []
  type: TYPE_NORMAL
