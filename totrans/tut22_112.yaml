- en: PyTorch Distributed Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/beginner/dist_overview.html](https://pytorch.org/tutorials/beginner/dist_overview.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Author**: [Shen Li](https://mrshenli.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '[![edit](../Images/a8aa37bcc5edbf2ba5fcf18dba1e55f9.png)](../_images/pencil-16.png)
    View and edit this tutorial in [github](https://github.com/pytorch/tutorials/blob/main/beginner_source/dist_overview.rst).'
  prefs: []
  type: TYPE_NORMAL
- en: This is the overview page for the `torch.distributed` package. The goal of this
    page is to categorize documents into different topics and briefly describe each
    of them. If this is your first time building distributed training applications
    using PyTorch, it is recommended to use this document to navigate to the technology
    that can best serve your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As of PyTorch v1.6.0, features in `torch.distributed` can be categorized into
    three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Distributed Data-Parallel Training](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    (DDP) is a widely adopted single-program multiple-data training paradigm. With
    DDP, the model is replicated on every process, and every model replica will be
    fed with a different set of input data samples. DDP takes care of gradient communication
    to keep model replicas synchronized and overlaps it with the gradient computations
    to speed up training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RPC-Based Distributed Training](https://pytorch.org/docs/stable/rpc.html)
    (RPC) supports general training structures that cannot fit into data-parallel
    training such as distributed pipeline parallelism, parameter server paradigm,
    and combinations of DDP with other training paradigms. It helps manage remote
    object lifetime and extends the [autograd engine](https://pytorch.org/docs/stable/autograd.html)
    beyond machine boundaries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Collective Communication](https://pytorch.org/docs/stable/distributed.html)
    (c10d) library supports sending tensors across processes within a group. It offers
    both collective communication APIs (e.g., [all_reduce](https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce)
    and [all_gather](https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather))
    and P2P communication APIs (e.g., [send](https://pytorch.org/docs/stable/distributed.html#torch.distributed.send)
    and [isend](https://pytorch.org/docs/stable/distributed.html#torch.distributed.isend)).
    DDP and RPC ([ProcessGroup Backend](https://pytorch.org/docs/stable/rpc.html#process-group-backend))
    are built on c10d, where the former uses collective communications and the latter
    uses P2P communications. Usually, developers do not need to directly use this
    raw communication API, as the DDP and RPC APIs can serve many distributed training
    scenarios. However, there are use cases where this API is still helpful. One example
    would be distributed parameter averaging, where applications would like to compute
    the average values of all model parameters after the backward pass instead of
    using DDP to communicate gradients. This can decouple communications from computations
    and allow finer-grain control over what to communicate, but on the other hand,
    it also gives up the performance optimizations offered by DDP. [Writing Distributed
    Applications with PyTorch](../intermediate/dist_tuto.html) shows examples of using
    c10d communication APIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Parallel Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PyTorch provides several options for data-parallel training. For applications
    that gradually grow from simple to complex and from prototype to production, the
    common development trajectory would be:'
  prefs: []
  type: TYPE_NORMAL
- en: Use single-device training if the data and model can fit in one GPU, and training
    speed is not a concern.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use single-machine multi-GPU [DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)
    to make use of multiple GPUs on a single machine to speed up training with minimal
    code changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use single-machine multi-GPU [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html),
    if you would like to further speed up training and are willing to write a little
    more code to set it up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use multi-machine [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    and the [launching script](https://github.com/pytorch/examples/blob/master/distributed/ddp/README.md),
    if the application needs to scale across machine boundaries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use multi-GPU [FullyShardedDataParallel](https://pytorch.org/docs/stable/fsdp.html)
    training on a single-machine or multi-machine when the data and model cannot fit
    on one GPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use [torch.distributed.elastic](https://pytorch.org/docs/stable/distributed.elastic.html)
    to launch distributed training if errors (e.g., out-of-memory) are expected or
    if resources can join and leave dynamically during training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Data-parallel training also works with [Automatic Mixed Precision (AMP)](https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-multiple-gpus).
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.DataParallel`'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The [DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)
    package enables single-machine multi-GPU parallelism with the lowest coding hurdle.
    It only requires a one-line change to the application code. The tutorial [Optional:
    Data Parallelism](../beginner/blitz/data_parallel_tutorial.html) shows an example.
    Although `DataParallel` is very easy to use, it usually does not offer the best
    performance because it replicates the model in every forward pass, and its single-process
    multi-thread parallelism naturally suffers from [GIL](https://wiki.python.org/moin/GlobalInterpreterLock)
    contention. To get better performance, consider using [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.parallel.DistributedDataParallel`'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to [DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html),
    [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    requires one more step to set up, i.e., calling [init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group).
    DDP uses multi-process parallelism, and hence there is no GIL contention across
    model replicas. Moreover, the model is broadcast at DDP construction time instead
    of in every forward pass, which also helps to speed up training. DDP is shipped
    with several performance optimization technologies. For a more in-depth explanation,
    refer to this [paper](http://www.vldb.org/pvldb/vol13/p3005-li.pdf) (VLDB’20).
  prefs: []
  type: TYPE_NORMAL
- en: 'DDP materials are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[DDP notes](https://pytorch.org/docs/stable/notes/ddp.html) offer a starter
    example and some brief descriptions of its design and implementation. If this
    is your first time using DDP, start from this document.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Getting Started with Distributed Data Parallel](../intermediate/ddp_tutorial.html)
    explains some common problems with DDP training, including unbalanced workload,
    checkpointing, and multi-device models. Note that, DDP can be easily combined
    with single-machine multi-device model parallelism which is described in the [Single-Machine
    Model Parallel Best Practices](../intermediate/model_parallel_tutorial.html) tutorial.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The [Launching and configuring distributed data parallel applications](https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md)
    document shows how to use the DDP launching script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The [Shard Optimizer States With ZeroRedundancyOptimizer](../recipes/zero_redundancy_optimizer.html)
    recipe demonstrates how [ZeroRedundancyOptimizer](https://pytorch.org/docs/stable/distributed.optim.html)
    helps to reduce optimizer memory footprint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The [Distributed Training with Uneven Inputs Using the Join Context Manager](../advanced/generic_join.html)
    tutorial walks through using the generic join context for distributed training
    with uneven inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`torch.distributed.FullyShardedDataParallel`'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [FullyShardedDataParallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP)
    is a type of data parallelism paradigm which maintains a per-GPU copy of a model’s
    parameters, gradients and optimizer states, it shards all of these states across
    data-parallel workers. The support for FSDP was added starting PyTorch v1.11\.
    The tutorial [Getting Started with FSDP](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)
    provides in depth explanation and example of how FSDP works.
  prefs: []
  type: TYPE_NORMAL
- en: torch.distributed.elastic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the growth of the application complexity and scale, failure recovery becomes
    a requirement. Sometimes it is inevitable to hit errors like out-of-memory (OOM)
    when using DDP, but DDP itself cannot recover from those errors, and it is not
    possible to handle them using a standard `try-except` construct. This is because
    DDP requires all processes to operate in a closely synchronized manner and all
    `AllReduce` communications launched in different processes must match. If one
    of the processes in the group throws an exception, it is likely to lead to desynchronization
    (mismatched `AllReduce` operations) which would then cause a crash or hang. [torch.distributed.elastic](https://pytorch.org/docs/stable/distributed.elastic.html)
    adds fault tolerance and the ability to make use of a dynamic pool of machines
    (elasticity).
  prefs: []
  type: TYPE_NORMAL
- en: RPC-Based Distributed Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many training paradigms do not fit into data parallelism, e.g., parameter server
    paradigm, distributed pipeline parallelism, reinforcement learning applications
    with multiple observers or agents, etc. [torch.distributed.rpc](https://pytorch.org/docs/stable/rpc.html)
    aims at supporting general distributed training scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '[torch.distributed.rpc](https://pytorch.org/docs/stable/rpc.html) has four
    main pillars:'
  prefs: []
  type: TYPE_NORMAL
- en: '[RPC](https://pytorch.org/docs/stable/rpc.html#rpc) supports running a given
    function on a remote worker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RRef](https://pytorch.org/docs/stable/rpc.html#rref) helps to manage the lifetime
    of a remote object. The reference counting protocol is presented in the [RRef
    notes](https://pytorch.org/docs/stable/rpc/rref.html#remote-reference-protocol).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Distributed Autograd](https://pytorch.org/docs/stable/rpc.html#distributed-autograd-framework)
    extends the autograd engine beyond machine boundaries. Please refer to [Distributed
    Autograd Design](https://pytorch.org/docs/stable/rpc/distributed_autograd.html#distributed-autograd-design)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Distributed Optimizer](https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.optim)
    automatically reaches out to all participating workers to update parameters using
    gradients computed by the distributed autograd engine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RPC Tutorials are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: The [Getting Started with Distributed RPC Framework](../intermediate/rpc_tutorial.html)
    tutorial first uses a simple Reinforcement Learning (RL) example to demonstrate
    RPC and RRef. Then, it applies a basic distributed model parallelism to an RNN
    example to show how to use distributed autograd and distributed optimizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The [Implementing a Parameter Server Using Distributed RPC Framework](../intermediate/rpc_param_server_tutorial.html)
    tutorial borrows the spirit of [HogWild! training](https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf)
    and applies it to an asynchronous parameter server (PS) training application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The [Distributed Pipeline Parallelism Using RPC](../intermediate/dist_pipeline_parallel_tutorial.html)
    tutorial extends the single-machine pipeline parallel example (presented in [Single-Machine
    Model Parallel Best Practices](../intermediate/model_parallel_tutorial.html))
    to a distributed environment and shows how to implement it using RPC.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The [Implementing Batch RPC Processing Using Asynchronous Executions](../intermediate/rpc_async_execution.html)
    tutorial demonstrates how to implement RPC batch processing using the [@rpc.functions.async_execution](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.functions.async_execution)
    decorator, which can help speed up inference and training. It uses RL and PS examples
    similar to those in the above tutorials 1 and 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The [Combining Distributed DataParallel with Distributed RPC Framework](../advanced/rpc_ddp_tutorial.html)
    tutorial demonstrates how to combine DDP with RPC to train a model using distributed
    data parallelism combined with distributed model parallelism.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PyTorch Distributed Developers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’d like to contribute to PyTorch Distributed, please refer to our [Developer
    Guide](https://github.com/pytorch/pytorch/blob/master/torch/distributed/CONTRIBUTING.md).
  prefs: []
  type: TYPE_NORMAL
