- en: Distributed Data Parallel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式数据并行
- en: 原文：[https://pytorch.org/docs/stable/notes/ddp.html](https://pytorch.org/docs/stable/notes/ddp.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/notes/ddp.html](https://pytorch.org/docs/stable/notes/ddp.html)
- en: Warning
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: The implementation of [`torch.nn.parallel.DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") evolves over time. This design note
    is written based on the state as of v1.4.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[`torch.nn.parallel.DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel")的实现随时间推移而发展。本设计说明是基于v1.4状态编写的。'
- en: '[`torch.nn.parallel.DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") (DDP) transparently performs distributed
    data parallel training. This page describes how it works and reveals implementation
    details.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[`torch.nn.parallel.DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel")（DDP）透明地执行分布式数据并行训练。本页描述了它的工作原理并揭示了实现细节。'
- en: Example
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: Let us start with a simple [`torch.nn.parallel.DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") example. This example uses a [`torch.nn.Linear`](../generated/torch.nn.Linear.html#torch.nn.Linear
    "torch.nn.Linear") as the local model, wraps it with DDP, and then runs one forward
    pass, one backward pass, and an optimizer step on the DDP model. After that, parameters
    on the local model will be updated, and all models on different processes should
    be exactly the same.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的[`torch.nn.parallel.DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel")示例开始。这个示例使用一个[`torch.nn.Linear`](../generated/torch.nn.Linear.html#torch.nn.Linear
    "torch.nn.Linear")作为本地模型，将其与DDP包装起来，然后在DDP模型上运行一次前向传递，一次反向传递和一个优化器步骤。之后，本地模型上的参数将被更新，并且不同进程上的所有模型应该完全相同。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: DDP works with TorchDynamo. When used with TorchDynamo, apply the DDP model
    wrapper before compiling the model, such that torchdynamo can apply `DDPOptimizer`
    (graph-break optimizations) based on DDP bucket sizes. (See [TorchDynamo DDPOptimizer](./ddp.html#torchdynamo-ddpoptimizer)
    for more information.)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: DDP与TorchDynamo一起使用。当与TorchDynamo一起使用时，在编译模型之前应用DDP模型包装器，以便torchdynamo可以根据DDP桶大小应用`DDPOptimizer`（基于DDP桶大小的图断点优化）。
    （有关更多信息，请参见[TorchDynamo DDPOptimizer](./ddp.html#torchdynamo-ddpoptimizer)。）
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Internal Design
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内部设计
- en: This section reveals how it works under the hood of [`torch.nn.parallel.DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel") by diving into details of every step
    in one iteration.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过深入探讨每个迭代步骤的细节，揭示了[`torch.nn.parallel.DistributedDataParallel`](../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel
    "torch.nn.parallel.DistributedDataParallel")的内部工作原理。
- en: '**Prerequisite**: DDP relies on c10d `ProcessGroup` for communications. Hence,
    applications must create `ProcessGroup` instances before constructing DDP.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**先决条件**：DDP依赖于c10d`ProcessGroup`进行通信。因此，应用程序在构造DDP之前必须创建`ProcessGroup`实例。'
- en: '**Construction**: The DDP constructor takes a reference to the local module,
    and broadcasts `state_dict()` from the process with rank 0 to all other processes
    in the group to make sure that all model replicas start from the exact same state.
    Then, each DDP process creates a local `Reducer`, which later will take care of
    the gradients synchronization during the backward pass. To improve communication
    efficiency, the `Reducer` organizes parameter gradients into buckets, and reduces
    one bucket at a time. Bucket size can be configured by setting the bucket_cap_mb
    argument in DDP constructor. The mapping from parameter gradients to buckets is
    determined at the construction time, based on the bucket size limit and parameter
    sizes. Model parameters are allocated into buckets in (roughly) the reverse order
    of `Model.parameters()` from the given model. The reason for using the reverse
    order is because DDP expects gradients to become ready during the backward pass
    in approximately that order. The figure below shows an example. Note that, the
    `grad0` and `grad1` are in `bucket1`, and the other two gradients are in `bucket0`.
    Of course, this assumption might not always be true, and when that happens it
    could hurt DDP backward speed as the `Reducer` cannot kick off the communication
    at the earliest possible time. Besides bucketing, the `Reducer` also registers
    autograd hooks during construction, one hook per parameter. These hooks will be
    triggered during the backward pass when the gradient becomes ready.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构造**：DDP构造函数接受对本地模块的引用，并从排名为0的进程向组中的所有其他进程广播`state_dict()`，以确保所有模型副本从完全相同的状态开始。然后，每个DDP进程创建一个本地的`Reducer`，后者将在反向传递期间负责梯度同步。为了提高通信效率，`Reducer`将参数梯度组织成桶，并一次减少一个桶。可以通过在DDP构造函数中设置bucket_cap_mb参数来配置桶大小。参数梯度到桶的映射是在构造时确定的，基于桶大小限制和参数大小。模型参数按照给定模型的`Model.parameters()`的（大致）相反顺序分配到桶中。使用相反顺序的原因是因为DDP期望梯度在反向传递期间以大致相同的顺序准备就绪。下面的图显示了一个示例。请注意，`grad0`和`grad1`在`bucket1`中，另外两个梯度在`bucket0`中。当然，这种假设可能并不总是正确，当发生这种情况时，可能会影响DDP反向传递速度，因为`Reducer`无法在可能的最早时间开始通信。除了分桶，`Reducer`还在构造过程中注册自动求导钩子，每个参数一个钩子。这些钩子将在梯度准备就绪时在反向传递期间触发。'
- en: '**Forward Pass**: The DDP takes the input and passes it to the local model,
    and then analyzes the output from the local model if `find_unused_parameters`
    is set to `True`. This mode allows running backward on a subgraph of the model,
    and DDP finds out which parameters are involved in the backward pass by traversing
    the autograd graph from the model output and marking all unused parameters as
    ready for reduction. During the backward pass, the `Reducer` would only wait for
    unready parameters, but it would still reduce all buckets. Marking a parameter
    gradient as ready does not help DDP skip buckets as for now, but it will prevent
    DDP from waiting for absent gradients forever during the backward pass. Note that
    traversing the autograd graph introduces extra overheads, so applications should
    only set `find_unused_parameters` to `True` when necessary.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前向传播**：DDP接受输入并将其传递给本地模型，然后分析本地模型的输出，如果`find_unused_parameters`设置为`True`。此模式允许在模型的子图上运行反向传播，DDP通过从模型输出遍历自动求导图并标记所有未使用的参数为准备好进行减少。在反向传播期间，`Reducer`只会等待未准备好的参数，但仍会减少所有桶。将参数梯度标记为准备好不会帮助DDP跳过桶，但它将防止DDP在反向传播期间永远等待缺失的梯度。请注意，遍历自动求导图会引入额外的开销，因此应用程序只应在必要时将`find_unused_parameters`设置为`True`。'
- en: '**Backward Pass**: The `backward()` function is directly invoked on the loss
    `Tensor`, which is out of DDP’s control, and DDP uses autograd hooks registered
    at construction time to trigger gradients synchronizations. When one gradient
    becomes ready, its corresponding DDP hook on that grad accumulator will fire,
    and DDP will then mark that parameter gradient as ready for reduction. When gradients
    in one bucket are all ready, the `Reducer` kicks off an asynchronous `allreduce`
    on that bucket to calculate mean of gradients across all processes. When all buckets
    are ready, the `Reducer` will block waiting for all `allreduce` operations to
    finish. When this is done, averaged gradients are written to the `param.grad`
    field of all parameters. So after the backward pass, the grad field on the same
    corresponding parameter across different DDP processes should be the same.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**：`backward()`函数直接在损失`Tensor`上调用，这是DDP无法控制的，DDP在构造时使用的自动求导钩子来触发梯度同步。当一个梯度准备就绪时，其对应的DDP钩子将触发该梯度累加器上的梯度，并且DDP将标记该参数梯度为准备好进行减少。当一个桶中的梯度都准备就绪时，`Reducer`会在该桶上启动一个异步的`allreduce`来计算所有进程中梯度的平均值。当所有桶都准备就绪时，`Reducer`将阻塞等待所有`allreduce`操作完成。完成后，平均梯度将写入所有参数的`param.grad`字段。因此，在反向传播之后，不同DDP进程中相应参数的grad字段应该是相同的。'
- en: '**Optimizer Step**: From the optimizer’s perspective, it is optimizing a local
    model. Model replicas on all DDP processes can keep in sync because they all start
    from the same state and they have the same averaged gradients in every iteration.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器步骤**：从优化器的角度来看，它正在优化一个本地模型。所有DDP进程上的模型副本可以保持同步，因为它们都从相同的状态开始，并且它们在每次迭代中具有相同的平均梯度。'
- en: '[![ddp_grad_sync.png](../Images/7fd0c1da1ad18ef7e8187a73ace04695.png)](https://user-images.githubusercontent.com/16999635/72401724-d296d880-371a-11ea-90ab-737f86543df9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[![ddp_grad_sync.png](../Images/7fd0c1da1ad18ef7e8187a73ace04695.png)](https://user-images.githubusercontent.com/16999635/72401724-d296d880-371a-11ea-90ab-737f86543df9.png)'
- en: Note
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: DDP requires `Reducer` instances on all processes to invoke `allreduce` in exactly
    the same order, which is done by always running `allreduce` in the bucket index
    order instead of actual bucket ready order. Mismatched `allreduce` order across
    processes can lead to wrong results or DDP backward hang.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: DDP要求所有进程上的`Reducer`实例以完全相同的顺序调用`allreduce`，这是通过始终按照桶索引顺序而不是实际桶准备就绪顺序来运行`allreduce`来实现的。跨进程的`allreduce`顺序不匹配可能导致错误的结果或DDP反向传播挂起。
- en: Implementation
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: Below are pointers to the DDP implementation components. The stacked graph shows
    the structure of the code.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DDP实现组件的指针。堆叠图显示了代码的结构。
- en: ProcessGroup
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ProcessGroup
- en: '[ProcessGroup.hpp](https://github.com/pytorch/pytorch/blob/v1.7.0/torch/lib/c10d/ProcessGroup.hpp):
    contains the abstract API of all process group implementations. The `c10d` library
    provides 3 implementations out of the box, namely, ProcessGroupGloo, ProcessGroupNCCL,
    and ProcessGroupMPI. `DistributedDataParallel` uses `ProcessGroup::broadcast()`
    to send model states from the process with rank 0 to others during initialization
    and `ProcessGroup::allreduce()` to sum gradients.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ProcessGroup.hpp](https://github.com/pytorch/pytorch/blob/v1.7.0/torch/lib/c10d/ProcessGroup.hpp)：包含所有进程组实现的抽象API。`c10d`库提供了3种开箱即用的实现，即ProcessGroupGloo、ProcessGroupNCCL和ProcessGroupMPI。`DistributedDataParallel`使用`ProcessGroup::broadcast()`在初始化期间从排名为0的进程向其他进程发送模型状态，并使用`ProcessGroup::allreduce()`来求和梯度。'
- en: '[Store.hpp](https://github.com/pytorch/pytorch/blob/v1.7.0/torch/lib/c10d/Store.hpp):
    assists the rendezvous service for process group instances to find each other.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Store.hpp](https://github.com/pytorch/pytorch/blob/v1.7.0/torch/lib/c10d/Store.hpp)：协助进程组实例的会合服务找到彼此。'
- en: DistributedDataParallel
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DistributedDataParallel
- en: '[distributed.py](https://github.com/pytorch/pytorch/blob/v1.7.0/torch/nn/parallel/distributed.py):
    is the Python entry point for DDP. It implements the initialization steps and
    the `forward` function for the `nn.parallel.DistributedDataParallel` module which
    call into C++ libraries. Its `_sync_param` function performs intra-process parameter
    synchronization when one DDP process works on multiple devices, and it also broadcasts
    model buffers from the process with rank 0 to all other processes. The inter-process
    parameter synchronization happens in `Reducer.cpp`.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[distributed.py](https://github.com/pytorch/pytorch/blob/v1.7.0/torch/nn/parallel/distributed.py)：是DDP的Python入口点。它实现了初始化步骤和`nn.parallel.DistributedDataParallel`模块的`forward`函数，该函数调用C++库。其`_sync_param`函数在一个DDP进程在多个设备上工作时执行进程内参数同步，并且它还会将模型缓冲区从排名为0的进程广播到所有其他进程。进程间参数同步发生在`Reducer.cpp`中。'
- en: '[comm.h](https://github.com/pytorch/pytorch/blob/v1.7.0/torch/csrc/distributed/c10d/comm.h):
    implements the coalesced broadcast helper function which is invoked to broadcast
    model states during initialization and synchronize model buffers before the forward
    pass.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[comm.h](https://github.com/pytorch/pytorch/blob/v1.7.0/torch/csrc/distributed/c10d/comm.h)：实现了合并广播辅助函数，用于在初始化期间广播模型状态并在前向传递之前同步模型缓冲区。'
- en: '[reducer.h](https://github.com/pytorch/pytorch/blob/v1.7.0/torch/csrc/distributed/c10d/reducer.h):
    provides the core implementation for gradient synchronization in the backward
    pass. It has three entry point functions:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[reducer.h](https://github.com/pytorch/pytorch/blob/v1.7.0/torch/csrc/distributed/c10d/reducer.h)：提供了在反向传递中进行梯度同步的核心实现。它有三个入口点函数：'
- en: '`Reducer`: The constructor is called in `distributed.py` which registers `Reducer::autograd_hook()`
    to gradient accumulators.'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Reducer`：构造函数在`distributed.py`中被调用，注册`Reducer::autograd_hook()`到梯度累加器。'
- en: '`autograd_hook()` function will be invoked by the autograd engine when a gradient
    becomes ready.'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`autograd_hook()`函数将在梯度准备就绪时被自动求导引擎调用。'
- en: '`prepare_for_backward()` is called at the end of DDP forward pass in `distributed.py`.
    It traverses the autograd graph to find unused parameters when `find_unused_parameters`
    is set to `True` in DDP constructor.'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prepare_for_backward()`在`distributed.py`中的DDP前向传递结束时被调用。当在DDP构造函数中将`find_unused_parameters`设置为`True`时，它会遍历自动求导图以找到未使用的参数。'
- en: '[![ddp_code.png](../Images/0b2511513fe6a3326d13ebd545cfb730.png)](https://user-images.githubusercontent.com/16999635/72313120-4e7c1c80-3658-11ea-9c6d-44336b2daeac.png)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[![ddp_code.png](../Images/0b2511513fe6a3326d13ebd545cfb730.png)](https://user-images.githubusercontent.com/16999635/72313120-4e7c1c80-3658-11ea-9c6d-44336b2daeac.png)'
- en: TorchDynamo DDPOptimizer
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TorchDynamo DDPOptimizer
- en: DDP’s performance advantage comes from overlapping allreduce collectives with
    computations during backwards. AotAutograd prevents this overlap when used with
    TorchDynamo for compiling a whole forward and whole backward graph, because allreduce
    ops are launched by autograd hooks _after_ the whole optimized backwards computation
    finishes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: DDP的性能优势来自在反向传递期间将allreduce集体操作与计算重叠。当与TorchDynamo一起使用时，AotAutograd会阻止这种重叠，因为它用于编译整个前向和整个反向图形，这会导致在整个优化的反向计算完成后，梯度同步操作由自动求导钩子在之后启动。
- en: 'TorchDynamo’s DDPOptimizer helps by breaking the forward graph at the logical
    boundaries of DDP’s allreduce buckets during backwards. Note: the goal is to break
    the graph during backwards, and the simplest implementation is to break the forward
    graphs and then call AotAutograd and compilation on each section. This allows
    DDP’s allreduce hooks to fire in-between sections of backwards, and schedule communications
    to overlap with compute.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TorchDynamo的DDPOptimizer通过在反向传递期间在DDP的allreduce桶的逻辑边界处中断前向图来帮助。注意：目标是在反向传递期间中断图形，最简单的实现方式是在前向图形中断，然后在每个部分上调用AotAutograd和编译。这允许DDP的allreduce钩子在反向传递的各个部分之间触发，并安排通信与计算重叠。
- en: See [this blog post](https://dev-discuss.pytorch.org/t/torchdynamo-update-9-making-ddp-work-with-torchdynamo/860/1)
    for a more in-depth explanation and experimental results, or read the docs and
    code at [torch/_dynamo/optimizations/distributed.py](https://github.com/pytorch/pytorch/blob/4908a12542798a3e8641faae6b74f068fdfc6778/torch/_dynamo/optimizations/distributed.py#L56)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[这篇博客文章](https://dev-discuss.pytorch.org/t/torchdynamo-update-9-making-ddp-work-with-torchdynamo/860/1)以获取更深入的解释和实验结果，或者在[torch/_dynamo/optimizations/distributed.py](https://github.com/pytorch/pytorch/blob/4908a12542798a3e8641faae6b74f068fdfc6778/torch/_dynamo/optimizations/distributed.py#L56)中阅读文档和代码
- en: To Debug DDPOptimizer, set torch._dynamo.config.log_level to DEBUG (for full
    graph dumps) or INFO (for basic info about bucket boundaries). To disable DDPOptimizer,
    set torch._dynamo.config.optimize_ddp=False. DDP and TorchDynamo should still
    work correctly without DDPOptimizer, but with performance degradation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要调试DDPOptimizer，请将torch._dynamo.config.log_level设置为DEBUG（用于完整图形转储）或INFO（用于有关桶边界的基本信息）。要禁用DDPOptimizer，请将torch._dynamo.config.optimize_ddp设置为False。DDP和TorchDynamo应该在没有DDPOptimizer的情况下仍能正常工作，但性能会下降。
