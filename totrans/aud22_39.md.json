["```py\nimport numpy as np\nimport sentencepiece as spm\nimport torch\nimport torchaudio\nimport torchvision \n```", "```py\ndef stream(q, format, option, src, segment_length, sample_rate):\n    print(\"Building StreamReader...\")\n    streamer = torchaudio.io.StreamReader(src=src, format=format, option=option)\n    streamer.add_basic_video_stream(frames_per_chunk=segment_length, buffer_chunk_size=500, width=600, height=340)\n    streamer.add_basic_audio_stream(frames_per_chunk=segment_length * 640, sample_rate=sample_rate)\n\n    print(streamer.get_src_stream_info(0))\n    print(streamer.get_src_stream_info(1))\n    print(\"Streaming...\")\n    print()\n    for (chunk_v, chunk_a) in streamer.stream(timeout=-1, backoff=1.0):\n        q.put([chunk_v, chunk_a])\n\nclass ContextCacher:\n    def __init__(self, segment_length: int, context_length: int, rate_ratio: int):\n        self.segment_length = segment_length\n        self.context_length = context_length\n\n        self.context_length_v = context_length\n        self.context_length_a = context_length * rate_ratio\n        self.context_v = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros \"torch.zeros\")([self.context_length_v, 3, 340, 600])\n        self.context_a = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros \"torch.zeros\")([self.context_length_a, 1])\n\n    def __call__(self, chunk_v, chunk_a):\n        if chunk_v.size(0) < self.segment_length:\n            chunk_v = [torch.nn.functional.pad](https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad \"torch.nn.functional.pad\")(chunk_v, (0, 0, 0, 0, 0, 0, 0, self.segment_length - chunk_v.size(0)))\n        if chunk_a.size(0) < self.segment_length * 640:\n            chunk_a = [torch.nn.functional.pad](https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad \"torch.nn.functional.pad\")(chunk_a, (0, 0, 0, self.segment_length * 640 - chunk_a.size(0)))\n\n        if self.context_length == 0:\n            return chunk_v.float(), chunk_a.float()\n        else:\n            chunk_with_context_v = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")((self.context_v, chunk_v))\n            chunk_with_context_a = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")((self.context_a, chunk_a))\n            self.context_v = chunk_v[-self.context_length_v :]\n            self.context_a = chunk_a[-self.context_length_a :]\n            return chunk_with_context_v.float(), chunk_with_context_a.float() \n```", "```py\nimport sys\n\nsys.path.insert(0, \"../../examples\")\n\nfrom avsr.data_prep.detectors.mediapipe.detector import LandmarksDetector\nfrom avsr.data_prep.detectors.mediapipe.video_process import VideoProcess\n\nclass FunctionalModule([torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, functional):\n        super().__init__()\n        self.functional = functional\n\n    def forward(self, input):\n        return self.functional(input)\n\nclass Preprocessing([torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self):\n        super().__init__()\n        self.landmarks_detector = LandmarksDetector()\n        self.video_process = VideoProcess()\n        self.video_transform = [torch.nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            FunctionalModule(\n                lambda n: [(lambda x: torchvision.transforms.functional.resize(x, 44, antialias=True))(i) for i in n]\n            ),\n            FunctionalModule(lambda x: [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html#torch.stack \"torch.stack\")(x)),\n            torchvision.transforms.Normalize(0.0, 255.0),\n            torchvision.transforms.Grayscale(),\n            torchvision.transforms.Normalize(0.421, 0.165),\n        )\n\n    def forward(self, audio, video):\n        video = video.permute(0, 2, 3, 1).cpu().numpy().astype([np.uint8](https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.uint8 \"numpy.uint8\"))\n        landmarks = self.landmarks_detector(video)\n        video = self.video_process(video, landmarks)\n        video = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")(video).permute(0, 3, 1, 2).float()\n        video = self.video_transform(video)\n        audio = audio.mean(axis=-1, keepdim=True)\n        return audio, video \n```", "```py\nclass SentencePieceTokenProcessor:\n    def __init__(self, sp_model):\n        self.sp_model = sp_model\n        self.post_process_remove_list = {\n            self.sp_model.unk_id(),\n            self.sp_model.eos_id(),\n            self.sp_model.pad_id(),\n        }\n\n    def __call__(self, tokens, lstrip: bool = True) -> str:\n        filtered_hypo_tokens = [\n            token_index for token_index in tokens[1:] if token_index not in self.post_process_remove_list\n        ]\n        output_string = \"\".join(self.sp_model.id_to_piece(filtered_hypo_tokens)).replace(\"\\u2581\", \" \")\n\n        if lstrip:\n            return output_string.lstrip()\n        else:\n            return output_string\n\nclass InferencePipeline([torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, preprocessor, model, decoder, token_processor):\n        super().__init__()\n        self.preprocessor = preprocessor\n        self.model = model\n        self.decoder = decoder\n        self.token_processor = token_processor\n\n        self.state = None\n        self.hypotheses = None\n\n    def forward(self, audio, video):\n        audio, video = self.preprocessor(audio, video)\n        feats = self.model(audio.unsqueeze(0), video.unsqueeze(0))\n        length = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")([feats.size(1)], device=audio.device)\n        self.hypotheses, self.state = self.decoder.infer(feats, length, 10, state=self.state, hypothesis=self.hypotheses)\n        transcript = self.token_processor(self.hypotheses[0][0], lstrip=False)\n        return transcript\n\ndef _get_inference_pipeline(model_path, spm_model_path):\n    model = [torch.jit.load](https://pytorch.org/docs/stable/generated/torch.jit.load.html#torch.jit.load \"torch.jit.load\")(model_path)\n    model.eval()\n\n    sp_model = spm.SentencePieceProcessor(model_file=spm_model_path)\n    token_processor = SentencePieceTokenProcessor(sp_model)\n\n    decoder = torchaudio.models.RNNTBeamSearch(model.model, sp_model.get_piece_size())\n\n    return InferencePipeline(\n        preprocessor=Preprocessing(),\n        model=model,\n        decoder=decoder,\n        token_processor=token_processor,\n    ) \n```", "```py\nfrom torchaudio.utils import download_asset\n\ndef main(device, src, option=None):\n    print(\"Building pipeline...\")\n    model_path = download_asset(\"tutorial-assets/device_avsr_model.pt\")\n    spm_model_path = download_asset(\"tutorial-assets/spm_unigram_1023.model\")\n\n    pipeline = _get_inference_pipeline(model_path, spm_model_path)\n\n    BUFFER_SIZE = 32\n    segment_length = 8\n    context_length = 4\n    sample_rate = 19200\n    frame_rate = 30\n    rate_ratio = sample_rate // frame_rate\n    cacher = ContextCacher(BUFFER_SIZE, context_length, rate_ratio)\n\n    import torch.multiprocessing as mp\n\n    ctx = mp.get_context(\"spawn\")\n\n    @torch.inference_mode()\n    def infer():\n        num_video_frames = 0\n        video_chunks = []\n        audio_chunks = []\n        while True:\n            chunk_v, chunk_a = q.get()\n            num_video_frames += chunk_a.size(0) // 640\n            video_chunks.append(chunk_v)\n            audio_chunks.append(chunk_a)\n            if num_video_frames < BUFFER_SIZE:\n                continue\n            video = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")(video_chunks)\n            audio = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")(audio_chunks)\n            video, audio = cacher(video, audio)\n            pipeline.state, pipeline.hypotheses = None, None\n            transcript = pipeline(audio, video.float())\n            print(transcript, end=\"\", flush=True)\n            num_video_frames = 0\n            video_chunks = []\n            audio_chunks = []\n\n    q = ctx.Queue()\n    p = ctx.Process(target=stream, args=(q, device, option, src, segment_length, sample_rate))\n    p.start()\n    infer()\n    p.join()\n\nif __name__ == \"__main__\":\n    main(\n        device=\"avfoundation\",\n        src=\"0:1\",\n        option={\"framerate\": \"30\", \"pixel_format\": \"rgb24\"},\n    ) \n```", "```py\nBuilding pipeline...\nBuilding StreamReader...\nSourceVideoStream(media_type='video', codec='rawvideo', codec_long_name='raw video', format='uyvy422', bit_rate=0, num_frames=0, bits_per_sample=0, metadata={}, width=1552, height=1552, frame_rate=1000000.0)\nSourceAudioStream(media_type='audio', codec='pcm_f32le', codec_long_name='PCM 32-bit floating point little-endian', format='flt', bit_rate=1536000, num_frames=0, bits_per_sample=0, metadata={}, sample_rate=48000.0, num_channels=1)\nStreaming...\n\nhello world \n```"]