- en: PyTorch Profiler With TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](#sphx-glr-download-intermediate-tensorboard-profiler-tutorial-py)
    to download the full example code
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial demonstrates how to use TensorBoard plugin with PyTorch Profiler
    to detect performance bottlenecks of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch 1.8 includes an updated profiler API capable of recording the CPU side
    operations as well as the CUDA kernel launches on the GPU side. The profiler can
    visualize this information in TensorBoard Plugin and provide analysis of the performance
    bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will use a simple Resnet model to demonstrate how to use
    TensorBoard plugin to analyze model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To install `torch` and `torchvision` use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prepare the data and model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use profiler to record execution events
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the profiler
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use TensorBoard to view results and analyze model performance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improve performance with the help of profiler
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze performance with other advanced features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Additional Practices: Profiling PyTorch on AMD GPUs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Prepare the data and model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, import all necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then prepare the input data. For this tutorial, we use the CIFAR10 dataset.
    Transform it to the desired format and use `DataLoader` to load each batch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, create Resnet model, loss function, and optimizer objects. To run on GPU,
    move model and loss to GPU device.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Define the training step for each batch of input data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Use profiler to record execution events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The profiler is enabled through the context manager and accepts several parameters,
    some of the most useful are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`schedule` - callable that takes step (int) as a single parameter and returns
    the profiler action to perform at each step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example with `wait=1, warmup=1, active=3, repeat=1`, profiler will skip
    the first step/iteration, start warming up on the second, record the following
    three iterations, after which the trace will become available and on_trace_ready
    (when set) is called. In total, the cycle repeats once. Each cycle is called a
    “span” in TensorBoard plugin.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: During `wait` steps, the profiler is disabled. During `warmup` steps, the profiler
    starts tracing but the results are discarded. This is for reducing the profiling
    overhead. The overhead at the beginning of profiling is high and easy to bring
    skew to the profiling result. During `active` steps, the profiler works and records
    events.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`on_trace_ready` - callable that is called at the end of each cycle; In this
    example we use `torch.profiler.tensorboard_trace_handler` to generate result files
    for TensorBoard. After profiling, result files will be saved into the `./log/resnet18`
    directory. Specify this directory as a `logdir` parameter to analyze profile in
    TensorBoard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`record_shapes` - whether to record shapes of the operator inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`profile_memory` - Track tensor memory allocation/deallocation. Note, for old
    version of pytorch with version before 1.10, if you suffer long profiling time,
    please disable it or upgrade to new version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with_stack` - Record source information (file and line number) for the ops.
    If the TensorBoard is launched in VS Code ([reference](https://code.visualstudio.com/docs/datascience/pytorch-support#_tensorboard-integration)),
    clicking a stack frame will navigate to the specific code line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, the following non-context manager start/stop is supported as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Run the profiler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Run the above code. The profiling result will be saved under `./log/resnet18`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Use TensorBoard to view results and analyze model performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard Plugin support has been deprecated, so some of these functions may
    not work as previously. Please take a look at the replacement, [HTA](https://github.com/pytorch/kineto/tree/main#holistic-trace-analysis).
  prefs: []
  type: TYPE_NORMAL
- en: Install PyTorch Profiler TensorBoard Plugin.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Launch the TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Open the TensorBoard profile URL in Google Chrome browser or Microsoft Edge
    browser (**Safari is not supported**).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You could see Profiler plugin page as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_overview1.png](../Images/7bf5bbd17de6da63afc38b29b8c8f0d2.png)](../_static/img/profiler_overview1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The overview shows a high-level summary of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: The “GPU Summary” panel shows the GPU configuration, GPU usage and Tensor Cores
    usage. In this example, the GPU Utilization is low. The details of these metrics
    are [here](https://github.com/pytorch/kineto/blob/main/tb_plugin/docs/gpu_utilization.md).
  prefs: []
  type: TYPE_NORMAL
- en: The “Step Time Breakdown” shows distribution of time spent in each step over
    different categories of execution. In this example, you can see the `DataLoader`
    overhead is significant.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom “Performance Recommendation” uses the profiling data to automatically
    highlight likely bottlenecks, and gives you actionable optimization suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: You can change the view page in left “Views” dropdown list.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8e0d2a9bdc16d06848a55ae706a357e.png)'
  prefs: []
  type: TYPE_IMG
- en: Operator view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operator view displays the performance of every PyTorch operator that is
    executed either on the host or device.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_operator_view.png](../Images/4fae99315367a1998f977b76a2fc6526.png)](../_static/img/profiler_operator_view.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The “Self” duration does not include its child operators’ time. The “Total”
    duration includes its child operators’ time.
  prefs: []
  type: TYPE_NORMAL
- en: View call stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click the `View Callstack` of an operator, the operators with same name but
    different call stacks will be shown. Then click a `View Callstack` in this sub-table,
    the call stack frames will be shown.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_callstack.png](../Images/0d8e7045d34fb23f544d1fdb71ccb79b.png)](../_static/img/profiler_callstack.png)'
  prefs: []
  type: TYPE_NORMAL
- en: If the TensorBoard is launched inside VS Code ([Launch Guide](https://devblogs.microsoft.com/python/python-in-visual-studio-code-february-2021-release/#tensorboard-integration)),
    clicking a call stack frame will navigate to the specific code line.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_vscode.png](../Images/75f42648d12a47e893905f678287a967.png)](../_static/img/profiler_vscode.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GPU kernel view shows all kernels’ time spent on GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_kernel_view.png](../Images/5122dd95514210b1325de9e54574173f.png)](../_static/img/profiler_kernel_view.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensor Cores Used: Whether this kernel uses Tensor Cores.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean Blocks per SM: Blocks per SM = Blocks of this kernel / SM number of this
    GPU. If this number is less than 1, it indicates the GPU multiprocessors are not
    fully utilized. “Mean Blocks per SM” is weighted average of all runs of this kernel
    name, using each run’s duration as weight.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean Est. Achieved Occupancy: Est. Achieved Occupancy is defined in this column’s
    tooltip. For most cases such as memory bandwidth bounded kernels, the higher the
    better. “Mean Est. Achieved Occupancy” is weighted average of all runs of this
    kernel name, using each run’s duration as weight.'
  prefs: []
  type: TYPE_NORMAL
- en: Trace view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trace view shows timeline of profiled operators and GPU kernels. You can
    select it to see details as below.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_trace_view1.png](../Images/be1bf500afaf7c10bd7f7f8a30fa1ef9.png)](../_static/img/profiler_trace_view1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: You can move the graph and zoom in/out with the help of right side toolbar.
    And keyboard can also be used to zoom and move around inside the timeline. The
    ‘w’ and ‘s’ keys zoom in centered around the mouse, and the ‘a’ and ‘d’ keys move
    the timeline left and right. You can hit these keys multiple times until you see
    a readable representation.
  prefs: []
  type: TYPE_NORMAL
- en: If a backward operator’s “Incoming Flow” field is with value “forward correspond
    to backward”, you can click the text to get its launching forward operator.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_trace_view_fwd_bwd.png](../Images/cb82608044c7382139065f9e79f1a99d.png)](../_static/img/profiler_trace_view_fwd_bwd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we can see the event prefixed with `enumerate(DataLoader)`
    costs a lot of time. And during most of this period, the GPU is idle. Because
    this function is loading data and transforming data on host side, during which
    the GPU resource is wasted.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Improve performance with the help of profiler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the bottom of “Overview” page, the suggestion in “Performance Recommendation”
    hints the bottleneck is `DataLoader`. The PyTorch `DataLoader` uses single process
    by default. User could enable multi-process data loading by setting the parameter
    `num_workers`. [Here](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)
    is more details.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we follow the “Performance Recommendation” and set `num_workers`
    as below, pass a different name such as `./log/resnet18_4workers` to `tensorboard_trace_handler`,
    and run it again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Then let’s choose the recently profiled run in left “Runs” dropdown list.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_overview2.png](../Images/837967744e5997b8debc071b27685596.png)](../_static/img/profiler_overview2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: From the above view, we can find the step time is reduced to about 76ms comparing
    with previous run’s 132ms, and the time reduction of `DataLoader` mainly contributes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_trace_view2.png](../Images/9126a2827ef47b32d4dd38a1e813505e.png)](../_static/img/profiler_trace_view2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: From the above view, we can see that the runtime of `enumerate(DataLoader)`
    is reduced, and the GPU utilization is increased.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Analyze performance with other advanced features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Memory view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To profile memory, `profile_memory` must be set to `True` in arguments of `torch.profiler.profile`.
  prefs: []
  type: TYPE_NORMAL
- en: You can try it by using existing example on Azure
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The profiler records all memory allocation/release events and allocator’s internal
    state during profiling. The memory view consists of three components as shown
    in the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_memory_view.png](../Images/c6251499e3b25e142059d0e53c1c3007.png)](../_static/img/profiler_memory_view.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The components are memory curve graph, memory events table and memory statistics
    table, from top to bottom, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The memory type could be selected in “Device” selection box. For example, “GPU0”
    means the following table only shows each operator’s memory usage on GPU 0, not
    including CPU or other GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory curve shows the trends of memory consumption. The “Allocated” curve
    shows the total memory that is actually in use, e.g., tensors. In PyTorch, caching
    mechanism is employed in CUDA allocator and some other allocators. The “Reserved”
    curve shows the total memory that is reserved by the allocator. You can left click
    and drag on the graph to select events in the desired range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_memory_curve_selecting.png](../Images/e9ec73bd94cda9e0afe2f7d66988efb3.png)](../_static/img/profiler_memory_curve_selecting.png)'
  prefs: []
  type: TYPE_NORMAL
- en: After selection, the three components will be updated for the restricted time
    range, so that you can gain more information about it. By repeating this process,
    you can zoom into a very fine-grained detail. Right click on the graph will reset
    the graph to the initial state.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_memory_curve_single.png](../Images/b34a9076e55573e9c29e772fd4fc8238.png)](../_static/img/profiler_memory_curve_single.png)'
  prefs: []
  type: TYPE_NORMAL
- en: In the memory events table, the allocation and release events are paired into
    one entry. The “operator” column shows the immediate ATen operator that is causing
    the allocation. Notice that in PyTorch, ATen operators commonly use `aten::empty`
    to allocate memory. For example, `aten::ones` is implemented as `aten::empty`
    followed by an `aten::fill_`. Solely display the operator name as `aten::empty`
    is of little help. It will be shown as `aten::ones (aten::empty)` in this special
    case. The “Allocation Time”, “Release Time” and “Duration” columns’ data might
    be missing if the event occurs outside of the time range.
  prefs: []
  type: TYPE_NORMAL
- en: In the memory statistics table, the “Size Increase” column sums up all allocation
    size and minus all the memory release size, that is, the net increase of memory
    usage after this operator. The “Self Size Increase” column is similar to “Size
    Increase”, but it does not count children operators’ allocation. With regards
    to ATen operators’ implementation detail, some operators might call other operators,
    so memory allocations can happen at any level of the call stack. That says, “Self
    Size Increase” only count the memory usage increase at current level of call stack.
    Finally, the “Allocation Size” column sums up all allocation without considering
    the memory release.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The plugin now supports distributed view on profiling DDP with NCCL/GLOO as
    backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can try it by using existing example on Azure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![../_static/img/profiler_distributed_view.png](../Images/bc5ec09af445c3714c07c9bc3c7fb515.png)](../_static/img/profiler_distributed_view.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The “Computation/Communication Overview” shows computation/communication ratio
    and their overlapping degree. From this view, User can figure out load balance
    issue among workers. For example, if the computation + overlapping time of one
    worker is much larger than others, there may be a problem of load balance or this
    worker may be a straggler.
  prefs: []
  type: TYPE_NORMAL
- en: The “Synchronizing/Communication Overview” shows the efficiency of communication.
    “Data Transfer Time” is the time for actual data exchanging. “Synchronizing Time”
    is the time for waiting and synchronizing with other workers.
  prefs: []
  type: TYPE_NORMAL
- en: If one worker’s “Synchronizing Time” is much shorter than that of other workers’,
    this worker may be a straggler which may have more computation workload than other
    workers’.
  prefs: []
  type: TYPE_NORMAL
- en: The “Communication Operations Stats” summarizes the detailed statistics of all
    communication ops in each worker.
  prefs: []
  type: TYPE_NORMAL
- en: '7\. Additional Practices: Profiling PyTorch on AMD GPUs'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The AMD ROCm Platform is an open-source software stack designed for GPU computation,
    consisting of drivers, development tools, and APIs. We can run the above mentioned
    steps on AMD GPUs. In this section, we will use Docker to install the ROCm base
    development image before installing PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of example, let’s create a directory called `profiler_tutorial`,
    and save the code in **Step 1** as `test_cifar10.py` in this directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: At the time of this writing, the Stable(`2.1.1`) Linux version of PyTorch on
    ROCm Platform is [ROCm 5.6](https://pytorch.org/get-started/locally/).
  prefs: []
  type: TYPE_NORMAL
- en: Obtain a base Docker image with the correct user-space ROCm version installed
    from [Docker Hub](https://hub.docker.com/repository/docker/rocm/dev-ubuntu-20.04).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is `rocm/dev-ubuntu-20.04:5.6`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the ROCm base Docker container:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Inside the container, install any dependencies needed for installing the wheels
    package.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the wheels:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the `torch_tb_profiler`, and then, run the Python file `test_cifar10.py`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have all the data needed to view in TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Choose different views as described in **Step 4**. For example, below is the
    **Operator** View:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_rocm_tensorboard_operartor_view.png](../Images/766def45c853a562ade085a166bc7a98.png)](../_static/img/profiler_rocm_tensorboard_operartor_view.png)'
  prefs: []
  type: TYPE_NORMAL
- en: At the time this section is written, **Trace** view does not work and it displays
    nothing. You can work around by typing `chrome://tracing` in your Chrome Browser.
  prefs: []
  type: TYPE_NORMAL
- en: Copy the `trace.json` file under `~/profiler_tutorial/log/resnet18` directory
    to the Windows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may need to copy the file by using `scp` if the file is located in a remote
    location.
  prefs: []
  type: TYPE_NORMAL
- en: Click **Load** button to load the trace JSON file from the `chrome://tracing`
    page in the browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![../_static/img/profiler_rocm_chrome_trace_view.png](../Images/576f0fdbe384c09bd227cc973cbf6ecd.png)](../_static/img/profiler_rocm_chrome_trace_view.png)'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, you can move the graph and zoom in and out. You can
    also use keyboard to zoom and move around inside the timeline. The `w` and `s`
    keys zoom in centered around the mouse, and the `a` and `d` keys move the timeline
    left and right. You can hit these keys multiple times until you see a readable
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: Learn More
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Take a look at the following documents to continue your learning, and feel free
    to open an issue [here](https://github.com/pytorch/kineto/issues).
  prefs: []
  type: TYPE_NORMAL
- en: '[PyTorch TensorBoard Profiler Github](https://github.com/pytorch/kineto/tree/master/tb_plugin)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[torch.profiler API](https://pytorch.org/docs/master/profiler.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HTA](https://github.com/pytorch/kineto/tree/main#holistic-trace-analysis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total running time of the script:** ( 0 minutes 0.000 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Python source code: tensorboard_profiler_tutorial.py`](../_downloads/67e47b6d6793c700666471b688068f72/tensorboard_profiler_tutorial.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '[`Download Jupyter notebook: tensorboard_profiler_tutorial.ipynb`](../_downloads/0aec568a42e89122e5ca293c86289287/tensorboard_profiler_tutorial.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  prefs: []
  type: TYPE_NORMAL
