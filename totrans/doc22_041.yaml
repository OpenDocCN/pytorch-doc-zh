- en: torch.cuda
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torch.cuda
- en: 原文：[https://pytorch.org/docs/stable/cuda.html](https://pytorch.org/docs/stable/cuda.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/cuda.html](https://pytorch.org/docs/stable/cuda.html)
- en: This package adds support for CUDA tensor types.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 此软件包添加了对CUDA张量类型的支持。
- en: It implements the same function as CPU tensors, but they utilize GPUs for computation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 它实现了与CPU张量相同的功能，但利用GPU进行计算。
- en: It is lazily initialized, so you can always import it, and use [`is_available()`](generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available") to determine if your system supports CUDA.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 它是懒惰初始化的，所以您可以随时导入它，并使用[`is_available()`](generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available")来确定您的系统是否支持CUDA。
- en: '[CUDA semantics](notes/cuda.html#cuda-semantics) has more details about working
    with CUDA.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDA语义](notes/cuda.html#cuda-semantics)有关使用CUDA的更多详细信息。'
- en: '| [`StreamContext`](generated/torch.cuda.StreamContext.html#torch.cuda.StreamContext
    "torch.cuda.StreamContext") | Context-manager that selects a given stream. |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| [`StreamContext`](generated/torch.cuda.StreamContext.html#torch.cuda.StreamContext
    "torch.cuda.StreamContext") | 选择给定流的上下文管理器。 |'
- en: '| [`can_device_access_peer`](generated/torch.cuda.can_device_access_peer.html#torch.cuda.can_device_access_peer
    "torch.cuda.can_device_access_peer") | Check if peer access between two devices
    is possible. |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| [`can_device_access_peer`](generated/torch.cuda.can_device_access_peer.html#torch.cuda.can_device_access_peer
    "torch.cuda.can_device_access_peer") | 检查两个设备之间是否可以进行对等访问。 |'
- en: '| [`current_blas_handle`](generated/torch.cuda.current_blas_handle.html#torch.cuda.current_blas_handle
    "torch.cuda.current_blas_handle") | Return cublasHandle_t pointer to current cuBLAS
    handle |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| [`current_blas_handle`](generated/torch.cuda.current_blas_handle.html#torch.cuda.current_blas_handle
    "torch.cuda.current_blas_handle") | 返回当前cuBLAS句柄的cublasHandle_t指针 |'
- en: '| [`current_device`](generated/torch.cuda.current_device.html#torch.cuda.current_device
    "torch.cuda.current_device") | Return the index of a currently selected device.
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| [`current_device`](generated/torch.cuda.current_device.html#torch.cuda.current_device
    "torch.cuda.current_device") | 返回当前选择设备的索引。 |'
- en: '| [`current_stream`](generated/torch.cuda.current_stream.html#torch.cuda.current_stream
    "torch.cuda.current_stream") | Return the currently selected [`Stream`](generated/torch.cuda.Stream.html#torch.cuda.Stream
    "torch.cuda.Stream") for a given device. |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| [`current_stream`](generated/torch.cuda.current_stream.html#torch.cuda.current_stream
    "torch.cuda.current_stream") | 返回给定设备当前选择的[`Stream`](generated/torch.cuda.Stream.html#torch.cuda.Stream
    "torch.cuda.Stream")。 |'
- en: '| [`default_stream`](generated/torch.cuda.default_stream.html#torch.cuda.default_stream
    "torch.cuda.default_stream") | Return the default [`Stream`](generated/torch.cuda.Stream.html#torch.cuda.Stream
    "torch.cuda.Stream") for a given device. |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| [`default_stream`](generated/torch.cuda.default_stream.html#torch.cuda.default_stream
    "torch.cuda.default_stream") | 返回给定设备的默认[`Stream`](generated/torch.cuda.Stream.html#torch.cuda.Stream
    "torch.cuda.Stream")。 |'
- en: '| [`device`](generated/torch.cuda.device.html#torch.cuda.device "torch.cuda.device")
    | Context-manager that changes the selected device. |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| [`device`](generated/torch.cuda.device.html#torch.cuda.device "torch.cuda.device")
    | 上下文管理器，更改所选设备。 |'
- en: '| [`device_count`](generated/torch.cuda.device_count.html#torch.cuda.device_count
    "torch.cuda.device_count") | Return the number of GPUs available. |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| [`device_count`](generated/torch.cuda.device_count.html#torch.cuda.device_count
    "torch.cuda.device_count") | 返回可用的GPU数量。 |'
- en: '| [`device_of`](generated/torch.cuda.device_of.html#torch.cuda.device_of "torch.cuda.device_of")
    | Context-manager that changes the current device to that of given object. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| [`device_of`](generated/torch.cuda.device_of.html#torch.cuda.device_of "torch.cuda.device_of")
    | 上下文管理器，将当前设备更改为给定对象的设备。 |'
- en: '| [`get_arch_list`](generated/torch.cuda.get_arch_list.html#torch.cuda.get_arch_list
    "torch.cuda.get_arch_list") | Return list CUDA architectures this library was
    compiled for. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| [`get_arch_list`](generated/torch.cuda.get_arch_list.html#torch.cuda.get_arch_list
    "torch.cuda.get_arch_list") | 返回此库编译的CUDA架构列表。 |'
- en: '| [`get_device_capability`](generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability
    "torch.cuda.get_device_capability") | Get the cuda capability of a device. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| [`get_device_capability`](generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability
    "torch.cuda.get_device_capability") | 获取设备的cuda能力。 |'
- en: '| [`get_device_name`](generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name
    "torch.cuda.get_device_name") | Get the name of a device. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| [`get_device_name`](generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name
    "torch.cuda.get_device_name") | 获取设备的名称。 |'
- en: '| [`get_device_properties`](generated/torch.cuda.get_device_properties.html#torch.cuda.get_device_properties
    "torch.cuda.get_device_properties") | Get the properties of a device. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| [`get_device_properties`](generated/torch.cuda.get_device_properties.html#torch.cuda.get_device_properties
    "torch.cuda.get_device_properties") | 获取设备的属性。 |'
- en: '| [`get_gencode_flags`](generated/torch.cuda.get_gencode_flags.html#torch.cuda.get_gencode_flags
    "torch.cuda.get_gencode_flags") | Return NVCC gencode flags this library was compiled
    with. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| [`get_gencode_flags`](generated/torch.cuda.get_gencode_flags.html#torch.cuda.get_gencode_flags
    "torch.cuda.get_gencode_flags") | 返回此库编译时使用的NVCC gencode标志。 |'
- en: '| [`get_sync_debug_mode`](generated/torch.cuda.get_sync_debug_mode.html#torch.cuda.get_sync_debug_mode
    "torch.cuda.get_sync_debug_mode") | Return current value of debug mode for cuda
    synchronizing operations. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| [`get_sync_debug_mode`](generated/torch.cuda.get_sync_debug_mode.html#torch.cuda.get_sync_debug_mode
    "torch.cuda.get_sync_debug_mode") | 返回cuda同步操作的调试模式的当前值。 |'
- en: '| [`init`](generated/torch.cuda.init.html#torch.cuda.init "torch.cuda.init")
    | Initialize PyTorch''s CUDA state. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| [`init`](generated/torch.cuda.init.html#torch.cuda.init "torch.cuda.init")
    | 初始化PyTorch的CUDA状态。 |'
- en: '| [`ipc_collect`](generated/torch.cuda.ipc_collect.html#torch.cuda.ipc_collect
    "torch.cuda.ipc_collect") | Force collects GPU memory after it has been released
    by CUDA IPC. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| [`ipc_collect`](generated/torch.cuda.ipc_collect.html#torch.cuda.ipc_collect
    "torch.cuda.ipc_collect") | 在CUDA IPC释放GPU内存后强制收集。 |'
- en: '| [`is_available`](generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available") | Return a bool indicating if CUDA is currently available.
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| [`is_available`](generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available") | 返回一个布尔值，指示当前是否可用CUDA。 |'
- en: '| [`is_initialized`](generated/torch.cuda.is_initialized.html#torch.cuda.is_initialized
    "torch.cuda.is_initialized") | Return whether PyTorch''s CUDA state has been initialized.
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [`is_initialized`](generated/torch.cuda.is_initialized.html#torch.cuda.is_initialized
    "torch.cuda.is_initialized") | 返回PyTorch的CUDA状态是否已初始化。 |'
- en: '| [`memory_usage`](generated/torch.cuda.memory_usage.html#torch.cuda.memory_usage
    "torch.cuda.memory_usage") | Return the percent of time over the past sample period
    during which global (device) memory was being read or written as given by nvidia-smi.
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| [`memory_usage`](generated/torch.cuda.memory_usage.html#torch.cuda.memory_usage
    "torch.cuda.memory_usage") | 返回过去采样周期内全局（设备）内存被读取或写入的时间百分比，由 nvidia-smi 给出。 |'
- en: '| [`set_device`](generated/torch.cuda.set_device.html#torch.cuda.set_device
    "torch.cuda.set_device") | Set the current device. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| [`set_device`](generated/torch.cuda.set_device.html#torch.cuda.set_device
    "torch.cuda.set_device") | 设置当前设备。 |'
- en: '| [`set_stream`](generated/torch.cuda.set_stream.html#torch.cuda.set_stream
    "torch.cuda.set_stream") | Set the current stream.This is a wrapper API to set
    the stream. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| [`set_stream`](generated/torch.cuda.set_stream.html#torch.cuda.set_stream
    "torch.cuda.set_stream") | 设置当前流。这是一个包装 API，用于设置流。 |'
- en: '| [`set_sync_debug_mode`](generated/torch.cuda.set_sync_debug_mode.html#torch.cuda.set_sync_debug_mode
    "torch.cuda.set_sync_debug_mode") | Set the debug mode for cuda synchronizing
    operations. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| [`set_sync_debug_mode`](generated/torch.cuda.set_sync_debug_mode.html#torch.cuda.set_sync_debug_mode
    "torch.cuda.set_sync_debug_mode") | 设置 cuda 同步操作的调试模式。 |'
- en: '| [`stream`](generated/torch.cuda.stream.html#torch.cuda.stream "torch.cuda.stream")
    | Wrap around the Context-manager StreamContext that selects a given stream. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| [`stream`](generated/torch.cuda.stream.html#torch.cuda.stream "torch.cuda.stream")
    | 包装上下文管理器 StreamContext，选择给定的流。 |'
- en: '| [`synchronize`](generated/torch.cuda.synchronize.html#torch.cuda.synchronize
    "torch.cuda.synchronize") | Wait for all kernels in all streams on a CUDA device
    to complete. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| [`synchronize`](generated/torch.cuda.synchronize.html#torch.cuda.synchronize
    "torch.cuda.synchronize") | 等待 CUDA 设备上所有流中的所有内核完成。 |'
- en: '| [`utilization`](generated/torch.cuda.utilization.html#torch.cuda.utilization
    "torch.cuda.utilization") | Return the percent of time over the past sample period
    during which one or more kernels was executing on the GPU as given by nvidia-smi.
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| [`utilization`](generated/torch.cuda.utilization.html#torch.cuda.utilization
    "torch.cuda.utilization") | 返回过去采样周期内 GPU 上一个或多个内核执行的时间百分比，由 nvidia-smi 给出。 |'
- en: '| [`temperature`](generated/torch.cuda.temperature.html#torch.cuda.temperature
    "torch.cuda.temperature") | Return the average temperature of the GPU sensor in
    Degrees C (Centigrades). |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| [`temperature`](generated/torch.cuda.temperature.html#torch.cuda.temperature
    "torch.cuda.temperature") | 返回 GPU 传感器的平均温度，单位为摄氏度（C）。 |'
- en: '| [`power_draw`](generated/torch.cuda.power_draw.html#torch.cuda.power_draw
    "torch.cuda.power_draw") | Return the average power draw of the GPU sensor in
    mW (MilliWatts) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| [`power_draw`](generated/torch.cuda.power_draw.html#torch.cuda.power_draw
    "torch.cuda.power_draw") | 返回 GPU 传感器的平均功耗，单位为毫瓦（mW） |'
- en: '| [`clock_rate`](generated/torch.cuda.clock_rate.html#torch.cuda.clock_rate
    "torch.cuda.clock_rate") | Return the clock speed of the GPU SM in Hz Hertz over
    the past sample period as given by nvidia-smi. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| [`clock_rate`](generated/torch.cuda.clock_rate.html#torch.cuda.clock_rate
    "torch.cuda.clock_rate") | 返回 GPU SM 的时钟速度，单位为赫兹（Hz），在过去的采样周期内由 nvidia-smi 给出。
    |'
- en: '| [`OutOfMemoryError`](generated/torch.cuda.OutOfMemoryError.html#torch.cuda.OutOfMemoryError
    "torch.cuda.OutOfMemoryError") | Exception raised when CUDA is out of memory |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| [`OutOfMemoryError`](generated/torch.cuda.OutOfMemoryError.html#torch.cuda.OutOfMemoryError
    "torch.cuda.OutOfMemoryError") | 当 CUDA 内存不足时引发的异常 |'
- en: Random Number Generator
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机数生成器
- en: '| [`get_rng_state`](generated/torch.cuda.get_rng_state.html#torch.cuda.get_rng_state
    "torch.cuda.get_rng_state") | Return the random number generator state of the
    specified GPU as a ByteTensor. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| [`get_rng_state`](generated/torch.cuda.get_rng_state.html#torch.cuda.get_rng_state
    "torch.cuda.get_rng_state") | 返回指定 GPU 的随机数生成器状态，作为 ByteTensor。 |'
- en: '| [`get_rng_state_all`](generated/torch.cuda.get_rng_state_all.html#torch.cuda.get_rng_state_all
    "torch.cuda.get_rng_state_all") | Return a list of ByteTensor representing the
    random number states of all devices. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| [`get_rng_state_all`](generated/torch.cuda.get_rng_state_all.html#torch.cuda.get_rng_state_all
    "torch.cuda.get_rng_state_all") | 返回表示所有设备的随机数状态的 ByteTensor 列表。 |'
- en: '| [`set_rng_state`](generated/torch.cuda.set_rng_state.html#torch.cuda.set_rng_state
    "torch.cuda.set_rng_state") | Set the random number generator state of the specified
    GPU. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| [`set_rng_state`](generated/torch.cuda.set_rng_state.html#torch.cuda.set_rng_state
    "torch.cuda.set_rng_state") | 设置指定 GPU 的随机数生成器状态。 |'
- en: '| [`set_rng_state_all`](generated/torch.cuda.set_rng_state_all.html#torch.cuda.set_rng_state_all
    "torch.cuda.set_rng_state_all") | Set the random number generator state of all
    devices. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| [`set_rng_state_all`](generated/torch.cuda.set_rng_state_all.html#torch.cuda.set_rng_state_all
    "torch.cuda.set_rng_state_all") | 设置所有设备的随机数生成器状态。 |'
- en: '| [`manual_seed`](generated/torch.cuda.manual_seed.html#torch.cuda.manual_seed
    "torch.cuda.manual_seed") | Set the seed for generating random numbers for the
    current GPU. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| [`manual_seed`](generated/torch.cuda.manual_seed.html#torch.cuda.manual_seed
    "torch.cuda.manual_seed") | 为当前 GPU 设置生成随机数的种子。 |'
- en: '| [`manual_seed_all`](generated/torch.cuda.manual_seed_all.html#torch.cuda.manual_seed_all
    "torch.cuda.manual_seed_all") | Set the seed for generating random numbers on
    all GPUs. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| [`manual_seed_all`](generated/torch.cuda.manual_seed_all.html#torch.cuda.manual_seed_all
    "torch.cuda.manual_seed_all") | 在所有 GPU 上设置生成随机数的种子。 |'
- en: '| [`seed`](generated/torch.cuda.seed.html#torch.cuda.seed "torch.cuda.seed")
    | Set the seed for generating random numbers to a random number for the current
    GPU. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| [`seed`](generated/torch.cuda.seed.html#torch.cuda.seed "torch.cuda.seed")
    | 为当前 GPU 将生成随机数的种子设置为随机数。 |'
- en: '| [`seed_all`](generated/torch.cuda.seed_all.html#torch.cuda.seed_all "torch.cuda.seed_all")
    | Set the seed for generating random numbers to a random number on all GPUs. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| [`seed_all`](generated/torch.cuda.seed_all.html#torch.cuda.seed_all "torch.cuda.seed_all")
    | 将生成随机数的种子设置为所有 GPU 上的随机数。 |'
- en: '| [`initial_seed`](generated/torch.cuda.initial_seed.html#torch.cuda.initial_seed
    "torch.cuda.initial_seed") | Return the current random seed of the current GPU.
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| [`initial_seed`](generated/torch.cuda.initial_seed.html#torch.cuda.initial_seed
    "torch.cuda.initial_seed") | 返回当前 GPU 的当前随机种子。 |'
- en: Communication collectives
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通信集合
- en: '| [`comm.broadcast`](generated/torch.cuda.comm.broadcast.html#torch.cuda.comm.broadcast
    "torch.cuda.comm.broadcast") | Broadcasts a tensor to specified GPU devices. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| [`comm.broadcast`](generated/torch.cuda.comm.broadcast.html#torch.cuda.comm.broadcast
    "torch.cuda.comm.broadcast") | 将张量广播到指定的 GPU 设备。 |'
- en: '| [`comm.broadcast_coalesced`](generated/torch.cuda.comm.broadcast_coalesced.html#torch.cuda.comm.broadcast_coalesced
    "torch.cuda.comm.broadcast_coalesced") | Broadcast a sequence of tensors to the
    specified GPUs. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| [`comm.broadcast_coalesced`](generated/torch.cuda.comm.broadcast_coalesced.html#torch.cuda.comm.broadcast_coalesced
    "torch.cuda.comm.broadcast_coalesced") | 将一系列张量广播到指定的 GPU。 |'
- en: '| [`comm.reduce_add`](generated/torch.cuda.comm.reduce_add.html#torch.cuda.comm.reduce_add
    "torch.cuda.comm.reduce_add") | Sum tensors from multiple GPUs. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| [`comm.reduce_add`](generated/torch.cuda.comm.reduce_add.html#torch.cuda.comm.reduce_add
    "torch.cuda.comm.reduce_add") | 对多个 GPU 的张量求和。 |'
- en: '| [`comm.scatter`](generated/torch.cuda.comm.scatter.html#torch.cuda.comm.scatter
    "torch.cuda.comm.scatter") | Scatters tensor across multiple GPUs. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| [`comm.scatter`](generated/torch.cuda.comm.scatter.html#torch.cuda.comm.scatter
    "torch.cuda.comm.scatter") | 在多个 GPU 上分散张量。 |'
- en: '| [`comm.gather`](generated/torch.cuda.comm.gather.html#torch.cuda.comm.gather
    "torch.cuda.comm.gather") | Gathers tensors from multiple GPU devices. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| [`comm.gather`](generated/torch.cuda.comm.gather.html#torch.cuda.comm.gather
    "torch.cuda.comm.gather") | 从多个 GPU 设备中收集张量。 |'
- en: Streams and events
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流和事件
- en: '| [`Stream`](generated/torch.cuda.Stream.html#torch.cuda.Stream "torch.cuda.Stream")
    | Wrapper around a CUDA stream. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| [`Stream`](generated/torch.cuda.Stream.html#torch.cuda.Stream "torch.cuda.Stream")
    | CUDA 流的包装器。 |'
- en: '| [`ExternalStream`](generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream
    "torch.cuda.ExternalStream") | Wrapper around an externally allocated CUDA stream.
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| [`ExternalStream`](generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream
    "torch.cuda.ExternalStream") | 外部分配的 CUDA 流的包装器。 |'
- en: '| [`Event`](generated/torch.cuda.Event.html#torch.cuda.Event "torch.cuda.Event")
    | Wrapper around a CUDA event. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| [`Event`](generated/torch.cuda.Event.html#torch.cuda.Event "torch.cuda.Event")
    | CUDA 事件的包装器。 |'
- en: Graphs (beta)
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图（测试版）
- en: '| [`is_current_stream_capturing`](generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing
    "torch.cuda.is_current_stream_capturing") | Return True if CUDA graph capture
    is underway on the current CUDA stream, False otherwise. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| [`is_current_stream_capturing`](generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing
    "torch.cuda.is_current_stream_capturing") | 如果当前 CUDA 流正在进行 CUDA 图捕获，则返回 True，否则返回
    False。 |'
- en: '| [`graph_pool_handle`](generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle
    "torch.cuda.graph_pool_handle") | Return an opaque token representing the id of
    a graph memory pool. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| [`graph_pool_handle`](generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle
    "torch.cuda.graph_pool_handle") | 返回表示图形内存池 id 的不透明令牌。 |'
- en: '| [`CUDAGraph`](generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph "torch.cuda.CUDAGraph")
    | Wrapper around a CUDA graph. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| [`CUDAGraph`](generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph "torch.cuda.CUDAGraph")
    | CUDA 图的包装器。 |'
- en: '| [`graph`](generated/torch.cuda.graph.html#torch.cuda.graph "torch.cuda.graph")
    | Context-manager that captures CUDA work into a [`torch.cuda.CUDAGraph`](generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph
    "torch.cuda.CUDAGraph") object for later replay. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| [`graph`](generated/torch.cuda.graph.html#torch.cuda.graph "torch.cuda.graph")
    | 上下文管理器，将 CUDA 工作捕获到一个 [`torch.cuda.CUDAGraph`](generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph
    "torch.cuda.CUDAGraph") 对象中以供以后重播。 |'
- en: '| [`make_graphed_callables`](generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") | Accept callables (functions or [`nn.Module`](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")s) and returns graphed versions. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| [`make_graphed_callables`](generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") | 接受可调用对象（函数或 [`nn.Module`](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")）并返回图形化版本。 |'
- en: '## Memory management'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '## 内存管理'
- en: '| [`empty_cache`](generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache
    "torch.cuda.empty_cache") | Release all unoccupied cached memory currently held
    by the caching allocator so that those can be used in other GPU application and
    visible in nvidia-smi. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| [`empty_cache`](generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache
    "torch.cuda.empty_cache") | 释放缓存分配器当前持有的所有未使用内存，以便其他 GPU 应用程序可以使用，并在 nvidia-smi
    中可见。 |'
- en: '| [`list_gpu_processes`](generated/torch.cuda.list_gpu_processes.html#torch.cuda.list_gpu_processes
    "torch.cuda.list_gpu_processes") | Return a human-readable printout of the running
    processes and their GPU memory use for a given device. |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| [`list_gpu_processes`](generated/torch.cuda.list_gpu_processes.html#torch.cuda.list_gpu_processes
    "torch.cuda.list_gpu_processes") | 返回给定设备的正在运行进程及其 GPU 内存使用情况的人类可读打印输出。 |'
- en: '| [`mem_get_info`](generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info
    "torch.cuda.mem_get_info") | Return the global free and total GPU memory for a
    given device using cudaMemGetInfo. |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [`mem_get_info`](generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info
    "torch.cuda.mem_get_info") | 使用 cudaMemGetInfo 返回给定设备的全局空闲和总 GPU 内存。 |'
- en: '| [`memory_stats`](generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats
    "torch.cuda.memory_stats") | Return a dictionary of CUDA memory allocator statistics
    for a given device. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [`memory_stats`](generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats
    "torch.cuda.memory_stats") | 返回给定设备的 CUDA 内存分配器统计信息字典。 |'
- en: '| [`memory_summary`](generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary
    "torch.cuda.memory_summary") | Return a human-readable printout of the current
    memory allocator statistics for a given device. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| [`memory_summary`](generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary
    "torch.cuda.memory_summary") | 返回给定设备的当前内存分配器统计信息的人类可读打印输出。 |'
- en: '| [`memory_snapshot`](generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot
    "torch.cuda.memory_snapshot") | Return a snapshot of the CUDA memory allocator
    state across all devices. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| [`memory_snapshot`](generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot
    "torch.cuda.memory_snapshot") | 返回跨所有设备的 CUDA 内存分配器状态的快照。 |'
- en: '| [`memory_allocated`](generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated
    "torch.cuda.memory_allocated") | Return the current GPU memory occupied by tensors
    in bytes for a given device. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| [`memory_allocated`](generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated
    "torch.cuda.memory_allocated") | 返回给定设备上张量占用的当前 GPU 内存（以字节为单位）。 |'
- en: '| [`max_memory_allocated`](generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated
    "torch.cuda.max_memory_allocated") | Return the maximum GPU memory occupied by
    tensors in bytes for a given device. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| [`max_memory_allocated`](generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated
    "torch.cuda.max_memory_allocated") | 返回给定设备张量占用的最大GPU内存（以字节为单位）。 |'
- en: '| [`reset_max_memory_allocated`](generated/torch.cuda.reset_max_memory_allocated.html#torch.cuda.reset_max_memory_allocated
    "torch.cuda.reset_max_memory_allocated") | Reset the starting point in tracking
    maximum GPU memory occupied by tensors for a given device. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| [`reset_max_memory_allocated`](generated/torch.cuda.reset_max_memory_allocated.html#torch.cuda.reset_max_memory_allocated
    "torch.cuda.reset_max_memory_allocated") | 重置跟踪给定设备张量占用的最大GPU内存的起始点。 |'
- en: '| [`memory_reserved`](generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved
    "torch.cuda.memory_reserved") | Return the current GPU memory managed by the caching
    allocator in bytes for a given device. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| [`memory_reserved`](generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved
    "torch.cuda.memory_reserved") | 返回由缓存分配器管理的给定设备的当前GPU内存（以字节为单位）。 |'
- en: '| [`max_memory_reserved`](generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved
    "torch.cuda.max_memory_reserved") | Return the maximum GPU memory managed by the
    caching allocator in bytes for a given device. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| [`max_memory_reserved`](generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved
    "torch.cuda.max_memory_reserved") | 返回由缓存分配器管理的给定设备的最大GPU内存（以字节为单位）。 |'
- en: '| [`set_per_process_memory_fraction`](generated/torch.cuda.set_per_process_memory_fraction.html#torch.cuda.set_per_process_memory_fraction
    "torch.cuda.set_per_process_memory_fraction") | Set memory fraction for a process.
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| [`set_per_process_memory_fraction`](generated/torch.cuda.set_per_process_memory_fraction.html#torch.cuda.set_per_process_memory_fraction
    "torch.cuda.set_per_process_memory_fraction") | 为进程设置内存分数。 |'
- en: '| [`memory_cached`](generated/torch.cuda.memory_cached.html#torch.cuda.memory_cached
    "torch.cuda.memory_cached") | Deprecated; see [`memory_reserved()`](generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved
    "torch.cuda.memory_reserved"). |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| [`memory_cached`](generated/torch.cuda.memory_cached.html#torch.cuda.memory_cached
    "torch.cuda.memory_cached") | 已弃用；请参阅 [`memory_reserved()`](generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved
    "torch.cuda.memory_reserved")。 |'
- en: '| [`max_memory_cached`](generated/torch.cuda.max_memory_cached.html#torch.cuda.max_memory_cached
    "torch.cuda.max_memory_cached") | Deprecated; see [`max_memory_reserved()`](generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved
    "torch.cuda.max_memory_reserved"). |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| [`max_memory_cached`](generated/torch.cuda.max_memory_cached.html#torch.cuda.max_memory_cached
    "torch.cuda.max_memory_cached") | 已弃用；请参阅 [`max_memory_reserved()`](generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved
    "torch.cuda.max_memory_reserved")。 |'
- en: '| [`reset_max_memory_cached`](generated/torch.cuda.reset_max_memory_cached.html#torch.cuda.reset_max_memory_cached
    "torch.cuda.reset_max_memory_cached") | Reset the starting point in tracking maximum
    GPU memory managed by the caching allocator for a given device. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| [`reset_max_memory_cached`](generated/torch.cuda.reset_max_memory_cached.html#torch.cuda.reset_max_memory_cached
    "torch.cuda.reset_max_memory_cached") | 重置跟踪由缓存分配器管理的给定设备的最大GPU内存的起始点。 |'
- en: '| [`reset_peak_memory_stats`](generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats
    "torch.cuda.reset_peak_memory_stats") | Reset the "peak" stats tracked by the
    CUDA memory allocator. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| [`reset_peak_memory_stats`](generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats
    "torch.cuda.reset_peak_memory_stats") | 重置CUDA内存分配器跟踪的“峰值”统计信息。 |'
- en: '| [`caching_allocator_alloc`](generated/torch.cuda.caching_allocator_alloc.html#torch.cuda.caching_allocator_alloc
    "torch.cuda.caching_allocator_alloc") | Perform a memory allocation using the
    CUDA memory allocator. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| [`caching_allocator_alloc`](generated/torch.cuda.caching_allocator_alloc.html#torch.cuda.caching_allocator_alloc
    "torch.cuda.caching_allocator_alloc") | 使用CUDA内存分配器执行内存分配。 |'
- en: '| [`caching_allocator_delete`](generated/torch.cuda.caching_allocator_delete.html#torch.cuda.caching_allocator_delete
    "torch.cuda.caching_allocator_delete") | Delete memory allocated using the CUDA
    memory allocator. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| [`caching_allocator_delete`](generated/torch.cuda.caching_allocator_delete.html#torch.cuda.caching_allocator_delete
    "torch.cuda.caching_allocator_delete") | 删除使用CUDA内存分配器分配的内存。 |'
- en: '| [`get_allocator_backend`](generated/torch.cuda.get_allocator_backend.html#torch.cuda.get_allocator_backend
    "torch.cuda.get_allocator_backend") | Return a string describing the active allocator
    backend as set by `PYTORCH_CUDA_ALLOC_CONF`. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| [`get_allocator_backend`](generated/torch.cuda.get_allocator_backend.html#torch.cuda.get_allocator_backend
    "torch.cuda.get_allocator_backend") | 返回一个描述由 `PYTORCH_CUDA_ALLOC_CONF` 设置的活动分配器后端的字符串。
    |'
- en: '| [`CUDAPluggableAllocator`](generated/torch.cuda.CUDAPluggableAllocator.html#torch.cuda.CUDAPluggableAllocator
    "torch.cuda.CUDAPluggableAllocator") | CUDA memory allocator loaded from a so
    file. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| [`CUDAPluggableAllocator`](generated/torch.cuda.CUDAPluggableAllocator.html#torch.cuda.CUDAPluggableAllocator
    "torch.cuda.CUDAPluggableAllocator") | 从so文件加载的CUDA内存分配器。 |'
- en: '| [`change_current_allocator`](generated/torch.cuda.change_current_allocator.html#torch.cuda.change_current_allocator
    "torch.cuda.change_current_allocator") | Change the currently used memory allocator
    to be the one provided. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [`change_current_allocator`](generated/torch.cuda.change_current_allocator.html#torch.cuda.change_current_allocator
    "torch.cuda.change_current_allocator") | 将当前使用的内存分配器更改为提供的内存分配器。 |'
- en: NVIDIA Tools Extension (NVTX)[](#nvidia-tools-extension-nvtx "Permalink to this
    heading")
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NVIDIA工具扩展（NVTX）[](#nvidia-tools-extension-nvtx "跳转到此标题")
- en: '| [`nvtx.mark`](generated/torch.cuda.nvtx.mark.html#torch.cuda.nvtx.mark "torch.cuda.nvtx.mark")
    | Describe an instantaneous event that occurred at some point. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| [`nvtx.mark`](generated/torch.cuda.nvtx.mark.html#torch.cuda.nvtx.mark "torch.cuda.nvtx.mark")
    | 描述在某个时间点发生的瞬时事件。 |'
- en: '| [`nvtx.range_push`](generated/torch.cuda.nvtx.range_push.html#torch.cuda.nvtx.range_push
    "torch.cuda.nvtx.range_push") | Push a range onto a stack of nested range span.
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| [`nvtx.range_push`](generated/torch.cuda.nvtx.range_push.html#torch.cuda.nvtx.range_push
    "torch.cuda.nvtx.range_push") | 将范围推送到嵌套范围跨度的堆栈上。 |'
- en: '| [`nvtx.range_pop`](generated/torch.cuda.nvtx.range_pop.html#torch.cuda.nvtx.range_pop
    "torch.cuda.nvtx.range_pop") | Pop a range off of a stack of nested range spans.
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| [`nvtx.range_pop`](generated/torch.cuda.nvtx.range_pop.html#torch.cuda.nvtx.range_pop
    "torch.cuda.nvtx.range_pop") | 从嵌套范围跨度堆栈中弹出范围。 |'
- en: Jiterator (beta)
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jiterator（beta）
- en: '| [`jiterator._create_jit_fn`](generated/torch.cuda.jiterator._create_jit_fn.html#torch.cuda.jiterator._create_jit_fn
    "torch.cuda.jiterator._create_jit_fn") | Create a jiterator-generated cuda kernel
    for an elementwise op. |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| [`jiterator._create_jit_fn`](generated/torch.cuda.jiterator._create_jit_fn.html#torch.cuda.jiterator._create_jit_fn
    "torch.cuda.jiterator._create_jit_fn") | 创建一个由jiterator生成的cuda内核，用于逐元素操作。 |'
- en: '| [`jiterator._create_multi_output_jit_fn`](generated/torch.cuda.jiterator._create_multi_output_jit_fn.html#torch.cuda.jiterator._create_multi_output_jit_fn
    "torch.cuda.jiterator._create_multi_output_jit_fn") | Create a jiterator-generated
    cuda kernel for an elementwise op that supports returning one or more outputs.
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| [`jiterator._create_multi_output_jit_fn`](generated/torch.cuda.jiterator._create_multi_output_jit_fn.html#torch.cuda.jiterator._create_multi_output_jit_fn
    "torch.cuda.jiterator._create_multi_output_jit_fn") | 创建一个由jiterator生成的cuda内核，用于支持返回一个或多个输出的逐元素操作。
    |'
- en: Stream Sanitizer (prototype)[](#stream-sanitizer-prototype "Permalink to this
    heading")
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流消毒器（原型）[](#stream-sanitizer-prototype "跳转到此标题的永久链接")
- en: CUDA Sanitizer is a prototype tool for detecting synchronization errors between
    streams in PyTorch. See the [documentation](cuda._sanitizer.html) for information
    on how to use it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA消毒器是一个用于检测PyTorch中流之间同步错误的原型工具。请查看[文档](cuda._sanitizer.html)以获取如何使用它的信息。
