- en: torch.cuda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/cuda.html](https://pytorch.org/docs/stable/cuda.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This package adds support for CUDA tensor types.
  prefs: []
  type: TYPE_NORMAL
- en: It implements the same function as CPU tensors, but they utilize GPUs for computation.
  prefs: []
  type: TYPE_NORMAL
- en: It is lazily initialized, so you can always import it, and use [`is_available()`](generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available") to determine if your system supports CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA semantics](notes/cuda.html#cuda-semantics) has more details about working
    with CUDA.'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`StreamContext`](generated/torch.cuda.StreamContext.html#torch.cuda.StreamContext
    "torch.cuda.StreamContext") | Context-manager that selects a given stream. |'
  prefs: []
  type: TYPE_TB
- en: '| [`can_device_access_peer`](generated/torch.cuda.can_device_access_peer.html#torch.cuda.can_device_access_peer
    "torch.cuda.can_device_access_peer") | Check if peer access between two devices
    is possible. |'
  prefs: []
  type: TYPE_TB
- en: '| [`current_blas_handle`](generated/torch.cuda.current_blas_handle.html#torch.cuda.current_blas_handle
    "torch.cuda.current_blas_handle") | Return cublasHandle_t pointer to current cuBLAS
    handle |'
  prefs: []
  type: TYPE_TB
- en: '| [`current_device`](generated/torch.cuda.current_device.html#torch.cuda.current_device
    "torch.cuda.current_device") | Return the index of a currently selected device.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`current_stream`](generated/torch.cuda.current_stream.html#torch.cuda.current_stream
    "torch.cuda.current_stream") | Return the currently selected [`Stream`](generated/torch.cuda.Stream.html#torch.cuda.Stream
    "torch.cuda.Stream") for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`default_stream`](generated/torch.cuda.default_stream.html#torch.cuda.default_stream
    "torch.cuda.default_stream") | Return the default [`Stream`](generated/torch.cuda.Stream.html#torch.cuda.Stream
    "torch.cuda.Stream") for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`device`](generated/torch.cuda.device.html#torch.cuda.device "torch.cuda.device")
    | Context-manager that changes the selected device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`device_count`](generated/torch.cuda.device_count.html#torch.cuda.device_count
    "torch.cuda.device_count") | Return the number of GPUs available. |'
  prefs: []
  type: TYPE_TB
- en: '| [`device_of`](generated/torch.cuda.device_of.html#torch.cuda.device_of "torch.cuda.device_of")
    | Context-manager that changes the current device to that of given object. |'
  prefs: []
  type: TYPE_TB
- en: '| [`get_arch_list`](generated/torch.cuda.get_arch_list.html#torch.cuda.get_arch_list
    "torch.cuda.get_arch_list") | Return list CUDA architectures this library was
    compiled for. |'
  prefs: []
  type: TYPE_TB
- en: '| [`get_device_capability`](generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability
    "torch.cuda.get_device_capability") | Get the cuda capability of a device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`get_device_name`](generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name
    "torch.cuda.get_device_name") | Get the name of a device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`get_device_properties`](generated/torch.cuda.get_device_properties.html#torch.cuda.get_device_properties
    "torch.cuda.get_device_properties") | Get the properties of a device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`get_gencode_flags`](generated/torch.cuda.get_gencode_flags.html#torch.cuda.get_gencode_flags
    "torch.cuda.get_gencode_flags") | Return NVCC gencode flags this library was compiled
    with. |'
  prefs: []
  type: TYPE_TB
- en: '| [`get_sync_debug_mode`](generated/torch.cuda.get_sync_debug_mode.html#torch.cuda.get_sync_debug_mode
    "torch.cuda.get_sync_debug_mode") | Return current value of debug mode for cuda
    synchronizing operations. |'
  prefs: []
  type: TYPE_TB
- en: '| [`init`](generated/torch.cuda.init.html#torch.cuda.init "torch.cuda.init")
    | Initialize PyTorch''s CUDA state. |'
  prefs: []
  type: TYPE_TB
- en: '| [`ipc_collect`](generated/torch.cuda.ipc_collect.html#torch.cuda.ipc_collect
    "torch.cuda.ipc_collect") | Force collects GPU memory after it has been released
    by CUDA IPC. |'
  prefs: []
  type: TYPE_TB
- en: '| [`is_available`](generated/torch.cuda.is_available.html#torch.cuda.is_available
    "torch.cuda.is_available") | Return a bool indicating if CUDA is currently available.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`is_initialized`](generated/torch.cuda.is_initialized.html#torch.cuda.is_initialized
    "torch.cuda.is_initialized") | Return whether PyTorch''s CUDA state has been initialized.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`memory_usage`](generated/torch.cuda.memory_usage.html#torch.cuda.memory_usage
    "torch.cuda.memory_usage") | Return the percent of time over the past sample period
    during which global (device) memory was being read or written as given by nvidia-smi.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`set_device`](generated/torch.cuda.set_device.html#torch.cuda.set_device
    "torch.cuda.set_device") | Set the current device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`set_stream`](generated/torch.cuda.set_stream.html#torch.cuda.set_stream
    "torch.cuda.set_stream") | Set the current stream.This is a wrapper API to set
    the stream. |'
  prefs: []
  type: TYPE_TB
- en: '| [`set_sync_debug_mode`](generated/torch.cuda.set_sync_debug_mode.html#torch.cuda.set_sync_debug_mode
    "torch.cuda.set_sync_debug_mode") | Set the debug mode for cuda synchronizing
    operations. |'
  prefs: []
  type: TYPE_TB
- en: '| [`stream`](generated/torch.cuda.stream.html#torch.cuda.stream "torch.cuda.stream")
    | Wrap around the Context-manager StreamContext that selects a given stream. |'
  prefs: []
  type: TYPE_TB
- en: '| [`synchronize`](generated/torch.cuda.synchronize.html#torch.cuda.synchronize
    "torch.cuda.synchronize") | Wait for all kernels in all streams on a CUDA device
    to complete. |'
  prefs: []
  type: TYPE_TB
- en: '| [`utilization`](generated/torch.cuda.utilization.html#torch.cuda.utilization
    "torch.cuda.utilization") | Return the percent of time over the past sample period
    during which one or more kernels was executing on the GPU as given by nvidia-smi.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`temperature`](generated/torch.cuda.temperature.html#torch.cuda.temperature
    "torch.cuda.temperature") | Return the average temperature of the GPU sensor in
    Degrees C (Centigrades). |'
  prefs: []
  type: TYPE_TB
- en: '| [`power_draw`](generated/torch.cuda.power_draw.html#torch.cuda.power_draw
    "torch.cuda.power_draw") | Return the average power draw of the GPU sensor in
    mW (MilliWatts) |'
  prefs: []
  type: TYPE_TB
- en: '| [`clock_rate`](generated/torch.cuda.clock_rate.html#torch.cuda.clock_rate
    "torch.cuda.clock_rate") | Return the clock speed of the GPU SM in Hz Hertz over
    the past sample period as given by nvidia-smi. |'
  prefs: []
  type: TYPE_TB
- en: '| [`OutOfMemoryError`](generated/torch.cuda.OutOfMemoryError.html#torch.cuda.OutOfMemoryError
    "torch.cuda.OutOfMemoryError") | Exception raised when CUDA is out of memory |'
  prefs: []
  type: TYPE_TB
- en: Random Number Generator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`get_rng_state`](generated/torch.cuda.get_rng_state.html#torch.cuda.get_rng_state
    "torch.cuda.get_rng_state") | Return the random number generator state of the
    specified GPU as a ByteTensor. |'
  prefs: []
  type: TYPE_TB
- en: '| [`get_rng_state_all`](generated/torch.cuda.get_rng_state_all.html#torch.cuda.get_rng_state_all
    "torch.cuda.get_rng_state_all") | Return a list of ByteTensor representing the
    random number states of all devices. |'
  prefs: []
  type: TYPE_TB
- en: '| [`set_rng_state`](generated/torch.cuda.set_rng_state.html#torch.cuda.set_rng_state
    "torch.cuda.set_rng_state") | Set the random number generator state of the specified
    GPU. |'
  prefs: []
  type: TYPE_TB
- en: '| [`set_rng_state_all`](generated/torch.cuda.set_rng_state_all.html#torch.cuda.set_rng_state_all
    "torch.cuda.set_rng_state_all") | Set the random number generator state of all
    devices. |'
  prefs: []
  type: TYPE_TB
- en: '| [`manual_seed`](generated/torch.cuda.manual_seed.html#torch.cuda.manual_seed
    "torch.cuda.manual_seed") | Set the seed for generating random numbers for the
    current GPU. |'
  prefs: []
  type: TYPE_TB
- en: '| [`manual_seed_all`](generated/torch.cuda.manual_seed_all.html#torch.cuda.manual_seed_all
    "torch.cuda.manual_seed_all") | Set the seed for generating random numbers on
    all GPUs. |'
  prefs: []
  type: TYPE_TB
- en: '| [`seed`](generated/torch.cuda.seed.html#torch.cuda.seed "torch.cuda.seed")
    | Set the seed for generating random numbers to a random number for the current
    GPU. |'
  prefs: []
  type: TYPE_TB
- en: '| [`seed_all`](generated/torch.cuda.seed_all.html#torch.cuda.seed_all "torch.cuda.seed_all")
    | Set the seed for generating random numbers to a random number on all GPUs. |'
  prefs: []
  type: TYPE_TB
- en: '| [`initial_seed`](generated/torch.cuda.initial_seed.html#torch.cuda.initial_seed
    "torch.cuda.initial_seed") | Return the current random seed of the current GPU.
    |'
  prefs: []
  type: TYPE_TB
- en: Communication collectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`comm.broadcast`](generated/torch.cuda.comm.broadcast.html#torch.cuda.comm.broadcast
    "torch.cuda.comm.broadcast") | Broadcasts a tensor to specified GPU devices. |'
  prefs: []
  type: TYPE_TB
- en: '| [`comm.broadcast_coalesced`](generated/torch.cuda.comm.broadcast_coalesced.html#torch.cuda.comm.broadcast_coalesced
    "torch.cuda.comm.broadcast_coalesced") | Broadcast a sequence of tensors to the
    specified GPUs. |'
  prefs: []
  type: TYPE_TB
- en: '| [`comm.reduce_add`](generated/torch.cuda.comm.reduce_add.html#torch.cuda.comm.reduce_add
    "torch.cuda.comm.reduce_add") | Sum tensors from multiple GPUs. |'
  prefs: []
  type: TYPE_TB
- en: '| [`comm.scatter`](generated/torch.cuda.comm.scatter.html#torch.cuda.comm.scatter
    "torch.cuda.comm.scatter") | Scatters tensor across multiple GPUs. |'
  prefs: []
  type: TYPE_TB
- en: '| [`comm.gather`](generated/torch.cuda.comm.gather.html#torch.cuda.comm.gather
    "torch.cuda.comm.gather") | Gathers tensors from multiple GPU devices. |'
  prefs: []
  type: TYPE_TB
- en: Streams and events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`Stream`](generated/torch.cuda.Stream.html#torch.cuda.Stream "torch.cuda.Stream")
    | Wrapper around a CUDA stream. |'
  prefs: []
  type: TYPE_TB
- en: '| [`ExternalStream`](generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream
    "torch.cuda.ExternalStream") | Wrapper around an externally allocated CUDA stream.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`Event`](generated/torch.cuda.Event.html#torch.cuda.Event "torch.cuda.Event")
    | Wrapper around a CUDA event. |'
  prefs: []
  type: TYPE_TB
- en: Graphs (beta)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`is_current_stream_capturing`](generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing
    "torch.cuda.is_current_stream_capturing") | Return True if CUDA graph capture
    is underway on the current CUDA stream, False otherwise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`graph_pool_handle`](generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle
    "torch.cuda.graph_pool_handle") | Return an opaque token representing the id of
    a graph memory pool. |'
  prefs: []
  type: TYPE_TB
- en: '| [`CUDAGraph`](generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph "torch.cuda.CUDAGraph")
    | Wrapper around a CUDA graph. |'
  prefs: []
  type: TYPE_TB
- en: '| [`graph`](generated/torch.cuda.graph.html#torch.cuda.graph "torch.cuda.graph")
    | Context-manager that captures CUDA work into a [`torch.cuda.CUDAGraph`](generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph
    "torch.cuda.CUDAGraph") object for later replay. |'
  prefs: []
  type: TYPE_TB
- en: '| [`make_graphed_callables`](generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables
    "torch.cuda.make_graphed_callables") | Accept callables (functions or [`nn.Module`](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")s) and returns graphed versions. |'
  prefs: []
  type: TYPE_TB
- en: '## Memory management'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`empty_cache`](generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache
    "torch.cuda.empty_cache") | Release all unoccupied cached memory currently held
    by the caching allocator so that those can be used in other GPU application and
    visible in nvidia-smi. |'
  prefs: []
  type: TYPE_TB
- en: '| [`list_gpu_processes`](generated/torch.cuda.list_gpu_processes.html#torch.cuda.list_gpu_processes
    "torch.cuda.list_gpu_processes") | Return a human-readable printout of the running
    processes and their GPU memory use for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`mem_get_info`](generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info
    "torch.cuda.mem_get_info") | Return the global free and total GPU memory for a
    given device using cudaMemGetInfo. |'
  prefs: []
  type: TYPE_TB
- en: '| [`memory_stats`](generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats
    "torch.cuda.memory_stats") | Return a dictionary of CUDA memory allocator statistics
    for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`memory_summary`](generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary
    "torch.cuda.memory_summary") | Return a human-readable printout of the current
    memory allocator statistics for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`memory_snapshot`](generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot
    "torch.cuda.memory_snapshot") | Return a snapshot of the CUDA memory allocator
    state across all devices. |'
  prefs: []
  type: TYPE_TB
- en: '| [`memory_allocated`](generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated
    "torch.cuda.memory_allocated") | Return the current GPU memory occupied by tensors
    in bytes for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`max_memory_allocated`](generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated
    "torch.cuda.max_memory_allocated") | Return the maximum GPU memory occupied by
    tensors in bytes for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`reset_max_memory_allocated`](generated/torch.cuda.reset_max_memory_allocated.html#torch.cuda.reset_max_memory_allocated
    "torch.cuda.reset_max_memory_allocated") | Reset the starting point in tracking
    maximum GPU memory occupied by tensors for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`memory_reserved`](generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved
    "torch.cuda.memory_reserved") | Return the current GPU memory managed by the caching
    allocator in bytes for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`max_memory_reserved`](generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved
    "torch.cuda.max_memory_reserved") | Return the maximum GPU memory managed by the
    caching allocator in bytes for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`set_per_process_memory_fraction`](generated/torch.cuda.set_per_process_memory_fraction.html#torch.cuda.set_per_process_memory_fraction
    "torch.cuda.set_per_process_memory_fraction") | Set memory fraction for a process.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`memory_cached`](generated/torch.cuda.memory_cached.html#torch.cuda.memory_cached
    "torch.cuda.memory_cached") | Deprecated; see [`memory_reserved()`](generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved
    "torch.cuda.memory_reserved"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`max_memory_cached`](generated/torch.cuda.max_memory_cached.html#torch.cuda.max_memory_cached
    "torch.cuda.max_memory_cached") | Deprecated; see [`max_memory_reserved()`](generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved
    "torch.cuda.max_memory_reserved"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`reset_max_memory_cached`](generated/torch.cuda.reset_max_memory_cached.html#torch.cuda.reset_max_memory_cached
    "torch.cuda.reset_max_memory_cached") | Reset the starting point in tracking maximum
    GPU memory managed by the caching allocator for a given device. |'
  prefs: []
  type: TYPE_TB
- en: '| [`reset_peak_memory_stats`](generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats
    "torch.cuda.reset_peak_memory_stats") | Reset the "peak" stats tracked by the
    CUDA memory allocator. |'
  prefs: []
  type: TYPE_TB
- en: '| [`caching_allocator_alloc`](generated/torch.cuda.caching_allocator_alloc.html#torch.cuda.caching_allocator_alloc
    "torch.cuda.caching_allocator_alloc") | Perform a memory allocation using the
    CUDA memory allocator. |'
  prefs: []
  type: TYPE_TB
- en: '| [`caching_allocator_delete`](generated/torch.cuda.caching_allocator_delete.html#torch.cuda.caching_allocator_delete
    "torch.cuda.caching_allocator_delete") | Delete memory allocated using the CUDA
    memory allocator. |'
  prefs: []
  type: TYPE_TB
- en: '| [`get_allocator_backend`](generated/torch.cuda.get_allocator_backend.html#torch.cuda.get_allocator_backend
    "torch.cuda.get_allocator_backend") | Return a string describing the active allocator
    backend as set by `PYTORCH_CUDA_ALLOC_CONF`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`CUDAPluggableAllocator`](generated/torch.cuda.CUDAPluggableAllocator.html#torch.cuda.CUDAPluggableAllocator
    "torch.cuda.CUDAPluggableAllocator") | CUDA memory allocator loaded from a so
    file. |'
  prefs: []
  type: TYPE_TB
- en: '| [`change_current_allocator`](generated/torch.cuda.change_current_allocator.html#torch.cuda.change_current_allocator
    "torch.cuda.change_current_allocator") | Change the currently used memory allocator
    to be the one provided. |'
  prefs: []
  type: TYPE_TB
- en: NVIDIA Tools Extension (NVTX)[](#nvidia-tools-extension-nvtx "Permalink to
    this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`nvtx.mark`](generated/torch.cuda.nvtx.mark.html#torch.cuda.nvtx.mark "torch.cuda.nvtx.mark")
    | Describe an instantaneous event that occurred at some point. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nvtx.range_push`](generated/torch.cuda.nvtx.range_push.html#torch.cuda.nvtx.range_push
    "torch.cuda.nvtx.range_push") | Push a range onto a stack of nested range span.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`nvtx.range_pop`](generated/torch.cuda.nvtx.range_pop.html#torch.cuda.nvtx.range_pop
    "torch.cuda.nvtx.range_pop") | Pop a range off of a stack of nested range spans.
    |'
  prefs: []
  type: TYPE_TB
- en: Jiterator (beta)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`jiterator._create_jit_fn`](generated/torch.cuda.jiterator._create_jit_fn.html#torch.cuda.jiterator._create_jit_fn
    "torch.cuda.jiterator._create_jit_fn") | Create a jiterator-generated cuda kernel
    for an elementwise op. |'
  prefs: []
  type: TYPE_TB
- en: '| [`jiterator._create_multi_output_jit_fn`](generated/torch.cuda.jiterator._create_multi_output_jit_fn.html#torch.cuda.jiterator._create_multi_output_jit_fn
    "torch.cuda.jiterator._create_multi_output_jit_fn") | Create a jiterator-generated
    cuda kernel for an elementwise op that supports returning one or more outputs.
    |'
  prefs: []
  type: TYPE_TB
- en: Stream Sanitizer (prototype)[](#stream-sanitizer-prototype "Permalink to this
    heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CUDA Sanitizer is a prototype tool for detecting synchronization errors between
    streams in PyTorch. See the [documentation](cuda._sanitizer.html) for information
    on how to use it.
  prefs: []
  type: TYPE_NORMAL
