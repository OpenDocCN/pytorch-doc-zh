["```py\nimport torch\nfrom torch import nn\nfrom torch.utils.data import [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")\nfrom torchvision import datasets\nfrom torchvision.transforms import [ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor \"torchvision.transforms.ToTensor\")\n\n[training_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\") = [datasets.FashionMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\")(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=[ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor \"torchvision.transforms.ToTensor\")()\n)\n\n[test_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\") = [datasets.FashionMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\")(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=[ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor \"torchvision.transforms.ToTensor\")()\n)\n\n[train_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")([training_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\"), batch_size=64)\n[test_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")([test_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\"), batch_size=64)\n\nclass NeuralNetwork([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self):\n        super().__init__()\n        self.flatten = [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten \"torch.nn.Flatten\")()\n        self.linear_relu_stack = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(28*28, 512),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(512, 512),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = [NeuralNetwork](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")() \n```", "```py\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/26421880 [00:00<?, ?it/s]\n  0%|          | 65536/26421880 [00:00<01:12, 365539.83it/s]\n  1%|          | 229376/26421880 [00:00<00:38, 684511.48it/s]\n  3%|3         | 884736/26421880 [00:00<00:12, 2030637.83it/s]\n 12%|#1        | 3080192/26421880 [00:00<00:03, 6027159.86it/s]\n 31%|###       | 8060928/26421880 [00:00<00:01, 16445259.09it/s]\n 42%|####1     | 11075584/26421880 [00:00<00:00, 16871356.21it/s]\n 64%|######3   | 16908288/26421880 [00:01<00:00, 24452744.90it/s]\n 76%|#######6  | 20086784/26421880 [00:01<00:00, 24276135.68it/s]\n 98%|#########7| 25788416/26421880 [00:01<00:00, 32055536.22it/s]\n100%|##########| 26421880/26421880 [00:01<00:00, 18270891.31it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/29515 [00:00<?, ?it/s]\n100%|##########| 29515/29515 [00:00<00:00, 326183.74it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00<?, ?it/s]\n  1%|1         | 65536/4422102 [00:00<00:12, 361771.90it/s]\n  5%|5         | 229376/4422102 [00:00<00:06, 680798.45it/s]\n 21%|##        | 917504/4422102 [00:00<00:01, 2100976.96it/s]\n 70%|#######   | 3112960/4422102 [00:00<00:00, 6040440.05it/s]\n100%|##########| 4422102/4422102 [00:00<00:00, 6047736.61it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/5148 [00:00<?, ?it/s]\n100%|##########| 5148/5148 [00:00<00:00, 36846889.06it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw \n```", "```py\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5 \n```", "```py\n# Initialize the loss function\n[loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\") = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")() \n```", "```py\n[optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\") = [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")(), lr=learning_rate) \n```", "```py\ndef train_loop(dataloader, model, [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\"), [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\")):\n    size = len(dataloader.dataset)\n    # Set the model to training mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    [model.train](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train \"torch.nn.Module.train\")()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        [optimizer.step](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.step \"torch.optim.SGD.step\")()\n        [optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.zero_grad \"torch.optim.SGD.zero_grad\")()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(X)\n            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n\ndef test_loop(dataloader, model, [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")):\n    # Set the model to evaluation mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    [model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")(pred, y).item()\n            correct += (pred.argmax(1) == y).type([torch.float](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}  \\n\") \n```", "```py\n[loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\") = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\n[optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\") = [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop([train_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), model, [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\"), [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\"))\n    test_loop([test_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), model, [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\"))\nprint(\"Done!\") \n```", "```py\nEpoch 1\n-------------------------------\nloss: 2.298730  [   64/60000]\nloss: 2.289123  [ 6464/60000]\nloss: 2.273286  [12864/60000]\nloss: 2.269406  [19264/60000]\nloss: 2.249603  [25664/60000]\nloss: 2.229407  [32064/60000]\nloss: 2.227368  [38464/60000]\nloss: 2.204261  [44864/60000]\nloss: 2.206193  [51264/60000]\nloss: 2.166651  [57664/60000]\nTest Error:\n Accuracy: 50.9%, Avg loss: 2.166725\n\nEpoch 2\n-------------------------------\nloss: 2.176750  [   64/60000]\nloss: 2.169595  [ 6464/60000]\nloss: 2.117500  [12864/60000]\nloss: 2.129272  [19264/60000]\nloss: 2.079674  [25664/60000]\nloss: 2.032928  [32064/60000]\nloss: 2.050115  [38464/60000]\nloss: 1.985236  [44864/60000]\nloss: 1.987887  [51264/60000]\nloss: 1.907162  [57664/60000]\nTest Error:\n Accuracy: 55.9%, Avg loss: 1.915486\n\nEpoch 3\n-------------------------------\nloss: 1.951612  [   64/60000]\nloss: 1.928685  [ 6464/60000]\nloss: 1.815709  [12864/60000]\nloss: 1.841552  [19264/60000]\nloss: 1.732467  [25664/60000]\nloss: 1.692914  [32064/60000]\nloss: 1.701714  [38464/60000]\nloss: 1.610632  [44864/60000]\nloss: 1.632870  [51264/60000]\nloss: 1.514263  [57664/60000]\nTest Error:\n Accuracy: 58.8%, Avg loss: 1.541525\n\nEpoch 4\n-------------------------------\nloss: 1.616448  [   64/60000]\nloss: 1.582892  [ 6464/60000]\nloss: 1.427595  [12864/60000]\nloss: 1.487950  [19264/60000]\nloss: 1.359332  [25664/60000]\nloss: 1.364817  [32064/60000]\nloss: 1.371491  [38464/60000]\nloss: 1.298706  [44864/60000]\nloss: 1.336201  [51264/60000]\nloss: 1.232145  [57664/60000]\nTest Error:\n Accuracy: 62.2%, Avg loss: 1.260237\n\nEpoch 5\n-------------------------------\nloss: 1.345538  [   64/60000]\nloss: 1.327798  [ 6464/60000]\nloss: 1.153802  [12864/60000]\nloss: 1.254829  [19264/60000]\nloss: 1.117322  [25664/60000]\nloss: 1.153248  [32064/60000]\nloss: 1.171765  [38464/60000]\nloss: 1.110263  [44864/60000]\nloss: 1.154467  [51264/60000]\nloss: 1.070921  [57664/60000]\nTest Error:\n Accuracy: 64.1%, Avg loss: 1.089831\n\nEpoch 6\n-------------------------------\nloss: 1.166889  [   64/60000]\nloss: 1.170514  [ 6464/60000]\nloss: 0.979435  [12864/60000]\nloss: 1.113774  [19264/60000]\nloss: 0.973411  [25664/60000]\nloss: 1.015192  [32064/60000]\nloss: 1.051113  [38464/60000]\nloss: 0.993591  [44864/60000]\nloss: 1.039709  [51264/60000]\nloss: 0.971077  [57664/60000]\nTest Error:\n Accuracy: 65.8%, Avg loss: 0.982440\n\nEpoch 7\n-------------------------------\nloss: 1.045165  [   64/60000]\nloss: 1.070583  [ 6464/60000]\nloss: 0.862304  [12864/60000]\nloss: 1.022265  [19264/60000]\nloss: 0.885213  [25664/60000]\nloss: 0.919528  [32064/60000]\nloss: 0.972762  [38464/60000]\nloss: 0.918728  [44864/60000]\nloss: 0.961629  [51264/60000]\nloss: 0.904379  [57664/60000]\nTest Error:\n Accuracy: 66.9%, Avg loss: 0.910167\n\nEpoch 8\n-------------------------------\nloss: 0.956964  [   64/60000]\nloss: 1.002171  [ 6464/60000]\nloss: 0.779057  [12864/60000]\nloss: 0.958409  [19264/60000]\nloss: 0.827240  [25664/60000]\nloss: 0.850262  [32064/60000]\nloss: 0.917320  [38464/60000]\nloss: 0.868384  [44864/60000]\nloss: 0.905506  [51264/60000]\nloss: 0.856353  [57664/60000]\nTest Error:\n Accuracy: 68.3%, Avg loss: 0.858248\n\nEpoch 9\n-------------------------------\nloss: 0.889765  [   64/60000]\nloss: 0.951220  [ 6464/60000]\nloss: 0.717035  [12864/60000]\nloss: 0.911042  [19264/60000]\nloss: 0.786085  [25664/60000]\nloss: 0.798370  [32064/60000]\nloss: 0.874939  [38464/60000]\nloss: 0.832796  [44864/60000]\nloss: 0.863254  [51264/60000]\nloss: 0.819742  [57664/60000]\nTest Error:\n Accuracy: 69.5%, Avg loss: 0.818780\n\nEpoch 10\n-------------------------------\nloss: 0.836395  [   64/60000]\nloss: 0.910220  [ 6464/60000]\nloss: 0.668506  [12864/60000]\nloss: 0.874338  [19264/60000]\nloss: 0.754805  [25664/60000]\nloss: 0.758453  [32064/60000]\nloss: 0.840451  [38464/60000]\nloss: 0.806153  [44864/60000]\nloss: 0.830360  [51264/60000]\nloss: 0.790281  [57664/60000]\nTest Error:\n Accuracy: 71.0%, Avg loss: 0.787271\n\nDone! \n```"]