["```py\ntorchtext.data.functional.generate_sp_model(filename, vocab_size=20000, model_type='unigram', model_prefix='m_user')\u00b6\n```", "```py\n>>> from torchtext.data.functional import generate_sp_model\n>>> generate_sp_model('test.csv', vocab_size=23456, model_prefix='spm_user') \n```", "```py\ntorchtext.data.functional.load_sp_model(spm)\u00b6\n```", "```py\n>>> from torchtext.data.functional import load_sp_model\n>>> sp_model = load_sp_model(\"m_user.model\")\n>>> sp_model = load_sp_model(open(\"m_user.model\", 'rb')) \n```", "```py\ntorchtext.data.functional.sentencepiece_numericalizer(sp_model)\u00b6\n```", "```py\n>>> from torchtext.data.functional import sentencepiece_numericalizer\n>>> sp_id_generator = sentencepiece_numericalizer(sp_model)\n>>> list_a = [\"sentencepiece encode as pieces\", \"examples to   try!\"]\n>>> list(sp_id_generator(list_a))\n [[9858, 9249, 1629, 1305, 1809, 53, 842],\n [2347, 13, 9, 150, 37]] \n```", "```py\ntorchtext.data.functional.sentencepiece_tokenizer(sp_model)\u00b6\n```", "```py\n>>> from torchtext.data.functional import sentencepiece_tokenizer\n>>> sp_tokens_generator = sentencepiece_tokenizer(sp_model)\n>>> list_a = [\"sentencepiece encode as pieces\", \"examples to   try!\"]\n>>> list(sp_tokens_generator(list_a))\n [['_sentence', 'piece', '_en', 'co', 'de', '_as', '_pieces'],\n ['_example', 's', '_to', '_try', '!']] \n```", "```py\ntorchtext.data.functional.custom_replace(replace_pattern)\u00b6\n```", "```py\n>>> from torchtext.data.functional import custom_replace\n>>> custom_replace_transform = custom_replace([(r'S', 's'), (r'\\s+', ' ')])\n>>> list_a = [\"Sentencepiece encode  aS  pieces\", \"exampleS to   try!\"]\n>>> list(custom_replace_transform(list_a))\n ['sentencepiece encode as pieces', 'examples to try!'] \n```", "```py\ntorchtext.data.functional.simple_space_split(iterator)\u00b6\n```", "```py\n>>> from torchtext.data.functional import simple_space_split\n>>> list_a = [\"Sentencepiece encode as pieces\", \"example to try!\"]\n>>> list(simple_space_split(list_a))\n [['Sentencepiece', 'encode', 'as', 'pieces'], ['example', 'to', 'try!']] \n```", "```py\ntorchtext.data.functional.numericalize_tokens_from_iterator(vocab, iterator, removed_tokens=None)\u00b6\n```", "```py\n>>> from torchtext.data.functional import simple_space_split\n>>> from torchtext.data.functional import numericalize_tokens_from_iterator\n>>> vocab = {'Sentencepiece' : 0, 'encode' : 1, 'as' : 2, 'pieces' : 3}\n>>> ids_iter = numericalize_tokens_from_iterator(vocab,\n>>>                               simple_space_split([\"Sentencepiece as pieces\",\n>>>                                                   \"as pieces\"]))\n>>> for ids in ids_iter:\n>>>     print([num for num in ids])\n>>> [0, 2, 3]\n>>> [2, 3] \n```", "```py\ntorchtext.data.functional.filter_wikipedia_xml(text_iterator)\u00b6\n```", "```py\n>>> from torchtext.data.functional import filter_wikipedia_xml\n>>> from torchtext.datasets import EnWik9\n>>> data_iter = EnWik9(split='train')\n>>> filter_data_iter = filter_wikipedia_xml(data_iter)\n>>> file_name = '.data/EnWik9/enwik9'\n>>> filter_data_iter = filter_wikipedia_xml(open(file_name,'r')) \n```", "```py\ntorchtext.data.functional.to_map_style_dataset(iter_data)\u00b6\n```", "```py\n>>> from torchtext.datasets import IMDB\n>>> from torchtext.data import to_map_style_dataset\n>>> train_iter = IMDB(split='train')\n>>> train_dataset = to_map_style_dataset(train_iter)\n>>> file_name = '.data/EnWik9/enwik9'\n>>> data_iter = to_map_style_dataset(open(file_name,'r')) \n```"]