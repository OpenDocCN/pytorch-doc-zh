["```py\nimport sys\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, [TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer \"torch.nn.TransformerEncoderLayer\")\n\nclass PositionalEncoding([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(p=dropout)\n\n        pe = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros \"torch.zeros\")(max_len, d_model)\n        position = [torch.arange](https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange \"torch.arange\")(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = [torch.exp](https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp \"torch.exp\")([torch.arange](https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange \"torch.arange\")(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = [torch.sin](https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin \"torch.sin\")(position * div_term)\n        pe[:, 1::2] = [torch.cos](https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos \"torch.cos\")(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.pe = nn.Parameter(pe, requires_grad=False)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x) \n```", "```py\nif sys.platform == 'win32':\n    print('Windows platform is not supported for pipeline parallelism')\n    sys.exit(0)\nif [torch.cuda.device_count](https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count \"torch.cuda.device_count\")() < 4:\n    print('Need at least four GPU devices for this tutorial')\n    sys.exit(0)\n\nclass Encoder([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, ntoken, ninp, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        self.encoder = [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding \"torch.nn.Embedding\")(ntoken, ninp)\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        # Need (S, N) format for encoder.\n        src = src.t()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        return self.pos_encoder(src)\n\nclass Decoder([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, ntoken, ninp):\n        super(Decoder, self).__init__()\n        self.decoder = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(ninp, ntoken)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, inp):\n        # Need batch dimension first for output of pipeline.\n        return self.decoder(inp).permute(1, 0, 2) \n```", "```py\ndef run_worker(rank, world_size): \n```", "```py\n# In 'run_worker'\n    def print_with_rank(msg):\n        print('[RANK {}]: {}'.format(rank, msg))\n\n    from torchtext.datasets import [WikiText2](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2 \"torchtext.datasets.WikiText2\")\n    from torchtext.data.utils import [get_tokenizer](https://pytorch.org/text/stable/data_utils.html#torchtext.data.utils.get_tokenizer \"torchtext.data.utils.get_tokenizer\")\n    from torchtext.vocab import [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")\n\n    train_iter = [WikiText2](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2 \"torchtext.datasets.WikiText2\")(split='train')\n    tokenizer = [get_tokenizer](https://pytorch.org/text/stable/data_utils.html#torchtext.data.utils.get_tokenizer \"torchtext.data.utils.get_tokenizer\")('basic_english')\n    vocab = [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")(map(tokenizer, train_iter), specials=[\"<unk>\"])\n    vocab.set_default_index(vocab[\"<unk>\"])\n\n    def data_process(raw_text_iter):\n      data = [[torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n      return [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")(tuple(filter(lambda t: t.numel() > 0, data)))\n\n    train_iter, val_iter, test_iter = [WikiText2](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2 \"torchtext.datasets.WikiText2\")()\n    train_data = data_process(train_iter)\n    val_data = data_process(val_iter)\n    test_data = data_process(test_iter)\n\n    device = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")(2 * rank)\n\n    def batchify(data, bsz, rank, world_size, is_train=False):\n        # Divide the dataset into ``bsz`` parts.\n        nbatch = data.size(0) // bsz\n        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n        data = data.narrow(0, 0, nbatch * bsz)\n        # Evenly divide the data across the ``bsz`` batches.\n        data = data.view(bsz, -1).t().contiguous()\n        # Divide the data across the ranks only for training data.\n        if is_train:\n            data_per_rank = data.size(0) // world_size\n            data = data[rank * data_per_rank : (rank + 1) * data_per_rank]\n        return data.to(device)\n\n    batch_size = 20\n    eval_batch_size = 10\n    train_data = batchify(train_data, batch_size, rank, world_size, True)\n    val_data = batchify(val_data, eval_batch_size, rank, world_size)\n    test_data = batchify(test_data, eval_batch_size, rank, world_size) \n```", "```py\n# In 'run_worker'\n    bptt = 35\n    def get_batch(source, i):\n        seq_len = min(bptt, len(source) - 1 - i)\n        data = source[i:i+seq_len]\n        target = source[i+1:i+1+seq_len].view(-1)\n        # Need batch dimension first for pipeline parallelism.\n        return data.t(), target \n```", "```py\n# In 'run_worker'\n    ntokens = len(vocab) # the size of vocabulary\n    emsize = 4096 # embedding dimension\n    nhid = 4096 # the dimension of the feedforward network model in ``nn.TransformerEncoder``\n    nlayers = 8 # the number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n    nhead = 16 # the number of heads in the Multihead Attention models\n    dropout = 0.2 # the dropout value\n\n    from torch.distributed import rpc\n    tmpfile = tempfile.NamedTemporaryFile()\n    [rpc.init_rpc](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.init_rpc \"torch.distributed.rpc.init_rpc\")(\n        name=\"worker\",\n        rank=0,\n        world_size=1,\n        rpc_backend_options=[rpc.TensorPipeRpcBackendOptions](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions \"torch.distributed.rpc.TensorPipeRpcBackendOptions\")(\n            init_method=\"file://{}\".format(tmpfile.name),\n            # Specifying _transports and _channels is a workaround and we no longer\n            # will have to specify _transports and _channels for PyTorch\n            # versions >= 1.8.1\n            _transports=[\"ibv\", \"uv\"],\n            _channels=[\"cuda_ipc\", \"cuda_basic\"],\n        )\n    )\n\n    # Number of GPUs for model parallelism.\n    num_gpus = 2\n    partition_len = ((nlayers - 1) // num_gpus) + 1\n\n    # Add encoder in the beginning.\n    tmp_list = [Encoder(ntokens, emsize, dropout).cuda(2 * rank)]\n    module_list = []\n\n    # Add all the necessary transformer blocks.\n    for i in range(nlayers):\n        transformer_block = [TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer \"torch.nn.TransformerEncoderLayer\")(emsize, nhead, nhid, dropout)\n        if i != 0 and i % (partition_len) == 0:\n            module_list.append([nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(*tmp_list))\n            tmp_list = []\n        device = i // (partition_len)\n        tmp_list.append(transformer_block.to(2 * rank + device))\n\n    # Add decoder in the end.\n    tmp_list.append(Decoder(ntokens, emsize).cuda(2 * rank + num_gpus - 1))\n    module_list.append([nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(*tmp_list))\n\n    # Need to use 'checkpoint=never' since as of PyTorch 1.8, Pipe checkpointing\n    # doesn't work with DDP.\n    from torch.distributed.pipeline.sync import [Pipe](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\")\n    chunks = 8\n    model = [Pipe](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\")([torch.nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n        *module_list), chunks = chunks, checkpoint=\"never\")\n\n    # Initialize process group and wrap model in DDP.\n    from torch.nn.parallel import [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel \"torch.nn.parallel.DistributedDataParallel\")\n    import torch.distributed as dist\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    [dist.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group \"torch.distributed.init_process_group\")(\n                backend=\"nccl\", rank=rank, world_size=world_size)\n    model = [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel \"torch.nn.parallel.DistributedDataParallel\")(model)\n\n    def get_total_params(module: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n        total_params = 0\n        for param in module.parameters():\n            total_params += param.numel()\n        return total_params\n\n    print_with_rank('Total parameters in model: {:,}'.format(get_total_params(model))) \n```", "```py\n# In 'run_worker'\n    criterion = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\n    lr = 5.0 # learning rate\n    optimizer = [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\")(model.parameters(), lr=lr)\n    scheduler = [torch.optim.lr_scheduler.StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\")(optimizer, 1.0, gamma=0.95)\n\n    import time\n    def train():\n        model.train() # Turn on the train mode\n        total_loss = 0.\n        start_time = time.time()\n        ntokens = len(vocab)\n\n        # Train only for 50 batches to keep script execution time low.\n        nbatches = min(50 * bptt, train_data.size(0) - 1)\n\n        for batch, i in enumerate(range(0, nbatches, bptt)):\n            data, targets = get_batch(train_data, i)\n            optimizer.zero_grad()\n            # Since the Pipe is only within a single host and process the ``RRef``\n            # returned by forward method is local to this node and can simply\n            # retrieved via ``RRef.local_value()``.\n            output = model(data).local_value()\n            # Need to move targets to the device where the output of the\n            # pipeline resides.\n            loss = criterion(output.view(-1, ntokens), targets.cuda(2 * rank + 1))\n            loss.backward()\n            [torch.nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_ \"torch.nn.utils.clip_grad_norm_\")(model.parameters(), 0.5)\n            optimizer.step()\n\n            total_loss += loss.item()\n            log_interval = 10\n            if batch % log_interval == 0 and batch > 0:\n                cur_loss = total_loss / log_interval\n                elapsed = time.time() - start_time\n                print_with_rank('| epoch {:3d} | {:5d}/{:5d} batches | '\n                      'lr {:02.2f} | ms/batch {:5.2f} | '\n                      'loss {:5.2f} | ppl {:8.2f}'.format(\n                        epoch, batch, nbatches // bptt, scheduler.get_last_lr()[0],\n                        elapsed * 1000 / log_interval,\n                        cur_loss, math.exp(cur_loss)))\n                total_loss = 0\n                start_time = time.time()\n\n    def evaluate(eval_model, data_source):\n        eval_model.eval() # Turn on the evaluation mode\n        total_loss = 0.\n        ntokens = len(vocab)\n        # Evaluate only for 50 batches to keep script execution time low.\n        nbatches = min(50 * bptt, data_source.size(0) - 1)\n        with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n            for i in range(0, nbatches, bptt):\n                data, targets = get_batch(data_source, i)\n                output = eval_model(data).local_value()\n                output_flat = output.view(-1, ntokens)\n                # Need to move targets to the device where the output of the\n                # pipeline resides.\n                total_loss += len(data) * criterion(output_flat, targets.cuda(2 * rank + 1)).item()\n        return total_loss / (len(data_source) - 1) \n```", "```py\n# In 'run_worker'\n    best_val_loss = float(\"inf\")\n    epochs = 3 # The number of epochs\n    best_model = None\n\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train()\n        val_loss = evaluate(model, val_data)\n        print_with_rank('-' * 89)\n        print_with_rank('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                         val_loss, math.exp(val_loss)))\n        print_with_rank('-' * 89)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model = model\n\n        scheduler.step() \n```", "```py\n# In 'run_worker'\n    test_loss = evaluate(best_model, test_data)\n    print_with_rank('=' * 89)\n    print_with_rank('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n        test_loss, math.exp(test_loss)))\n    print_with_rank('=' * 89)\n\n# Main execution\nimport torch.multiprocessing as mp\n\nif __name__==\"__main__\":\n    world_size = 2\n    [mp.spawn](https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing.spawn \"torch.multiprocessing.spawn\")(run_worker, args=(world_size, ), nprocs=world_size, join=True) \n```", "```py\n[RANK 0]: | epoch   1 |    10/   50 batches | lr 5.00 | ms/batch 778.97 | loss 43.31 | ppl 6432469059895903232.00\n[RANK 1]: | epoch   1 |    10/   50 batches | lr 5.00 | ms/batch 778.90 | loss 44.50 | ppl 21245447128217366528.00\n[RANK 0]: | epoch   1 |    20/   50 batches | lr 5.00 | ms/batch 699.89 | loss 44.50 | ppl 21176949187407757312.00\n[RANK 1]: | epoch   1 |    20/   50 batches | lr 5.00 | ms/batch 699.87 | loss 44.62 | ppl 23975861229620961280.00\n[RANK 0]: | epoch   1 |    30/   50 batches | lr 5.00 | ms/batch 698.86 | loss 41.62 | ppl 1193312915629888256.00\n[RANK 1]: | epoch   1 |    30/   50 batches | lr 5.00 | ms/batch 698.87 | loss 40.69 | ppl 471605759847546240.00\n[RANK 0]: | epoch   1 |    40/   50 batches | lr 5.00 | ms/batch 698.34 | loss 45.20 | ppl 42812308420836458496.00\n[RANK 1]: | epoch   1 |    40/   50 batches | lr 5.00 | ms/batch 698.33 | loss 45.68 | ppl 68839569686012223488.00\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 1]: | end of epoch   1 | time: 40.08s | valid loss  0.80 | valid ppl     2.22\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | end of epoch   1 | time: 40.09s | valid loss  0.80 | valid ppl     2.22\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | epoch   2 |    10/   50 batches | lr 4.75 | ms/batch 768.51 | loss 36.34 | ppl 6063529544668166.00\n[RANK 1]: | epoch   2 |    10/   50 batches | lr 4.75 | ms/batch 769.23 | loss 37.41 | ppl 17651211266236086.00\n[RANK 0]: | epoch   2 |    20/   50 batches | lr 4.75 | ms/batch 699.57 | loss 28.97 | ppl 3798441739584.11\n[RANK 1]: | epoch   2 |    20/   50 batches | lr 4.75 | ms/batch 699.56 | loss 29.28 | ppl 5203636967575.47\n[RANK 0]: | epoch   2 |    30/   50 batches | lr 4.75 | ms/batch 699.04 | loss 28.43 | ppl 2212498693571.25\n[RANK 1]: | epoch   2 |    30/   50 batches | lr 4.75 | ms/batch 699.05 | loss 28.33 | ppl 2015144761281.48\n[RANK 0]: | epoch   2 |    40/   50 batches | lr 4.75 | ms/batch 699.10 | loss 23.30 | ppl 13121380184.92\n[RANK 1]: | epoch   2 |    40/   50 batches | lr 4.75 | ms/batch 699.09 | loss 23.41 | ppl 14653799192.87\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | end of epoch   2 | time: 39.97s | valid loss  0.24 | valid ppl     1.27\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 1]: | end of epoch   2 | time: 39.98s | valid loss  0.24 | valid ppl     1.27\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 0]: | epoch   3 |    10/   50 batches | lr 4.51 | ms/batch 769.36 | loss 12.80 | ppl 361681.11\n[RANK 1]: | epoch   3 |    10/   50 batches | lr 4.51 | ms/batch 768.97 | loss 12.57 | ppl 287876.61\n[RANK 0]: | epoch   3 |    20/   50 batches | lr 4.51 | ms/batch 698.27 | loss 12.01 | ppl 164364.60\n[RANK 1]: | epoch   3 |    20/   50 batches | lr 4.51 | ms/batch 698.30 | loss 11.98 | ppl 159095.89\n[RANK 0]: | epoch   3 |    30/   50 batches | lr 4.51 | ms/batch 697.75 | loss 10.90 | ppl 54261.91\n[RANK 1]: | epoch   3 |    30/   50 batches | lr 4.51 | ms/batch 697.72 | loss 10.89 | ppl 53372.39\n[RANK 0]: | epoch   3 |    40/   50 batches | lr 4.51 | ms/batch 699.49 | loss 10.78 | ppl 47948.35\n[RANK 1]: | epoch   3 |    40/   50 batches | lr 4.51 | ms/batch 699.50 | loss 10.79 | ppl 48664.42\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | end of epoch   3 | time: 39.96s | valid loss  0.38 | valid ppl     1.46\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 1]: | end of epoch   3 | time: 39.96s | valid loss  0.38 | valid ppl     1.46\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 0]: =========================================================================================\n[RANK 0]: | End of training | test loss  0.33 | test ppl     1.39\n[RANK 0]: =========================================================================================\n[RANK 1]: =========================================================================================\n[RANK 1]: | End of training | test loss  0.33 | test ppl     1.39\n[RANK 1]: ========================================================================================= \n```"]