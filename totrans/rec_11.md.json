["```py\nclass torchrec.optim.clipping.GradientClipping(value)\u00b6\n```", "```py\nNONE = 'none'\u00b6\n```", "```py\nNORM = 'norm'\u00b6\n```", "```py\nVALUE = 'value'\u00b6\n```", "```py\nclass torchrec.optim.clipping.GradientClippingOptimizer(optimizer: KeyedOptimizer, clipping: GradientClipping = GradientClipping.NONE, max_gradient: float = 0.1)\u00b6\n```", "```py\nstep(closure: Optional[Any] = None) \u2192 None\u00b6\n```", "```py\nclass torchrec.optim.fused.EmptyFusedOptimizer\u00b6\n```", "```py\nstep(closure: Optional[Any] = None) \u2192 None\u00b6\n```", "```py\nzero_grad(set_to_none: bool = False) \u2192 None\u00b6\n```", "```py\nclass torchrec.optim.fused.FusedOptimizer(params: Mapping[str, Union[Tensor, ShardedTensor]], state: Mapping[Any, Any], param_groups: Collection[Mapping[str, Any]])\u00b6\n```", "```py\nabstract step(closure: Optional[Any] = None) \u2192 None\u00b6\n```", "```py\nabstract zero_grad(set_to_none: bool = False) \u2192 None\u00b6\n```", "```py\nclass torchrec.optim.fused.FusedOptimizerModule\u00b6\n```", "```py\nabstract property fused_optimizer: KeyedOptimizer\u00b6\n```", "```py\nclass torchrec.optim.keyed.CombinedOptimizer(optims: List[Union[KeyedOptimizer, Tuple[str, KeyedOptimizer]]])\u00b6\n```", "```py\nproperty optimizers: List[Tuple[str, KeyedOptimizer]]\u00b6\n```", "```py\nproperty param_groups: Collection[Mapping[str, Any]]\u00b6\n```", "```py\nproperty params: Mapping[str, Union[Tensor, ShardedTensor]]\u00b6\n```", "```py\npost_load_state_dict() \u2192 None\u00b6\n```", "```py\nstatic prepend_opt_key(name: str, opt_key: str) \u2192 str\u00b6\n```", "```py\nsave_param_groups(save: bool) \u2192 None\u00b6\n```", "```py\nproperty state: Mapping[Tensor, Any]\u00b6\n```", "```py\nstep(closure: Optional[Any] = None) \u2192 None\u00b6\n```", "```py\nzero_grad(set_to_none: bool = False) \u2192 None\u00b6\n```", "```py\nclass torchrec.optim.keyed.KeyedOptimizer(params: Mapping[str, Union[Tensor, ShardedTensor]], state: Mapping[Any, Any], param_groups: Collection[Mapping[str, Any]])\u00b6\n```", "```py\nadd_param_group(param_group: Any) \u2192 None\u00b6\n```", "```py\ninit_state(sparse_grad_parameter_names: Optional[Set[str]] = None) \u2192 None\u00b6\n```", "```py\nload_state_dict(state_dict: Mapping[str, Any]) \u2192 None\u00b6\n```", "```py\npost_load_state_dict() \u2192 None\u00b6\n```", "```py\nsave_param_groups(save: bool) \u2192 None\u00b6\n```", "```py\nstate_dict() \u2192 Dict[str, Any]\u00b6\n```", "```py\nclass torchrec.optim.keyed.KeyedOptimizerWrapper(params: Mapping[str, Union[Tensor, ShardedTensor]], optim_factory: Callable[[List[Union[Tensor, ShardedTensor]]], Optimizer])\u00b6\n```", "```py\nstep(closure: Optional[Any] = None) \u2192 None\u00b6\n```", "```py\nzero_grad(set_to_none: bool = False) \u2192 None\u00b6\n```", "```py\nclass torchrec.optim.keyed.OptimizerWrapper(optimizer: KeyedOptimizer)\u00b6\n```", "```py\nadd_param_group(param_group: Any) \u2192 None\u00b6\n```", "```py\nload_state_dict(state_dict: Mapping[str, Any]) \u2192 None\u00b6\n```", "```py\npost_load_state_dict() \u2192 None\u00b6\n```", "```py\nsave_param_groups(save: bool) \u2192 None\u00b6\n```", "```py\nstate_dict() \u2192 Dict[str, Any]\u00b6\n```", "```py\nstep(closure: Optional[Any] = None) \u2192 None\u00b6\n```", "```py\nzero_grad(set_to_none: bool = False) \u2192 None\u00b6\n```", "```py\nclass torchrec.optim.warmup.WarmupOptimizer(optimizer: KeyedOptimizer, stages: List[WarmupStage], lr: float = 0.1, lr_param: str = 'lr', param_name: str = '__warmup')\u00b6\n```", "```py\npost_load_state_dict() \u2192 None\u00b6\n```", "```py\nstep(closure: Optional[Any] = None) \u2192 None\u00b6\n```", "```py\nclass torchrec.optim.warmup.WarmupPolicy(value)\u00b6\n```", "```py\nCONSTANT = 'constant'\u00b6\n```", "```py\nINVSQRT = 'inv_sqrt'\u00b6\n```", "```py\nLINEAR = 'linear'\u00b6\n```", "```py\nNONE = 'none'\u00b6\n```", "```py\nPOLY = 'poly'\u00b6\n```", "```py\nSTEP = 'step'\u00b6\n```", "```py\nclass torchrec.optim.warmup.WarmupStage(policy: torchrec.optim.warmup.WarmupPolicy = <WarmupPolicy.LINEAR: 'linear'>, max_iters: int = 1, value: float = 1.0, lr_scale: float = 1.0, decay_iters: int = -1)\u00b6\n```", "```py\ndecay_iters: int = -1\u00b6\n```", "```py\nlr_scale: float = 1.0\u00b6\n```", "```py\nmax_iters: int = 1\u00b6\n```", "```py\npolicy: WarmupPolicy = 'linear'\u00b6\n```", "```py\nvalue: float = 1.0\u00b6\n```"]