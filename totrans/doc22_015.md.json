["```py\nimport torch\nimport numpy as np\n\ndef to_numpy(tensor):\n    return tensor.cpu().numpy()\n\nclass NumpySort(torch.autograd.Function):\n    # Note that forward does not take ctx\n    @staticmethod\n    def forward(x, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = np.argsort(x, axis=dim)\n        ind_inv = np.argsort(ind, axis=dim)\n        result = np.take_along_axis(x, ind, axis=dim)\n        # Any intermediates to be saved in backward must be returned as\n        # outputs.\n        return (\n            # The desired output\n            torch.tensor(result, device=device),\n            # intermediate to save for backward\n            torch.tensor(ind, device=device),\n            # intermediate to save for backward\n            torch.tensor(ind_inv, device=device),\n        )\n\n    # setup_context is responsible for calling methods and/or assigning to\n    # the ctx object. Please do not do additional compute (e.g. add\n    # Tensors together) in setup_context.\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, dim = inputs\n        # Note that output is whatever you returned from forward.\n        # If you returned multiple values, then output is a Tuple of multiple values.\n        # If you returned a single Tensor, then output is a Tensor.\n        # If you returned a Tuple with a single Tensor, then output is a\n        # Tuple with a single Tensor.\n        _, ind, ind_inv = output\n        ctx.mark_non_differentiable(ind, ind_inv)\n        # Tensors must be saved via ctx.save_for_backward. Please do not\n        # assign them directly onto the ctx object.\n        ctx.save_for_backward(ind, ind_inv)\n        # Non-tensors may be saved by assigning them as attributes on the ctx object.\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output, _0, _1):\n        # For the autograd.Function to be arbitrarily composable with function\n        # transforms, all staticmethod other than forward and setup_context\n        # must be implemented in a \"transformable\" way; that is, they must\n        # only consist of PyTorch operations or autograd.Function.\n        #\n        # For example, this allows us to do double backwards and/or compute\n        # second order gradients.\n        #\n        # We've written the backward pass of NumpySort in terms of another\n        # autograd.Function, NumpyTake.\n        ind, ind_inv = ctx.saved_tensors\n        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\nclass NumpyTake(torch.autograd.Function):\n    @staticmethod\n    def forward(x, ind, ind_inv, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = to_numpy(ind)\n        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, ind, ind_inv, dim = inputs\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ind, ind_inv = ctx.saved_tensors\n        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n        return result, None, None, None \n```", "```py\ndef numpy_sort(x, dim=-1):\n    result, _, _ = NumpySort.apply(x, dim)\n    return result \n```", "```py\nx = torch.randn(2, 3)\ngrad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x)\nassert torch.allclose(grad_x, torch.ones_like(x)) \n```", "```py\nclass MyCube(torch.autograd.Function):\n    @staticmethod\n    def forward(x):\n        result = x ** 3\n        # In regular PyTorch, if we had just run y = x ** 3, then the backward\n        # pass computes dx = 3 * x ** 2\\. In this autograd.Function, we've done\n        # that computation here in the forward pass instead.\n        dx = 3 * x ** 2\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`.\n        result = grad_output * dx + grad_dx * 6 * x\n        return result \n```", "```py\ndef my_cube(x):\n    result, _ = MyCube.apply(x)\n    return result \n```", "```py\nx = torch.randn([])\nggx = torch.func.grad(torch.func.grad(my_cube))(x)\nassert torch.allclose(ggx, 6 * x) \n```", "```py\nclass MyCube(torch.autograd.Function):\n    # Set generate_vmap_rule to True to ask PyTorch to automatically generate\n    # a vmap rule.\n    generate_vmap_rule = True\n\n    @staticmethod\n    def forward(x):\n        result = x ** 3\n        dx = 3 * x ** 2\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\ndef my_cube(x):\n    result, dx = MyCube.apply(x)\n    return result\n\nx = torch.randn(3)\nresult = torch.vmap(my_cube)(x)\nassert torch.allclose(result, x ** 3) \n```", "```py\ndef to_numpy(tensor):\n    return tensor.cpu().numpy()\n\nclass NumpySort(torch.autograd.Function):\n    @staticmethod\n    def forward(x, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = np.argsort(x, axis=dim)\n        ind_inv = np.argsort(ind, axis=dim)\n        result = np.take_along_axis(x, ind, axis=dim)\n        return (\n            torch.tensor(result, device=device),\n            torch.tensor(ind, device=device),\n            torch.tensor(ind_inv, device=device),\n        )\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, dim = inputs\n        _, ind, ind_inv = output\n        ctx.mark_non_differentiable(ind, ind_inv)\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output, _0, _1):\n        ind, ind_inv = ctx.saved_tensors\n        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\n    # The signature of the vmap staticmethod is:\n    # vmap(info, in_dims: Tuple[Optional[int]], *args)\n    # where *args is the same as the arguments to `forward`.\n    @staticmethod\n    def vmap(info, in_dims, x, dim):\n        # For every input (x and dim), in_dims stores an Optional[int]\n        # that is:\n        # - None if the input is not being vmapped over or if the input\n        #   is not a Tensor\n        # - an integer if the input is being vmapped over that represents\n        #   the index of the dimension being vmapped over.\n        x_bdim, _ = in_dims\n\n        # A \"vmap rule\" is the logic of how to perform the operation given\n        # inputs with one additional dimension. In NumpySort, x has an\n        # additional dimension (x_bdim). The vmap rule is simply\n        # to call NumpySort again but pass it a different `dim`.\n        x = x.movedim(x_bdim, 0)\n        # Handle negative dims correctly\n        dim = dim if dim >= 0 else dim + x.dim() - 1\n        result = NumpySort.apply(x, dim + 1)\n\n        # The vmap rule must return a tuple of two things\n        # 1\\. the output. Should be the same amount of things\n        #    as returned by the forward().\n        # 2\\. one Optional[int] for each output specifying if each output\n        # is being vmapped over, and if so, the index of the\n        # dimension being vmapped over.\n        #\n        # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the\n        # dimension being vmapped over to the front of `x`, that appears at\n        # dimension 0 of all outputs.\n        # The return is (output, out_dims) -- output is a tuple of 3 Tensors\n        # and out_dims is a Tuple of 3 Optional[int]\n        return NumpySort.apply(x, dim + 1), (0, 0, 0)\n\nclass NumpyTake(torch.autograd.Function):\n    @staticmethod\n    def forward(x, ind, ind_inv, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = to_numpy(ind)\n        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, ind, ind_inv, dim = inputs\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ind, ind_inv = ctx.saved_tensors\n        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n        return result, None, None, None\n\n    @staticmethod\n    def vmap(info, in_dims, x, ind, ind_inv, dim):\n        x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims\n\n        # The strategy is: expand {x, ind, ind_inv} to all have the dimension\n        # being vmapped over.\n        # Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim).\n\n        # Handle negative dims by wrapping them to be positive\n        logical_dim = x.dim() if x_bdim is None else x_bdim - 1\n        dim = dim if dim >= 0 else dim + logical_dim\n\n        def maybe_expand_bdim_at_front(x, x_bdim):\n            if x_bdim is None:\n                return x.expand(info.batch_size, *x.shape)\n            return x.movedim(x_bdim, 0)\n\n        # If the Tensor doesn't have the dimension being vmapped over,\n        # expand it out. Otherwise, move it to the front of the Tensor\n        x = maybe_expand_bdim_at_front(x, x_bdim)\n        ind = maybe_expand_bdim_at_front(ind, ind_bdim)\n        ind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim)\n\n        # The return is a tuple (output, out_dims). Since output is a Tensor,\n        # then out_dims is an Optional[int] (instead of being a Tuple).\n        return NumpyTake.apply(x, ind, ind_inv, dim + 1), 0\n\ndef numpy_sort(x, dim=-1):\n    result, _, _ = NumpySort.apply(x, dim)\n    return result\n\nx = torch.randn(2, 3)\nresult = torch.vmap(numpy_sort)(x)\nassert torch.allclose(result, numpy_sort(result, 1)) \n```"]