- en: Fast Transformer Inference with Better Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html](https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Author**: [Michael Gschwind](https://github.com/mikekgfb)'
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial introduces Better Transformer (BT) as part of the PyTorch 1.12
    release. In this tutorial, we show how to use Better Transformer for production
    inference with torchtext. Better Transformer is a production ready fastpath to
    accelerate deployment of Transformer models with high performance on CPU and GPU.
    The fastpath feature works transparently for models based either directly on PyTorch
    core `nn.module` or with torchtext.
  prefs: []
  type: TYPE_NORMAL
- en: Models which can be accelerated by Better Transformer fastpath execution are
    those using the following PyTorch core `torch.nn.module` classes `TransformerEncoder`,
    `TransformerEncoderLayer`, and `MultiHeadAttention`. In addition, torchtext has
    been updated to use the core library modules to benefit from fastpath acceleration.
    (Additional modules may be enabled with fastpath execution in the future.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Better Transformer offers two types of acceleration:'
  prefs: []
  type: TYPE_NORMAL
- en: Native multihead attention (MHA) implementation for CPU and GPU to improve overall
    execution efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploiting sparsity in NLP inference. Because of variable input lengths, input
    tokens may contain a large number of padding tokens for which processing may be
    skipped, delivering significant speedups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fastpath execution is subject to some criteria. Most importantly, the model
    must be executed in inference mode and operate on input tensors that do not collect
    gradient tape information (e.g., running with torch.no_grad).
  prefs: []
  type: TYPE_NORMAL
- en: To follow this example in Google Colab, [click here](https://colab.research.google.com/drive/1KZnMJYhYkOMYtNIX5S3AGIYnjyG0AojN?usp=sharing).
  prefs: []
  type: TYPE_NORMAL
- en: Better Transformer Features in This Tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Load pretrained models (created before PyTorch version 1.12 without Better Transformer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run and benchmark inference on CPU with and without BT fastpath (native MHA
    only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run and benchmark inference on (configurable) DEVICE with and without BT fastpath
    (native MHA only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable sparsity support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run and benchmark inference on (configurable) DEVICE with and without BT fastpath
    (native MHA + sparsity)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Additional information about Better Transformer may be found in the PyTorch.Org
    blog [A Better Transformer for Fast Transformer Inference](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference//).
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1.1 Load pretrained models
  prefs: []
  type: TYPE_NORMAL
- en: We download the XLM-R model from the predefined torchtext models by following
    the instructions in [torchtext.models](https://pytorch.org/text/main/models.html).
    We also set the DEVICE to execute on-accelerator tests. (Enable GPU execution
    for your environment as appropriate.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 1.2 Dataset Setup
  prefs: []
  type: TYPE_NORMAL
- en: 'We set up two types of inputs: a small input batch and a big input batch with
    sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, we select either the small or large input batch, preprocess the inputs
    and test the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we set the benchmark iteration count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Execution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2.1 Run and benchmark inference on CPU with and without BT fastpath (native
    MHA only)
  prefs: []
  type: TYPE_NORMAL
- en: 'We run the model on CPU, and collect profile information:'
  prefs: []
  type: TYPE_NORMAL
- en: The first run uses traditional (“slow path”) execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second run enables BT fastpath execution by putting the model in inference
    mode using model.eval() and disables gradient collection with torch.no_grad().
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see an improvement (whose magnitude will depend on the CPU model) when
    the model is executing on CPU. Notice that the fastpath profile shows most of
    the execution time in the native TransformerEncoderLayer implementation aten::_transformer_encoder_layer_fwd.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 2.2 Run and benchmark inference on (configurable) DEVICE with and without BT
    fastpath (native MHA only)
  prefs: []
  type: TYPE_NORMAL
- en: 'We check the BT sparsity setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We disable the BT sparsity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We run the model on DEVICE, and collect profile information for native MHA
    execution on DEVICE:'
  prefs: []
  type: TYPE_NORMAL
- en: The first run uses traditional (“slow path”) execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second run enables BT fastpath execution by putting the model in inference
    mode using model.eval() and disables gradient collection with torch.no_grad().
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When executing on a GPU, you should see a significant speedup, in particular
    for the small input batch setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 2.3 Run and benchmark inference on (configurable) DEVICE with and without BT
    fastpath (native MHA + sparsity)
  prefs: []
  type: TYPE_NORMAL
- en: 'We enable sparsity support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We run the model on DEVICE, and collect profile information for native MHA
    and sparsity support execution on DEVICE:'
  prefs: []
  type: TYPE_NORMAL
- en: The first run uses traditional (“slow path”) execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second run enables BT fastpath execution by putting the model in inference
    mode using model.eval() and disables gradient collection with torch.no_grad().
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When executing on a GPU, you should see a significant speedup, in particular
    for the large input batch setting which includes sparsity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we have introduced fast transformer inference with Better
    Transformer fastpath execution in torchtext using PyTorch core Better Transformer
    support for Transformer Encoder models. We have demonstrated the use of Better
    Transformer with models trained prior to the availability of BT fastpath execution.
    We have demonstrated and benchmarked the use of both BT fastpath execution modes,
    native MHA execution and BT sparsity acceleration.
  prefs: []
  type: TYPE_NORMAL
