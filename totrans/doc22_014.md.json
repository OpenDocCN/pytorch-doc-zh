["```py\n# Inherit from Function\nclass LinearFunction(Function):\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(input, weight, bias):\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    # inputs is a Tuple of all of the inputs passed to forward.\n    # output is the output of the forward().\n    def setup_context(ctx, inputs, output):\n        input, weight, bias = inputs\n        ctx.save_for_backward(input, weight, bias)\n\n    # This function has only a single output, so it gets only one gradient\n    @staticmethod\n    def backward(ctx, grad_output):\n        # This is a pattern that is very convenient - at the top of backward\n        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n        # None. Thanks to the fact that additional trailing Nones are\n        # ignored, the return statement is simple even when the function has\n        # optional inputs.\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        # These needs_input_grad checks are optional and there only to\n        # improve efficiency. If you want to make your code simpler, you can\n        # skip them. Returning gradients for inputs that don't require it is\n        # not an error.\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n\n        return grad_input, grad_weight, grad_bias \n```", "```py\n# Option 1: alias\nlinear = LinearFunction.apply\n\n# Option 2: wrap in a function, to support default args and keyword args.\ndef linear(input, weight, bias=None):\n    return LinearFunction.apply(input, weight, bias) \n```", "```py\nclass MulConstant(Function):\n    @staticmethod\n    def forward(tensor, constant):\n        return tensor * constant\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        # ctx is a context object that can be used to stash information\n        # for backward computation\n        tensor, constant = inputs\n        ctx.constant = constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None \n```", "```py\nclass MulConstant(Function):\n    @staticmethod\n    def forward(tensor, constant):\n        return tensor * constant\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        tensor, constant = inputs\n        ctx.set_materialize_grads(False)\n        ctx.constant = constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Here we must handle None grad_output tensor. In this case we\n        # can skip unnecessary computations and just return None.\n        if grad_output is None:\n            return None, None\n\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None \n```", "```py\nclass MyCube(torch.autograd.Function):\n    @staticmethod\n    def forward(x):\n        # We wish to save dx for backward. In order to do so, it must\n        # be returned as an output.\n        dx = 3 * x ** 2\n        result = x ** 3\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`,\n        # which is grad_dx * 6 * x.\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\n# Wrap MyCube in a function so that it is clearer what the output is\ndef my_cube(x):\n    result, dx = MyCube.apply(x)\n    return result \n```", "```py\nfrom torch.autograd import gradcheck\n\n# gradcheck takes a tuple of tensors as input, check if your gradient\n# evaluated with these tensors are close enough to numerical\n# approximations and returns True if they all verify this condition.\ninput = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\ntest = gradcheck(linear, input, eps=1e-6, atol=1e-4)\nprint(test) \n```", "```py\nclass LinearFunction(Function):\n    @staticmethod\n    # ctx is the first argument to forward\n    def forward(ctx, input, weight, bias=None):\n        # The forward pass can use ctx.\n        ctx.save_for_backward(input, weight, bias)\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n\n        return grad_input, grad_weight, grad_bias \n```", "```py\nclass Linear(nn.Module):\n    def __init__(self, input_features, output_features, bias=True):\n        super().__init__()\n        self.input_features = input_features\n        self.output_features = output_features\n\n        # nn.Parameter is a special kind of Tensor, that will get\n        # automatically registered as Module's parameter once it's assigned\n        # as an attribute. Parameters and buffers need to be registered, or\n        # they won't appear in .parameters() (doesn't apply to buffers), and\n        # won't be converted when e.g. .cuda() is called. You can use\n        # .register_buffer() to register buffers.\n        # nn.Parameters require gradients by default.\n        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(output_features))\n        else:\n            # You should always register all possible parameters, but the\n            # optional ones can be None if you want.\n            self.register_parameter('bias', None)\n\n        # Not a very smart way to initialize weights\n        nn.init.uniform_(self.weight, -0.1, 0.1)\n        if self.bias is not None:\n            nn.init.uniform_(self.bias, -0.1, 0.1)\n\n    def forward(self, input):\n        # See the autograd section for explanation of what happens here.\n        return LinearFunction.apply(input, self.weight, self.bias)\n\n    def extra_repr(self):\n        # (Optional)Set the extra information about this module. You can test\n        # it by printing an object of this class.\n        return 'input_features={}, output_features={}, bias={}'.format(\n            self.input_features, self.output_features, self.bias is not None\n        ) \n```", "```py\nclass ScalarTensor(object):\n   def __init__(self, N, value):\n       self._N = N\n       self._value = value\n\n   def __repr__(self):\n       return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n   def tensor(self):\n       return self._value * torch.eye(self._N) \n```", "```py\n>>> d = ScalarTensor(5, 2)\n>>> d\nScalarTensor(N=5, value=2)\n>>> d.tensor()\ntensor([[2., 0., 0., 0., 0.],\n [0., 2., 0., 0., 0.],\n [0., 0., 2., 0., 0.],\n [0., 0., 0., 2., 0.],\n [0., 0., 0., 0., 2.]]) \n```", "```py\n>>> import torch\n>>> torch.mean(d)\nTypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor \n```", "```py\nHANDLED_FUNCTIONS = {}\nclass ScalarTensor(object):\n    def __init__(self, N, value):\n        self._N = N\n        self._value = value\n\n    def __repr__(self):\n        return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n    def tensor(self):\n        return self._value * torch.eye(self._N)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        if func not in HANDLED_FUNCTIONS or not all(\n            issubclass(t, (torch.Tensor, ScalarTensor))\n            for t in types\n        ):\n            return NotImplemented\n        return HANDLED_FUNCTIONS[func](*args, **kwargs) \n```", "```py\nimport functools\ndef implements(torch_function):\n  \"\"\"Register a torch function override for ScalarTensor\"\"\"\n    def decorator(func):\n        functools.update_wrapper(func, torch_function)\n        HANDLED_FUNCTIONS[torch_function] = func\n        return func\n    return decorator \n```", "```py\n@implements(torch.mean)\ndef mean(input):\n    return float(input._value) / input._N \n```", "```py\n>>> d = ScalarTensor(5, 2)\n>>> torch.mean(d)\n0.4 \n```", "```py\ndef ensure_tensor(data):\n    if isinstance(data, ScalarTensor):\n        return data.tensor()\n    return torch.as_tensor(data)\n\n@implements(torch.add)\ndef add(input, other):\n   try:\n       if input._N == other._N:\n           return ScalarTensor(input._N, input._value + other._value)\n       else:\n           raise ValueError(\"Shape mismatch!\")\n   except AttributeError:\n       return torch.add(ensure_tensor(input), ensure_tensor(other)) \n```", "```py\n>>> s = ScalarTensor(2, 2)\n>>> torch.add(s, s)\nScalarTensor(N=2, value=4)\n>>> t = torch.tensor([[1, 1,], [1, 1]])\n>>> torch.add(s, t)\ntensor([[3., 1.],\n [1., 3.]]) \n```", "```py\n>>> torch.add(s, s, alpha=2)\nTypeError: add() got an unexpected keyword argument 'alpha' \n```", "```py\n>>> torch.mul(s, 3)\nTypeError: no implementation found for 'torch.mul' on types that\nimplement __torch_function__: [ScalarTensor] \n```", "```py\n@classmethod\ndef __torch_function__(cls, func, types, args=(), kwargs=None):\n    if kwargs is None:\n        kwargs = {}\n    if func not in HANDLED_FUNCTIONS or not all(\n            issubclass(t, (torch.Tensor, ScalarTensor))\n            for t in types\n        ):\n        args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]\n        return func(*args, **kwargs)\n    return HANDLED_FUNCTIONS[func](*args, **kwargs) \n```", "```py\n>>> s = ScalarTensor(2, 2)\n>>> torch.mul(s, s)\ntensor([[4., 0.],\n [0., 4.]]) \n```", "```py\n>>> class SubTensor(torch.Tensor):\n...     pass\n>>> type(torch.add(SubTensor([0]), SubTensor([1]))).__name__\n'SubTensor'\n>>> type(torch.add(SubTensor([0]), torch.tensor([1]))).__name__\n'SubTensor' \n```", "```py\n>>> type(torch.add(SubTensor2([0]), SubTensor([1]))).__name__\n'SubTensor2'\n>>> type(torch.add(SubTensor2([0]), torch.tensor([1]))).__name__\n'SubTensor2'\n>>> torch.add(SubTensor([0]), OtherSubTensor([1]))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor] \n```", "```py\nclass LoggingTensor(torch.Tensor):\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion\n        if func is not torch.Tensor.__repr__:\n            logging.info(f\"func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}\")\n        if kwargs is None:\n            kwargs = {}\n        return super().__torch_function__(func, types, args, kwargs) \n```", "```py\nclass MetadataTensor(object):\n    def __init__(self, data, metadata=None, **kwargs):\n        self._t = torch.as_tensor(data, **kwargs)\n        self._metadata = metadata\n\n    def __repr__(self):\n        return \"Metadata:\\n{}\\n\\ndata:\\n{}\".format(self._metadata, self._t)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata'))\n        args = [getattr(a, '_t', a) for a in args]\n        assert len(metadatas) > 0\n        ret = func(*args, **kwargs)\n        return MetadataTensor(ret, metadata=metadatas[0]) \n```", "```py\n>>> metadata = {'owner': 'Ministry of Silly Walks'}\n>>> m = MetadataTensor([[1, 2], [3, 4]], metadata=metadata)\n>>> t = torch.tensor([[1, 2], [1, 2]])\n>>> torch.add(t, m)\nMetadata:\n{'owner': 'Ministry of Silly Walks'}\n\ndata:\ntensor([[2, 4],\n [4, 6]])\n>>> torch.mul(t, m)\nMetadata:\n{'owner': 'Ministry of Silly Walks'}\n\ndata:\ntensor([[1, 4],\n [3, 8]]) \n```", "```py\n>>> from torch.overrides import get_overridable_functions\n>>> func_dict = get_overridable_functions()\n>>> nn_funcs = func_dict[torch.nn.functional]\n>>> print([f.__name__ for f in nn_funcs[:5])\n['adaptive_avg_pool1d', 'adaptive_avg_pool2d', 'adaptive_avg_pool3d',\n 'adaptive_max_pool1d', 'adaptive_max_pool1d_with_indices'] \n```", "```py\n>>> import inspect\n>>> from torch.overrides import get_testing_overrides\n>>> override_dict = get_testing_overrides()\n>>> dummy_add = override_dict[torch.add]\n>>> inspect.signature(dummy_add)\n<Signature (input, other, out=None)> \n```", "```py\nimport torch\nfrom torch.overrides import TorchFunctionMode, resolve_name\nfrom torch.utils._python_dispatch import TorchDispatchMode\n\nclass FunctionLog(TorchFunctionMode):\n    def __torch_function__(self, func, types, args, kwargs=None):\n        print(f\"Function Log: {resolve_name(func)}(*{args}, **{kwargs})\")\n        return func(*args, **(kwargs or {}))\n\nclass DispatchLog(TorchDispatchMode):\n    def __torch_dispatch__(self, func, types, args, kwargs=None):\n        print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\")\n        return func(*args, **(kwargs or {}))\n\ndef f():\n    a = torch.rand(10, requires_grad=True)\n    b = a * 2\n    b.sum().backward()\n\nprint(\"TorchFunctionMode logging:\")\nwith FunctionLog():\n    f()\n\nprint(\"TorchDispatchMode logging:\")\nwith DispatchLog():\n    f() \n```", "```py\nTorchFunctionMode logging:\nFunction Log: torch.rand(*(10,), **{'requires_grad': True})\nFunction Log: torch.Tensor.mul(*(tensor([0.7164, 0.9897, 0.1745, 0.9336, 0.4287, 0.7989, 0.2169, 0.7474, 0.5624,\n        0.5970], requires_grad=True), 2), **None)\nFunction Log: torch.Tensor.sum(*(tensor([1.4328, 1.9794, 0.3490, 1.8671, 0.8573, 1.5977, 0.4338, 1.4948, 1.1249,\n        1.1939], grad_fn=<MulBackward0>),), **None)\n# Note that at the python level, we only see the call to backward but not what happens in the autograd engine.\nFunction Log: torch.Tensor.backward(*(tensor(12.3307, grad_fn=<SumBackward0>),), **{'gradient': None, 'retain_graph': None, 'create_graph': False, 'inputs': None})\n\nTorchDispatchMode logging:\n# Here the requires_grad flag from autograd is removed while default arguments were populated.\nDispatch Log: aten.rand.default(*([10],), **{'device': device(type='cpu'), 'pin_memory': False})\nDispatch Log: aten.mul.Tensor(*(tensor([0.2151, 0.6018, 0.8415, 0.9060, 0.2974, 0.7708, 0.6668, 0.0352, 0.7948,\n        0.6023], requires_grad=True), 2), **{})\nDispatch Log: aten.sum.default(*(tensor([0.4303, 1.2036, 1.6831, 1.8120, 0.5949, 1.5416, 1.3335, 0.0705, 1.5897,\n        1.2046], grad_fn=<MulBackward0>),), **{})\n# Here we don't see the call to backward itself, but its constituents. Starting here with the factory function that creates the initial gradient.\nDispatch Log: aten.ones_like.default(*(tensor(11.4637, grad_fn=<SumBackward0>),), **{'pin_memory': False, 'memory_format': torch.preserve_format})\n# This is the backward of the sum\nDispatch Log: aten.expand.default(*(tensor(1.), [10]), **{})\nDispatch Log: aten.mul.Tensor(*(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 2), **{})\nDispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})\nDispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{}) \n```"]