["```py\n>>> t = torch.tensor([1., 2.])\n>>> torch.save(t, 'tensor.pt')\n>>> torch.load('tensor.pt')\ntensor([1., 2.]) \n```", "```py\n>>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}\n>>> torch.save(d, 'tensor_dict.pt')\n>>> torch.load('tensor_dict.pt')\n{'a': tensor([1., 2.]), 'b': tensor([3., 4.])} \n```", "```py\n>>> numbers = torch.arange(1, 10)\n>>> evens = numbers[1::2]\n>>> torch.save([numbers, evens], 'tensors.pt')\n>>> loaded_numbers, loaded_evens = torch.load('tensors.pt')\n>>> loaded_evens *= 2\n>>> loaded_numbers\ntensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9]) \n```", "```py\n>>> large = torch.arange(1, 1000)\n>>> small = large[0:5]\n>>> torch.save(small, 'small.pt')\n>>> loaded_small = torch.load('small.pt')\n>>> loaded_small.storage().size()\n999 \n```", "```py\n>>> large = torch.arange(1, 1000)\n>>> small = large[0:5]\n>>> torch.save(small.clone(), 'small.pt')  # saves a clone of small\n>>> loaded_small = torch.load('small.pt')\n>>> loaded_small.storage().size()\n5 \n```", "```py\n>>> bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n>>> list(bn.named_parameters())\n[('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),\n ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]\n\n>>> list(bn.named_buffers())\n[('running_mean', tensor([0., 0., 0.])),\n ('running_var', tensor([1., 1., 1.])),\n ('num_batches_tracked', tensor(0))]\n\n>>> bn.state_dict()\nOrderedDict([('weight', tensor([1., 1., 1.])),\n ('bias', tensor([0., 0., 0.])),\n ('running_mean', tensor([0., 0., 0.])),\n ('running_var', tensor([1., 1., 1.])),\n ('num_batches_tracked', tensor(0))]) \n```", "```py\n>>> torch.save(bn.state_dict(), 'bn.pt')\n>>> bn_state_dict = torch.load('bn.pt')\n>>> new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n>>> new_bn.load_state_dict(bn_state_dict)\n<All keys matched successfully> \n```", "```py\n# A module with two linear layers\n>>> class MyModule(torch.nn.Module):\n      def __init__(self):\n        super().__init__()\n        self.l0 = torch.nn.Linear(4, 2)\n        self.l1 = torch.nn.Linear(2, 1)\n\n      def forward(self, input):\n        out0 = self.l0(input)\n        out0_relu = torch.nn.functional.relu(out0)\n        return self.l1(out0_relu)\n\n>>> m = MyModule()\n>>> m.state_dict()\nOrderedDict([('l0.weight', tensor([[ 0.1400, 0.4563, -0.0271, -0.4406],\n                                   [-0.3289, 0.2827, 0.4588, 0.2031]])),\n             ('l0.bias', tensor([ 0.0300, -0.1316])),\n             ('l1.weight', tensor([[0.6533, 0.3413]])),\n             ('l1.bias', tensor([-0.1112]))])\n\n>>> torch.save(m.state_dict(), 'mymodule.pt')\n>>> m_state_dict = torch.load('mymodule.pt')\n>>> new_m = MyModule()\n>>> new_m.load_state_dict(m_state_dict)\n<All keys matched successfully> \n```", "```py\n>>> scripted_module = torch.jit.script(MyModule())\n>>> torch.jit.save(scripted_module, 'mymodule.pt')\n>>> torch.jit.load('mymodule.pt')\nRecursiveScriptModule( original_name=MyModule\n (l0): RecursiveScriptModule(original_name=Linear)\n (l1): RecursiveScriptModule(original_name=Linear) ) \n```", "```py\n# A module with control flow\n>>> class ControlFlowModule(torch.nn.Module):\n      def __init__(self):\n        super().__init__()\n        self.l0 = torch.nn.Linear(4, 2)\n        self.l1 = torch.nn.Linear(2, 1)\n\n      def forward(self, input):\n        if input.dim() > 1:\n            return torch.tensor(0)\n\n        out0 = self.l0(input)\n        out0_relu = torch.nn.functional.relu(out0)\n        return self.l1(out0_relu)\n\n>>> traced_module = torch.jit.trace(ControlFlowModule(), torch.randn(4))\n>>> torch.jit.save(traced_module, 'controlflowmodule_traced.pt')\n>>> loaded = torch.jit.load('controlflowmodule_traced.pt')\n>>> loaded(torch.randn(2, 4)))\ntensor([[-0.1571], [-0.3793]], grad_fn=<AddBackward0>)\n\n>>> scripted_module = torch.jit.script(ControlFlowModule(), torch.randn(4))\n>>> torch.jit.save(scripted_module, 'controlflowmodule_scripted.pt')\n>>> loaded = torch.jit.load('controlflowmodule_scripted.pt')\n>> loaded(torch.randn(2, 4))\ntensor(0) \n```", "```py\n>>> torch::jit::script::Module module;\n>>> module = torch::jit::load('controlflowmodule_scripted.pt'); \n```", "```py\n# PyTorch 1.5 (and earlier)\n>>> a = torch.tensor(5)\n>>> b = torch.tensor(3)\n>>> a / b\ntensor(1) \n```", "```py\n# PyTorch 1.7\n>>> a = torch.tensor(5)\n>>> b = torch.tensor(3)\n>>> a / b\ntensor(1.6667) \n```", "```py\n# PyTorch 1.5 and earlier\n>>> torch.full((3,), 1)  # Note the integer fill value...\ntensor([1., 1., 1.])     # ...but float tensor! \n```", "```py\n# PyTorch 1.7\n>>> torch.full((3,), 1)\ntensor([1, 1, 1])\n\n>>> torch.full((3,), True)\ntensor([True, True, True])\n\n>>> torch.full((3,), 1.)\ntensor([1., 1., 1.])\n\n>>> torch.full((3,), 1 + 1j)\ntensor([1.+1.j, 1.+1.j, 1.+1.j]) \n```", "```py\ntorch.serialization.register_package(priority, tagger, deserializer)\u00b6\n```", "```py\n>>> def ipu_tag(obj):\n>>>     if obj.device.type == 'ipu':\n>>>         return 'ipu'\n>>> def ipu_deserialize(obj, location):\n>>>     if location.startswith('ipu'):\n>>>         ipu = getattr(torch, \"ipu\", None)\n>>>         assert ipu is not None, \"IPU device module is not loaded\"\n>>>         assert torch.ipu.is_available(), \"ipu is not available\"\n>>>         return obj.ipu(location)\n>>> torch.serialization.register_package(11, ipu_tag, ipu_deserialize) \n```", "```py\ntorch.serialization.get_default_load_endianness()\u00b6\n```", "```py\ntorch.serialization.set_default_load_endianness(endianness)\u00b6\n```"]