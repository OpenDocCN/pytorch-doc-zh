- en: torch._logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/logging.html](https://pytorch.org/docs/stable/logging.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PyTorch has a configurable logging system, where different components can be
    given different log level settings. For instance, one component’s log messages
    can be completely disabled, while another component’s log messages can be set
    to maximum verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This feature is a prototype and may have compatibility breaking changes in the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This feature has not been expanded to control the log messages of all components
    in PyTorch yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to configure the logging system: through the environment
    variable `TORCH_LOGS` or the python API torch._logging.set_logs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`set_logs`](generated/torch._logging.set_logs.html#torch._logging.set_logs
    "torch._logging.set_logs") | Sets the log level for individual components and
    toggles individual log artifact types. |'
  prefs: []
  type: TYPE_TB
- en: 'The environment variable `TORCH_LOGS` is a comma-separated list of `[+-]<component>`
    pairs, where `<component>` is a component specified below. The `+` prefix will
    decrease the log level of the component, displaying more log messages while the
    `-` prefix will increase the log level of the component and display fewer log
    messages. The default setting is the behavior when a component is not specified
    in `TORCH_LOGS`. In addition to components, there are also artifacts. Artifacts
    are specific pieces of debug information associated with a component that are
    either displayed or not displayed, so prefixing an artifact with `+` or `-` will
    be a no-op. Since they are associated with a component, enabling that component
    will typically also enable that artifact, unless that artifact was specified to
    be off_by_default. This option is specified in _registrations.py for artifacts
    that are so spammy they should only be displayed when explicitly enabled. The
    following components and artifacts are configurable through the `TORCH_LOGS` environment
    variable (see torch._logging.set_logs for the python API):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`all`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Special component which configures the default log level of all components.
    Default: `logging.WARN`'
  prefs: []
  type: TYPE_NORMAL
- en: '`dynamo`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The log level for the TorchDynamo component. Default: `logging.WARN`'
  prefs: []
  type: TYPE_NORMAL
- en: '`aot`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The log level for the AOTAutograd component. Default: `logging.WARN`'
  prefs: []
  type: TYPE_NORMAL
- en: '`inductor`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The log level for the TorchInductor component. Default: `logging.WARN`'
  prefs: []
  type: TYPE_NORMAL
- en: '`your.custom.module`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The log level for an arbitrary unregistered module. Provide the fully qualified
    name and the module will be enabled. Default: `logging.WARN`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Artifacts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bytecode`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit the original and generated bytecode from TorchDynamo. Default:
    `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`aot_graphs`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit the graphs generated by AOTAutograd. Default: `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`aot_joint_graph`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit the joint forward-backward graph generated by AOTAutograd.
    Default: `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`compiled_autograd`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit logs from compiled_autograd. Defaults: `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ddp_graphs`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit graphs generated by DDPOptimizer. Default: `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`graph`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit the graph captured by TorchDynamo in tabular format. Default:
    `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`graph_code`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit the python source of the graph captured by TorchDynamo. Default:
    `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`graph_breaks`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit a message when a unique graph break is encountered during TorchDynamo
    tracing. Default: `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`guards`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit the guards generated by TorchDynamo for each compiled function.
    Default: `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`recompiles`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit a guard failure reason and message every time TorchDynamo recompiles
    a function. Default: `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`output_code`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit the TorchInductor output code. Default: `False`'
  prefs: []
  type: TYPE_NORMAL
- en: '`schedule`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether to emit the TorchInductor schedule. Default: `False`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TORCH_LOGS="+dynamo,aot"` will set the log level of TorchDynamo to `logging.DEBUG`
    and AOT to `logging.INFO`'
  prefs: []
  type: TYPE_NORMAL
- en: '`TORCH_LOGS="-dynamo,+inductor"` will set the log level of TorchDynamo to `logging.ERROR`
    and TorchInductor to `logging.DEBUG`'
  prefs: []
  type: TYPE_NORMAL
- en: '`TORCH_LOGS="aot_graphs"` will enable the `aot_graphs` artifact'
  prefs: []
  type: TYPE_NORMAL
- en: '`TORCH_LOGS="+dynamo,schedule"` will enable set the log level of TorchDynamo
    to `logging.DEBUG` and enable the `schedule` artifact'
  prefs: []
  type: TYPE_NORMAL
- en: '`TORCH_LOGS="+some.random.module,schedule"` will set the log level of some.random.module
    to `logging.DEBUG` and enable the `schedule` artifact'
  prefs: []
  type: TYPE_NORMAL
