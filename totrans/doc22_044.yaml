- en: torch.backends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/backends.html](https://pytorch.org/docs/stable/backends.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: torch.backends controls the behavior of various backends that PyTorch supports.
  prefs: []
  type: TYPE_NORMAL
- en: 'These backends include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.backends.cpu`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.backends.cuda`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.backends.cudnn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.backends.mps`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.backends.mkl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.backends.mkldnn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.backends.openmp`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.backends.opt_einsum`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.backends.xeon`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '## torch.backends.cpu'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Return cpu capability as a string value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Possible values: - “DEFAULT” - “VSX” - “Z VECTOR” - “NO AVX” - “AVX2” - “AVX512”'
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)")  ##
    torch.backends.cuda'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Return whether PyTorch is built with CUDA support.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this doesn’t necessarily mean CUDA is available; just that if this
    PyTorch binary were run on a machine with working CUDA drivers and devices, we
    would be able to use it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that controls whether TensorFloat-32 tensor cores may be used in matrix
    multiplications on Ampere or newer GPUs. See [TensorFloat-32 (TF32) on Ampere
    (and later) devices](notes/cuda.html#tf32-on-ampere).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that controls whether reduced precision reductions (e.g., with fp16 accumulation
    type) are allowed with fp16 GEMMs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that controls whether reduced precision reductions are allowed with bf16
    GEMMs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`cufft_plan_cache` contains the cuFFT plan caches for each CUDA device. Query
    a specific device i’s cache via torch.backends.cuda.cufft_plan_cache[i].'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A readonly [`int`](https://docs.python.org/3/library/functions.html#int "(in
    Python v3.12)") that shows the number of plans currently in a cuFFT plan cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: A [`int`](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)")
    that controls the capacity of a cuFFT plan cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Clears a cuFFT plan cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Override the heuristic PyTorch uses to choose between cuSOLVER and MAGMA for
    CUDA linear algebra operations.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This flag is experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER
    or MAGMA libraries, and if both are available it decides which to use with a heuristic.
    This flag (a [`str`](https://docs.python.org/3/library/stdtypes.html#str "(in
    Python v3.12)")) allows overriding those heuristics.
  prefs: []
  type: TYPE_NORMAL
- en: If “cusolver” is set then cuSOLVER will be used wherever possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If “magma” is set then MAGMA will be used wherever possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If “default” (the default) is set then heuristics will be used to pick between
    cuSOLVER and MAGMA if both are available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When no input is given, this function returns the currently preferred library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set
    the preferred library to cuSOLVER globally. This flag only sets the initial value
    of the preferred library and the preferred library may still be overridden by
    this function call later in your script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: When a library is preferred other libraries may still be used if the
    preferred library doesn’t implement the operation(s) called. This flag may achieve
    better performance if PyTorch’s heuristic library selection is incorrect for your
    application’s inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently supported linalg operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[`torch.linalg.inv()`](generated/torch.linalg.inv.html#torch.linalg.inv "torch.linalg.inv")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.linalg.inv_ex()`](generated/torch.linalg.inv_ex.html#torch.linalg.inv_ex
    "torch.linalg.inv_ex")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.linalg.cholesky()`](generated/torch.linalg.cholesky.html#torch.linalg.cholesky
    "torch.linalg.cholesky")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.linalg.cholesky_ex()`](generated/torch.linalg.cholesky_ex.html#torch.linalg.cholesky_ex
    "torch.linalg.cholesky_ex")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.cholesky_solve()`](generated/torch.cholesky_solve.html#torch.cholesky_solve
    "torch.cholesky_solve")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.cholesky_inverse()`](generated/torch.cholesky_inverse.html#torch.cholesky_inverse
    "torch.cholesky_inverse")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.linalg.lu_factor()`](generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor
    "torch.linalg.lu_factor")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.linalg.lu()`](generated/torch.linalg.lu.html#torch.linalg.lu "torch.linalg.lu")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.linalg.lu_solve()`](generated/torch.linalg.lu_solve.html#torch.linalg.lu_solve
    "torch.linalg.lu_solve")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.linalg.qr()`](generated/torch.linalg.qr.html#torch.linalg.qr "torch.linalg.qr")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.linalg.eigh()`](generated/torch.linalg.eigh.html#torch.linalg.eigh
    "torch.linalg.eigh")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.linalg.eighvals()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.linalg.svd()`](generated/torch.linalg.svd.html#torch.linalg.svd "torch.linalg.svd")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`torch.linalg.svdvals()`](generated/torch.linalg.svdvals.html#torch.linalg.svdvals
    "torch.linalg.svdvals")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '*_LinalgBackend*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: alias of `_SDPBackend`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: alias of `_SDPAParams`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This flag is beta and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: Returns whether flash scaled dot product attention is enabled or not.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This flag is beta and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: Enables or disables memory efficient scaled dot product attention.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This flag is beta and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: Returns whether memory efficient scaled dot product attention is enabled or
    not.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This flag is beta and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: Enables or disables flash scaled dot product attention.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This flag is beta and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: Returns whether math scaled dot product attention is enabled or not.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This flag is beta and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: Enables or disables math scaled dot product attention.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Check if FlashAttention can be utilized in scaled_dot_product_attention.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**params** (*_SDPAParams*) – An instance of SDPAParams containing the tensors
    for query, key, value, an optional attention mask, dropout rate, and a flag indicating
    if the attention is causal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**debug** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in
    Python v3.12)")) – Whether to logging.warn debug information as to why FlashAttention
    could not be run. Defaults to False.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: True if FlashAttention can be used with the given parameters; otherwise, False.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This function is dependent on a CUDA-enabled build of PyTorch. It will return
    False in non-CUDA environments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Check if efficient_attention can be utilized in scaled_dot_product_attention.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**params** (*_SDPAParams*) – An instance of SDPAParams containing the tensors
    for query, key, value, an optional attention mask, dropout rate, and a flag indicating
    if the attention is causal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**debug** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in
    Python v3.12)")) – Whether to logging.warn with information as to why efficient_attention
    could not be run. Defaults to False.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: True if efficient_attention can be used with the given parameters; otherwise,
    False.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This function is dependent on a CUDA-enabled build of PyTorch. It will return
    False in non-CUDA environments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This flag is beta and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: 'This context manager can be used to temporarily enable or disable any of the
    three backends for scaled dot product attention. Upon exiting the context manager,
    the previous state of the flags will be restored.  ## torch.backends.cudnn[](#module-torch.backends.cudnn
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Return the version of cuDNN.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Return a bool indicating if CUDNN is currently available.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that controls whether cuDNN is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that controls where TensorFloat-32 tensor cores may be used in cuDNN
    convolutions on Ampere or newer GPUs. See [TensorFloat-32 (TF32) on Ampere (and
    later) devices](notes/cuda.html#tf32-on-ampere).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that, if True, causes cuDNN to only use deterministic convolution algorithms.
    See also [`torch.are_deterministic_algorithms_enabled()`](generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled
    "torch.are_deterministic_algorithms_enabled") and [`torch.use_deterministic_algorithms()`](generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms
    "torch.use_deterministic_algorithms").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that, if True, causes cuDNN to benchmark multiple convolution algorithms
    and select the fastest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'A [`int`](https://docs.python.org/3/library/functions.html#int "(in Python
    v3.12)") that specifies the maximum number of cuDNN convolution algorithms to
    try when torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to
    try every available algorithm. Note that this setting only affects convolutions
    dispatched via the cuDNN v8 API.  ## torch.backends.mps[](#module-torch.backends.mps
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Return a bool indicating if MPS is currently available.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Return whether PyTorch is built with MPS support.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this doesn’t necessarily mean MPS is available; just that if this
    PyTorch binary were run a machine with working MPS drivers and devices, we would
    be able to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")  ##
    torch.backends.mkl'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Return whether PyTorch is built with MKL support.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: On-demand oneMKL verbosing functionality.
  prefs: []
  type: TYPE_NORMAL
- en: To make it easier to debug performance issues, oneMKL can dump verbose messages
    containing execution information like duration while executing the kernel. The
    verbosing functionality can be invoked via an environment variable named MKL_VERBOSE.
    However, this methodology dumps messages in all steps. Those are a large amount
    of verbose messages. Moreover, for investigating the performance issues, generally
    taking verbose messages for one single iteration is enough. This on-demand verbosing
    functionality makes it possible to control scope for verbose message dumping.
    In the following example, verbose messages will be dumped out for the second inference
    only.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**level** – Verbose level - `VERBOSE_OFF`: Disable verbosing - `VERBOSE_ON`:
    Enable verbosing  ## torch.backends.mkldnn[](#module-torch.backends.mkldnn "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Return whether PyTorch is built with MKL-DNN support.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: On-demand oneDNN (former MKL-DNN) verbosing functionality.
  prefs: []
  type: TYPE_NORMAL
- en: To make it easier to debug performance issues, oneDNN can dump verbose messages
    containing information like kernel size, input data size and execution duration
    while executing the kernel. The verbosing functionality can be invoked via an
    environment variable named DNNL_VERBOSE. However, this methodology dumps messages
    in all steps. Those are a large amount of verbose messages. Moreover, for investigating
    the performance issues, generally taking verbose messages for one single iteration
    is enough. This on-demand verbosing functionality makes it possible to control
    scope for verbose message dumping. In the following example, verbose messages
    will be dumped out for the second inference only.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**level** – Verbose level - `VERBOSE_OFF`: Disable verbosing - `VERBOSE_ON`:
    Enable verbosing - `VERBOSE_ON_CREATION`: Enable verbosing, including oneDNN kernel
    creation  ## torch.backends.openmp[](#module-torch.backends.openmp "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Return whether PyTorch is built with OpenMP support.  ## torch.backends.opt_einsum[](#module-torch.backends.opt_einsum
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Return a bool indicating if opt_einsum is currently available.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Return the opt_einsum package if opt_einsum is currently available, else None.
  prefs: []
  type: TYPE_NORMAL
- en: Return type
  prefs: []
  type: TYPE_NORMAL
- en: '[*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in Python
    v3.12)")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: A :class:`bool` that controls whether opt_einsum is enabled (`True` by default).
    If so, torch.einsum will use opt_einsum ([https://optimized-einsum.readthedocs.io/en/stable/path_finding.html](https://optimized-einsum.readthedocs.io/en/stable/path_finding.html))
    if available to calculate an optimal path of contraction for faster performance.
  prefs: []
  type: TYPE_NORMAL
- en: If opt_einsum is not available, torch.einsum will fall back to the default contraction
    path of left to right.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'A :class:`str` that specifies which strategies to try when `torch.backends.opt_einsum.enabled`
    is `True`. By default, torch.einsum will try the “auto” strategy, but the “greedy”
    and “optimal” strategies are also supported. Note that the “optimal” strategy
    is factorial on the number of inputs as it tries all possible paths. See more
    details in opt_einsum’s docs ([https://optimized-einsum.readthedocs.io/en/stable/path_finding.html](https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)).  ##
    torch.backends.xeon'
  prefs: []
  type: TYPE_NORMAL
