["```py\npython  -m  spacy  download  en_core_web_sm\npython  -m  spacy  download  de_core_news_sm \n```", "```py\nimport torchdata.datapipes as dp\nimport torchtext.transforms as T\nimport spacy\nfrom torchtext.vocab import [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")\neng = spacy.load(\"en_core_web_sm\") # Load the English model to tokenize English text\nde = spacy.load(\"de_core_news_sm\") # Load the German model to tokenize German text \n```", "```py\nFILE_PATH = 'data/deu.txt'\ndata_pipe = [dp.iter.IterableWrapper](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset \"torch.utils.data.IterableDataset\")([FILE_PATH])\ndata_pipe = [dp.iter.FileOpener](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset \"torch.utils.data.IterableDataset\")(data_pipe, mode='rb')\ndata_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True) \n```", "```py\nfor sample in data_pipe:\n    print(sample)\n    break \n```", "```py\n('Go.', 'Geh.', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)') \n```", "```py\ndef removeAttribution(row):\n  \"\"\"\n Function to keep the first two elements in a tuple\n \"\"\"\n    return row[:2]\ndata_pipe = data_pipe.map(removeAttribution) \n```", "```py\nfor sample in data_pipe:\n    print(sample)\n    break \n```", "```py\n('Go.', 'Geh.') \n```", "```py\ndef engTokenize(text):\n  \"\"\"\n Tokenize an English text and return a list of tokens\n \"\"\"\n    return [token.text for token in eng.tokenizer(text)]\n\ndef deTokenize(text):\n  \"\"\"\n Tokenize a German text and return a list of tokens\n \"\"\"\n    return [token.text for token in de.tokenizer(text)] \n```", "```py\nprint(engTokenize(\"Have a good day!!!\"))\nprint(deTokenize(\"Haben Sie einen guten Tag!!!\")) \n```", "```py\n['Have', 'a', 'good', 'day', '!', '!', '!']\n['Haben', 'Sie', 'einen', 'guten', 'Tag', '!', '!', '!'] \n```", "```py\ndef getTokens(data_iter, place):\n  \"\"\"\n Function to yield tokens from an iterator. Since, our iterator contains\n tuple of sentences (source and target), `place` parameters defines for which\n index to return the tokens for. `place=0` for source and `place=1` for target\n \"\"\"\n    for english, german in data_iter:\n        if place == 0:\n            yield engTokenize(english)\n        else:\n            yield deTokenize(german) \n```", "```py\n[source_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\") = [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")(\n    getTokens(data_pipe,0),\n    min_freq=2,\n    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n    special_first=True\n)\n[source_vocab.set_default_index](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.set_default_index \"torchtext.vocab.Vocab.set_default_index\")([source_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\")['<unk>']) \n```", "```py\n[target_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\") = [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")(\n    getTokens(data_pipe,1),\n    min_freq=2,\n    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n    special_first=True\n)\n[target_vocab.set_default_index](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.set_default_index \"torchtext.vocab.Vocab.set_default_index\")([target_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\")['<unk>']) \n```", "```py\nprint([source_vocab.get_itos](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.get_itos \"torchtext.vocab.Vocab.get_itos\")()[:9]) \n```", "```py\n['<pad>', '<sos>', '<eos>', '<unk>', '.', 'I', 'Tom', 'to', 'you'] \n```", "```py\ndef getTransform(vocab):\n  \"\"\"\n Create transforms based on given vocabulary. The returned transform is applied to sequence\n of tokens.\n \"\"\"\n    text_tranform = [T.Sequential](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.Sequential \"torchtext.transforms.Sequential\")(\n        ## converts the sentences to indices based on given vocabulary\n        [T.VocabTransform](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.VocabTransform \"torchtext.transforms.VocabTransform\")(vocab=vocab),\n        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n        # 1 as seen in previous section\n        [T.AddToken](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.AddToken \"torchtext.transforms.AddToken\")(1, begin=True),\n        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n        # 2 as seen in previous section\n        [T.AddToken](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.AddToken \"torchtext.transforms.AddToken\")(2, begin=False)\n    )\n    return text_tranform \n```", "```py\ntemp_list = list(data_pipe)\nsome_sentence = temp_list[798][0]\nprint(\"Some sentence=\", end=\"\")\nprint(some_sentence)\ntransformed_sentence = getTransform([source_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\"))(engTokenize(some_sentence))\nprint(\"Transformed sentence=\", end=\"\")\nprint(transformed_sentence)\nindex_to_string = [source_vocab.get_itos](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.get_itos \"torchtext.vocab.Vocab.get_itos\")()\nfor index in transformed_sentence:\n    print(index_to_string[index], end=\" \") \n```", "```py\nSome sentence=I fainted.\nTransformed sentence=[1, 5, 2897, 4, 2]\n<sos> I fainted . <eos> \n```", "```py\ndef applyTransform(sequence_pair):\n  \"\"\"\n Apply transforms to sequence of tokens in a sequence pair\n \"\"\"\n\n    return (\n        getTransform([source_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\"))(engTokenize(sequence_pair[0])),\n        getTransform([target_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\"))(deTokenize(sequence_pair[1]))\n    )\ndata_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\ntemp_list = list(data_pipe)\nprint(temp_list[0]) \n```", "```py\n([1, 616, 4, 2], [1, 739, 4, 2]) \n```", "```py\ndef sortBucket(bucket):\n  \"\"\"\n Function to sort a given bucket. Here, we want to sort based on the length of\n source and target sequence.\n \"\"\"\n    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1]))) \n```", "```py\ndata_pipe = data_pipe.bucketbatch(\n    batch_size = 4, batch_num=5,  bucket_num=1,\n    use_in_batch_shuffle=False, sort_key=sortBucket\n) \n```", "```py\nprint(list(data_pipe)[0]) \n```", "```py\n[([1, 11105, 17, 4, 2], [1, 507, 29, 24, 2]), ([1, 11105, 17, 4, 2], [1, 7994, 1487, 24, 2]), ([1, 5335, 21, 4, 2], [1, 6956, 32, 24, 2]), ([1, 5335, 21, 4, 2], [1, 16003, 32, 24, 2])] \n```", "```py\ndef separateSourceTarget(sequence_pairs):\n  \"\"\"\n input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n \"\"\"\n    sources,targets = zip(*sequence_pairs)\n    return sources,targets\n\n## Apply the function to each element in the iterator\ndata_pipe = data_pipe.map(separateSourceTarget)\nprint(list(data_pipe)[0]) \n```", "```py\n(([1, 6860, 23, 10, 2], [1, 6860, 23, 10, 2], [1, 29, 466, 4, 2], [1, 29, 466, 4, 2]), ([1, 20825, 8, 2], [1, 11118, 8, 2], [1, 31, 1152, 4, 2], [1, 31, 1035, 4, 2])) \n```", "```py\ndef applyPadding(pair_of_sequences):\n  \"\"\"\n Convert sequences to tensors and apply padding\n \"\"\"\n    return ([T.ToTensor](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.ToTensor \"torchtext.transforms.ToTensor\")(0)(list(pair_of_sequences[0])), [T.ToTensor](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.ToTensor \"torchtext.transforms.ToTensor\")(0)(list(pair_of_sequences[1])))\n## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n# padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the\n# vocabulary.\ndata_pipe = data_pipe.map(applyPadding) \n```", "```py\nsource_index_to_string = [source_vocab.get_itos](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.get_itos \"torchtext.vocab.Vocab.get_itos\")()\ntarget_index_to_string = [target_vocab.get_itos](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.get_itos \"torchtext.vocab.Vocab.get_itos\")()\n\ndef showSomeTransformedSentences(data_pipe):\n  \"\"\"\n Function to show how the sentences look like after applying all transforms.\n Here we try to print actual words instead of corresponding index\n \"\"\"\n    for sources,targets in data_pipe:\n        if sources[0][-1] != 0:\n            continue # Just to visualize padding of shorter sentences\n        for i in range(4):\n            source = \"\"\n            for token in sources[i]:\n                source += \" \" + source_index_to_string[token]\n            target = \"\"\n            for token in targets[i]:\n                target += \" \" + target_index_to_string[token]\n            print(f\"Source: {source}\")\n            print(f\"Traget: {target}\")\n        break\n\nshowSomeTransformedSentences(data_pipe) \n```", "```py\nSource:  <sos> Freeze ! <eos> <pad>\nTraget:  <sos> Stehenbleiben ! <eos> <pad>\nSource:  <sos> <unk> ! <eos> <pad>\nTraget:  <sos> Zum Wohl ! <eos>\nSource:  <sos> Freeze ! <eos> <pad>\nTraget:  <sos> Keine Bewegung ! <eos>\nSource:  <sos> Got it ! <eos>\nTraget:  <sos> Verstanden ! <eos> <pad> \n```"]