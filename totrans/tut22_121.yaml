- en: Implementing a Parameter Server Using Distributed RPC Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html](https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Author**: [Rohan Varma](https://github.com/rohan-varma)'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '![edit](../Images/a8aa37bcc5edbf2ba5fcf18dba1e55f9.png)
    View and edit this tutorial in [github](https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PyTorch Distributed Overview](../beginner/dist_overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RPC API documents](https://pytorch.org/docs/master/rpc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This tutorial walks through a simple example of implementing a parameter server
    using PyTorch’s [Distributed RPC framework](https://pytorch.org/docs/stable/rpc.html).
    The parameter server framework is a paradigm in which a set of servers store parameters,
    such as large embedding tables, and several trainers query the parameter servers
    in order to retrieve the most up to date parameters. These trainers can run a
    training loop locally and occasionally synchronize with the parameter server to
    get the latest parameters. For more reading on the parameter server approach,
    check out [this paper](https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Using the Distributed RPC Framework, we’ll build an example where multiple trainers
    use RPC to communicate with the same parameter server and use [RRef](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RRef)
    to access states on the remote parameter server instance. Each trainer will launch
    its dedicated backward pass in a distributed fashion through stitching of the
    autograd graph across multiple nodes using distributed autograd.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: This tutorial covers the use of the Distributed RPC Framework, which
    is useful for splitting a model onto multiple machines, or for implementing a
    parameter-server training strategy where network trainers fetch parameters hosted
    on a different machine. If instead you are looking for replicating your model
    across many GPUs, please see the [Distributed Data Parallel tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html).
    There is also another [RPC tutorial](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)
    that covers reinforcement learning and RNN use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the familiar: importing our required modules and defining
    a simple ConvNet that will train on the MNIST dataset. The below network is largely
    adopted from the network defined in the [pytorch/examples repo](https://github.com/pytorch/examples/tree/master/mnist).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s define some helper functions that will be useful for the rest of
    our script. The following uses [rpc_sync](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.rpc_sync)
    and [RRef](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RRef)
    in order to define a function that invokes a given method on an object living
    on a remote node. Below, our handle to the remote object is given by the `rref`
    argument, and we run it on its owning node: `rref.owner()`. On the caller node,
    we run this command synchronously through the use of `rpc_sync`, meaning that
    we will block until a response is received.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’re ready to define our parameter server. We will subclass `nn.Module`
    and save a handle to our network defined above. We’ll also save an input device
    which will be the device our input is transferred to before invoking the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll define our forward pass. Note that regardless of the device of the
    model output, we move the output to CPU, as the Distributed RPC Framework currently
    only supports sending CPU tensors over RPC. We have intentionally disabled sending
    CUDA tensors over RPC due to the potential for different devices (CPU/GPU) on
    on the caller/callee, but may support this in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll define a few miscellaneous functions useful for training and verification
    purposes. The first, `get_dist_gradients`, will take in a Distributed Autograd
    context ID and call into the `dist_autograd.get_gradients` API in order to retrieve
    gradients computed by distributed autograd. More information can be found in the
    [distributed autograd documentation](https://pytorch.org/docs/stable/rpc.html#distributed-autograd-framework).
    Note that we also iterate through the resulting dictionary and convert each tensor
    to a CPU tensor, as the framework currently only supports sending tensors over
    RPC. Next, `get_param_rrefs` will iterate through our model parameters and wrap
    them as a (local) [RRef](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RRef).
    This method will be invoked over RPC by trainer nodes and will return a list of
    the parameters to be optimized. This is required as input to the [Distributed
    Optimizer](https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.optim),
    which requires all parameters it must optimize as a list of `RRef`s.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we’ll create methods to initialize our parameter server. Note that
    there will only be one instance of a parameter server across all processes, and
    all trainers will talk to the same parameter server and update the same stored
    model. As seen in `run_parameter_server`, the server itself does not take any
    independent actions; it waits for requests from trainers (which are yet to be
    defined) and responds to them by running the requested function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that above, `rpc.shutdown()` will not immediately shut down the Parameter
    Server. Instead, it will wait for all workers (trainers in this case) to also
    call into `rpc.shutdown()`. This gives us the guarantee that the parameter server
    will not go offline before all trainers (yet to be define) have completed their
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll define our `TrainerNet` class. This will also be a subclass of `nn.Module`,
    and our `__init__` method will use the `rpc.remote` API to obtain an RRef, or
    Remote Reference, to our parameter server. Note that here we are not copying the
    parameter server to our local process, instead, we can think of `self.param_server_rref`
    as a distributed shared pointer to the parameter server that lives on a separate
    process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll define a method called `get_global_param_rrefs`. To motivate the
    need for this method, it is worth it to read through the documentation on [DistributedOptimizer](https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.optim),
    specifically the API signature. The optimizer must be passed a list of `RRef`s
    corresponding to the remote parameters to be optimized, so here we obtain the
    necessary `RRef`s. Since the only remote worker that a given `TrainerNet` interacts
    with is the `ParameterServer`, we simply invoke a `remote_method` on the `ParameterServer`.
    We use the `get_param_rrefs` method which we defined in the `ParameterServer`
    class. This method will return a list of `RRef`s to the parameters that need to
    be optimized. Note that in this case our `TrainerNet` does not define its own
    paramaters; if it did, we would need to wrap each parameter in an `RRef` as well
    and include it into our input to `DistributedOptimizer`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’re ready to define our `forward` method, which will invoke (synchronous)
    RPC to run the forward pass of the network defined on the `ParameterServer`. Note
    that we pass in `self.param_server_rref`, which is a remote handle to our `ParameterServer`,
    to our RPC call. This call will send an RPC to the node on which our `ParameterServer`
    is running, invoke the `forward` pass, and return the `Tensor` corresponding to
    the model’s output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With our trainer fully defined, it’s now time to write our neural network training
    loop that will create our network and optimizer, run some inputs through the network
    and compute the loss. The training loop looks a lot like that of a local training
    program, with some modifications due to the nature of our network being distributed
    across machines.
  prefs: []
  type: TYPE_NORMAL
- en: Below, we initialize our `TrainerNet` and build a `DistributedOptimizer`. Note
    that as mentioned above, we must pass in all of the global (across all nodes participating
    in distributed training) parameters that we want to be optimized. In addition,
    we pass in the local optimizer to be used, in this case, SGD. Note that we can
    configure the underlying optimizer algorithm in the same way as creating a local
    optimizer - all arguments for `optimizer.SGD` will be forwarded properly. As an
    example, we pass in a custom learning rate that will be used as the learning rate
    for all local optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Next, we define our main training loop. We loop through iterables given by PyTorch’s
    [DataLoader](https://pytorch.org/docs/stable/data.html). Before writing our typical
    forward/backward/optimizer loop, we first wrap the logic within a [Distributed
    Autograd context](https://pytorch.org/docs/stable/rpc.html#torch.distributed.autograd.context).
    Note that this is needed to record RPCs invoked in the model’s forward pass, so
    that an appropriate graph can be constructed which includes all participating
    distributed workers in the backward pass. The distributed autograd context returns
    a `context_id` which serves as an identifier for accumulating and optimizing gradients
    corresponding to a particular iteration.
  prefs: []
  type: TYPE_NORMAL
- en: As opposed to calling the typical `loss.backward()` which would kick off the
    backward pass on this local worker, we call `dist_autograd.backward()` and pass
    in our context_id as well as `loss`, which is the root at which we want the backward
    pass to begin. In addition, we pass this `context_id` into our optimizer call,
    which is required to be able to look up the corresponding gradients computed by
    this particular backwards pass across all nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The following simply computes the accuracy of our model after we’re done training,
    much like a traditional local model. However, note that the `net` we pass into
    this function above is an instance of `TrainerNet` and therefore the forward pass
    invokes RPC in a transparent fashion.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, similar to how we defined `run_parameter_server` as the main loop for
    our `ParameterServer` that is responsible for initializing RPC, let’s define a
    similar loop for our trainers. The difference will be that our trainers must run
    the training loop we defined above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that similar to `run_parameter_server`, `rpc.shutdown()` will by default
    wait for all workers, both trainers and ParameterServers, to call into `rpc.shutdown()`
    before this node exits. This ensures that nodes are terminated gracefully and
    no node goes offline while another is expecting it to be online.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now completed our trainer and parameter server specific code, and all
    that’s left is to add code to launch trainers and parameter servers. First, we
    must take in various arguments that apply to our parameter server and trainers.
    `world_size` corresponds to the total number of nodes that will participate in
    training, and is the sum of all trainers and the parameter server. We also must
    pass in a unique `rank` for each individual process, from 0 (where we will run
    our single parameter server) to `world_size - 1`. `master_addr` and `master_port`
    are arguments that can be used to identify where the rank 0 process is running,
    and will be used by individual nodes to discover each other. To test this example
    out locally, simply pass in `localhost` and the same `master_port` to all instances
    spawned. Note that for demonstration purposes, this example supports only between
    0-2 GPUs, although the pattern can be extended to make use of additional GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’ll create a process corresponding to either a parameter server or trainer
    depending on our command line arguments. We’ll create a `ParameterServer` if our
    passed in rank is 0, and a `TrainerNet` otherwise. Note that we’re using `torch.multiprocessing`
    to launch a subprocess corresponding to the function that we want to execute,
    and waiting on this process’s completion from the main thread with `p.join()`.
    In the case of initializing our trainers, we also use PyTorch’s [dataloaders](https://pytorch.org/docs/stable/data.html)
    in order to specify train and test data loaders on the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the example locally, run the following command worker for the server
    and each worker you wish to spawn, in separate terminal windows: `python rpc_parameter_server.py
    --world_size=WORLD_SIZE --rank=RANK`. For example, for a master node with world
    size of 2, the command would be `python rpc_parameter_server.py --world_size=2
    --rank=0`. The trainer can then be launched with the command `python rpc_parameter_server.py
    --world_size=2 --rank=1` in a separate window, and this will begin training with
    one server and a single trainer. Note that this tutorial assumes that training
    occurs using between 0 and 2 GPUs, and this argument can be configured by passing
    `--num_gpus=N` into the training script.'
  prefs: []
  type: TYPE_NORMAL
- en: You can pass in the command line arguments `--master_addr=ADDRESS` and `--master_port=PORT`
    to indicate the address and port that the master worker is listening on, for example,
    to test functionality where trainers and master nodes run on different machines.
  prefs: []
  type: TYPE_NORMAL
