["```py\nclass torchrec.modules.activation.SwishLayerNorm(input_dims: Union[int, List[int], Size], device: Optional[device] = None)\u00b6\n```", "```py\nsln = SwishLayerNorm(100) \n```", "```py\nforward(input: Tensor) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.crossnet.CrossNet(in_features: int, num_layers: int)\u00b6\n```", "```py\nbatch_size = 3\nnum_layers = 2\nin_features = 10\ninput = torch.randn(batch_size, in_features)\ndcn = CrossNet(num_layers=num_layers)\noutput = dcn(input) \n```", "```py\nforward(input: Tensor) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.crossnet.LowRankCrossNet(in_features: int, num_layers: int, low_rank: int = 1)\u00b6\n```", "```py\nbatch_size = 3\nnum_layers = 2\nin_features = 10\ninput = torch.randn(batch_size, in_features)\ndcn = LowRankCrossNet(num_layers=num_layers, low_rank=3)\noutput = dcn(input) \n```", "```py\nforward(input: Tensor) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.crossnet.LowRankMixtureCrossNet(in_features: int, num_layers: int, num_experts: int = 1, low_rank: int = 1, activation: ~typing.Union[~torch.nn.modules.module.Module, ~typing.Callable[[~torch.Tensor], ~torch.Tensor]] = <built-in method relu of type object>)\u00b6\n```", "```py\nbatch_size = 3\nnum_layers = 2\nin_features = 10\ninput = torch.randn(batch_size, in_features)\ndcn = LowRankCrossNet(num_layers=num_layers, num_experts=5, low_rank=3)\noutput = dcn(input) \n```", "```py\nforward(input: Tensor) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.crossnet.VectorCrossNet(in_features: int, num_layers: int)\u00b6\n```", "```py\nbatch_size = 3\nnum_layers = 2\nin_features = 10\ninput = torch.randn(batch_size, in_features)\ndcn = VectorCrossNet(num_layers=num_layers)\noutput = dcn(input) \n```", "```py\nforward(input: Tensor) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.deepfm.DeepFM(dense_module: Module)\u00b6\n```", "```py\n 1 x 10                  output\n         /|\\\n          |                     pass into `dense_module`\n          |\n        1 x 90\n         /|\\\n          |                     concat\n          |\n1 x 20, 1 x 30, 1 x 40          list of embeddings \n```", "```py\nimport torch\nfrom torchrec.fb.modules.deepfm import DeepFM\nfrom torchrec.fb.modules.mlp import LazyMLP\nbatch_size = 3\noutput_dim = 30\n# the input embedding are a torch.Tensor of [batch_size, num_embeddings, embedding_dim]\ninput_embeddings = [\n    torch.randn(batch_size, 2, 64),\n    torch.randn(batch_size, 2, 32),\n]\ndense_module = nn.Linear(192, output_dim)\ndeepfm = DeepFM(dense_module=dense_module)\ndeep_fm_output = deepfm(embeddings=input_embeddings) \n```", "```py\nforward(embeddings: List[Tensor]) \u2192 Tensor\u00b6\n```", "```py\n(batch_size, num_embeddings, embedding_dim) \n```", "```py\ntensor(B, 1, 32) (trained_embedding with num_embeddings=1, embedding_dim=32)\ntensor(B, 5, 64) (native_embedding with num_embeddings=5, embedding_dim=64)\ntensor(B, 3, 16) (dense_features with num_embeddings=3, embedding_dim=32) \n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.deepfm.FactorizationMachine\u00b6\n```", "```py\n 1 x 10                  output\n         /|\\\n          |                     pass into `dense_module`\n          |\n        1 x 90\n         /|\\\n          |                     concat\n          |\n1 x 20, 1 x 30, 1 x 40          list of embeddings \n```", "```py\nbatch_size = 3\n# the input embedding are in torch.Tensor of [batch_size, num_embeddings, embedding_dim]\ninput_embeddings = [\n    torch.randn(batch_size, 2, 64),\n    torch.randn(batch_size, 2, 32),\n]\nfm = FactorizationMachine()\noutput = fm(embeddings=input_embeddings) \n```", "```py\nforward(embeddings: List[Tensor]) \u2192 Tensor\u00b6\n```", "```py\n(batch_size, num_embeddings, embedding_dim) \n```", "```py\ntensor(B, 1, 32) (trained_embedding with num_embeddings=1, embedding_dim=32)\ntensor(B, 5, 64) (native_embedding with num_embeddings=5, embedding_dim=64)\ntensor(B, 3, 16) (dense_features with num_embeddings=3, embedding_dim=32) \n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.embedding_configs.BaseEmbeddingConfig(num_embeddings: int, embedding_dim: int, name: str = '', data_type: torchrec.types.DataType = <DataType.FP32: 'FP32'>, feature_names: List[str] = <factory>, weight_init_max: Union[float, NoneType] = None, weight_init_min: Union[float, NoneType] = None, pruning_indices_remapping: Union[torch.Tensor, NoneType] = None, init_fn: Union[Callable[[torch.Tensor], Union[torch.Tensor, NoneType]], NoneType] = None, need_pos: bool = False)\u00b6\n```", "```py\ndata_type: DataType = 'FP32'\u00b6\n```", "```py\nembedding_dim: int\u00b6\n```", "```py\nfeature_names: List[str]\u00b6\n```", "```py\nget_weight_init_max() \u2192 float\u00b6\n```", "```py\nget_weight_init_min() \u2192 float\u00b6\n```", "```py\ninit_fn: Optional[Callable[[Tensor], Optional[Tensor]]] = None\u00b6\n```", "```py\nname: str = ''\u00b6\n```", "```py\nneed_pos: bool = False\u00b6\n```", "```py\nnum_embeddings: int\u00b6\n```", "```py\nnum_features() \u2192 int\u00b6\n```", "```py\npruning_indices_remapping: Optional[Tensor] = None\u00b6\n```", "```py\nweight_init_max: Optional[float] = None\u00b6\n```", "```py\nweight_init_min: Optional[float] = None\u00b6\n```", "```py\nclass torchrec.modules.embedding_configs.EmbeddingBagConfig(num_embeddings: int, embedding_dim: int, name: str = '', data_type: torchrec.types.DataType = <DataType.FP32: 'FP32'>, feature_names: List[str] = <factory>, weight_init_max: Union[float, NoneType] = None, weight_init_min: Union[float, NoneType] = None, pruning_indices_remapping: Union[torch.Tensor, NoneType] = None, init_fn: Union[Callable[[torch.Tensor], Union[torch.Tensor, NoneType]], NoneType] = None, need_pos: bool = False, pooling: torchrec.modules.embedding_configs.PoolingType = <PoolingType.SUM: 'SUM'>)\u00b6\n```", "```py\npooling: PoolingType = 'SUM'\u00b6\n```", "```py\nclass torchrec.modules.embedding_configs.EmbeddingConfig(num_embeddings: int, embedding_dim: int, name: str = '', data_type: torchrec.types.DataType = <DataType.FP32: 'FP32'>, feature_names: List[str] = <factory>, weight_init_max: Union[float, NoneType] = None, weight_init_min: Union[float, NoneType] = None, pruning_indices_remapping: Union[torch.Tensor, NoneType] = None, init_fn: Union[Callable[[torch.Tensor], Union[torch.Tensor, NoneType]], NoneType] = None, need_pos: bool = False)\u00b6\n```", "```py\nembedding_dim: int\u00b6\n```", "```py\nfeature_names: List[str]\u00b6\n```", "```py\nnum_embeddings: int\u00b6\n```", "```py\nclass torchrec.modules.embedding_configs.EmbeddingTableConfig(num_embeddings: int, embedding_dim: int, name: str = '', data_type: torchrec.types.DataType = <DataType.FP32: 'FP32'>, feature_names: List[str] = <factory>, weight_init_max: Union[float, NoneType] = None, weight_init_min: Union[float, NoneType] = None, pruning_indices_remapping: Union[torch.Tensor, NoneType] = None, init_fn: Union[Callable[[torch.Tensor], Union[torch.Tensor, NoneType]], NoneType] = None, need_pos: bool = False, pooling: torchrec.modules.embedding_configs.PoolingType = <PoolingType.SUM: 'SUM'>, is_weighted: bool = False, has_feature_processor: bool = False, embedding_names: List[str] = <factory>)\u00b6\n```", "```py\nembedding_names: List[str]\u00b6\n```", "```py\nhas_feature_processor: bool = False\u00b6\n```", "```py\nis_weighted: bool = False\u00b6\n```", "```py\npooling: PoolingType = 'SUM'\u00b6\n```", "```py\nclass torchrec.modules.embedding_configs.PoolingType(value)\u00b6\n```", "```py\nMEAN = 'MEAN'\u00b6\n```", "```py\nNONE = 'NONE'\u00b6\n```", "```py\nSUM = 'SUM'\u00b6\n```", "```py\nclass torchrec.modules.embedding_configs.QuantConfig(activation, weight, per_table_weight_dtype)\u00b6\n```", "```py\nactivation: PlaceholderObserver\u00b6\n```", "```py\nper_table_weight_dtype: Optional[Dict[str, dtype]]\u00b6\n```", "```py\nweight: PlaceholderObserver\u00b6\n```", "```py\ntorchrec.modules.embedding_configs.data_type_to_dtype(data_type: DataType) \u2192 dtype\u00b6\n```", "```py\ntorchrec.modules.embedding_configs.data_type_to_sparse_type(data_type: DataType) \u2192 SparseType\u00b6\n```", "```py\ntorchrec.modules.embedding_configs.dtype_to_data_type(dtype: dtype) \u2192 DataType\u00b6\n```", "```py\ntorchrec.modules.embedding_configs.pooling_type_to_pooling_mode(pooling_type: PoolingType) \u2192 PoolingMode\u00b6\n```", "```py\ntorchrec.modules.embedding_configs.pooling_type_to_str(pooling_type: PoolingType) \u2192 str\u00b6\n```", "```py\nclass torchrec.modules.embedding_modules.EmbeddingBagCollection(tables: List[EmbeddingBagConfig], is_weighted: bool = False, device: Optional[device] = None)\u00b6\n```", "```py\ntable_0 = EmbeddingBagConfig(\n    name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n)\ntable_1 = EmbeddingBagConfig(\n    name=\"t2\", embedding_dim=4, num_embeddings=10, feature_names=[\"f2\"]\n)\n\nebc = EmbeddingBagCollection(tables=[table_0, table_1])\n\n#        0       1        2  <-- batch\n# \"f1\"   [0,1] None    [2]\n# \"f2\"   [3]    [4]    [5,6,7]\n#  ^\n# feature\n\nfeatures = KeyedJaggedTensor(\n    keys=[\"f1\", \"f2\"],\n    values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n    offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n)\n\npooled_embeddings = ebc(features)\nprint(pooled_embeddings.values())\ntensor([[-0.8899, -0.1342, -1.9060, -0.0905, -0.2814, -0.9369, -0.7783],\n    [ 0.0000,  0.0000,  0.0000,  0.1598,  0.0695,  1.3265, -0.1011],\n    [-0.4256, -1.1846, -2.1648, -1.0893,  0.3590, -1.9784, -0.7681]],\n    grad_fn=<CatBackward0>)\nprint(pooled_embeddings.keys())\n['f1', 'f2']\nprint(pooled_embeddings.offset_per_key())\ntensor([0, 3, 7]) \n```", "```py\nproperty device: device\u00b6\n```", "```py\nembedding_bag_configs() \u2192 List[EmbeddingBagConfig]\u00b6\n```", "```py\nforward(features: KeyedJaggedTensor) \u2192 KeyedTensor\u00b6\n```", "```py\nis_weighted() \u2192 bool\u00b6\n```", "```py\nreset_parameters() \u2192 None\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.embedding_modules.EmbeddingBagCollectionInterface(*args, **kwargs)\u00b6\n```", "```py\nabstract embedding_bag_configs() \u2192 List[EmbeddingBagConfig]\u00b6\n```", "```py\nabstract forward(features: KeyedJaggedTensor) \u2192 KeyedTensor\u00b6\n```", "```py\nabstract is_weighted() \u2192 bool\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.embedding_modules.EmbeddingCollection(tables: List[EmbeddingConfig], device: Optional[device] = None, need_indices: bool = False)\u00b6\n```", "```py\ne1_config = EmbeddingConfig(\n    name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n)\ne2_config = EmbeddingConfig(\n    name=\"t2\", embedding_dim=3, num_embeddings=10, feature_names=[\"f2\"]\n)\n\nec = EmbeddingCollection(tables=[e1_config, e2_config])\n\n#     0       1        2  <-- batch\n# 0   [0,1] None    [2]\n# 1   [3]    [4]    [5,6,7]\n# ^\n# feature\n\nfeatures = KeyedJaggedTensor.from_offsets_sync(\n    keys=[\"f1\", \"f2\"],\n    values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n    offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n)\nfeature_embeddings = ec(features)\nprint(feature_embeddings['f2'].values())\ntensor([[-0.2050,  0.5478,  0.6054],\n[ 0.7352,  0.3210, -3.0399],\n[ 0.1279, -0.1756, -0.4130],\n[ 0.7519, -0.4341, -0.0499],\n[ 0.9329, -1.0697, -0.8095]], grad_fn=<EmbeddingBackward>) \n```", "```py\nproperty device: device\u00b6\n```", "```py\nembedding_configs() \u2192 List[EmbeddingConfig]\u00b6\n```", "```py\nembedding_dim() \u2192 int\u00b6\n```", "```py\nembedding_names_by_table() \u2192 List[List[str]]\u00b6\n```", "```py\nforward(features: KeyedJaggedTensor) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\nneed_indices() \u2192 bool\u00b6\n```", "```py\nreset_parameters() \u2192 None\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.embedding_modules.EmbeddingCollectionInterface(*args, **kwargs)\u00b6\n```", "```py\nabstract embedding_configs() \u2192 List[EmbeddingConfig]\u00b6\n```", "```py\nabstract embedding_dim() \u2192 int\u00b6\n```", "```py\nabstract embedding_names_by_table() \u2192 List[List[str]]\u00b6\n```", "```py\nabstract forward(features: KeyedJaggedTensor) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\nabstract need_indices() \u2192 bool\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\ntorchrec.modules.embedding_modules.get_embedding_names_by_table(tables: Union[List[EmbeddingBagConfig], List[EmbeddingConfig]]) \u2192 List[List[str]]\u00b6\n```", "```py\ntorchrec.modules.embedding_modules.process_pooled_embeddings(pooled_embeddings: List[Tensor], inverse_indices: Tensor) \u2192 Tensor\u00b6\n```", "```py\ntorchrec.modules.embedding_modules.reorder_inverse_indices(inverse_indices: Optional[Tuple[List[str], Tensor]], feature_names: List[str]) \u2192 Tensor\u00b6\n```", "```py\nclass torchrec.modules.feature_processor.BaseFeatureProcessor(*args, **kwargs)\u00b6\n```", "```py\nabstract forward(features: Dict[str, JaggedTensor]) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.feature_processor.BaseGroupedFeatureProcessor(*args, **kwargs)\u00b6\n```", "```py\nabstract forward(features: KeyedJaggedTensor) \u2192 KeyedJaggedTensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.feature_processor.PositionWeightedModule(max_feature_lengths: Dict[str, int], device: Optional[device] = None)\u00b6\n```", "```py\nforward(features: Dict[str, JaggedTensor]) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\nreset_parameters() \u2192 None\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.feature_processor.PositionWeightedProcessor(max_feature_lengths: Dict[str, int], device: Optional[device] = None)\u00b6\n```", "```py\nkeys=[\"Feature0\", \"Feature1\", \"Feature2\"]\nvalues=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 3, 4, 5, 6, 7])\nlengths=torch.tensor([2, 0, 1, 1, 1, 3, 2, 3, 0])\nfeatures = KeyedJaggedTensor.from_lengths_sync(keys=keys, values=values, lengths=lengths)\npw = FeatureProcessorCollection(\n    feature_processor_modules={key: PositionWeightedFeatureProcessor(max_feature_length=100) for key in keys}\n)\nresult = pw(features)\n# result is\n# KeyedJaggedTensor({\n#     \"Feature0\": {\n#         \"values\": [[0, 1], [], [2]],\n#         \"weights\": [[1.0, 1.0], [], [1.0]]\n#     },\n#     \"Feature1\": {\n#         \"values\": [[3], [4], [5, 6, 7]],\n#         \"weights\": [[1.0], [1.0], [1.0, 1.0, 1.0]]\n#     },\n#     \"Feature2\": {\n#         \"values\": [[3, 4], [5, 6, 7], []],\n#         \"weights\": [[1.0, 1.0], [1.0, 1.0, 1.0], []]\n#     }\n# }) \n```", "```py\nforward(features: KeyedJaggedTensor) \u2192 KeyedJaggedTensor\u00b6\n```", "```py\nnamed_buffers(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) \u2192 Iterator[Tuple[str, Tensor]]\u00b6\n```", "```py\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size()) \n```", "```py\nstate_dict(destination: Optional[Dict[str, Any]] = None, prefix: str = '', keep_vars: bool = False) \u2192 Dict[str, Any]\u00b6\n```", "```py\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> module.state_dict().keys()\n['bias', 'weight'] \n```", "```py\ntraining: bool\u00b6\n```", "```py\ntorchrec.modules.feature_processor.position_weighted_module_update_features(features: Dict[str, JaggedTensor], weighted_features: Dict[str, JaggedTensor]) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\nclass torchrec.modules.lazy_extension.LazyModuleExtensionMixin(*args, **kwargs)\u00b6\n```", "```py\napply(fn: Callable[[Module], None]) \u2192 Module\u00b6\n```", "```py\n@torch.no_grad()\ndef init_weights(m):\n    print(m)\n    if type(m) == torch.nn.LazyLinear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\nlinear = torch.nn.LazyLinear(2)\nlinear.apply(init_weights)  # this fails, because `linear` (a lazy-module) hasn't been initialized yet\n\ninput = torch.randn(2, 10)\nlinear(input)  # run a dummy forward pass to initialize the lazy-module\n\nlinear.apply(init_weights)  # this works now \n```", "```py\ntorchrec.modules.lazy_extension.lazy_apply(module: Module, fn: Callable[[Module], None]) \u2192 Module\u00b6\n```", "```py\n@torch.no_grad()\ndef init_weights(m):\n    print(m)\n    if type(m) == torch.nn.LazyLinear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\nlinear = torch.nn.LazyLinear(2)\nlazy_apply(linear, init_weights)  # doesn't run `init_weights` immediately\ninput = torch.randn(2, 10)\nlinear(input)  # runs `init_weights` only once, right after first forward pass\n\nseq = torch.nn.Sequential(torch.nn.LazyLinear(2), torch.nn.LazyLinear(2))\nlazy_apply(seq, init_weights)  # doesn't run `init_weights` immediately\ninput = torch.randn(2, 10)\nseq(input)  # runs `init_weights` only once, right after first forward pass \n```", "```py\nclass torchrec.modules.mlp.MLP(in_size: int, layer_sizes: ~typing.List[int], bias: bool = True, activation: ~typing.Union[str, ~typing.Callable[[], ~torch.nn.modules.module.Module], ~torch.nn.modules.module.Module, ~typing.Callable[[~torch.Tensor], ~torch.Tensor]] = <built-in method relu of type object>, device: ~typing.Optional[~torch.device] = None, dtype: ~torch.dtype = torch.float32)\u00b6\n```", "```py\nbatch_size = 3\nin_size = 40\ninput = torch.randn(batch_size, in_size)\n\nlayer_sizes = [16, 8, 4]\nmlp_module = MLP(in_size, layer_sizes, bias=True)\noutput = mlp_module(input)\nassert list(output.shape) == [batch_size, layer_sizes[-1]] \n```", "```py\nforward(input: Tensor) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.mlp.Perceptron(in_size: int, out_size: int, bias: bool = True, activation: ~typing.Union[~torch.nn.modules.module.Module, ~typing.Callable[[~torch.Tensor], ~torch.Tensor]] = <built-in method relu of type object>, device: ~typing.Optional[~torch.device] = None, dtype: ~torch.dtype = torch.float32)\u00b6\n```", "```py\nbatch_size = 3\nin_size = 40\ninput = torch.randn(batch_size, in_size)\n\nout_size = 16\nperceptron = Perceptron(in_size, out_size, bias=True)\n\noutput = perceptron(input)\nassert list(output) == [batch_size, out_size] \n```", "```py\nforward(input: Tensor) \u2192 Tensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\ntorchrec.modules.utils.check_module_output_dimension(module: Union[Iterable[Module], Module], in_features: int, out_features: int) \u2192 bool\u00b6\n```", "```py\ntorchrec.modules.utils.construct_jagged_tensors(embeddings: Tensor, features: KeyedJaggedTensor, embedding_names: List[str], need_indices: bool = False, features_to_permute_indices: Optional[Dict[str, List[int]]] = None, original_features: Optional[KeyedJaggedTensor] = None, reverse_indices: Optional[Tensor] = None) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\ntorchrec.modules.utils.construct_modulelist_from_single_module(module: Module, sizes: Tuple[int, ...]) \u2192 Module\u00b6\n```", "```py\ntorchrec.modules.utils.convert_list_of_modules_to_modulelist(modules: Iterable[Module], sizes: Tuple[int, ...]) \u2192 Module\u00b6\n```", "```py\ntorchrec.modules.utils.extract_module_or_tensor_callable(module_or_callable: Union[Callable[[], Module], Module, Callable[[Tensor], Tensor]]) \u2192 Union[Module, Callable[[Tensor], Tensor]]\u00b6\n```", "```py\ntorchrec.modules.utils.get_module_output_dimension(module: Union[Callable[[Tensor], Tensor], Module], in_features: int) \u2192 int\u00b6\n```", "```py\ntorchrec.modules.utils.init_mlp_weights_xavier_uniform(m: Module) \u2192 None\u00b6\n```", "```py\nclass torchrec.modules.mc_modules.DistanceLFU_EvictionPolicy(decay_exponent: float = 1.0, threshold_filtering_func: Optional[Callable[[Tensor], Tuple[Tensor, Union[float, Tensor]]]] = None)\u00b6\n```", "```py\ncoalesce_history_metadata(current_iter: int, history_metadata: Dict[str, Tensor], unique_ids_counts: Tensor, unique_inverse_mapping: Tensor, additional_ids: Optional[Tensor] = None, threshold_mask: Optional[Tensor] = None) \u2192 Dict[str, Tensor]\u00b6\n```", "```py\nproperty metadata_info: List[MCHEvictionPolicyMetadataInfo]\u00b6\n```", "```py\nrecord_history_metadata(current_iter: int, incoming_ids: Tensor, history_metadata: Dict[str, Tensor]) \u2192 None\u00b6\n```", "```py\nupdate_metadata_and_generate_eviction_scores(current_iter: int, mch_size: int, coalesced_history_argsort_mapping: Tensor, coalesced_history_sorted_unique_ids_counts: Tensor, coalesced_history_mch_matching_elements_mask: Tensor, coalesced_history_mch_matching_indices: Tensor, mch_metadata: Dict[str, Tensor], coalesced_history_metadata: Dict[str, Tensor]) \u2192 Tuple[Tensor, Tensor]\u00b6\n```", "```py\nclass torchrec.modules.mc_modules.LFU_EvictionPolicy(threshold_filtering_func: Optional[Callable[[Tensor], Tuple[Tensor, Union[float, Tensor]]]] = None)\u00b6\n```", "```py\ncoalesce_history_metadata(current_iter: int, history_metadata: Dict[str, Tensor], unique_ids_counts: Tensor, unique_inverse_mapping: Tensor, additional_ids: Optional[Tensor] = None, threshold_mask: Optional[Tensor] = None) \u2192 Dict[str, Tensor]\u00b6\n```", "```py\nproperty metadata_info: List[MCHEvictionPolicyMetadataInfo]\u00b6\n```", "```py\nrecord_history_metadata(current_iter: int, incoming_ids: Tensor, history_metadata: Dict[str, Tensor]) \u2192 None\u00b6\n```", "```py\nupdate_metadata_and_generate_eviction_scores(current_iter: int, mch_size: int, coalesced_history_argsort_mapping: Tensor, coalesced_history_sorted_unique_ids_counts: Tensor, coalesced_history_mch_matching_elements_mask: Tensor, coalesced_history_mch_matching_indices: Tensor, mch_metadata: Dict[str, Tensor], coalesced_history_metadata: Dict[str, Tensor]) \u2192 Tuple[Tensor, Tensor]\u00b6\n```", "```py\nclass torchrec.modules.mc_modules.LRU_EvictionPolicy(decay_exponent: float = 1.0, threshold_filtering_func: Optional[Callable[[Tensor], Tuple[Tensor, Union[float, Tensor]]]] = None)\u00b6\n```", "```py\ncoalesce_history_metadata(current_iter: int, history_metadata: Dict[str, Tensor], unique_ids_counts: Tensor, unique_inverse_mapping: Tensor, additional_ids: Optional[Tensor] = None, threshold_mask: Optional[Tensor] = None) \u2192 Dict[str, Tensor]\u00b6\n```", "```py\nproperty metadata_info: List[MCHEvictionPolicyMetadataInfo]\u00b6\n```", "```py\nrecord_history_metadata(current_iter: int, incoming_ids: Tensor, history_metadata: Dict[str, Tensor]) \u2192 None\u00b6\n```", "```py\nupdate_metadata_and_generate_eviction_scores(current_iter: int, mch_size: int, coalesced_history_argsort_mapping: Tensor, coalesced_history_sorted_unique_ids_counts: Tensor, coalesced_history_mch_matching_elements_mask: Tensor, coalesced_history_mch_matching_indices: Tensor, mch_metadata: Dict[str, Tensor], coalesced_history_metadata: Dict[str, Tensor]) \u2192 Tuple[Tensor, Tensor]\u00b6\n```", "```py\nclass torchrec.modules.mc_modules.MCHEvictionPolicy(metadata_info: List[MCHEvictionPolicyMetadataInfo], threshold_filtering_func: Optional[Callable[[Tensor], Tuple[Tensor, Union[float, Tensor]]]] = None)\u00b6\n```", "```py\nabstract coalesce_history_metadata(current_iter: int, history_metadata: Dict[str, Tensor], unique_ids_counts: Tensor, unique_inverse_mapping: Tensor, additional_ids: Optional[Tensor] = None, threshold_mask: Optional[Tensor] = None) \u2192 Dict[str, Tensor]\u00b6\n```", "```py\nabstract property metadata_info: List[MCHEvictionPolicyMetadataInfo]\u00b6\n```", "```py\nabstract record_history_metadata(current_iter: int, incoming_ids: Tensor, history_metadata: Dict[str, Tensor]) \u2192 None\u00b6\n```", "```py\nabstract update_metadata_and_generate_eviction_scores(current_iter: int, mch_size: int, coalesced_history_argsort_mapping: Tensor, coalesced_history_sorted_unique_ids_counts: Tensor, coalesced_history_mch_matching_elements_mask: Tensor, coalesced_history_mch_matching_indices: Tensor, mch_metadata: Dict[str, Tensor], coalesced_history_metadata: Dict[str, Tensor]) \u2192 Tuple[Tensor, Tensor]\u00b6\n```", "```py\nclass torchrec.modules.mc_modules.MCHEvictionPolicyMetadataInfo(metadata_name, is_mch_metadata, is_history_metadata)\u00b6\n```", "```py\nis_history_metadata: bool\u00b6\n```", "```py\nis_mch_metadata: bool\u00b6\n```", "```py\nmetadata_name: str\u00b6\n```", "```py\nclass torchrec.modules.mc_modules.MCHManagedCollisionModule(zch_size: int, device: device, eviction_policy: MCHEvictionPolicy, eviction_interval: int, input_hash_size: int = 9223372036854775808, input_hash_func: Optional[Callable[[Tensor, int], Tensor]] = None, mch_size: Optional[int] = None, mch_hash_func: Optional[Callable[[Tensor, int], Tensor]] = None, name: Optional[str] = None, output_global_offset: int = 0)\u00b6\n```", "```py\nevict() \u2192 Optional[Tensor]\u00b6\n```", "```py\nforward(features: Dict[str, JaggedTensor]) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\ninput_size() \u2192 int\u00b6\n```", "```py\noutput_size() \u2192 int\u00b6\n```", "```py\npreprocess(features: Dict[str, JaggedTensor]) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\nprofile(features: Dict[str, JaggedTensor]) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\nrebuild_with_output_id_range(output_id_range: Tuple[int, int], device: Optional[device] = None) \u2192 MCHManagedCollisionModule\u00b6\n```", "```py\nremap(features: Dict[str, JaggedTensor]) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.mc_modules.ManagedCollisionCollection(managed_collision_modules: Dict[str, ManagedCollisionModule], embedding_configs: List[BaseEmbeddingConfig])\u00b6\n```", "```py\nembedding_configs() \u2192 List[BaseEmbeddingConfig]\u00b6\n```", "```py\nevict() \u2192 Dict[str, Optional[Tensor]]\u00b6\n```", "```py\nforward(features: KeyedJaggedTensor) \u2192 KeyedJaggedTensor\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.mc_modules.ManagedCollisionModule(device: device)\u00b6\n```", "```py\nproperty device: device\u00b6\n```", "```py\nabstract evict() \u2192 Optional[Tensor]\u00b6\n```", "```py\nabstract forward(features: Dict[str, JaggedTensor]) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\nabstract input_size() \u2192 int\u00b6\n```", "```py\nabstract output_size() \u2192 int\u00b6\n```", "```py\nabstract preprocess(features: Dict[str, JaggedTensor]) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\nabstract rebuild_with_output_id_range(output_id_range: Tuple[int, int], device: Optional[device] = None) \u2192 ManagedCollisionModule\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\ntorchrec.modules.mc_modules.apply_mc_method_to_jt_dict(method: str, features_dict: Dict[str, JaggedTensor], table_to_features: Dict[str, List[str]], managed_collisions: ModuleDict) \u2192 Dict[str, JaggedTensor]\u00b6\n```", "```py\ntorchrec.modules.mc_modules.average_threshold_filter(id_counts: Tensor) \u2192 Tuple[Tensor, Tensor]\u00b6\n```", "```py\ntorchrec.modules.mc_modules.dynamic_threshold_filter(id_counts: Tensor, threshold_skew_multiplier: float = 10.0) \u2192 Tuple[Tensor, Tensor]\u00b6\n```", "```py\nclass torchrec.modules.mc_embedding_modules.BaseManagedCollisionEmbeddingCollection(embedding_module: Union[EmbeddingBagCollection, EmbeddingCollection], managed_collision_collection: ManagedCollisionCollection, return_remapped_features: bool = False)\u00b6\n```", "```py\nforward(features: KeyedJaggedTensor) \u2192 Tuple[Union[KeyedTensor, Dict[str, JaggedTensor]], Optional[KeyedJaggedTensor]]\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingBagCollection(embedding_bag_collection: EmbeddingBagCollection, managed_collision_collection: ManagedCollisionCollection, return_remapped_features: bool = False)\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\nclass torchrec.modules.mc_embedding_modules.ManagedCollisionEmbeddingCollection(embedding_collection: EmbeddingCollection, managed_collision_collection: ManagedCollisionCollection, return_remapped_features: bool = False)\u00b6\n```", "```py\ntraining: bool\u00b6\n```", "```py\ntorchrec.modules.mc_embedding_modules.evict(evictions: Dict[str, Optional[Tensor]], ebc: Union[EmbeddingBagCollection, EmbeddingCollection]) \u2192 None\u00b6\n```"]