["```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndevice = \"cuda\" if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available \"torch.cuda.is_available\")() else \"cpu\"\n\n# Example Usage:\n[query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(2, 3, 8, device=device), [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(2, 3, 8, device=device), [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(2, 3, 8, device=device)\n[F.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention \"torch.nn.functional.scaled_dot_product_attention\")([query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) \n```", "```py\ntensor([[[-1.3321, -0.3489,  0.3015, -0.3912,  0.9867,  0.3137, -0.0691,\n          -1.2593],\n         [-1.0882,  0.2506,  0.6491,  0.1360,  0.5238, -0.2448, -0.0820,\n          -0.6171],\n         [-1.0012,  0.3990,  0.6441, -0.0277,  0.5325, -0.2564, -0.0607,\n          -0.6404]],\n\n        [[ 0.6091,  0.0708,  0.6188,  0.3252, -0.1598,  0.4197, -0.2335,\n           0.0630],\n         [ 0.5285,  0.3890, -0.2649,  0.3706, -0.3839,  0.1963, -0.6242,\n           0.2312],\n         [ 0.4048,  0.0762,  0.3777,  0.4689, -0.2978,  0.2754, -0.6429,\n           0.1037]]], device='cuda:0') \n```", "```py\n# Lets define a helpful benchmarking function:\nimport torch.utils.benchmark as benchmark\ndef benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n    t0 = [benchmark.Timer](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer \"torch.utils.benchmark.utils.timer.Timer\")(\n        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n    )\n    return t0.blocked_autorange().mean * 1e6\n\n# Lets define the hyper-parameters of our input\nbatch_size = 32\nmax_sequence_len = 1024\nnum_heads = 32\nembed_dimension = 32\n\n[dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\") = [torch.float16](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")\n\n[query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")=[dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n[key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")=[dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n[value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")=[dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\n\nprint(f\"The default implementation runs in {benchmark_torch_function_in_microseconds([F.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention \"torch.nn.functional.scaled_dot_product_attention\"),  [query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),  [key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),  [value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):.3f} microseconds\")\n\n# Lets explore the speed of each of the 3 implementations\nfrom torch.backends.cuda import [sdp_kernel](https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.sdp_kernel \"torch.backends.cuda.sdp_kernel\"), SDPBackend\n\n# Helpful arguments mapper\nbackend_map = {\n    SDPBackend.MATH: {\"enable_math\": True, \"enable_flash\": False, \"enable_mem_efficient\": False},\n    SDPBackend.FLASH_ATTENTION: {\"enable_math\": False, \"enable_flash\": True, \"enable_mem_efficient\": False},\n    SDPBackend.EFFICIENT_ATTENTION: {\n        \"enable_math\": False, \"enable_flash\": False, \"enable_mem_efficient\": True}\n}\n\nwith [sdp_kernel](https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.sdp_kernel \"torch.backends.cuda.sdp_kernel\")(**backend_map[SDPBackend.MATH]):\n    print(f\"The math implementation runs in {benchmark_torch_function_in_microseconds([F.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention \"torch.nn.functional.scaled_dot_product_attention\"),  [query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),  [key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),  [value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):.3f} microseconds\")\n\nwith [sdp_kernel](https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.sdp_kernel \"torch.backends.cuda.sdp_kernel\")(**backend_map[SDPBackend.FLASH_ATTENTION]):\n    try:\n        print(f\"The flash attention implementation runs in {benchmark_torch_function_in_microseconds([F.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention \"torch.nn.functional.scaled_dot_product_attention\"),  [query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),  [key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),  [value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")\n\nwith [sdp_kernel](https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.sdp_kernel \"torch.backends.cuda.sdp_kernel\")(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n    try:\n        print(f\"The memory efficient implementation runs in {benchmark_torch_function_in_microseconds([F.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention \"torch.nn.functional.scaled_dot_product_attention\"),  [query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),  [key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"),  [value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):.3f} microseconds\")\n    except RuntimeError:\n        print(\"EfficientAttention is not supported. See warnings for reasons.\") \n```", "```py\nThe default implementation runs in 2263.405 microseconds\nThe math implementation runs in 19254.524 microseconds\nThe flash attention implementation runs in 2262.901 microseconds\nThe memory efficient implementation runs in 4143.146 microseconds \n```", "```py\nclass CausalSelfAttention([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n\n    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0):\n        super().__init__()\n        assert embed_dimension % num_heads == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(embed_dimension, 3 * embed_dimension, bias=bias)\n        # output projection\n        self.c_proj = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(embed_dimension, embed_dimension, bias=bias)\n        # regularization\n        self.dropout = dropout\n        self.resid_dropout = [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(dropout)\n        self.num_heads = num_heads\n        self.embed_dimension = embed_dimension\n        # Perform causal masking\n        self.is_causal = is_causal\n\n    def forward(self, [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        query_projected = self.c_attn([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\n        batch_size = query_projected.size(0)\n        embed_dim = query_projected.size(2)\n        head_dim = embed_dim // (self.num_heads * 3)\n\n        [query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = query_projected.chunk(3, -1)\n        [query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n        [key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n        [value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n\n        if self.training:\n            dropout = self.dropout\n            is_causal = self.is_causal\n        else:\n            dropout = 0.0\n            is_causal = False\n\n        y = [F.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention \"torch.nn.functional.scaled_dot_product_attention\")([query](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [key](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n        y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim)\n\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nnum_heads = 8\nheads_per_dim = 64\nembed_dimension = num_heads * heads_per_dim\n[dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\") = [torch.float16](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")\nmodel = [CausalSelfAttention](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to([dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")).eval()\nprint(model) \n```", "```py\nCausalSelfAttention(\n  (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n  (c_proj): Linear(in_features=512, out_features=512, bias=False)\n  (resid_dropout): Dropout(p=0.1, inplace=False)\n) \n```", "```py\nimport random\ndef generate_rand_batch(\n    batch_size,\n    max_sequence_len,\n    embed_dimension,\n    pad_percentage=None,\n    [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")=[torch.float16](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"),\n    device=\"cuda\",\n):\n    if not pad_percentage:\n        return (\n            [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(\n                batch_size,\n                max_sequence_len,\n                embed_dimension,\n                [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")=[dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"),\n                device=device,\n            ),\n            None,\n        )\n    # Random sequence lengths\n    seq_len_list = [\n        int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))\n        for _ in range(batch_size)\n    ]\n    # Make random entry in the batch have max sequence length\n    seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len\n    return (\n        [torch.nested.nested_tensor](https://pytorch.org/docs/stable/nested.html#torch.nested.nested_tensor \"torch.nested.nested_tensor\")(\n            [\n                [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(seq_len, embed_dimension,\n                            [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")=[dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"), device=device)\n                for seq_len in seq_len_list\n            ]\n        ),\n        seq_len_list,\n    )\n\n[random_nt](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=0.5, [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")=[dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"), device=device)\n[random_dense](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=None, [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")=[dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"), device=device)\n\n# Currently the fused implementations don't support ``NestedTensor`` for training\n[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")()\n\nwith [sdp_kernel](https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.sdp_kernel \"torch.backends.cuda.sdp_kernel\")(**backend_map[SDPBackend.FLASH_ATTENTION]):\n    try:\n        print(f\"Random NT runs in {benchmark_torch_function_in_microseconds(model,  [random_nt](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):.3f} microseconds\")\n        print(f\"Random Dense runs in {benchmark_torch_function_in_microseconds(model,  [random_dense](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\") \n```", "```py\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nested/__init__.py:166: UserWarning:\n\nThe PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n\nRandom NT runs in 560.000 microseconds\nRandom Dense runs in 938.743 microseconds \n```", "```py\nbatch_size = 32\nmax_sequence_len = 256\n[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand \"torch.rand\")(batch_size, max_sequence_len,\n               embed_dimension, device=device, [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")=[dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\nprint(\n    f\"The non compiled module runs in {benchmark_torch_function_in_microseconds(model,  [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):.3f} microseconds\")\n\ncompiled_model = [torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile \"torch.compile\")(model)\n# Let's compile it\ncompiled_model([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nprint(\n    f\"The compiled module runs in {benchmark_torch_function_in_microseconds(compiled_model,  [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):.3f} microseconds\") \n```", "```py\nThe non compiled module runs in  407.788 microseconds\nThe compiled module runs in  521.239 microseconds \n```", "```py\nfrom torch.profiler import [profile](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile \"torch.profiler.profile\"), record_function, ProfilerActivity\nactivities = [[ProfilerActivity.CPU](https://pytorch.org/docs/stable/profiler.html#torch.profiler.ProfilerActivity \"torch._C._profiler.ProfilerActivity\")]\nif device == 'cuda':\n    activities.append([ProfilerActivity.CUDA](https://pytorch.org/docs/stable/profiler.html#torch.profiler.ProfilerActivity \"torch._C._profiler.ProfilerActivity\"))\n\nwith [profile](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile \"torch.profiler.profile\")(activities=activities, record_shapes=False) as [prof](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile \"torch.profiler.profile\"):\n    with record_function(\" Non-Compilied Causal Attention\"):\n        for _ in range(25):\n            model([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nprint([prof.key_averages](https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.key_averages \"torch.profiler._KinetoProfile.key_averages\")().table(sort_by=\"cuda_time_total\", row_limit=10))\n\nwith [profile](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile \"torch.profiler.profile\")(activities=activities, record_shapes=False) as [prof](https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile \"torch.profiler.profile\"):\n    with record_function(\"Compiled Causal Attention\"):\n        for _ in range(25):\n            compiled_model([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nprint([prof.key_averages](https://pytorch.org/docs/stable/profiler.html#torch.profiler._KinetoProfile.key_averages \"torch.profiler._KinetoProfile.key_averages\")().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n#\n# .. code-block:: python\n#\n#    prof.export_chrome_trace(\"compiled_causal_attention_trace.json\"). \n```", "```py\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                         Non-Compilied Causal Attention        18.51%       2.124ms        75.85%       8.703ms       8.703ms       0.000us         0.00%      11.033ms      11.033ms             1\n                                           aten::matmul         2.23%     256.000us        27.21%       3.122ms      62.440us       0.000us         0.00%       8.156ms     163.120us            50\n                                               aten::mm        19.17%       2.200ms        23.15%       2.656ms      53.120us       7.752ms        76.53%       8.156ms     163.120us            50\n                                           aten::linear         1.83%     210.000us        30.51%       3.501ms      70.020us       0.000us         0.00%       7.846ms     156.920us            50\n         ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us       5.554ms        54.83%       5.554ms     222.160us            25\n                     aten::scaled_dot_product_attention         1.97%     226.000us        18.83%       2.161ms      86.440us       0.000us         0.00%       2.877ms     115.080us            25\n              aten::_scaled_dot_product_flash_attention         3.51%     403.000us        16.86%       1.935ms      77.400us       0.000us         0.00%       2.877ms     115.080us            25\n                         aten::_flash_attention_forward         4.62%     530.000us        12.10%       1.388ms      55.520us       2.377ms        23.47%       2.877ms     115.080us            25\nvoid pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us       2.377ms        23.47%       2.377ms      95.080us            25\nampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us       2.198ms        21.70%       2.198ms      87.920us            25\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 11.474ms\nSelf CUDA time total: 10.129ms\n\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                              Compiled Causal Attention         9.79%       1.158ms        93.81%      11.091ms      11.091ms       0.000us         0.00%      10.544ms      10.544ms             1\n                                  Torch-Compiled Region         8.51%       1.006ms        82.19%       9.717ms     388.680us       0.000us         0.00%      10.544ms     421.760us            25\n                                       CompiledFunction        41.11%       4.861ms        72.93%       8.622ms     344.880us       0.000us         0.00%      10.544ms     421.760us            25\n                                               aten::mm         7.96%     941.000us        12.70%       1.502ms      30.040us       7.755ms        76.49%       7.843ms     156.860us            50\n         ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us       5.556ms        54.80%       5.556ms     222.240us            25\n              aten::_scaled_dot_product_flash_attention         2.30%     272.000us        15.12%       1.788ms      71.520us       0.000us         0.00%       2.701ms     108.040us            25\n                         aten::_flash_attention_forward         4.58%     541.000us        11.52%       1.362ms      54.480us       2.383ms        23.51%       2.701ms     108.040us            25\nvoid pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us       2.383ms        23.51%       2.383ms      95.320us            25\nampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us       2.199ms        21.69%       2.199ms      87.960us            25\n                                  cudaStreamIsCapturing         0.24%      28.000us         0.24%      28.000us       1.120us     222.000us         2.19%     222.000us       8.880us            25\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 11.823ms\nSelf CUDA time total: 10.138ms \n```"]