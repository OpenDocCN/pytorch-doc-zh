["```py\nimport torch\nfrom torch.utils.data import [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset \"torch.utils.data.Dataset\")\nfrom torchvision import datasets\nfrom torchvision.transforms import [ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor \"torchvision.transforms.ToTensor\")\nimport matplotlib.pyplot as plt\n\n[training_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\") = [datasets.FashionMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\")(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=[ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor \"torchvision.transforms.ToTensor\")()\n)\n\n[test_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\") = [datasets.FashionMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\")(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=[ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor \"torchvision.transforms.ToTensor\")()\n) \n```", "```py\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/26421880 [00:00<?, ?it/s]\n  0%|          | 65536/26421880 [00:00<01:12, 365057.28it/s]\n  1%|          | 229376/26421880 [00:00<00:37, 693381.35it/s]\n  2%|2         | 655360/26421880 [00:00<00:14, 1837266.59it/s]\n  6%|5         | 1507328/26421880 [00:00<00:07, 3214435.37it/s]\n 17%|#6        | 4489216/26421880 [00:00<00:02, 10348304.36it/s]\n 31%|###       | 8126464/26421880 [00:00<00:01, 14655512.28it/s]\n 50%|####9     | 13107200/26421880 [00:00<00:00, 23379028.97it/s]\n 65%|######5   | 17235968/26421880 [00:01<00:00, 23640128.44it/s]\n 85%|########4 | 22347776/26421880 [00:01<00:00, 30209848.84it/s]\n100%|#########9| 26312704/26421880 [00:01<00:00, 27567395.90it/s]\n100%|##########| 26421880/26421880 [00:01<00:00, 18266988.23it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/29515 [00:00<?, ?it/s]\n100%|##########| 29515/29515 [00:00<00:00, 327415.67it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00<?, ?it/s]\n  1%|1         | 65536/4422102 [00:00<00:11, 364864.90it/s]\n  5%|5         | 229376/4422102 [00:00<00:06, 685739.55it/s]\n 21%|##1       | 950272/4422102 [00:00<00:01, 2200229.85it/s]\n 87%|########6 | 3833856/4422102 [00:00<00:00, 7656139.70it/s]\n100%|##########| 4422102/4422102 [00:00<00:00, 6118794.98it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/5148 [00:00<?, ?it/s]\n100%|##########| 5148/5148 [00:00<00:00, 41443909.77it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw \n```", "```py\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = [torch.randint](https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint \"torch.randint\")(len([training_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\")), size=(1,)).item()\n    [img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [training_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\")[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[[label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")])\n    plt.axis(\"off\")\n    plt.imshow([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").squeeze(), cmap=\"gray\")\nplt.show() \n```", "```py\nimport os\nimport pandas as pd\nfrom torchvision.io import [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image \"torchvision.io.read_image\")\n\nclass CustomImageDataset([Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset \"torch.utils.data.Dataset\")):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image \"torchvision.io.read_image\")(img_path)\n        [label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            [label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = self.target_transform([label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n        return image, [label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") \n```", "```py\ntshirt1.jpg, 0\ntshirt2.jpg, 0\n......\nankleboot999.jpg, 9 \n```", "```py\ndef __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n    self.img_labels = pd.read_csv(annotations_file)\n    self.img_dir = img_dir\n    self.transform = transform\n    self.target_transform = target_transform \n```", "```py\ndef __len__(self):\n    return len(self.img_labels) \n```", "```py\ndef __getitem__(self, idx):\n    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n    image = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image \"torchvision.io.read_image\")(img_path)\n    [label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = self.img_labels.iloc[idx, 1]\n    if self.transform:\n        image = self.transform(image)\n    if self.target_transform:\n        [label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = self.target_transform([label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    return image, [label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") \n```", "```py\nfrom torch.utils.data import [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")\n\n[train_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")([training_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\"), batch_size=64, shuffle=True)\n[test_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")([test_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST \"torchvision.datasets.FashionMNIST\"), batch_size=64, shuffle=True) \n```", "```py\n# Display image and label.\n[train_features](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [train_labels](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = next(iter([train_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")))\nprint(f\"Feature batch shape: {[train_features](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").size()}\")\nprint(f\"Labels batch shape: {[train_labels](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").size()}\")\n[img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [train_features](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")[0].squeeze()\n[label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [train_labels](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")[0]\nplt.imshow([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), cmap=\"gray\")\nplt.show()\nprint(f\"Label: {[label](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")}\") \n```", "```py\nFeature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\nLabel: 5 \n```"]