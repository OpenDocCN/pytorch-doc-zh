["```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\n# Check if GPU is available, and if not, use the CPU\n[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")(\"cuda\" if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available \"torch.cuda.is_available\")() else \"cpu\") \n```", "```py\n# Below we are preprocessing data for CIFAR-10\\. We use an arbitrary batch size of 128.\n[transforms_cifar](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\") = [transforms.Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\")([\n    [transforms.ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor \"torchvision.transforms.ToTensor\")(),\n    [transforms.Normalize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize \"torchvision.transforms.Normalize\")(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Loading the CIFAR-10 dataset:\n[train_dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10 \"torchvision.datasets.CIFAR10\") = [datasets.CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10 \"torchvision.datasets.CIFAR10\")(root='./data', train=True, download=True, transform=[transforms_cifar](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\"))\n[test_dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10 \"torchvision.datasets.CIFAR10\") = [datasets.CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10 \"torchvision.datasets.CIFAR10\")(root='./data', train=False, download=True, transform=[transforms_cifar](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\")) \n```", "```py\nFiles already downloaded and verified\nFiles already downloaded and verified \n```", "```py\n#from torch.utils.data import Subset\n#num_images_to_keep = 2000\n#train_dataset = Subset(train_dataset, range(min(num_images_to_keep, 50_000)))\n#test_dataset = Subset(test_dataset, range(min(num_images_to_keep, 10_000))) \n```", "```py\n#Dataloaders\n[train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")([train_dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10 \"torchvision.datasets.CIFAR10\"), batch_size=128, shuffle=True, num_workers=2)\n[test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\") = [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")([test_dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10 \"torchvision.datasets.CIFAR10\"), batch_size=128, shuffle=False, num_workers=2) \n```", "```py\n# Deeper neural network class to be used as teacher:\nclass DeepNN([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, num_classes=10):\n        super([DeepNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.features = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(3, 128, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(128, 64, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(64, 64, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(64, 32, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n        )\n        self.classifier = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(2048, 512),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(0.1),\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten \"torch.flatten\")(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Lightweight neural network class to be used as student:\nclass LightNN([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, num_classes=10):\n        super([LightNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.features = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(3, 16, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(16, 16, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n        )\n        self.classifier = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(1024, 256),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(0.1),\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten \"torch.flatten\")(x, 1)\n        x = self.classifier(x)\n        return x \n```", "```py\ndef train(model, [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), epochs, learning_rate, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")):\n    criterion = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\n    optimizer = [optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam \"torch.optim.Adam\")(model.parameters(), lr=learning_rate)\n\n    model.train()\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"):\n            # inputs: A collection of batch_size images\n            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n            inputs, labels = inputs.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")), labels.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n\n            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n            # labels: The actual labels of the images. Vector of dimensionality batch_size\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss  /  len([train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"))}\")\n\ndef test(model, [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")):\n    model.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n    model.eval()\n\n    correct = 0\n    total = 0\n\n    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n        for inputs, labels in [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"):\n            inputs, labels = inputs.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")), labels.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\n            outputs = model(inputs)\n            _, predicted = [torch.max](https://pytorch.org/docs/stable/generated/torch.max.html#torch.max \"torch.max\")(outputs.data, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    return accuracy \n```", "```py\n[torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed \"torch.manual_seed\")(42)\nnn_deep = [DeepNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(num_classes=10).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\ntrain(nn_deep, [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), epochs=10, learning_rate=0.001, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\ntest_accuracy_deep = test(nn_deep, [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\n# Instantiate the lightweight network:\n[torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed \"torch.manual_seed\")(42)\nnn_light = [LightNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(num_classes=10).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")) \n```", "```py\nEpoch 1/10, Loss: 1.33431153483403\nEpoch 2/10, Loss: 0.8656839088100912\nEpoch 3/10, Loss: 0.6777699019597925\nEpoch 4/10, Loss: 0.5402812090371271\nEpoch 5/10, Loss: 0.4225304535663951\nEpoch 6/10, Loss: 0.3173445740243053\nEpoch 7/10, Loss: 0.2325386164324058\nEpoch 8/10, Loss: 0.17896929922539864\nEpoch 9/10, Loss: 0.1499793469581915\nEpoch 10/10, Loss: 0.12164150110310148\nTest Accuracy: 75.18% \n```", "```py\n[torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed \"torch.manual_seed\")(42)\nnew_nn_light = [LightNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(num_classes=10).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")) \n```", "```py\n# Print the norm of the first layer of the initial lightweight model\nprint(\"Norm of 1st layer of nn_light:\", [torch.norm](https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm \"torch.norm\")([nn_light.features](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")[0].weight).item())\n# Print the norm of the first layer of the new lightweight model\nprint(\"Norm of 1st layer of new_nn_light:\", [torch.norm](https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm \"torch.norm\")([new_nn_light.features](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")[0].weight).item()) \n```", "```py\nNorm of 1st layer of nn_light: 2.327361822128296\nNorm of 1st layer of new_nn_light: 2.327361822128296 \n```", "```py\ntotal_params_deep = \"{:,}\".format(sum(p.numel() for p in [nn_deep.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")()))\nprint(f\"DeepNN parameters: {total_params_deep}\")\ntotal_params_light = \"{:,}\".format(sum(p.numel() for p in [nn_light.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")()))\nprint(f\"LightNN parameters: {total_params_light}\") \n```", "```py\nDeepNN parameters: 1,186,986\nLightNN parameters: 267,738 \n```", "```py\ntrain(nn_light, [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), epochs=10, learning_rate=0.001, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\ntest_accuracy_light_ce = test(nn_light, [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")) \n```", "```py\nEpoch 1/10, Loss: 1.4691094873506394\nEpoch 2/10, Loss: 1.157914390344449\nEpoch 3/10, Loss: 1.0261659164867742\nEpoch 4/10, Loss: 0.9236082335567231\nEpoch 5/10, Loss: 0.8480177427191868\nEpoch 6/10, Loss: 0.7821924878508234\nEpoch 7/10, Loss: 0.7189932451833545\nEpoch 8/10, Loss: 0.6598629956050297\nEpoch 9/10, Loss: 0.6044211582759457\nEpoch 10/10, Loss: 0.5556994059201702\nTest Accuracy: 70.60% \n```", "```py\nprint(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\nprint(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\") \n```", "```py\nTeacher accuracy: 75.18%\nStudent accuracy: 70.60% \n```", "```py\ndef train_knowledge_distillation(teacher, student, [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")):\n    ce_loss = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\n    optimizer = [optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam \"torch.optim.Adam\")(student.parameters(), lr=learning_rate)\n\n    teacher.eval()  # Teacher set to evaluation mode\n    student.train() # Student to train mode\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"):\n            inputs, labels = inputs.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")), labels.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\n            optimizer.zero_grad()\n\n            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n            with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n                teacher_logits = teacher(inputs)\n\n            # Forward pass with the student model\n            student_logits = student(inputs)\n\n            #Soften the student logits by applying softmax first and log() second\n            soft_targets = [nn.functional.softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax \"torch.nn.functional.softmax\")(teacher_logits / T, dim=-1)\n            soft_prob = [nn.functional.log_softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax \"torch.nn.functional.log_softmax\")(student_logits / T, dim=-1)\n\n            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n            soft_targets_loss = -[torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")(soft_targets * soft_prob) / soft_prob.size()[0] * (T**2)\n\n            # Calculate the true label loss\n            label_loss = ce_loss(student_logits, labels)\n\n            # Weighted sum of the two losses\n            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss  /  len([train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"))}\")\n\n# Apply ``train_knowledge_distillation`` with a temperature of 2\\. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\ntrain_knowledge_distillation(teacher=nn_deep, student=new_nn_light, [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")=[train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\ntest_accuracy_light_ce_and_kd = test(new_nn_light, [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\n# Compare the student test accuracy with and without the teacher, after distillation\nprint(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\nprint(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\nprint(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\") \n```", "```py\nEpoch 1/10, Loss: 2.7032457148022666\nEpoch 2/10, Loss: 2.1822731882105093\nEpoch 3/10, Loss: 1.9572431745431613\nEpoch 4/10, Loss: 1.7957131417511065\nEpoch 5/10, Loss: 1.6697854071931766\nEpoch 6/10, Loss: 1.5559934722188185\nEpoch 7/10, Loss: 1.464548922865592\nEpoch 8/10, Loss: 1.379408223244845\nEpoch 9/10, Loss: 1.306471157409346\nEpoch 10/10, Loss: 1.2383389463814933\nTest Accuracy: 70.57%\nTeacher accuracy: 75.18%\nStudent accuracy without teacher: 70.60%\nStudent accuracy with CE + KD: 70.57% \n```", "```py\nclass ModifiedDeepNNCosine([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, num_classes=10):\n        super([ModifiedDeepNNCosine](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.features = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(3, 128, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(128, 64, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(64, 64, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(64, 32, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n        )\n        self.classifier = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(2048, 512),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(0.1),\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        flattened_conv_output = [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten \"torch.flatten\")(x, 1)\n        x = self.classifier(flattened_conv_output)\n        flattened_conv_output_after_pooling = [torch.nn.functional.avg_pool1d](https://pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool1d.html#torch.nn.functional.avg_pool1d \"torch.nn.functional.avg_pool1d\")(flattened_conv_output, 2)\n        return x, flattened_conv_output_after_pooling\n\n# Create a similar student class where we return a tuple. We do not apply pooling after flattening.\nclass ModifiedLightNNCosine([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, num_classes=10):\n        super([ModifiedLightNNCosine](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.features = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(3, 16, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(16, 16, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n        )\n        self.classifier = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(1024, 256),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(0.1),\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        flattened_conv_output = [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten \"torch.flatten\")(x, 1)\n        x = self.classifier(flattened_conv_output)\n        return x, flattened_conv_output\n\n# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\nmodified_nn_deep = [ModifiedDeepNNCosine](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(num_classes=10).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n[modified_nn_deep.load_state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict \"torch.nn.Module.load_state_dict\")([nn_deep.state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict \"torch.nn.Module.state_dict\")())\n\n# Once again ensure the norm of the first layer is the same for both networks\nprint(\"Norm of 1st layer for deep_nn:\", [torch.norm](https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm \"torch.norm\")([nn_deep.features](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")[0].weight).item())\nprint(\"Norm of 1st layer for modified_deep_nn:\", [torch.norm](https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm \"torch.norm\")([modified_nn_deep.features](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")[0].weight).item())\n\n# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.\n[torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed \"torch.manual_seed\")(42)\nmodified_nn_light = [ModifiedLightNNCosine](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(num_classes=10).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\nprint(\"Norm of 1st layer:\", [torch.norm](https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm \"torch.norm\")([modified_nn_light.features](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")[0].weight).item()) \n```", "```py\nNorm of 1st layer for deep_nn: 7.510530471801758\nNorm of 1st layer for modified_deep_nn: 7.510530471801758\nNorm of 1st layer: 2.327361822128296 \n```", "```py\n# Create a sample input tensor\n[sample_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn \"torch.randn\")(128, 3, 32, 32).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")) # Batch size: 128, Filters: 3, Image size: 32x32\n\n# Pass the input through the student\n[logits](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [hidden_representation](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = modified_nn_light([sample_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\n# Print the shapes of the tensors\nprint(\"Student logits shape:\", [logits](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").shape) # batch_size x total_classes\nprint(\"Student hidden representation shape:\", [hidden_representation](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").shape) # batch_size x hidden_representation_size\n\n# Pass the input through the teacher\n[logits](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), [hidden_representation](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = modified_nn_deep([sample_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\n# Print the shapes of the tensors\nprint(\"Teacher logits shape:\", [logits](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").shape) # batch_size x total_classes\nprint(\"Teacher hidden representation shape:\", [hidden_representation](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").shape) # batch_size x hidden_representation_size \n```", "```py\nStudent logits shape: torch.Size([128, 10])\nStudent hidden representation shape: torch.Size([128, 1024])\nTeacher logits shape: torch.Size([128, 10])\nTeacher hidden representation shape: torch.Size([128, 1024]) \n```", "```py\ndef train_cosine_loss(teacher, student, [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")):\n    ce_loss = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\n    cosine_loss = [nn.CosineEmbeddingLoss](https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss \"torch.nn.CosineEmbeddingLoss\")()\n    optimizer = [optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam \"torch.optim.Adam\")(student.parameters(), lr=learning_rate)\n\n    teacher.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n    student.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n    teacher.eval()  # Teacher set to evaluation mode\n    student.train() # Student to train mode\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"):\n            inputs, labels = inputs.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")), labels.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\n            optimizer.zero_grad()\n\n            # Forward pass with the teacher model and keep only the hidden representation\n            with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n                _, teacher_hidden_representation = teacher(inputs)\n\n            # Forward pass with the student model\n            student_logits, student_hidden_representation = student(inputs)\n\n            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=[torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones \"torch.ones\")(inputs.size(0)).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")))\n\n            # Calculate the true label loss\n            label_loss = ce_loss(student_logits, labels)\n\n            # Weighted sum of the two losses\n            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss  /  len([train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"))}\") \n```", "```py\ndef test_multiple_outputs(model, [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")):\n    model.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n    model.eval()\n\n    correct = 0\n    total = 0\n\n    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n        for inputs, labels in [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"):\n            inputs, labels = inputs.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")), labels.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\n            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n            _, predicted = [torch.max](https://pytorch.org/docs/stable/generated/torch.max.html#torch.max \"torch.max\")(outputs.data, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    return accuracy \n```", "```py\n# Train and test the lightweight network with cross entropy loss\ntrain_cosine_loss(teacher=modified_nn_deep, student=modified_nn_light, [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")=[train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), epochs=10, learning_rate=0.001, hidden_rep_loss_weight=0.25, ce_loss_weight=0.75, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\ntest_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_nn_light, [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")) \n```", "```py\nEpoch 1/10, Loss: 1.3057707054230867\nEpoch 2/10, Loss: 1.0680991774019988\nEpoch 3/10, Loss: 0.9685801694460232\nEpoch 4/10, Loss: 0.8937607102686792\nEpoch 5/10, Loss: 0.8375817691273701\nEpoch 6/10, Loss: 0.7915807698693726\nEpoch 7/10, Loss: 0.7496646805797391\nEpoch 8/10, Loss: 0.7140546901451658\nEpoch 9/10, Loss: 0.6746650690312885\nEpoch 10/10, Loss: 0.6464888599065258\nTest Accuracy: 71.47% \n```", "```py\n# Pass the sample input only from the convolutional feature extractor\n[convolutional_fe_output_student](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [nn_light.features](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")([sample_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n[convolutional_fe_output_teacher](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [nn_deep.features](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")([sample_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\n# Print their shapes\nprint(\"Student's feature extractor output shape: \", [convolutional_fe_output_student](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").shape)\nprint(\"Teacher's feature extractor output shape: \", [convolutional_fe_output_teacher](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").shape) \n```", "```py\nStudent's feature extractor output shape:  torch.Size([128, 16, 8, 8])\nTeacher's feature extractor output shape:  torch.Size([128, 32, 8, 8]) \n```", "```py\nclass ModifiedDeepNNRegressor([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, num_classes=10):\n        super([ModifiedDeepNNRegressor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.features = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(3, 128, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(128, 64, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(64, 64, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(64, 32, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n        )\n        self.classifier = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(2048, 512),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(0.1),\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        conv_feature_map = x\n        x = [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten \"torch.flatten\")(x, 1)\n        x = self.classifier(x)\n        return x, conv_feature_map\n\nclass ModifiedLightNNRegressor([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, num_classes=10):\n        super([ModifiedLightNNRegressor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.features = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(3, 16, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(16, 16, kernel_size=3, padding=1),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d \"torch.nn.MaxPool2d\")(kernel_size=2, stride=2),\n        )\n        # Include an extra regressor (in our case linear)\n        self.regressor = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(16, 32, kernel_size=3, padding=1)\n        )\n        self.classifier = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(1024, 256),\n            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU \"torch.nn.ReLU\")(),\n            [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(0.1),\n            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        regressor_output = self.regressor(x)\n        x = [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten \"torch.flatten\")(x, 1)\n        x = self.classifier(x)\n        return x, regressor_output \n```", "```py\ndef train_mse_loss(teacher, student, [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), epochs, learning_rate, feature_map_weight, ce_loss_weight, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")):\n    ce_loss = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\n    mse_loss = [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss \"torch.nn.MSELoss\")()\n    optimizer = [optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam \"torch.optim.Adam\")(student.parameters(), lr=learning_rate)\n\n    teacher.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n    student.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n    teacher.eval()  # Teacher set to evaluation mode\n    student.train() # Student to train mode\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"):\n            inputs, labels = inputs.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")), labels.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\n            optimizer.zero_grad()\n\n            # Again ignore teacher logits\n            with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n                _, teacher_feature_map = teacher(inputs)\n\n            # Forward pass with the student model\n            student_logits, regressor_feature_map = student(inputs)\n\n            # Calculate the loss\n            hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n\n            # Calculate the true label loss\n            label_loss = ce_loss(student_logits, labels)\n\n            # Weighted sum of the two losses\n            loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss  /  len([train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"))}\")\n\n# Notice how our test function remains the same here with the one we used in our previous case. We only care about the actual outputs because we measure accuracy.\n\n# Initialize a ModifiedLightNNRegressor\n[torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed \"torch.manual_seed\")(42)\nmodified_nn_light_reg = [ModifiedLightNNRegressor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(num_classes=10).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n\n# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\nmodified_nn_deep_reg = [ModifiedDeepNNRegressor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(num_classes=10).to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\n[modified_nn_deep_reg.load_state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict \"torch.nn.Module.load_state_dict\")([nn_deep.state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict \"torch.nn.Module.state_dict\")())\n\n# Train and test once again\ntrain_mse_loss(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, [train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")=[train_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), epochs=10, learning_rate=0.001, feature_map_weight=0.25, ce_loss_weight=0.75, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\"))\ntest_accuracy_light_ce_and_mse_loss = test_multiple_outputs(modified_nn_light_reg, [test_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")) \n```", "```py\nEpoch 1/10, Loss: 1.6985262717737262\nEpoch 2/10, Loss: 1.325937156787004\nEpoch 3/10, Loss: 1.1824340555064208\nEpoch 4/10, Loss: 1.0864463061322946\nEpoch 5/10, Loss: 1.009828634731605\nEpoch 6/10, Loss: 0.9486901266190707\nEpoch 7/10, Loss: 0.8957636421903625\nEpoch 8/10, Loss: 0.8455343330302811\nEpoch 9/10, Loss: 0.8041850715646963\nEpoch 10/10, Loss: 0.7668854673500256\nTest Accuracy: 70.80% \n```", "```py\nprint(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\nprint(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\nprint(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\nprint(f\"Student accuracy with CE + CosineLoss: {test_accuracy_light_ce_and_cosine_loss:.2f}%\")\nprint(f\"Student accuracy with CE + RegressorMSE: {test_accuracy_light_ce_and_mse_loss:.2f}%\") \n```", "```py\nTeacher accuracy: 75.18%\nStudent accuracy without teacher: 70.60%\nStudent accuracy with CE + KD: 70.57%\nStudent accuracy with CE + CosineLoss: 71.47%\nStudent accuracy with CE + RegressorMSE: 70.80% \n```"]