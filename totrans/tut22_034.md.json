["```py\npip install torch torchvision timm pandas requests \n```", "```py\n!pip install timm pandas requests \n```", "```py\nfrom PIL import Image\nimport torch\nimport timm\nimport requests\nimport torchvision.transforms as transforms\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\nprint(torch.__version__)\n# should be 1.8.0\n\nmodel = [torch.hub.load](https://pytorch.org/docs/stable/hub.html#torch.hub.load \"torch.hub.load\")('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")()\n\n[transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\") = [transforms.Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\")([\n    [transforms.Resize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html#torchvision.transforms.Resize \"torchvision.transforms.Resize\")(256, interpolation=3),\n    [transforms.CenterCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.CenterCrop.html#torchvision.transforms.CenterCrop \"torchvision.transforms.CenterCrop\")(224),\n    [transforms.ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor \"torchvision.transforms.ToTensor\")(),\n    [transforms.Normalize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize \"torchvision.transforms.Normalize\")(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n])\n\n[img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n[img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose \"torchvision.transforms.Compose\")([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))[None,]\n[out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n[clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax \"torch.argmax\")([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nprint([clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").item()) \n```", "```py\n2.2.0+cu121\nDownloading: \"https://github.com/facebookresearch/deit/zipball/main\" to /var/lib/jenkins/.cache/torch/hub/main.zip\n/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:63: UserWarning:\n\nOverwriting deit_tiny_patch16_224 in registry with models.deit_tiny_patch16_224\\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:78: UserWarning:\n\nOverwriting deit_small_patch16_224 in registry with models.deit_small_patch16_224\\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:93: UserWarning:\n\nOverwriting deit_base_patch16_224 in registry with models.deit_base_patch16_224\\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:108: UserWarning:\n\nOverwriting deit_tiny_distilled_patch16_224 in registry with models.deit_tiny_distilled_patch16_224\\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:123: UserWarning:\n\nOverwriting deit_small_distilled_patch16_224 in registry with models.deit_small_distilled_patch16_224\\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:138: UserWarning:\n\nOverwriting deit_base_distilled_patch16_224 in registry with models.deit_base_distilled_patch16_224\\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:153: UserWarning:\n\nOverwriting deit_base_patch16_384 in registry with models.deit_base_patch16_384\\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\n/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:168: UserWarning:\n\nOverwriting deit_base_distilled_patch16_384 in registry with models.deit_base_distilled_patch16_384\\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n\nDownloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n\n  0%|          | 0.00/330M [00:00<?, ?B/s]\n  4%|3         | 12.4M/330M [00:00<00:02, 130MB/s]\n  7%|7         | 24.7M/330M [00:00<00:02, 110MB/s]\n 11%|#1        | 36.8M/330M [00:00<00:02, 117MB/s]\n 15%|#4        | 49.2M/330M [00:00<00:02, 121MB/s]\n 19%|#8        | 62.2M/330M [00:00<00:02, 127MB/s]\n 23%|##3       | 76.7M/330M [00:00<00:01, 135MB/s]\n 27%|##7       | 90.6M/330M [00:00<00:01, 139MB/s]\n 32%|###1      | 106M/330M [00:00<00:01, 144MB/s]\n 36%|###6      | 119M/330M [00:00<00:01, 125MB/s]\n 40%|###9      | 132M/330M [00:01<00:01, 122MB/s]\n 45%|####4     | 147M/330M [00:01<00:01, 132MB/s]\n 49%|####8     | 162M/330M [00:01<00:01, 138MB/s]\n 53%|#####3    | 176M/330M [00:01<00:01, 142MB/s]\n 58%|#####7    | 190M/330M [00:01<00:01, 144MB/s]\n 62%|######2   | 205M/330M [00:01<00:00, 147MB/s]\n 67%|######6   | 220M/330M [00:01<00:00, 149MB/s]\n 71%|#######   | 234M/330M [00:01<00:00, 148MB/s]\n 76%|#######5  | 250M/330M [00:01<00:00, 155MB/s]\n 81%|########1 | 268M/330M [00:01<00:00, 162MB/s]\n 86%|########6 | 285M/330M [00:02<00:00, 168MB/s]\n 91%|#########1| 302M/330M [00:02<00:00, 172MB/s]\n 97%|#########6| 319M/330M [00:02<00:00, 175MB/s]\n100%|##########| 330M/330M [00:02<00:00, 147MB/s]\n269 \n```", "```py\nmodel = [torch.hub.load](https://pytorch.org/docs/stable/hub.html#torch.hub.load \"torch.hub.load\")('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval \"torch.nn.Module.eval\")()\nscripted_model = [torch.jit.script](https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script \"torch.jit.script\")(model)\n[scripted_model.save](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.save \"torch.jit.ScriptModule.save\")(\"fbdeit_scripted.pt\") \n```", "```py\nUsing cache found in /var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main \n```", "```py\n# Use 'x86' for server inference (the old 'fbgemm' is still available but 'x86' is the recommended default) and ``qnnpack`` for mobile inference.\nbackend = \"x86\" # replaced with ``qnnpack`` causing much worse inference speed for quantized model on this notebook\n[model.qconfig](https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig \"torch.ao.quantization.qconfig.QConfig\") = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\n\nquantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={[torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")}, dtype=[torch.qint8](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\"))\nscripted_quantized_model = [torch.jit.script](https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script \"torch.jit.script\")(quantized_model)\n[scripted_quantized_model.save](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.save \"torch.jit.ScriptModule.save\")(\"fbdeit_scripted_quantized.pt\") \n```", "```py\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/ao/quantization/observer.py:220: UserWarning:\n\nPlease use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch. \n```", "```py\n[out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = scripted_quantized_model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n[clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax \"torch.argmax\")([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nprint([clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").item())\n# The same output 269 should be printed \n```", "```py\n269 \n```", "```py\nfrom torch.utils.mobile_optimizer import [optimize_for_mobile](https://pytorch.org/docs/stable/mobile_optimizer.html#torch.utils.mobile_optimizer.optimize_for_mobile \"torch.utils.mobile_optimizer.optimize_for_mobile\")\noptimized_scripted_quantized_model = [optimize_for_mobile](https://pytorch.org/docs/stable/mobile_optimizer.html#torch.utils.mobile_optimizer.optimize_for_mobile \"torch.utils.mobile_optimizer.optimize_for_mobile\")(scripted_quantized_model)\n[optimized_scripted_quantized_model.save](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.save \"torch.jit.ScriptModule.save\")(\"fbdeit_optimized_scripted_quantized.pt\") \n```", "```py\n[out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = optimized_scripted_quantized_model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n[clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax \"torch.argmax\")([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nprint([clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").item())\n# Again, the same output 269 should be printed \n```", "```py\n269 \n```", "```py\noptimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\nptl = [torch.jit.load](https://pytorch.org/docs/stable/generated/torch.jit.load.html#torch.jit.load \"torch.jit.load\")(\"fbdeit_optimized_scripted_quantized_lite.ptl\") \n```", "```py\nwith [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\")(use_cuda=False) as [prof1](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\"):\n    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nwith [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\")(use_cuda=False) as [prof2](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\"):\n    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = scripted_model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nwith [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\")(use_cuda=False) as [prof3](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\"):\n    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = scripted_quantized_model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nwith [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\")(use_cuda=False) as [prof4](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\"):\n    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = optimized_scripted_quantized_model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nwith [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\")(use_cuda=False) as [prof5](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\"):\n    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = ptl([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n\nprint(\"original model: {:.2f}ms\".format([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")/1000))\nprint(\"scripted model: {:.2f}ms\".format([prof2.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")/1000))\nprint(\"scripted & quantized model: {:.2f}ms\".format([prof3.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")/1000))\nprint(\"scripted & quantized & optimized model: {:.2f}ms\".format([prof4.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")/1000))\nprint(\"lite model: {:.2f}ms\".format([prof5.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")/1000)) \n```", "```py\noriginal model: 123.27ms\nscripted model: 111.89ms\nscripted & quantized model: 129.99ms\nscripted & quantized & optimized model: 129.94ms\nlite model: 120.00ms \n```", "```py\noriginal  model:  1236.69ms\nscripted  model:  1226.72ms\nscripted  &  quantized  model:  593.19ms\nscripted  &  quantized  &  optimized  model:  598.01ms\nlite  model:  600.72ms \n```", "```py\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})\ndf = pd.concat([df, pd.DataFrame([\n    [\"{:.2f}ms\".format([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")/1000), \"0%\"],\n    [\"{:.2f}ms\".format([prof2.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")/1000),\n     \"{:.2f}%\".format(([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")-[prof2.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\"))/[prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")*100)],\n    [\"{:.2f}ms\".format([prof3.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")/1000),\n     \"{:.2f}%\".format(([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")-[prof3.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\"))/[prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")*100)],\n    [\"{:.2f}ms\".format([prof4.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")/1000),\n     \"{:.2f}%\".format(([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")-[prof4.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\"))/[prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")*100)],\n    [\"{:.2f}ms\".format([prof5.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")/1000),\n     \"{:.2f}%\".format(([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")-[prof5.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\"))/[prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total \"torch.autograd.profiler.profile.self_cpu_time_total\")*100)]],\n    columns=['Inference Time', 'Reduction'])], axis=1)\n\nprint(df)\n\n\"\"\"\n Model                             Inference Time    Reduction\n0   original model                             1236.69ms           0%\n1   scripted model                             1226.72ms        0.81%\n2   scripted & quantized model                  593.19ms       52.03%\n3   scripted & quantized & optimized model      598.01ms       51.64%\n4   lite model                                  600.72ms       51.43%\n\"\"\" \n```", "```py\n Model  ... Reduction\n0                          original model  ...        0%\n1                          scripted model  ...     9.23%\n2              scripted & quantized model  ...    -5.45%\n3  scripted & quantized & optimized model  ...    -5.41%\n4                              lite model  ...     2.65%\n\n[5 rows x 3 columns]\n\n'\\n        Model                             Inference Time    Reduction\\n0\\toriginal model                             1236.69ms           0%\\n1\\tscripted model                             1226.72ms        0.81%\\n2\\tscripted & quantized model                  593.19ms       52.03%\\n3\\tscripted & quantized & optimized model      598.01ms       51.64%\\n4\\tlite model                                  600.72ms       51.43%\\n' \n```"]