["```py\n# original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                 /\nlinear_weight_fp32\n\n# dynamically quantized model\n# linear and LSTM weights are in int8\nprevious_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32\n                     /\n   linear_weight_int8 \n```", "```py\nimport torch\n\n# define a floating point model\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(4, 4)\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n# create a quantized model instance\nmodel_int8 = torch.ao.quantization.quantize_dynamic(\n    model_fp32,  # the original model\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8)  # the target dtype for quantized weights\n\n# run the model\ninput_fp32 = torch.randn(4, 4, 4, 4)\nres = model_int8(input_fp32) \n```", "```py\n# original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                    /\n    linear_weight_fp32\n\n# statically quantized model\n# weights and activations are in int8\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                    /\n  linear_weight_int8 \n```", "```py\nimport torch\n\n# define a floating point model where some layers could be statically quantized\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.ao.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.ao.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # manually specify where tensors will be converted from floating\n        # point to quantized in the quantized model\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.relu(x)\n        # manually specify where tensors will be converted from quantized\n        # to floating point in the quantized model\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to eval mode for static quantization logic to work\nmodel_fp32.eval()\n\n# attach a global qconfig, which contains information about what kind\n# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n# for mobile inference. Other quantization configurations such as selecting\n# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n# can be specified here.\n# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n# for server inference.\n# model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\nmodel_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n\n# Fuse the activations to preceding layers, where applicable.\n# This needs to be done manually depending on the model architecture.\n# Common fusions include `conv + relu` and `conv + batchnorm + relu`\nmodel_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n\n# Prepare the model for static quantization. This inserts observers in\n# the model that will observe activation tensors during calibration.\nmodel_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)\n\n# calibrate the prepared model to determine quantization parameters for activations\n# in a real world setting, the calibration would be done with a representative dataset\ninput_fp32 = torch.randn(4, 1, 4, 4)\nmodel_fp32_prepared(input_fp32)\n\n# Convert the observed model to a quantized model. This does several things:\n# quantizes the weights, computes and stores the scale and bias value to be\n# used with each activation tensor, and replaces key operators with quantized\n# implementations.\nmodel_int8 = torch.ao.quantization.convert(model_fp32_prepared)\n\n# run the model, relevant calculations will happen in int8\nres = model_int8(input_fp32) \n```", "```py\n# original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                      /\n    linear_weight_fp32\n\n# model with fake_quants for modeling quantization numerics during training\nprevious_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32\n                           /\n   linear_weight_fp32 -- fq\n\n# quantized model\n# weights and activations are in int8\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                     /\n   linear_weight_int8 \n```", "```py\nimport torch\n\n# define a floating point model where some layers could benefit from QAT\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.ao.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.ao.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to eval for fusion to work\nmodel_fp32.eval()\n\n# attach a global qconfig, which contains information about what kind\n# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n# for mobile inference. Other quantization configurations such as selecting\n# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n# can be specified here.\n# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n# for server inference.\n# model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\nmodel_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n\n# fuse the activations to preceding layers, where applicable\n# this needs to be done manually depending on the model architecture\nmodel_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,\n    [['conv', 'bn', 'relu']])\n\n# Prepare the model for QAT. This inserts observers and fake_quants in\n# the model needs to be set to train for QAT logic to work\n# the model that will observe weight and activation tensors during calibration.\nmodel_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused.train())\n\n# run the training loop (not shown)\ntraining_loop(model_fp32_prepared)\n\n# Convert the observed model to a quantized model. This does several things:\n# quantizes the weights, computes and stores the scale and bias value to be\n# used with each activation tensor, fuses modules where appropriate,\n# and replaces key operators with quantized implementations.\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.ao.quantization.convert(model_fp32_prepared)\n\n# run the model, relevant calculations will happen in int8\nres = model_int8(input_fp32) \n```", "```py\nimport torch\nfrom torch.ao.quantization import (\n  get_default_qconfig_mapping,\n  get_default_qat_qconfig_mapping,\n  QConfigMapping,\n)\nimport torch.ao.quantization.quantize_fx as quantize_fx\nimport copy\n\nmodel_fp = UserModel()\n\n#\n# post training dynamic/weight_only quantization\n#\n\n# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_to_quantize.eval()\nqconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\n# a tuple of one or more example inputs are needed to trace the model\nexample_inputs = (input_fp32)\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n# no calibration needed when we only have dynamic/weight_only quantization\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# post training static quantization\n#\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nqconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\nmodel_to_quantize.eval()\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n# calibrate (not shown)\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# quantization aware training for static quantization\n#\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nqconfig_mapping = get_default_qat_qconfig_mapping(\"qnnpack\")\nmodel_to_quantize.train()\n# prepare\nmodel_prepared = quantize_fx.prepare_qat_fx(model_to_quantize, qconfig_mapping, example_inputs)\n# training loop (not shown)\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# fusion\n#\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_fused = quantize_fx.fuse_fx(model_to_quantize) \n```", "```py\nimport torch\nfrom torch.ao.quantization.quantize_pt2e import prepare_pt2e\nfrom torch._export import capture_pre_autograd_graph\nfrom torch.ao.quantization.quantizer import (\n    XNNPACKQuantizer,\n    get_symmetric_quantization_config,\n)\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n\n   def forward(self, x):\n       return self.linear(x)\n\n# initialize a floating point model\nfloat_model = M().eval()\n\n# define calibration function\ndef calibrate(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        for image, target in data_loader:\n            model(image)\n\n# Step 1\\. program capture\n# NOTE: this API will be updated to torch.export API in the future, but the captured\n# result shoud mostly stay the same\nm = capture_pre_autograd_graph(m, *example_inputs)\n# we get a model with aten ops\n\n# Step 2\\. quantization\n# backend developer will write their own Quantizer and expose methods to allow\n# users to express how they\n# want the model to be quantized\nquantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())\n# or prepare_qat_pt2e for Quantization Aware Training\nm = prepare_pt2e(m, quantizer)\n\n# run calibration\n# calibrate(m, sample_inference_data)\nm = convert_pt2e(m)\n\n# Step 3\\. lowering\n# lower to target backend \n```", "```py\n# set the qconfig for PTQ\n# Note: the old 'fbgemm' is still available but 'x86' is the recommended default on x86 CPUs\nqconfig = torch.ao.quantization.get_default_qconfig('x86')\n# or, set the qconfig for QAT\nqconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n# set the qengine to control weight packing\ntorch.backends.quantized.engine = 'x86' \n```", "```py\n# set the qconfig for PTQ\nqconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n# or, set the qconfig for QAT\nqconfig = torch.ao.quantization.get_default_qat_qconfig('qnnpack')\n# set the qengine to control weight packing\ntorch.backends.quantized.engine = 'qnnpack' \n```", "```py\nimport torch\nimport torch.ao.nn.quantized as nnq\nfrom torch.ao.quantization import QConfigMapping\nimport torch.ao.quantization.quantize_fx\n\n# original fp32 module to replace\nclass CustomModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n\n    def forward(self, x):\n        return self.linear(x)\n\n# custom observed module, provided by user\nclass ObservedCustomModule(torch.nn.Module):\n    def __init__(self, linear):\n        super().__init__()\n        self.linear = linear\n\n    def forward(self, x):\n        return self.linear(x)\n\n    @classmethod\n    def from_float(cls, float_module):\n        assert hasattr(float_module, 'qconfig')\n        observed = cls(float_module.linear)\n        observed.qconfig = float_module.qconfig\n        return observed\n\n# custom quantized module, provided by user\nclass StaticQuantCustomModule(torch.nn.Module):\n    def __init__(self, linear):\n        super().__init__()\n        self.linear = linear\n\n    def forward(self, x):\n        return self.linear(x)\n\n    @classmethod\n    def from_observed(cls, observed_module):\n        assert hasattr(observed_module, 'qconfig')\n        assert hasattr(observed_module, 'activation_post_process')\n        observed_module.linear.activation_post_process = \\\n            observed_module.activation_post_process\n        quantized = cls(nnq.Linear.from_float(observed_module.linear))\n        return quantized\n\n#\n# example API call (Eager mode quantization)\n#\n\nm = torch.nn.Sequential(CustomModule()).eval()\nprepare_custom_config_dict = {\n    \"float_to_observed_custom_module_class\": {\n        CustomModule: ObservedCustomModule\n    }\n}\nconvert_custom_config_dict = {\n    \"observed_to_quantized_custom_module_class\": {\n        ObservedCustomModule: StaticQuantCustomModule\n    }\n}\nm.qconfig = torch.ao.quantization.default_qconfig\nmp = torch.ao.quantization.prepare(\n    m, prepare_custom_config_dict=prepare_custom_config_dict)\n# calibration (not shown)\nmq = torch.ao.quantization.convert(\n    mp, convert_custom_config_dict=convert_custom_config_dict)\n#\n# example API call (FX graph mode quantization)\n#\nm = torch.nn.Sequential(CustomModule()).eval()\nqconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_qconfig)\nprepare_custom_config_dict = {\n    \"float_to_observed_custom_module_class\": {\n        \"static\": {\n            CustomModule: ObservedCustomModule,\n        }\n    }\n}\nconvert_custom_config_dict = {\n    \"observed_to_quantized_custom_module_class\": {\n        \"static\": {\n            ObservedCustomModule: StaticQuantCustomModule,\n        }\n    }\n}\nmp = torch.ao.quantization.quantize_fx.prepare_fx(\n    m, qconfig_mapping, torch.randn(3,3), prepare_custom_config=prepare_custom_config_dict)\n# calibration (not shown)\nmq = torch.ao.quantization.quantize_fx.convert_fx(\n    mp, convert_custom_config=convert_custom_config_dict) \n```", "```py\nRuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend... \n```", "```py\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.ao.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv(x)\n        return x \n```", "```py\nRuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend. \n```", "```py\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.ao.quantization.QuantStub()\n        self.conv1 = torch.nn.Conv2d(1, 1, 1)\n        # this module will not be quantized (see `qconfig = None` logic below)\n        self.conv2 = torch.nn.Conv2d(1, 1, 1)\n        self.dequant = torch.ao.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv1(x)\n        # during the convert step, this will be replaced with a\n        # `dequantize` call\n        x = self.dequant(x)\n        x = self.conv2(x)\n        return x\n\nm = M()\nm.qconfig = some_qconfig\n# turn off quantization for conv2\nm.conv2.qconfig = None \n```", "```py\nAttributeError: 'LinearPackedParams' object has no attribute '_modules' \n```", "```py\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(5, 5)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.relu(x)\n        return x\n\nm = M().eval()\nprepare_orig = prepare_fx(m, {'' : default_qconfig})\nprepare_orig(torch.rand(5, 5))\nquantized_orig = convert_fx(prepare_orig)\n\n# Save/load using state_dict\nb = io.BytesIO()\ntorch.save(quantized_orig.state_dict(), b)\n\nm2 = M().eval()\nprepared = prepare_fx(m2, {'' : default_qconfig})\nquantized = convert_fx(prepared)\nb.seek(0)\nquantized.load_state_dict(torch.load(b)) \n```", "```py\n# Note: using the same model M from previous example\nm = M().eval()\nprepare_orig = prepare_fx(m, {'' : default_qconfig})\nprepare_orig(torch.rand(5, 5))\nquantized_orig = convert_fx(prepare_orig)\n\n# save/load using scripted model\nscripted = torch.jit.script(quantized_orig)\nb = io.BytesIO()\ntorch.jit.save(scripted, b)\nb.seek(0)\nscripted_quantized = torch.jit.load(b) \n```", "```py\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow \n```"]