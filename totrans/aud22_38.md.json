["```py\n$ ffmpeg -f avfoundation -list_devices true -i dummy\n...\n[AVFoundation indev @ 0x126e049d0] AVFoundation video devices:\n[AVFoundation indev @ 0x126e049d0] [0] FaceTime HD Camera\n[AVFoundation indev @ 0x126e049d0] [1] Capture screen 0\n[AVFoundation indev @ 0x126e049d0] AVFoundation audio devices:\n[AVFoundation indev @ 0x126e049d0] [0] ZoomAudioDevice\n[AVFoundation indev @ 0x126e049d0] [1] MacBook Pro Microphone \n```", "```py\nStreamReader(\n    src = \":1\",  # no video, audio from device 1, \"MacBook Pro Microphone\"\n    format = \"avfoundation\",\n) \n```", "```py\n> ffmpeg -f dshow -list_devices true -i dummy\n...\n[dshow @ 000001adcabb02c0] DirectShow video devices (some may be both video and audio devices)\n[dshow @ 000001adcabb02c0]  \"TOSHIBA Web Camera - FHD\"\n[dshow @ 000001adcabb02c0]     Alternative name \"@device_pnp_\\\\?\\usb#vid_10f1&pid_1a42&mi_00#7&27d916e6&0&0000#{65e8773d-8f56-11d0-a3b9-00a0c9223196}\\global\"\n[dshow @ 000001adcabb02c0] DirectShow audio devices\n[dshow @ 000001adcabb02c0]  \"... (Realtek High Definition Audio)\"\n[dshow @ 000001adcabb02c0]     Alternative name \"@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\\wave_{BF2B8AE1-10B8-4CA4-A0DC-D02E18A56177}\" \n```", "```py\nStreamReader(\n    src = \"audio=@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\\wave_{BF2B8AE1-10B8-4CA4-A0DC-D02E18A56177}\",\n    format = \"dshow\",\n) \n```", "```py\nimport torch\nimport torchaudio\n\n# The data acquisition process will stop after this number of steps.\n# This eliminates the need of process synchronization and makes this\n# tutorial simple.\nNUM_ITER = 100\n\ndef stream(q, format, src, segment_length, sample_rate):\n    from torchaudio.io import StreamReader\n\n    print(\"Building StreamReader...\")\n    streamer = StreamReader(src, format=format)\n    streamer.add_basic_audio_stream(frames_per_chunk=segment_length, sample_rate=sample_rate)\n\n    print(streamer.get_src_stream_info(0))\n    print(streamer.get_out_stream_info(0))\n\n    print(\"Streaming...\")\n    print()\n    stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\n    for _ in range(NUM_ITER):\n        (chunk,) = next(stream_iterator)\n        q.put(chunk) \n```", "```py\nclass Pipeline:\n  \"\"\"Build inference pipeline from RNNTBundle.\n\n Args:\n bundle (torchaudio.pipelines.RNNTBundle): Bundle object\n beam_width (int): Beam size of beam search decoder.\n \"\"\"\n\n    def __init__(self, bundle: torchaudio.pipelines.RNNTBundle, beam_width: int = 10):\n        self.bundle = bundle\n        self.feature_extractor = bundle.get_streaming_feature_extractor()\n        self.decoder = bundle.get_decoder()\n        self.token_processor = bundle.get_token_processor()\n\n        self.beam_width = beam_width\n\n        self.state = None\n        self.hypotheses = None\n\n    def infer(self, segment: [torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) -> str:\n  \"\"\"Perform streaming inference\"\"\"\n        features, length = self.feature_extractor(segment)\n        self.hypotheses, self.state = self.decoder.infer(\n            features, length, self.beam_width, state=self.state, hypothesis=self.hypotheses\n        )\n        transcript = self.token_processor(self.hypotheses[0][0], lstrip=False)\n        return transcript \n```", "```py\nclass ContextCacher:\n  \"\"\"Cache the end of input data and prepend the next input data with it.\n\n Args:\n segment_length (int): The size of main segment.\n If the incoming segment is shorter, then the segment is padded.\n context_length (int): The size of the context, cached and appended.\n \"\"\"\n\n    def __init__(self, segment_length: int, context_length: int):\n        self.segment_length = segment_length\n        self.context_length = context_length\n        self.context = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros \"torch.zeros\")([context_length])\n\n    def __call__(self, chunk: [torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")):\n        if chunk.size(0) < self.segment_length:\n            chunk = [torch.nn.functional.pad](https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad \"torch.nn.functional.pad\")(chunk, (0, self.segment_length - chunk.size(0)))\n        chunk_with_context = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")((self.context, chunk))\n        self.context = chunk[-self.context_length :]\n        return chunk_with_context \n```", "```py\ndef main(device, src, bundle):\n    print(torch.__version__)\n    print(torchaudio.__version__)\n\n    print(\"Building pipeline...\")\n    pipeline = Pipeline(bundle)\n\n    sample_rate = bundle.sample_rate\n    segment_length = bundle.segment_length * bundle.hop_length\n    context_length = bundle.right_context_length * bundle.hop_length\n\n    print(f\"Sample rate: {sample_rate}\")\n    print(f\"Main segment: {segment_length} frames ({segment_length  /  sample_rate} seconds)\")\n    print(f\"Right context: {context_length} frames ({context_length  /  sample_rate} seconds)\")\n\n    cacher = ContextCacher(segment_length, context_length)\n\n    @torch.inference_mode()\n    def infer():\n        for _ in range(NUM_ITER):\n            chunk = q.get()\n            segment = cacher(chunk[:, 0])\n            transcript = pipeline.infer(segment)\n            print(transcript, end=\"\\r\", flush=True)\n\n    import torch.multiprocessing as mp\n\n    ctx = mp.get_context(\"spawn\")\n    q = ctx.Queue()\n    p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\n    p.start()\n    infer()\n    p.join()\n\nif __name__ == \"__main__\":\n    main(\n        device=\"avfoundation\",\n        src=\":1\",\n        bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH,\n    ) \n```", "```py\nBuilding pipeline...\nSample rate: 16000\nMain segment: 2560 frames (0.16 seconds)\nRight context: 640 frames (0.04 seconds)\nBuilding StreamReader...\nSourceAudioStream(media_type='audio', codec='pcm_f32le', codec_long_name='PCM 32-bit floating point little-endian', format='flt', bit_rate=1536000, sample_rate=48000.0, num_channels=1)\nOutputStream(source_index=0, filter_description='aresample=16000,aformat=sample_fmts=fltp')\nStreaming...\n\nhello world \n```"]