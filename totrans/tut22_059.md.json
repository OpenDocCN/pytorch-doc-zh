["```py\n!pip3  install  torchrl\n!pip3  install  gym[mujoco]\n!pip3  install  tqdm \n```", "```py\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom tensordict.nn import [TensorDictModule](https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule \"tensordict.nn.TensorDictModule\")\nfrom tensordict.nn.distributions import [NormalParamExtractor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")\nfrom torch import nn\nfrom torchrl.collectors import [SyncDataCollector](https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.SyncDataCollector.html#torchrl.collectors.collectors.SyncDataCollector \"torchrl.collectors.collectors.SyncDataCollector\")\nfrom torchrl.data.replay_buffers import [ReplayBuffer](https://pytorch.org/rl/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer \"torchrl.data.ReplayBuffer\")\nfrom torchrl.data.replay_buffers.samplers import [SamplerWithoutReplacement](https://pytorch.org/rl/reference/generated/torchrl.data.replay_buffers.SamplerWithoutReplacement.html#torchrl.data.replay_buffers.SamplerWithoutReplacement \"torchrl.data.replay_buffers.SamplerWithoutReplacement\")\nfrom torchrl.data.replay_buffers.storages import [LazyTensorStorage](https://pytorch.org/rl/reference/generated/torchrl.data.replay_buffers.LazyTensorStorage.html#torchrl.data.replay_buffers.LazyTensorStorage \"torchrl.data.replay_buffers.storages.LazyTensorStorage\")\nfrom torchrl.envs import ([Compose](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.Compose.html#torchrl.envs.transforms.Compose \"torchrl.envs.transforms.transforms.Compose\"), [DoubleToFloat](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.DoubleToFloat.html#torchrl.envs.transforms.DoubleToFloat \"torchrl.envs.transforms.transforms.DoubleToFloat\"), [ObservationNorm](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm \"torchrl.envs.transforms.transforms.ObservationNorm\"), [StepCounter](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.StepCounter.html#torchrl.envs.transforms.StepCounter \"torchrl.envs.transforms.transforms.StepCounter\"),\n                          [TransformedEnv](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv \"torchrl.envs.transforms.transforms.TransformedEnv\"))\nfrom torchrl.envs.libs.gym import [GymEnv](https://pytorch.org/rl/reference/generated/torchrl.envs.GymEnv.html#torchrl.envs.GymEnv \"torchrl.envs.GymEnv\")\nfrom torchrl.envs.utils import [check_env_specs](https://pytorch.org/rl/reference/generated/torchrl.envs.utils.check_env_specs.html#torchrl.envs.utils.check_env_specs \"torchrl.envs.utils.check_env_specs\"), [set_exploration_mode](https://pytorch.org/rl/reference/generated/torchrl.envs.utils.set_exploration_mode.html#torchrl.envs.utils.set_exploration_mode \"torchrl.envs.utils.set_exploration_mode\")\nfrom torchrl.modules import [ProbabilisticActor](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor \"torchrl.modules.tensordict_module.actors.ProbabilisticActor\"), [TanhNormal](https://pytorch.org/rl/reference/generated/torchrl.modules.TanhNormal.html#torchrl.modules.TanhNormal \"torchrl.modules.TanhNormal\"), [ValueOperator](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator \"torchrl.modules.tensordict_module.actors.ValueOperator\")\nfrom torchrl.objectives import [ClipPPOLoss](https://pytorch.org/rl/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss \"torchrl.objectives.ClipPPOLoss\")\nfrom torchrl.objectives.value import [GAE](https://pytorch.org/rl/reference/generated/torchrl.objectives.value.GAE.html#torchrl.objectives.value.GAE \"torchrl.objectives.value.GAE\")\nfrom tqdm import tqdm \n```", "```py\ndevice = \"cpu\" if not [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available \"torch.cuda.is_available\")() else \"cuda:0\"\nnum_cells = 256  # number of cells in each layer i.e. output dim.\nlr = 3e-4\nmax_grad_norm = 1.0 \n```", "```py\nframe_skip = 1\nframes_per_batch = 1000 // frame_skip\n# For a complete training, bring the number of frames up to 1M\ntotal_frames = 50_000 // frame_skip \n```", "```py\nsub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\nnum_epochs = 10  # optimization steps per batch of data collected\nclip_epsilon = (\n    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n)\ngamma = 0.99\nlmbda = 0.95\nentropy_eps = 1e-4 \n```", "```py\n[base_env](https://pytorch.org/rl/reference/generated/torchrl.envs.GymEnv.html#torchrl.envs.GymEnv \"torchrl.envs.GymEnv\") = [GymEnv](https://pytorch.org/rl/reference/generated/torchrl.envs.GymEnv.html#torchrl.envs.GymEnv \"torchrl.envs.GymEnv\")(\"InvertedDoublePendulum-v4\", device=device, frame_skip=frame_skip) \n```", "```py\n[env](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv \"torchrl.envs.transforms.transforms.TransformedEnv\") = [TransformedEnv](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv \"torchrl.envs.transforms.transforms.TransformedEnv\")(\n    [base_env](https://pytorch.org/rl/reference/generated/torchrl.envs.GymEnv.html#torchrl.envs.GymEnv \"torchrl.envs.GymEnv\"),\n    [Compose](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.Compose.html#torchrl.envs.transforms.Compose \"torchrl.envs.transforms.transforms.Compose\")(\n        # normalize observations\n        [ObservationNorm](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm \"torchrl.envs.transforms.transforms.ObservationNorm\")(in_keys=[\"observation\"]),\n        [DoubleToFloat](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.DoubleToFloat.html#torchrl.envs.transforms.DoubleToFloat \"torchrl.envs.transforms.transforms.DoubleToFloat\")(in_keys=[\"observation\"]),\n        [StepCounter](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.StepCounter.html#torchrl.envs.transforms.StepCounter \"torchrl.envs.transforms.transforms.StepCounter\")(),\n    ),\n) \n```", "```py\n[env](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv \"torchrl.envs.transforms.transforms.TransformedEnv\").transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0) \n```", "```py\nprint(\"normalization constant shape:\", [env](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv \"torchrl.envs.transforms.transforms.TransformedEnv\").transform[0].loc.shape) \n```", "```py\nnormalization constant shape: torch.Size([11]) \n```", "```py\nprint(\"observation_spec:\", [env.observation_spec](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.observation_spec \"torchrl.envs.EnvBase.observation_spec\"))\nprint(\"reward_spec:\", [env.reward_spec](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.reward_spec \"torchrl.envs.EnvBase.reward_spec\"))\nprint(\"input_spec:\", [env.input_spec](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv.input_spec \"torchrl.envs.transforms.TransformedEnv.input_spec\"))\nprint(\"action_spec (as defined by input_spec):\", [env.action_spec](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.action_spec \"torchrl.envs.EnvBase.action_spec\")) \n```", "```py\nobservation_spec: CompositeSpec(\n    observation: UnboundedContinuousTensorSpec(\n        shape=torch.Size([11]),\n        space=None,\n        device=cuda:0,\n        dtype=torch.float32,\n        domain=continuous),\n    step_count: BoundedTensorSpec(\n        shape=torch.Size([1]),\n        space=ContinuousBox(\n            low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True),\n            high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True)),\n        device=cuda:0,\n        dtype=torch.int64,\n        domain=continuous), device=cuda:0, shape=torch.Size([]))\nreward_spec: UnboundedContinuousTensorSpec(\n    shape=torch.Size([1]),\n    space=ContinuousBox(\n        low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n        high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n    device=cuda:0,\n    dtype=torch.float32,\n    domain=continuous)\ninput_spec: CompositeSpec(\n    full_state_spec: CompositeSpec(\n        step_count: BoundedTensorSpec(\n            shape=torch.Size([1]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True),\n                high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True)),\n            device=cuda:0,\n            dtype=torch.int64,\n            domain=continuous), device=cuda:0, shape=torch.Size([])),\n    full_action_spec: CompositeSpec(\n        action: BoundedTensorSpec(\n            shape=torch.Size([1]),\n            space=ContinuousBox(\n                low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n                high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n            device=cuda:0,\n            dtype=torch.float32,\n            domain=continuous), device=cuda:0, shape=torch.Size([])), device=cuda:0, shape=torch.Size([]))\naction_spec (as defined by input_spec): BoundedTensorSpec(\n    shape=torch.Size([1]),\n    space=ContinuousBox(\n        low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n        high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n    device=cuda:0,\n    dtype=torch.float32,\n    domain=continuous) \n```", "```py\n[check_env_specs](https://pytorch.org/rl/reference/generated/torchrl.envs.utils.check_env_specs.html#torchrl.envs.utils.check_env_specs \"torchrl.envs.utils.check_env_specs\")([env](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv \"torchrl.envs.transforms.transforms.TransformedEnv\")) \n```", "```py\ncheck_env_specs succeeded! \n```", "```py\n[rollout](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\") = [env.rollout](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id2 \"torchrl.envs.EnvBase.rollout\")(3)\nprint(\"rollout of three steps:\", [rollout](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\"))\nprint(\"Shape of the rollout TensorDict:\", [rollout.batch_size](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.batch_size \"tensordict.TensorDict.batch_size\")) \n```", "```py\nrollout of three steps: TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n                observation: Tensor(shape=torch.Size([3, 11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n                reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n                step_count: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n                terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n                truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n            batch_size=torch.Size([3]),\n            device=cuda:0,\n            is_shared=True),\n        observation: Tensor(shape=torch.Size([3, 11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        step_count: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n        terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n    batch_size=torch.Size([3]),\n    device=cuda:0,\n    is_shared=True)\nShape of the rollout TensorDict: torch.Size([3]) \n```", "```py\n[actor_net](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\") = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n    [nn.LazyLinear](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear \"torch.nn.LazyLinear\")(num_cells, device=device),\n    [nn.Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh \"torch.nn.Tanh\")(),\n    [nn.LazyLinear](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear \"torch.nn.LazyLinear\")(num_cells, device=device),\n    [nn.Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh \"torch.nn.Tanh\")(),\n    [nn.LazyLinear](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear \"torch.nn.LazyLinear\")(num_cells, device=device),\n    [nn.Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh \"torch.nn.Tanh\")(),\n    [nn.LazyLinear](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear \"torch.nn.LazyLinear\")(2 * [env.action_spec.shape](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.action_spec \"torchrl.envs.EnvBase.action_spec\")[-1], device=device),\n    [NormalParamExtractor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(),\n) \n```", "```py\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/lazy.py:181: UserWarning:\n\nLazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment. \n```", "```py\n[policy_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor \"torchrl.modules.tensordict_module.actors.ProbabilisticActor\") = [TensorDictModule](https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule \"tensordict.nn.TensorDictModule\")(\n    [actor_net](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\"), in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n) \n```", "```py\n[policy_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor \"torchrl.modules.tensordict_module.actors.ProbabilisticActor\") = [ProbabilisticActor](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor \"torchrl.modules.tensordict_module.actors.ProbabilisticActor\")(\n    module=[policy_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor \"torchrl.modules.tensordict_module.actors.ProbabilisticActor\"),\n    spec=[env.action_spec](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.action_spec \"torchrl.envs.EnvBase.action_spec\"),\n    in_keys=[\"loc\", \"scale\"],\n    distribution_class=[TanhNormal](https://pytorch.org/rl/reference/generated/torchrl.modules.TanhNormal.html#torchrl.modules.TanhNormal \"torchrl.modules.TanhNormal\"),\n    distribution_kwargs={\n        \"min\": [env.action_spec.space.minimum](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.action_spec \"torchrl.envs.EnvBase.action_spec\"),\n        \"max\": [env.action_spec.space.maximum](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.action_spec \"torchrl.envs.EnvBase.action_spec\"),\n    },\n    return_log_prob=True,\n    # we'll need the log-prob for the numerator of the importance weights\n) \n```", "```py\n[value_net](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\") = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(\n    [nn.LazyLinear](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear \"torch.nn.LazyLinear\")(num_cells, device=device),\n    [nn.Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh \"torch.nn.Tanh\")(),\n    [nn.LazyLinear](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear \"torch.nn.LazyLinear\")(num_cells, device=device),\n    [nn.Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh \"torch.nn.Tanh\")(),\n    [nn.LazyLinear](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear \"torch.nn.LazyLinear\")(num_cells, device=device),\n    [nn.Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh \"torch.nn.Tanh\")(),\n    [nn.LazyLinear](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear \"torch.nn.LazyLinear\")(1, device=device),\n)\n\n[value_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator \"torchrl.modules.tensordict_module.actors.ValueOperator\") = [ValueOperator](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator \"torchrl.modules.tensordict_module.actors.ValueOperator\")(\n    module=[value_net](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\"),\n    in_keys=[\"observation\"],\n) \n```", "```py\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/lazy.py:181: UserWarning:\n\nLazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment. \n```", "```py\nprint(\"Running policy:\", [policy_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor \"torchrl.modules.tensordict_module.actors.ProbabilisticActor\")([env.reset](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id1 \"torchrl.envs.EnvBase.reset\")()))\nprint(\"Running value:\", [value_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator \"torchrl.modules.tensordict_module.actors.ValueOperator\")([env.reset](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id1 \"torchrl.envs.EnvBase.reset\")())) \n```", "```py\nRunning policy: TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        loc: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        observation: Tensor(shape=torch.Size([11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        sample_log_prob: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        scale: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n    batch_size=torch.Size([]),\n    device=cuda:0,\n    is_shared=True)\nRunning value: TensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        observation: Tensor(shape=torch.Size([11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        state_value: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n    batch_size=torch.Size([]),\n    device=cuda:0,\n    is_shared=True) \n```", "```py\n[collector](https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.SyncDataCollector.html#torchrl.collectors.collectors.SyncDataCollector \"torchrl.collectors.collectors.SyncDataCollector\") = [SyncDataCollector](https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.SyncDataCollector.html#torchrl.collectors.collectors.SyncDataCollector \"torchrl.collectors.collectors.SyncDataCollector\")(\n    [env](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv \"torchrl.envs.transforms.transforms.TransformedEnv\"),\n    [policy_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor \"torchrl.modules.tensordict_module.actors.ProbabilisticActor\"),\n    frames_per_batch=frames_per_batch,\n    total_frames=total_frames,\n    split_trajs=False,\n    device=device,\n) \n```", "```py\n[replay_buffer](https://pytorch.org/rl/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer \"torchrl.data.ReplayBuffer\") = [ReplayBuffer](https://pytorch.org/rl/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer \"torchrl.data.ReplayBuffer\")(\n    storage=[LazyTensorStorage](https://pytorch.org/rl/reference/generated/torchrl.data.replay_buffers.LazyTensorStorage.html#torchrl.data.replay_buffers.LazyTensorStorage \"torchrl.data.replay_buffers.storages.LazyTensorStorage\")(frames_per_batch),\n    sampler=[SamplerWithoutReplacement](https://pytorch.org/rl/reference/generated/torchrl.data.replay_buffers.SamplerWithoutReplacement.html#torchrl.data.replay_buffers.SamplerWithoutReplacement \"torchrl.data.replay_buffers.SamplerWithoutReplacement\")(),\n) \n```", "```py\n[advantage_module](https://pytorch.org/rl/reference/generated/torchrl.objectives.value.GAE.html#torchrl.objectives.value.GAE \"torchrl.objectives.value.GAE\") = [GAE](https://pytorch.org/rl/reference/generated/torchrl.objectives.value.GAE.html#torchrl.objectives.value.GAE \"torchrl.objectives.value.GAE\")(\n    gamma=gamma, lmbda=lmbda, value_network=[value_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator \"torchrl.modules.tensordict_module.actors.ValueOperator\"), average_gae=True\n)\n\n[loss_module](https://pytorch.org/rl/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss \"torchrl.objectives.ClipPPOLoss\") = [ClipPPOLoss](https://pytorch.org/rl/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss \"torchrl.objectives.ClipPPOLoss\")(\n    actor=[policy_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor \"torchrl.modules.tensordict_module.actors.ProbabilisticActor\"),\n    critic=[value_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator \"torchrl.modules.tensordict_module.actors.ValueOperator\"),\n    advantage_key=\"advantage\",\n    clip_epsilon=clip_epsilon,\n    entropy_bonus=bool(entropy_eps),\n    entropy_coef=entropy_eps,\n    # these keys match by default but we set this for completeness\n    value_target_key=[advantage_module](https://pytorch.org/rl/reference/generated/torchrl.objectives.value.GAE.html#torchrl.objectives.value.GAE \"torchrl.objectives.value.GAE\").value_target_key,\n    critic_coef=1.0,\n    gamma=0.99,\n    loss_critic_type=\"smooth_l1\",\n)\n\n[optim](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam \"torch.optim.Adam\") = [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam \"torch.optim.Adam\")([loss_module.parameters](https://pytorch.org/rl/reference/generated/torchrl.objectives.LossModule.html#torchrl.objectives.LossModule.parameters \"torchrl.objectives.LossModule.parameters\")(), lr)\n[scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR \"torch.optim.lr_scheduler.CosineAnnealingLR\") = [torch.optim.lr_scheduler.CosineAnnealingLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR \"torch.optim.lr_scheduler.CosineAnnealingLR\")(\n    [optim](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam \"torch.optim.Adam\"), total_frames // frames_per_batch, 0.0\n) \n```", "```py\nlogs = defaultdict(list)\npbar = tqdm(total=total_frames * frame_skip)\neval_str = \"\"\n\n# We iterate over the collector until it reaches the total number of frames it was\n# designed to collect:\nfor i, [tensordict_data](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\") in enumerate([collector](https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.SyncDataCollector.html#torchrl.collectors.collectors.SyncDataCollector \"torchrl.collectors.collectors.SyncDataCollector\")):\n    # we now have a batch of data to work with. Let's learn something from it.\n    for _ in range(num_epochs):\n        # We'll need an \"advantage\" signal to make PPO work.\n        # We re-compute it at each epoch as its value depends on the value\n        # network which is updated in the inner loop.\n        [advantage_module](https://pytorch.org/rl/reference/generated/torchrl.objectives.value.GAE.html#torchrl.objectives.value.GAE \"torchrl.objectives.value.GAE\")([tensordict_data](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\"))\n        [data_view](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\") = [tensordict_data.reshape](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.reshape \"tensordict.TensorDict.reshape\")(-1)\n        [replay_buffer.extend](https://pytorch.org/rl/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.extend \"torchrl.data.ReplayBuffer.extend\")([data_view.cpu](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.cpu \"tensordict.TensorDict.cpu\")())\n        for _ in range(frames_per_batch // sub_batch_size):\n            [subdata](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\") = [replay_buffer.sample](https://pytorch.org/rl/reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.sample \"torchrl.data.ReplayBuffer.sample\")(sub_batch_size)\n            [loss_vals](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\") = [loss_module](https://pytorch.org/rl/reference/generated/torchrl.objectives.ClipPPOLoss.html#torchrl.objectives.ClipPPOLoss \"torchrl.objectives.ClipPPOLoss\")([subdata.to](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.to \"tensordict.TensorDict.to\")(device))\n            [loss_value](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = (\n                [loss_vals](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\")[\"loss_objective\"]\n                + [loss_vals](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\")[\"loss_critic\"]\n                + [loss_vals](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\")[\"loss_entropy\"]\n            )\n\n            # Optimization: backward, grad clipping and optimization step\n            [loss_value.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward \"torch.Tensor.backward\")()\n            # this is not strictly mandatory but it's good practice to keep\n            # your gradient norm bounded\n            [torch.nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_ \"torch.nn.utils.clip_grad_norm_\")([loss_module.parameters](https://pytorch.org/rl/reference/generated/torchrl.objectives.LossModule.html#torchrl.objectives.LossModule.parameters \"torchrl.objectives.LossModule.parameters\")(), max_grad_norm)\n            [optim](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam \"torch.optim.Adam\").step()\n            [optim.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad \"torch.optim.Adam.zero_grad\")()\n\n    logs[\"reward\"].append([tensordict_data](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\")[\"next\", \"reward\"].mean().item())\n    pbar.update([tensordict_data.numel](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.numel \"tensordict.TensorDict.numel\")() * frame_skip)\n    cum_reward_str = (\n        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n    )\n    logs[\"step_count\"].append([tensordict_data](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict \"tensordict.TensorDict\")[\"step_count\"].max().item())\n    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n    logs[\"lr\"].append([optim](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam \"torch.optim.Adam\").param_groups[0][\"lr\"])\n    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n    if i % 10 == 0:\n        # We evaluate the policy once every 10 batches of data.\n        # Evaluation is rather simple: execute the policy without exploration\n        # (take the expected value of the action distribution) for a given\n        # number of steps (1000, which is our ``env`` horizon).\n        # The ``rollout`` method of the ``env`` can take a policy as argument:\n        # it will then execute this policy at each step.\n        with [set_exploration_mode](https://pytorch.org/rl/reference/generated/torchrl.envs.utils.set_exploration_mode.html#torchrl.envs.utils.set_exploration_mode \"torchrl.envs.utils.set_exploration_mode\")(\"mean\"), [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n            # execute a rollout with the trained policy\n            eval_rollout = [env.rollout](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id2 \"torchrl.envs.EnvBase.rollout\")(1000, [policy_module](https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor \"torchrl.modules.tensordict_module.actors.ProbabilisticActor\"))\n            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n            logs[\"eval reward (sum)\"].append(\n                eval_rollout[\"next\", \"reward\"].sum().item()\n            )\n            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n            eval_str = (\n                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n                f\"eval step-count: {logs['eval step_count'][-1]}\"\n            )\n            del eval_rollout\n    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n\n    # We're also using a learning rate scheduler. Like the gradient clipping,\n    # this is a nice-to-have but nothing necessary for PPO to work.\n    [scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR \"torch.optim.lr_scheduler.CosineAnnealingLR\").step() \n```", "```py\n 0%|          | 0/50000 [00:00<?, ?it/s]\n  2%|2         | 1000/50000 [00:06<05:18, 153.98it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.0850 (init= 9.0850), step count (max): 16, lr policy:  0.0003:   2%|2         | 1000/50000 [00:06<05:18, 153.98it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.0850 (init= 9.0850), step count (max): 16, lr policy:  0.0003:   4%|4         | 2000/50000 [00:12<04:54, 162.80it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.1122 (init= 9.0850), step count (max): 12, lr policy:  0.0003:   4%|4         | 2000/50000 [00:12<04:54, 162.80it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.1122 (init= 9.0850), step count (max): 12, lr policy:  0.0003:   6%|6         | 3000/50000 [00:18<04:41, 166.90it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.1491 (init= 9.0850), step count (max): 18, lr policy:  0.0003:   6%|6         | 3000/50000 [00:18<04:41, 166.90it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.1491 (init= 9.0850), step count (max): 18, lr policy:  0.0003:   8%|8         | 4000/50000 [00:23<04:31, 169.41it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.1947 (init= 9.0850), step count (max): 24, lr policy:  0.0003:   8%|8         | 4000/50000 [00:23<04:31, 169.41it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.1947 (init= 9.0850), step count (max): 24, lr policy:  0.0003:  10%|#         | 5000/50000 [00:29<04:25, 169.30it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2093 (init= 9.0850), step count (max): 20, lr policy:  0.0003:  10%|#         | 5000/50000 [00:29<04:25, 169.30it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2093 (init= 9.0850), step count (max): 20, lr policy:  0.0003:  12%|#2        | 6000/50000 [00:35<04:17, 171.17it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2281 (init= 9.0850), step count (max): 27, lr policy:  0.0003:  12%|#2        | 6000/50000 [00:35<04:17, 171.17it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2281 (init= 9.0850), step count (max): 27, lr policy:  0.0003:  14%|#4        | 7000/50000 [00:41<04:09, 172.49it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2291 (init= 9.0850), step count (max): 30, lr policy:  0.0003:  14%|#4        | 7000/50000 [00:41<04:09, 172.49it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2291 (init= 9.0850), step count (max): 30, lr policy:  0.0003:  16%|#6        | 8000/50000 [00:46<04:02, 173.50it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2427 (init= 9.0850), step count (max): 39, lr policy:  0.0003:  16%|#6        | 8000/50000 [00:46<04:02, 173.50it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2427 (init= 9.0850), step count (max): 39, lr policy:  0.0003:  18%|#8        | 9000/50000 [00:52<03:55, 174.40it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2471 (init= 9.0850), step count (max): 42, lr policy:  0.0003:  18%|#8        | 9000/50000 [00:52<03:55, 174.40it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2471 (init= 9.0850), step count (max): 42, lr policy:  0.0003:  20%|##        | 10000/50000 [00:58<03:48, 175.06it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2578 (init= 9.0850), step count (max): 54, lr policy:  0.0003:  20%|##        | 10000/50000 [00:58<03:48, 175.06it/s]\neval cumulative reward:  101.1702 (init:  101.1702), eval step-count: 10, average reward= 9.2578 (init= 9.0850), step count (max): 54, lr policy:  0.0003:  22%|##2       | 11000/50000 [01:04<03:44, 173.84it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2618 (init= 9.0850), step count (max): 77, lr policy:  0.0003:  22%|##2       | 11000/50000 [01:04<03:44, 173.84it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2618 (init= 9.0850), step count (max): 77, lr policy:  0.0003:  24%|##4       | 12000/50000 [01:09<03:38, 173.96it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2594 (init= 9.0850), step count (max): 52, lr policy:  0.0003:  24%|##4       | 12000/50000 [01:09<03:38, 173.96it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2594 (init= 9.0850), step count (max): 52, lr policy:  0.0003:  26%|##6       | 13000/50000 [01:15<03:32, 174.41it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2604 (init= 9.0850), step count (max): 40, lr policy:  0.0003:  26%|##6       | 13000/50000 [01:15<03:32, 174.41it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2604 (init= 9.0850), step count (max): 40, lr policy:  0.0003:  28%|##8       | 14000/50000 [01:21<03:25, 174.83it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2739 (init= 9.0850), step count (max): 53, lr policy:  0.0003:  28%|##8       | 14000/50000 [01:21<03:25, 174.83it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2739 (init= 9.0850), step count (max): 53, lr policy:  0.0003:  30%|###       | 15000/50000 [01:26<03:19, 175.04it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2667 (init= 9.0850), step count (max): 49, lr policy:  0.0002:  30%|###       | 15000/50000 [01:26<03:19, 175.04it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2667 (init= 9.0850), step count (max): 49, lr policy:  0.0002:  32%|###2      | 16000/50000 [01:32<03:14, 175.01it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2706 (init= 9.0850), step count (max): 57, lr policy:  0.0002:  32%|###2      | 16000/50000 [01:32<03:14, 175.01it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2706 (init= 9.0850), step count (max): 57, lr policy:  0.0002:  34%|###4      | 17000/50000 [01:38<03:08, 174.87it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2566 (init= 9.0850), step count (max): 58, lr policy:  0.0002:  34%|###4      | 17000/50000 [01:38<03:08, 174.87it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2566 (init= 9.0850), step count (max): 58, lr policy:  0.0002:  36%|###6      | 18000/50000 [01:44<03:04, 173.30it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2628 (init= 9.0850), step count (max): 44, lr policy:  0.0002:  36%|###6      | 18000/50000 [01:44<03:04, 173.30it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2628 (init= 9.0850), step count (max): 44, lr policy:  0.0002:  38%|###8      | 19000/50000 [01:50<02:58, 173.94it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2692 (init= 9.0850), step count (max): 56, lr policy:  0.0002:  38%|###8      | 19000/50000 [01:50<02:58, 173.94it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2692 (init= 9.0850), step count (max): 56, lr policy:  0.0002:  40%|####      | 20000/50000 [01:55<02:52, 174.34it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2711 (init= 9.0850), step count (max): 83, lr policy:  0.0002:  40%|####      | 20000/50000 [01:55<02:52, 174.34it/s]\neval cumulative reward:  184.6869 (init:  101.1702), eval step-count: 19, average reward= 9.2711 (init= 9.0850), step count (max): 83, lr policy:  0.0002:  42%|####2     | 21000/50000 [02:01<02:45, 174.75it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2784 (init= 9.0850), step count (max): 62, lr policy:  0.0002:  42%|####2     | 21000/50000 [02:01<02:45, 174.75it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2784 (init= 9.0850), step count (max): 62, lr policy:  0.0002:  44%|####4     | 22000/50000 [02:07<02:41, 173.74it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2762 (init= 9.0850), step count (max): 60, lr policy:  0.0002:  44%|####4     | 22000/50000 [02:07<02:41, 173.74it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2762 (init= 9.0850), step count (max): 60, lr policy:  0.0002:  46%|####6     | 23000/50000 [02:12<02:34, 174.39it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2779 (init= 9.0850), step count (max): 69, lr policy:  0.0002:  46%|####6     | 23000/50000 [02:12<02:34, 174.39it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2779 (init= 9.0850), step count (max): 69, lr policy:  0.0002:  48%|####8     | 24000/50000 [02:18<02:30, 173.23it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2783 (init= 9.0850), step count (max): 52, lr policy:  0.0002:  48%|####8     | 24000/50000 [02:18<02:30, 173.23it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2783 (init= 9.0850), step count (max): 52, lr policy:  0.0002:  50%|#####     | 25000/50000 [02:24<02:23, 173.93it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2750 (init= 9.0850), step count (max): 50, lr policy:  0.0002:  50%|#####     | 25000/50000 [02:24<02:23, 173.93it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2750 (init= 9.0850), step count (max): 50, lr policy:  0.0002:  52%|#####2    | 26000/50000 [02:30<02:17, 174.39it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2738 (init= 9.0850), step count (max): 76, lr policy:  0.0001:  52%|#####2    | 26000/50000 [02:30<02:17, 174.39it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2738 (init= 9.0850), step count (max): 76, lr policy:  0.0001:  54%|#####4    | 27000/50000 [02:35<02:11, 174.76it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2835 (init= 9.0850), step count (max): 72, lr policy:  0.0001:  54%|#####4    | 27000/50000 [02:35<02:11, 174.76it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2835 (init= 9.0850), step count (max): 72, lr policy:  0.0001:  56%|#####6    | 28000/50000 [02:41<02:05, 174.97it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2823 (init= 9.0850), step count (max): 61, lr policy:  0.0001:  56%|#####6    | 28000/50000 [02:41<02:05, 174.97it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2823 (init= 9.0850), step count (max): 61, lr policy:  0.0001:  58%|#####8    | 29000/50000 [02:47<01:59, 175.14it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2865 (init= 9.0850), step count (max): 60, lr policy:  0.0001:  58%|#####8    | 29000/50000 [02:47<01:59, 175.14it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2865 (init= 9.0850), step count (max): 60, lr policy:  0.0001:  60%|######    | 30000/50000 [02:53<01:55, 173.69it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2899 (init= 9.0850), step count (max): 74, lr policy:  0.0001:  60%|######    | 30000/50000 [02:53<01:55, 173.69it/s]\neval cumulative reward:  277.6396 (init:  101.1702), eval step-count: 29, average reward= 9.2899 (init= 9.0850), step count (max): 74, lr policy:  0.0001:  62%|######2   | 31000/50000 [02:58<01:48, 174.42it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2936 (init= 9.0850), step count (max): 60, lr policy:  0.0001:  62%|######2   | 31000/50000 [02:59<01:48, 174.42it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2936 (init= 9.0850), step count (max): 60, lr policy:  0.0001:  64%|######4   | 32000/50000 [03:04<01:43, 173.40it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2996 (init= 9.0850), step count (max): 80, lr policy:  0.0001:  64%|######4   | 32000/50000 [03:04<01:43, 173.40it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2996 (init= 9.0850), step count (max): 80, lr policy:  0.0001:  66%|######6   | 33000/50000 [03:10<01:37, 174.25it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.3009 (init= 9.0850), step count (max): 93, lr policy:  0.0001:  66%|######6   | 33000/50000 [03:10<01:37, 174.25it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.3009 (init= 9.0850), step count (max): 93, lr policy:  0.0001:  68%|######8   | 34000/50000 [03:16<01:31, 174.77it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2965 (init= 9.0850), step count (max): 81, lr policy:  0.0001:  68%|######8   | 34000/50000 [03:16<01:31, 174.77it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2965 (init= 9.0850), step count (max): 81, lr policy:  0.0001:  70%|#######   | 35000/50000 [03:21<01:25, 175.08it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2899 (init= 9.0850), step count (max): 68, lr policy:  0.0001:  70%|#######   | 35000/50000 [03:21<01:25, 175.08it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2899 (init= 9.0850), step count (max): 68, lr policy:  0.0001:  72%|#######2  | 36000/50000 [03:27<01:19, 175.32it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2915 (init= 9.0850), step count (max): 50, lr policy:  0.0001:  72%|#######2  | 36000/50000 [03:27<01:19, 175.32it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2915 (init= 9.0850), step count (max): 50, lr policy:  0.0001:  74%|#######4  | 37000/50000 [03:33<01:14, 173.74it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2923 (init= 9.0850), step count (max): 115, lr policy:  0.0001:  74%|#######4  | 37000/50000 [03:33<01:14, 173.74it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2923 (init= 9.0850), step count (max): 115, lr policy:  0.0001:  76%|#######6  | 38000/50000 [03:38<01:08, 174.45it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2979 (init= 9.0850), step count (max): 57, lr policy:  0.0000:  76%|#######6  | 38000/50000 [03:38<01:08, 174.45it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2979 (init= 9.0850), step count (max): 57, lr policy:  0.0000:  78%|#######8  | 39000/50000 [03:44<01:02, 174.89it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2898 (init= 9.0850), step count (max): 57, lr policy:  0.0000:  78%|#######8  | 39000/50000 [03:44<01:02, 174.89it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2898 (init= 9.0850), step count (max): 57, lr policy:  0.0000:  80%|########  | 40000/50000 [03:50<00:57, 175.15it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2846 (init= 9.0850), step count (max): 67, lr policy:  0.0000:  80%|########  | 40000/50000 [03:50<00:57, 175.15it/s]\neval cumulative reward:  409.9215 (init:  101.1702), eval step-count: 43, average reward= 9.2846 (init= 9.0850), step count (max): 67, lr policy:  0.0000:  82%|########2 | 41000/50000 [03:56<00:51, 175.55it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.2923 (init= 9.0850), step count (max): 76, lr policy:  0.0000:  82%|########2 | 41000/50000 [03:56<00:51, 175.55it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.2923 (init= 9.0850), step count (max): 76, lr policy:  0.0000:  84%|########4 | 42000/50000 [04:01<00:45, 174.00it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.2962 (init= 9.0850), step count (max): 75, lr policy:  0.0000:  84%|########4 | 42000/50000 [04:01<00:45, 174.00it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.2962 (init= 9.0850), step count (max): 75, lr policy:  0.0000:  86%|########6 | 43000/50000 [04:07<00:40, 173.10it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.2913 (init= 9.0850), step count (max): 60, lr policy:  0.0000:  86%|########6 | 43000/50000 [04:07<00:40, 173.10it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.2913 (init= 9.0850), step count (max): 60, lr policy:  0.0000:  88%|########8 | 44000/50000 [04:13<00:34, 174.17it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.2912 (init= 9.0850), step count (max): 108, lr policy:  0.0000:  88%|########8 | 44000/50000 [04:13<00:34, 174.17it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.2912 (init= 9.0850), step count (max): 108, lr policy:  0.0000:  90%|######### | 45000/50000 [04:19<00:28, 174.85it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.2952 (init= 9.0850), step count (max): 58, lr policy:  0.0000:  90%|######### | 45000/50000 [04:19<00:28, 174.85it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.2952 (init= 9.0850), step count (max): 58, lr policy:  0.0000:  92%|#########2| 46000/50000 [04:24<00:22, 175.34it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.3059 (init= 9.0850), step count (max): 125, lr policy:  0.0000:  92%|#########2| 46000/50000 [04:24<00:22, 175.34it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.3059 (init= 9.0850), step count (max): 125, lr policy:  0.0000:  94%|#########3| 47000/50000 [04:30<00:17, 175.62it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.3046 (init= 9.0850), step count (max): 136, lr policy:  0.0000:  94%|#########3| 47000/50000 [04:30<00:17, 175.62it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.3046 (init= 9.0850), step count (max): 136, lr policy:  0.0000:  96%|#########6| 48000/50000 [04:36<00:11, 176.01it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.3019 (init= 9.0850), step count (max): 130, lr policy:  0.0000:  96%|#########6| 48000/50000 [04:36<00:11, 176.01it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.3019 (init= 9.0850), step count (max): 130, lr policy:  0.0000:  98%|#########8| 49000/50000 [04:41<00:05, 174.60it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.3142 (init= 9.0850), step count (max): 156, lr policy:  0.0000:  98%|#########8| 49000/50000 [04:41<00:05, 174.60it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.3142 (init= 9.0850), step count (max): 156, lr policy:  0.0000: 100%|##########| 50000/50000 [04:47<00:00, 175.27it/s]\neval cumulative reward:  503.3041 (init:  101.1702), eval step-count: 53, average reward= 9.3095 (init= 9.0850), step count (max): 144, lr policy:  0.0000: 100%|##########| 50000/50000 [04:47<00:00, 175.27it/s] \n```", "```py\nplt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show() \n```"]