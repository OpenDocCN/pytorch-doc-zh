- en: torchrec.optim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/torchrec/torchrec.optim.html](https://pytorch.org/torchrec/torchrec.optim.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Torchrec Optimizers
  prefs: []
  type: TYPE_NORMAL
- en: Torchrec contains a special optimizer called KeyedOptimizer. KeyedOptimizer
    exposes the state_dict with meaningful keys- it enables loading both torch.tensor
    and [ShardedTensor](https://github.com/pytorch/pytorch/issues/55207) in place,
    and it prohibits loading an empty state into already initialized KeyedOptimizer
    and vise versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also contains - several modules wrapping KeyedOptimizer, called CombinedOptimizer
    and OptimizerWrapper - Optimizers used in RecSys: e.g. rowwise adagrad/adam/etc'
  prefs: []
  type: TYPE_NORMAL
- en: '## torchrec.optim.clipping[](#module-torchrec.optim.clipping "Permalink to
    this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Enum`'
  prefs: []
  type: TYPE_NORMAL
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`OptimizerWrapper`](#torchrec.optim.keyed.OptimizerWrapper "torchrec.optim.keyed.OptimizerWrapper")'
  prefs: []
  type: TYPE_NORMAL
- en: Clips gradients before doing optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** ([*KeyedOptimizer*](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer"))
    – optimizer to wrap'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clipping** ([*GradientClipping*](#torchrec.optim.clipping.GradientClipping
    "torchrec.optim.clipping.GradientClipping")) – how to clip gradients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_gradient** (*float*) – max value for clipping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Performs a single optimization step (parameter update).
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.  ## torchrec.optim.fused[](#module-torchrec.optim.fused "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`FusedOptimizer`](#torchrec.optim.fused.FusedOptimizer "torchrec.optim.fused.FusedOptimizer")'
  prefs: []
  type: TYPE_NORMAL
- en: Fused Optimizer class with no-op step and no parameters to optimize over
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Performs a single optimization step (parameter update).
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Resets the gradients of all optimized `torch.Tensor` s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**set_to_none** (*bool*) – instead of setting to zero, set the grads to None.
    This will in general have lower memory footprint, and can modestly improve performance.
    However, it changes certain behaviors. For example: 1\. When the user tries to
    access a gradient and perform manual ops on it, a None attribute or a Tensor full
    of 0s will behave differently. 2\. If the user requests `zero_grad(set_to_none=True)`
    followed by a backward pass, `.grad`s are guaranteed to be None for params that
    did not receive a gradient. 3\. `torch.optim` optimizers have a different behavior
    if the gradient is 0 or None (in one case it does the step with a gradient of
    0 and in the other it skips the step altogether).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer"),
    `ABC`'
  prefs: []
  type: TYPE_NORMAL
- en: Assumes that weight update is done during backward pass, thus step() is a no-op.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Performs a single optimization step (parameter update).
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Resets the gradients of all optimized `torch.Tensor` s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**set_to_none** (*bool*) – instead of setting to zero, set the grads to None.
    This will in general have lower memory footprint, and can modestly improve performance.
    However, it changes certain behaviors. For example: 1\. When the user tries to
    access a gradient and perform manual ops on it, a None attribute or a Tensor full
    of 0s will behave differently. 2\. If the user requests `zero_grad(set_to_none=True)`
    followed by a backward pass, `.grad`s are guaranteed to be None for params that
    did not receive a gradient. 3\. `torch.optim` optimizers have a different behavior
    if the gradient is 0 or None (in one case it does the step with a gradient of
    0 and in the other it skips the step altogether).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `ABC`'
  prefs: []
  type: TYPE_NORMAL
- en: Module, which does weight update during backward pass.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]  ## torchrec.optim.keyed[](#module-torchrec.optim.keyed "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer")'
  prefs: []
  type: TYPE_NORMAL
- en: Combines multiple KeyedOptimizers into one.
  prefs: []
  type: TYPE_NORMAL
- en: Meant to combine different optimizers for different submodules
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Performs a single optimization step (parameter update).
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Resets the gradients of all optimized `torch.Tensor` s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**set_to_none** (*bool*) – instead of setting to zero, set the grads to None.
    This will in general have lower memory footprint, and can modestly improve performance.
    However, it changes certain behaviors. For example: 1\. When the user tries to
    access a gradient and perform manual ops on it, a None attribute or a Tensor full
    of 0s will behave differently. 2\. If the user requests `zero_grad(set_to_none=True)`
    followed by a backward pass, `.grad`s are guaranteed to be None for params that
    did not receive a gradient. 3\. `torch.optim` optimizers have a different behavior
    if the gradient is 0 or None (in one case it does the step with a gradient of
    0 and in the other it skips the step altogether).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Optimizer`'
  prefs: []
  type: TYPE_NORMAL
- en: Takes a dict of parameters and exposes state_dict by parameter key.
  prefs: []
  type: TYPE_NORMAL
- en: 'This implementation is much stricter than the one in torch.Optimizer: it requires
    implementations to fully initialize their state during first optimization iteration,
    and it prohibits loading an empty state into already initialized KeyedOptimizer
    and vise versa.'
  prefs: []
  type: TYPE_NORMAL
- en: It also doesn’t expose param_groups in state_dict() by default Old behavior
    can be switch on by setting save_param_groups flag. The reason is that during
    distributed training not all parameters are present on all ranks and we identify
    param_group by its parameters. In addition to that, param_groups are typically
    re-set during training initialization, so it makes little sense to save them as
    a part of the state to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Add a param group to the `Optimizer` s param_groups.
  prefs: []
  type: TYPE_NORMAL
- en: This can be useful when fine tuning a pre-trained network as frozen layers can
    be made trainable and added to the `Optimizer` as training progresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**param_group** (*dict*) – Specifies what Tensors should be optimized along
    with group specific optimization options.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Runs a dummy optimizer step, which allows to initialize optimizer state, which
    is typically lazy. This allows us to do in-place loading of optimizer state from
    a checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation is much stricter than the one in torch.Optimizer: it requires
    implementations to fully initialize their state during first optimization iteration,
    and it prohibits loading an empty state into already initialized KeyedOptimizer
    and vise versa.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of introduced strictness it allows us to:'
  prefs: []
  type: TYPE_NORMAL
- en: do compatibility checks for state and param_groups, which improves usability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'avoid state duplication by directly copying into state tensors, e.g. optimizer.step()
    # make sure optimizer is initialized sd = optimizer.state_dict() load_checkpoint(sd)
    # copy state directly into tensors, re-shard if needed optimizer.load_state_dict(sd)
    # replace param_groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Returned state and param_groups will contain parameter keys instead of parameter
    indices in torch.Optimizer. This allows for advanced functionality like optimizer
    re-sharding to be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Can also handle classes and supported data structures that follow the PyTorch
    stateful protocol.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer")'
  prefs: []
  type: TYPE_NORMAL
- en: Takes a dict of parameters and exposes state_dict by parameter key.
  prefs: []
  type: TYPE_NORMAL
- en: Convenience wrapper to take in optim_factory callable to create KeyedOptimizer
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Performs a single optimization step (parameter update).
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Resets the gradients of all optimized `torch.Tensor` s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**set_to_none** (*bool*) – instead of setting to zero, set the grads to None.
    This will in general have lower memory footprint, and can modestly improve performance.
    However, it changes certain behaviors. For example: 1\. When the user tries to
    access a gradient and perform manual ops on it, a None attribute or a Tensor full
    of 0s will behave differently. 2\. If the user requests `zero_grad(set_to_none=True)`
    followed by a backward pass, `.grad`s are guaranteed to be None for params that
    did not receive a gradient. 3\. `torch.optim` optimizers have a different behavior
    if the gradient is 0 or None (in one case it does the step with a gradient of
    0 and in the other it skips the step altogether).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer")'
  prefs: []
  type: TYPE_NORMAL
- en: Wrapper which takes in a KeyedOptimizer and is a KeyedOptimizer
  prefs: []
  type: TYPE_NORMAL
- en: Subclass for Optimizers like GradientClippingOptimizer and WarmupOptimizer
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Add a param group to the `Optimizer` s param_groups.
  prefs: []
  type: TYPE_NORMAL
- en: This can be useful when fine tuning a pre-trained network as frozen layers can
    be made trainable and added to the `Optimizer` as training progresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**param_group** (*dict*) – Specifies what Tensors should be optimized along
    with group specific optimization options.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation is much stricter than the one in torch.Optimizer: it requires
    implementations to fully initialize their state during first optimization iteration,
    and it prohibits loading an empty state into already initialized KeyedOptimizer
    and vise versa.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of introduced strictness it allows us to:'
  prefs: []
  type: TYPE_NORMAL
- en: do compatibility checks for state and param_groups, which improves usability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'avoid state duplication by directly copying into state tensors, e.g. optimizer.step()
    # make sure optimizer is initialized sd = optimizer.state_dict() load_checkpoint(sd)
    # copy state directly into tensors, re-shard if needed optimizer.load_state_dict(sd)
    # replace param_groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Returned state and param_groups will contain parameter keys instead of parameter
    indices in torch.Optimizer. This allows for advanced functionality like optimizer
    re-sharding to be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Can also handle classes and supported data structures that follow the PyTorch
    stateful protocol.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Performs a single optimization step (parameter update).
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Resets the gradients of all optimized `torch.Tensor` s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**set_to_none** (*bool*) – instead of setting to zero, set the grads to None.
    This will in general have lower memory footprint, and can modestly improve performance.
    However, it changes certain behaviors. For example: 1\. When the user tries to
    access a gradient and perform manual ops on it, a None attribute or a Tensor full
    of 0s will behave differently. 2\. If the user requests `zero_grad(set_to_none=True)`
    followed by a backward pass, `.grad`s are guaranteed to be None for params that
    did not receive a gradient. 3\. `torch.optim` optimizers have a different behavior
    if the gradient is 0 or None (in one case it does the step with a gradient of
    0 and in the other it skips the step altogether).  ## torchrec.optim.warmup[](#module-torchrec.optim.warmup
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: [`OptimizerWrapper`](#torchrec.optim.keyed.OptimizerWrapper "torchrec.optim.keyed.OptimizerWrapper")'
  prefs: []
  type: TYPE_NORMAL
- en: Adjusts learning rate according to the schedule.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** ([*KeyedOptimizer*](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer"))
    – optimizer to wrap'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stages** (*List**[*[*WarmupStage*](#torchrec.optim.warmup.WarmupStage "torchrec.optim.warmup.WarmupStage")*]*)
    – stages to go through'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lr** (*float*) – initial learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lr_param** (*str*) – learning rate parameter in parameter group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**param_name** – Name of fake parameter to hold warmup state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Performs a single optimization step (parameter update).
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `Enum`'
  prefs: []
  type: TYPE_NORMAL
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Bases: `object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]  ## Module contents[](#module-0 "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: Torchrec Optimizers
  prefs: []
  type: TYPE_NORMAL
- en: Torchrec contains a special optimizer called KeyedOptimizer. KeyedOptimizer
    exposes the state_dict with meaningful keys- it enables loading both torch.tensor
    and [ShardedTensor](https://github.com/pytorch/pytorch/issues/55207) in place,
    and it prohibits loading an empty state into already initialized KeyedOptimizer
    and vise versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also contains - several modules wrapping KeyedOptimizer, called CombinedOptimizer
    and OptimizerWrapper - Optimizers used in RecSys: e.g. rowwise adagrad/adam/etc'
  prefs: []
  type: TYPE_NORMAL
