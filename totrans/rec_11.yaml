- en: torchrec.optim
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torchrec.optim
- en: 原文：[https://pytorch.org/torchrec/torchrec.optim.html](https://pytorch.org/torchrec/torchrec.optim.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/torchrec/torchrec.optim.html](https://pytorch.org/torchrec/torchrec.optim.html)
- en: Torchrec Optimizers
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Torchrec 优化器
- en: Torchrec contains a special optimizer called KeyedOptimizer. KeyedOptimizer
    exposes the state_dict with meaningful keys- it enables loading both torch.tensor
    and [ShardedTensor](https://github.com/pytorch/pytorch/issues/55207) in place,
    and it prohibits loading an empty state into already initialized KeyedOptimizer
    and vise versa.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Torchrec包含一个名为KeyedOptimizer的特殊优化器。KeyedOptimizer公开具有有意义键的state_dict-它使得可以在原地加载torch.tensor和[ShardedTensor](https://github.com/pytorch/pytorch/issues/55207)，并且禁止将空状态加载到已初始化的KeyedOptimizer中，反之亦然。
- en: 'It also contains - several modules wrapping KeyedOptimizer, called CombinedOptimizer
    and OptimizerWrapper - Optimizers used in RecSys: e.g. rowwise adagrad/adam/etc'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 还包括-几个包装KeyedOptimizer的模块，称为CombinedOptimizer和OptimizerWrapper-RecSys中使用的优化器：例如逐行的adagrad/adam等
- en: '## torchrec.optim.clipping[](#module-torchrec.optim.clipping "Permalink to
    this heading")'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '## torchrec.optim.clipping[](#module-torchrec.optim.clipping "跳转到此标题的永久链接")'
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Bases: `Enum`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Enum`
- en: An enumeration.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个枚举。
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Bases: [`OptimizerWrapper`](#torchrec.optim.keyed.OptimizerWrapper "torchrec.optim.keyed.OptimizerWrapper")'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`OptimizerWrapper`](#torchrec.optim.keyed.OptimizerWrapper "torchrec.optim.keyed.OptimizerWrapper")
- en: Clips gradients before doing optimization step.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行优化步骤之前裁剪梯度。
- en: 'Parameters:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**optimizer** ([*KeyedOptimizer*](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer"))
    – optimizer to wrap'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optimizer** ([*KeyedOptimizer*](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer"))
    – 要包装的优化器'
- en: '**clipping** ([*GradientClipping*](#torchrec.optim.clipping.GradientClipping
    "torchrec.optim.clipping.GradientClipping")) – how to clip gradients'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**clipping** ([*GradientClipping*](#torchrec.optim.clipping.GradientClipping
    "torchrec.optim.clipping.GradientClipping")) – 如何裁剪梯度'
- en: '**max_gradient** (*float*) – max value for clipping'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_gradient** (*float*) – 裁剪的最大值'
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Performs a single optimization step (parameter update).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤（参数更新）。
- en: 'Parameters:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**closure** (*Callable*) – 重新评估模型并返回损失的闭包。对大多数优化器来说是可选的。'
- en: Note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.  ## torchrec.optim.fused[](#module-torchrec.optim.fused "Permalink
    to this heading")'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则此函数不应修改参数的`.grad`字段。## torchrec.optim.fused[](#module-torchrec.optim.fused
    "跳转到此标题的永久链接")
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Bases: [`FusedOptimizer`](#torchrec.optim.fused.FusedOptimizer "torchrec.optim.fused.FusedOptimizer")'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`FusedOptimizer`](#torchrec.optim.fused.FusedOptimizer "torchrec.optim.fused.FusedOptimizer")
- en: Fused Optimizer class with no-op step and no parameters to optimize over
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 融合优化器类，无操作步骤和无需优化的参数
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Performs a single optimization step (parameter update).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤（参数更新）。
- en: 'Parameters:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**closure** (*Callable*) – 重新评估模型并返回损失的闭包。对大多数优化器来说是可选的。'
- en: Note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则此函数不应修改参数的`.grad`字段。
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Resets the gradients of all optimized `torch.Tensor` s.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 重置所有优化过的`torch.Tensor`的梯度。
- en: 'Parameters:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**set_to_none** (*bool*) – instead of setting to zero, set the grads to None.
    This will in general have lower memory footprint, and can modestly improve performance.
    However, it changes certain behaviors. For example: 1\. When the user tries to
    access a gradient and perform manual ops on it, a None attribute or a Tensor full
    of 0s will behave differently. 2\. If the user requests `zero_grad(set_to_none=True)`
    followed by a backward pass, `.grad`s are guaranteed to be None for params that
    did not receive a gradient. 3\. `torch.optim` optimizers have a different behavior
    if the gradient is 0 or None (in one case it does the step with a gradient of
    0 and in the other it skips the step altogether).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**set_to_none** (*bool*) – 将梯度设置为None而不是零。这通常会减少内存占用，并可能略微提高性能。但是，它会改变某些行为。例如：1.
    当用户尝试访问梯度并对其执行手动操作时，具有None属性或全为0的张量会表现不同。2. 如果用户请求`zero_grad(set_to_none=True)`，然后进行反向传播，对于未接收梯度的参数，`.grad`保证为None。3.
    `torch.optim`优化器在梯度为0或None时具有不同的行为（在一种情况下，它使用梯度为0进行步骤，在另一种情况下，它完全跳过步骤）。'
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Bases: [`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer"),
    `ABC`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer"),
    `ABC`
- en: Assumes that weight update is done during backward pass, thus step() is a no-op.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设权重更新在反向传播期间完成，因此step()是一个无操作。
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Performs a single optimization step (parameter update).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤（参数更新）。
- en: 'Parameters:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**closure** (*Callable*) – 重新评估模型并返回损失的闭包。对大多数优化器来说是可选的。'
- en: Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则此函数不应修改参数的`.grad`字段。
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Resets the gradients of all optimized `torch.Tensor` s.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 重置所有优化过的`torch.Tensor`的梯度。
- en: 'Parameters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**set_to_none** (*bool*) – instead of setting to zero, set the grads to None.
    This will in general have lower memory footprint, and can modestly improve performance.
    However, it changes certain behaviors. For example: 1\. When the user tries to
    access a gradient and perform manual ops on it, a None attribute or a Tensor full
    of 0s will behave differently. 2\. If the user requests `zero_grad(set_to_none=True)`
    followed by a backward pass, `.grad`s are guaranteed to be None for params that
    did not receive a gradient. 3\. `torch.optim` optimizers have a different behavior
    if the gradient is 0 or None (in one case it does the step with a gradient of
    0 and in the other it skips the step altogether).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**set_to_none**（*bool*）-将梯度设置为None而不是设置为零。这通常会减少内存占用，并可能略微提高性能。但是，它会改变某些行为。例如：1.当用户尝试访问梯度并对其执行手动操作时，None属性或一个全为0的张量会有不同的行为。2.如果用户请求`zero_grad(set_to_none=True)`，然后进行反向传播，对于未接收梯度的参数，`.grad`保证为None。3.`torch.optim`优化器在梯度为0或None时具有不同的行为（在一种情况下，它使用梯度为0执行步骤，在另一种情况下，它完全跳过步骤）。'
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Bases: `ABC`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`ABC`
- en: Module, which does weight update during backward pass.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播期间执行权重更新的模块。
- en: '[PRE13]  ## torchrec.optim.keyed[](#module-torchrec.optim.keyed "Permalink
    to this heading")'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE13]  ## torchrec.optim.keyed[](#module-torchrec.optim.keyed "Permalink
    to this heading")'
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Bases: [`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer")'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer")
- en: Combines multiple KeyedOptimizers into one.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个KeyedOptimizers组合成一个。
- en: Meant to combine different optimizers for different submodules
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在将不同的优化器组合用于不同的子模块
- en: '[PRE15]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Performs a single optimization step (parameter update).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤（参数更新）。
- en: 'Parameters:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**closure**（*Callable*）-重新评估模型并返回损失的闭包。对于大多数优化器来说是可选的。'
- en: Note
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则此函数不应修改参数的`.grad`字段。
- en: '[PRE23]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Resets the gradients of all optimized `torch.Tensor` s.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 重置所有优化的`torch.Tensor`的梯度。
- en: 'Parameters:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**set_to_none** (*bool*) – instead of setting to zero, set the grads to None.
    This will in general have lower memory footprint, and can modestly improve performance.
    However, it changes certain behaviors. For example: 1\. When the user tries to
    access a gradient and perform manual ops on it, a None attribute or a Tensor full
    of 0s will behave differently. 2\. If the user requests `zero_grad(set_to_none=True)`
    followed by a backward pass, `.grad`s are guaranteed to be None for params that
    did not receive a gradient. 3\. `torch.optim` optimizers have a different behavior
    if the gradient is 0 or None (in one case it does the step with a gradient of
    0 and in the other it skips the step altogether).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**set_to_none**（*bool*）-将梯度设置为None而不是设置为零。这通常会减少内存占用，并可能略微提高性能。但是，它会改变某些行为。例如：1.当用户尝试访问梯度并对其执行手动操作时，None属性或一个全为0的张量会有不同的行为。2.如果用户请求`zero_grad(set_to_none=True)`，然后进行反向传播，对于未接收梯度的参数，`.grad`保证为None。3.`torch.optim`优化器在梯度为0或None时具有不同的行为（在一种情况下，它使用梯度为0执行步骤，在另一种情况下，它完全跳过步骤）。'
- en: '[PRE24]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Bases: `Optimizer`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Optimizer`
- en: Takes a dict of parameters and exposes state_dict by parameter key.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接受参数字典并按参数键公开state_dict。
- en: 'This implementation is much stricter than the one in torch.Optimizer: it requires
    implementations to fully initialize their state during first optimization iteration,
    and it prohibits loading an empty state into already initialized KeyedOptimizer
    and vise versa.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现比torch.Optimizer中的实现要严格得多：它要求实现在第一次优化迭代期间完全初始化其状态，并禁止将空状态加载到已初始化的KeyedOptimizer中，反之亦然。
- en: It also doesn’t expose param_groups in state_dict() by default Old behavior
    can be switch on by setting save_param_groups flag. The reason is that during
    distributed training not all parameters are present on all ranks and we identify
    param_group by its parameters. In addition to that, param_groups are typically
    re-set during training initialization, so it makes little sense to save them as
    a part of the state to begin with.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，它也不会在state_dict()中公开param_groups。可以通过设置save_param_groups标志来切换到旧行为。原因是在分布式训练期间，并非所有参数都存在于所有排名上，我们通过其参数来识别param_group。此外，param_groups通常在训练初始化期间重新设置，因此将它们保存为状态的一部分起初没有太多意义。
- en: '[PRE25]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Add a param group to the `Optimizer` s param_groups.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 向`Optimizer`的param_groups添加一个参数组。
- en: This can be useful when fine tuning a pre-trained network as frozen layers can
    be made trainable and added to the `Optimizer` as training progresses.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这在微调预训练网络时可能会很有用，因为冻结的层可以在训练过程中变为可训练，并添加到`Optimizer`中。
- en: 'Parameters:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**param_group** (*dict*) – Specifies what Tensors should be optimized along
    with group specific optimization options.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**param_group**（*dict*）-指定应优化的张量以及组特定的优化选项。'
- en: '[PRE26]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Runs a dummy optimizer step, which allows to initialize optimizer state, which
    is typically lazy. This allows us to do in-place loading of optimizer state from
    a checkpoint.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 运行一个虚拟的优化器步骤，允许初始化通常是懒惰的优化器状态。这使我们能够从检查点中就地加载优化器状态。
- en: '[PRE27]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This implementation is much stricter than the one in torch.Optimizer: it requires
    implementations to fully initialize their state during first optimization iteration,
    and it prohibits loading an empty state into already initialized KeyedOptimizer
    and vise versa.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现比torch.Optimizer中的实现要严格得多：它要求实现在第一次优化迭代期间完全初始化其状态，并禁止将空状态加载到已初始化的KeyedOptimizer中，反之亦然。
- en: 'Because of introduced strictness it allows us to:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于引入了严格性，它使我们能够：
- en: do compatibility checks for state and param_groups, which improves usability
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对状态和param_groups进行兼容性检查，以提高可用性
- en: 'avoid state duplication by directly copying into state tensors, e.g. optimizer.step()
    # make sure optimizer is initialized sd = optimizer.state_dict() load_checkpoint(sd)
    # copy state directly into tensors, re-shard if needed optimizer.load_state_dict(sd)
    # replace param_groups'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '通过直接复制到状态张量来避免状态重复，例如optimizer.step() # 确保优化器已初始化 sd = optimizer.state_dict()
    load_checkpoint(sd) # 直接将状态复制到张量中，如果需要，重新分片 optimizer.load_state_dict(sd) # 替换param_groups'
- en: '[PRE28]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Returned state and param_groups will contain parameter keys instead of parameter
    indices in torch.Optimizer. This allows for advanced functionality like optimizer
    re-sharding to be implemented.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的状态和param_groups将包含参数键而不是torch.Optimizer中的参数索引。这允许实现像优化器重新分片这样的高级功能。
- en: Can also handle classes and supported data structures that follow the PyTorch
    stateful protocol.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以处理遵循PyTorch有状态协议的类和支持的数据结构。
- en: '[PRE31]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Bases: [`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer")'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer")
- en: Takes a dict of parameters and exposes state_dict by parameter key.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接受参数字典并按参数键公开state_dict。
- en: Convenience wrapper to take in optim_factory callable to create KeyedOptimizer
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的包装器，接受optim_factory可调用以创建KeyedOptimizer
- en: '[PRE32]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Performs a single optimization step (parameter update).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤（参数更新）。
- en: 'Parameters:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**closure**（*Callable*）- 重新评估模型并返回损失的闭包。对于大多数优化器来说是可选的。'
- en: Note
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则此函数不应修改参数的`.grad`字段。
- en: '[PRE33]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Resets the gradients of all optimized `torch.Tensor` s.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 重置所有优化的`torch.Tensor`的梯度。
- en: 'Parameters:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**set_to_none** (*bool*) – instead of setting to zero, set the grads to None.
    This will in general have lower memory footprint, and can modestly improve performance.
    However, it changes certain behaviors. For example: 1\. When the user tries to
    access a gradient and perform manual ops on it, a None attribute or a Tensor full
    of 0s will behave differently. 2\. If the user requests `zero_grad(set_to_none=True)`
    followed by a backward pass, `.grad`s are guaranteed to be None for params that
    did not receive a gradient. 3\. `torch.optim` optimizers have a different behavior
    if the gradient is 0 or None (in one case it does the step with a gradient of
    0 and in the other it skips the step altogether).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**set_to_none**（*bool*）- 不设置为零，将梯度设置为None。这通常会减少内存占用，并可能略微提高性能。但是，它会改变某些行为。例如：1.
    当用户尝试访问梯度并对其执行手动操作时，一个None属性或一个全为0的张量会有不同的行为。2. 如果用户请求`zero_grad(set_to_none=True)`，然后进行反向传播，对于未接收梯度的参数，`.grad`将保证为None。3.
    `torch.optim`优化器在梯度为0或None时具有不同的行为（在一种情况下，它使用梯度为0进行步骤，在另一种情况下，它完全跳过步骤）。'
- en: '[PRE34]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Bases: [`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer")'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`KeyedOptimizer`](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer")
- en: Wrapper which takes in a KeyedOptimizer and is a KeyedOptimizer
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接受KeyedOptimizer并且是KeyedOptimizer的包装器
- en: Subclass for Optimizers like GradientClippingOptimizer and WarmupOptimizer
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 用于像GradientClippingOptimizer和WarmupOptimizer这样的优化器的子类
- en: '[PRE35]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Add a param group to the `Optimizer` s param_groups.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 向`Optimizer`的param_groups添加一个参数组。
- en: This can be useful when fine tuning a pre-trained network as frozen layers can
    be made trainable and added to the `Optimizer` as training progresses.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当微调预训练网络时，冻结的层可以在训练进行时变为可训练，并添加到`Optimizer`中。
- en: 'Parameters:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**param_group** (*dict*) – Specifies what Tensors should be optimized along
    with group specific optimization options.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**param_group**（*dict*）- 指定应优化的张量以及组特定的优化选项。'
- en: '[PRE36]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This implementation is much stricter than the one in torch.Optimizer: it requires
    implementations to fully initialize their state during first optimization iteration,
    and it prohibits loading an empty state into already initialized KeyedOptimizer
    and vise versa.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现比torch.Optimizer中的实现严格得多：它要求在第一次优化迭代期间完全初始化其状态，并禁止将空状态加载到已初始化的KeyedOptimizer中，反之亦然。
- en: 'Because of introduced strictness it allows us to:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于引入了严格性，它使我们能够：
- en: do compatibility checks for state and param_groups, which improves usability
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对状态和param_groups进行兼容性检查，以提高可用性
- en: 'avoid state duplication by directly copying into state tensors, e.g. optimizer.step()
    # make sure optimizer is initialized sd = optimizer.state_dict() load_checkpoint(sd)
    # copy state directly into tensors, re-shard if needed optimizer.load_state_dict(sd)
    # replace param_groups'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '通过直接复制到状态张量来避免状态重复，例如optimizer.step() # 确保优化器已初始化 sd = optimizer.state_dict()
    load_checkpoint(sd) # 直接将状态复制到张量中，如果需要，重新分片 optimizer.load_state_dict(sd) # 替换param_groups'
- en: '[PRE37]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Returned state and param_groups will contain parameter keys instead of parameter
    indices in torch.Optimizer. This allows for advanced functionality like optimizer
    re-sharding to be implemented.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的状态和param_groups将包含参数键而不是torch.Optimizer中的参数索引。这允许实现像优化器重新分片这样的高级功能。
- en: Can also handle classes and supported data structures that follow the PyTorch
    stateful protocol.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以处理遵循PyTorch有状态协议的类和支持的数据结构。
- en: '[PRE40]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Performs a single optimization step (parameter update).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤（参数更新）。
- en: 'Parameters:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**closure**（*Callable*）- 重新评估模型并返回损失的闭包。对于大多数优化器来说是可选的。'
- en: Note
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则此函数不应修改参数的`.grad`字段。
- en: '[PRE41]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Resets the gradients of all optimized `torch.Tensor` s.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 重置所有优化的`torch.Tensor`的梯度。
- en: 'Parameters:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**set_to_none** (*bool*) – instead of setting to zero, set the grads to None.
    This will in general have lower memory footprint, and can modestly improve performance.
    However, it changes certain behaviors. For example: 1\. When the user tries to
    access a gradient and perform manual ops on it, a None attribute or a Tensor full
    of 0s will behave differently. 2\. If the user requests `zero_grad(set_to_none=True)`
    followed by a backward pass, `.grad`s are guaranteed to be None for params that
    did not receive a gradient. 3\. `torch.optim` optimizers have a different behavior
    if the gradient is 0 or None (in one case it does the step with a gradient of
    0 and in the other it skips the step altogether).  ## torchrec.optim.warmup[](#module-torchrec.optim.warmup
    "Permalink to this heading")'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**set_to_none**（*bool*）- 将梯度设置为None而不是设置为零。这通常具有更低的内存占用，并且可以适度提高性能。但是，它会改变某些行为。例如：1\.
    当用户尝试访问梯度并对其执行手动操作时，具有None属性或全为0的张量会有不同的行为。2\. 如果用户请求`zero_grad(set_to_none=True)`，然后进行反向传播，对于未接收梯度的参数，`.grad`保证为None。3\.
    `torch.optim`优化器在梯度为0或None时具有不同的行为（在一种情况下，它使用梯度为0进行步骤，在另一种情况下，它完全跳过步骤）。  ## torchrec.optim.warmup[](#module-torchrec.optim.warmup
    "跳转到此标题")'
- en: '[PRE42]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Bases: [`OptimizerWrapper`](#torchrec.optim.keyed.OptimizerWrapper "torchrec.optim.keyed.OptimizerWrapper")'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`OptimizerWrapper`](#torchrec.optim.keyed.OptimizerWrapper "torchrec.optim.keyed.OptimizerWrapper")
- en: Adjusts learning rate according to the schedule.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 根据时间表调整学习率。
- en: 'Parameters:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**optimizer** ([*KeyedOptimizer*](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer"))
    – optimizer to wrap'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**optimizer**（[*KeyedOptimizer*](#torchrec.optim.keyed.KeyedOptimizer "torchrec.optim.keyed.KeyedOptimizer")）-
    要包装的优化器'
- en: '**stages** (*List**[*[*WarmupStage*](#torchrec.optim.warmup.WarmupStage "torchrec.optim.warmup.WarmupStage")*]*)
    – stages to go through'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**stages**（*List**[*[*WarmupStage*](#torchrec.optim.warmup.WarmupStage "torchrec.optim.warmup.WarmupStage")*]*）-
    要经过的阶段'
- en: '**lr** (*float*) – initial learning rate'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lr**（*float*）- 初始学习率'
- en: '**lr_param** (*str*) – learning rate parameter in parameter group.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lr_param**（*str*）- 参数组中的学习率参数。'
- en: '**param_name** – Name of fake parameter to hold warmup state.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**param_name** - 用于保存预热状态的虚拟参数的名称。'
- en: '[PRE43]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Performs a single optimization step (parameter update).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单个优化步骤（参数更新）。
- en: 'Parameters:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**closure** (*Callable*) – A closure that reevaluates the model and returns
    the loss. Optional for most optimizers.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**closure**（*Callable*）- 重新评估模型并返回损失的闭包。对于大多数优化器来说是可选的。'
- en: Note
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Unless otherwise specified, this function should not modify the `.grad` field
    of the parameters.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则此函数不应修改参数的`.grad`字段。
- en: '[PRE45]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Bases: `Enum`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Enum`
- en: An enumeration.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一个枚举。
- en: '[PRE46]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Bases: `object`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`object`
- en: '[PRE53]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]  ## Module contents[](#module-0 "Permalink to this heading")'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE57]  ## 模块内容[](#module-0 "跳转到此标题")'
- en: Torchrec Optimizers
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Torchrec优化器
- en: Torchrec contains a special optimizer called KeyedOptimizer. KeyedOptimizer
    exposes the state_dict with meaningful keys- it enables loading both torch.tensor
    and [ShardedTensor](https://github.com/pytorch/pytorch/issues/55207) in place,
    and it prohibits loading an empty state into already initialized KeyedOptimizer
    and vise versa.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Torchrec包含一个名为KeyedOptimizer的特殊优化器。KeyedOptimizer公开具有有意义键的state_dict- 它使得可以在原地加载torch.tensor和[ShardedTensor](https://github.com/pytorch/pytorch/issues/55207)，并且禁止将空状态加载到已初始化的KeyedOptimizer中，反之亦然。
- en: 'It also contains - several modules wrapping KeyedOptimizer, called CombinedOptimizer
    and OptimizerWrapper - Optimizers used in RecSys: e.g. rowwise adagrad/adam/etc'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 它还包含 - 几个包装KeyedOptimizer的模块，称为CombinedOptimizer和OptimizerWrapper - 用于RecSys的优化器：例如逐行adagrad/adam等
