["```py\nclass torchtext.vocab.Vocab(vocab)\u00b6\n```", "```py\n__contains__(token: str) \u2192 bool\u00b6\n```", "```py\n__getitem__(token: str) \u2192 int\u00b6\n```", "```py\n__init__(vocab) \u2192 None\u00b6\n```", "```py\n__jit_unused_properties__ = ['is_jitable']\u00b6\n```", "```py\n__len__() \u2192 int\u00b6\n```", "```py\n__prepare_scriptable__()\u00b6\n```", "```py\nappend_token(token: str) \u2192 None\u00b6\n```", "```py\nforward(tokens: List[str]) \u2192 List[int]\u00b6\n```", "```py\nget_default_index() \u2192 Optional[int]\u00b6\n```", "```py\nget_itos() \u2192 List[str]\u00b6\n```", "```py\nget_stoi() \u2192 Dict[str, int]\u00b6\n```", "```py\ninsert_token(token: str, index: int) \u2192 None\u00b6\n```", "```py\nlookup_indices(tokens: List[str]) \u2192 List[int]\u00b6\n```", "```py\nlookup_token(index: int) \u2192 str\u00b6\n```", "```py\nlookup_tokens(indices: List[int]) \u2192 List[str]\u00b6\n```", "```py\nset_default_index(index: Optional[int]) \u2192 None\u00b6\n```", "```py\ntorchtext.vocab.vocab(ordered_dict: Dict, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True) \u2192 Vocab\u00b6\n```", "```py\n>>> from torchtext.vocab import vocab\n>>> from collections import Counter, OrderedDict\n>>> counter = Counter([\"a\", \"a\", \"b\", \"b\", \"b\"])\n>>> sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n>>> ordered_dict = OrderedDict(sorted_by_freq_tuples)\n>>> v1 = vocab(ordered_dict)\n>>> print(v1['a']) #prints 1\n>>> print(v1['out of vocab']) #raise RuntimeError since default index is not set\n>>> tokens = ['e', 'd', 'c', 'b', 'a']\n>>> #adding <unk> token and default index\n>>> unk_token = '<unk>'\n>>> default_index = -1\n>>> v2 = vocab(OrderedDict([(token, 1) for token in tokens]), specials=[unk_token])\n>>> v2.set_default_index(default_index)\n>>> print(v2['<unk>']) #prints 0\n>>> print(v2['out of vocab']) #prints -1\n>>> #make default index same as index of unk_token\n>>> v2.set_default_index(v2[unk_token])\n>>> v2['out of vocab'] is v2[unk_token] #prints True \n```", "```py\ntorchtext.vocab.build_vocab_from_iterator(iterator: Iterable, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True, max_tokens: Optional[int] = None) \u2192 Vocab\u00b6\n```", "```py\n>>> #generating vocab from text file\n>>> import io\n>>> from torchtext.vocab import build_vocab_from_iterator\n>>> def yield_tokens(file_path):\n>>>     with io.open(file_path, encoding = 'utf-8') as f:\n>>>         for line in f:\n>>>             yield line.strip().split()\n>>> vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"]) \n```", "```py\nclass torchtext.vocab.Vectors(name, cache=None, url=None, unk_init=None, max_vectors=None)\u00b6\n```", "```py\n__init__(name, cache=None, url=None, unk_init=None, max_vectors=None) \u2192 None\u00b6\n```", "```py\nget_vecs_by_tokens(tokens, lower_case_backup=False)\u00b6\n```", "```py\n>>> examples = ['chip', 'baby', 'Beautiful']\n>>> vec = text.vocab.GloVe(name='6B', dim=50)\n>>> ret = vec.get_vecs_by_tokens(examples, lower_case_backup=True) \n```", "```py\nclass torchtext.vocab.GloVe(name='840B', dim=300, **kwargs)\u00b6\n```", "```py\nclass torchtext.vocab.FastText(language='en', **kwargs)\u00b6\n```", "```py\nclass torchtext.vocab.CharNGram(**kwargs)\u00b6\n```"]