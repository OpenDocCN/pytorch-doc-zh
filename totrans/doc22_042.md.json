["```py\n# enable memory history, which will\n# add tracebacks and event history to snapshots\ntorch.cuda.memory._record_memory_history()\n\nrun_your_code()\ntorch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\") \n```", "```py\ntorch.cuda.memory._record_memory_history(enabled='all', context='all', stacks='all', max_entries=9223372036854775807, device=None)\u00b6\n```", "```py\ntorch.cuda.memory._snapshot(device=None)\u00b6\n```", "```py\nclass Snapshot(TypedDict):\n    segments : List[Segment]\n    device_traces: List[List[TraceEntry]]\n\nclass Segment(TypedDict):\n    # Segments are memory returned from a cudaMalloc call.\n    # The size of reserved memory is the sum of all Segments.\n    # Segments are cached and reused for future allocations.\n    # If the reuse is smaller than the segment, the segment\n    # is split into more then one Block.\n    # empty_cache() frees Segments that are entirely inactive.\n    address: int\n    total_size: int #  cudaMalloc'd size of segment\n    stream: int\n    segment_type: Literal['small', 'large'] # 'large' (>1MB)\n    allocated_size: int # size of memory in use\n    active_size: int # size of memory in use or in active_awaiting_free state\n    blocks : List[Block]\n\nclass Block(TypedDict):\n    # A piece of memory returned from the allocator, or\n    # current cached but inactive.\n    size: int\n    requested_size: int # size requested during malloc, may be smaller than\n                        # size due to rounding\n    address: int\n    state: Literal['active_allocated', # used by a tensor\n                'active_awaiting_free', # waiting for another stream to finish using\n                                        # this, then it will become free\n                'inactive',] # free for reuse\n    frames: List[Frame] # stack trace from where the allocation occurred\n\nclass Frame(TypedDict):\n        filename: str\n        line: int\n        name: str\n\nclass TraceEntry(TypedDict):\n    # When `torch.cuda.memory._record_memory_history()` is enabled,\n    # the snapshot will contain TraceEntry objects that record each\n    # action the allocator took.\n    action: Literal[\n    'alloc'  # memory allocated\n    'free_requested', # the allocated received a call to free memory\n    'free_completed', # the memory that was requested to be freed is now\n                    # able to be used in future allocation calls\n    'segment_alloc', # the caching allocator ask cudaMalloc for more memory\n                    # and added it as a segment in its cache\n    'segment_free',  # the caching allocator called cudaFree to return memory\n                    # to cuda possibly trying free up memory to\n                    # allocate more segments or because empty_caches was called\n    'oom',          # the allocator threw an OOM exception. 'size' is\n                    # the requested number of bytes that did not succeed\n    'snapshot'      # the allocator generated a memory snapshot\n                    # useful to coorelate a previously taken\n                    # snapshot with this trace\n    ]\n    addr: int # not present for OOM\n    frames: List[Frame]\n    size: int\n    stream: int\n    device_free: int # only present for OOM, the amount of\n                    # memory cuda still reports to be free \n```", "```py\ntorch.cuda.memory._dump_snapshot(filename='dump_snapshot.pickle')\u00b6\n```"]