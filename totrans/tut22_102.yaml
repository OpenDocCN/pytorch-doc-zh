- en: Grokking PyTorch Intel CPU performance from first principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html](https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A case study on the TorchServe inference framework optimized with [Intel® Extension
    for PyTorch*](https://github.com/intel/intel-extension-for-pytorch).
  prefs: []
  type: TYPE_NORMAL
- en: 'Authors: Min Jean Cho, Mark Saroufim'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reviewers: Ashok Emani, Jiong Gong'
  prefs: []
  type: TYPE_NORMAL
- en: Getting a strong out-of-box performance for deep learning on CPUs can be tricky
    but it’s much easier if you’re aware of the main problems that affect performance,
    how to measure them and how to solve them.
  prefs: []
  type: TYPE_NORMAL
- en: TL;DR
  prefs: []
  type: TYPE_NORMAL
- en: '| Problem | How to measure it | Solution |'
  prefs: []
  type: TYPE_TB
- en: '| Bottlenecked GEMM execution units |'
  prefs: []
  type: TYPE_TB
- en: '[Imbalance or Serial Spinning](https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/top/reference/cpu-metrics-reference/spin-time/imbalance-or-serial-spinning-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Front-End Bound](https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/top/reference/cpu-metrics-reference/front-end-bound.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Core Bound](https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/top/reference/cpu-metrics-reference/back-end-bound.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Avoid using logical cores by setting thread affinity to physical cores via
    core pinning |'
  prefs: []
  type: TYPE_TB
- en: '| Non Uniform Memory Access (NUMA) |'
  prefs: []
  type: TYPE_TB
- en: Local vs. remote memory access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[UPI Utilization](https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/top/reference/cpu-metrics-reference/memory-bound/dram-bound/upi-utilization-bound.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency in memory accesses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread migration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Avoid cross-socket computation by setting thread affinity to a specific socket
    via core pinning |'
  prefs: []
  type: TYPE_TB
- en: '*GEMM (General Matrix Multiply)* run on fused-multiply-add (FMA) or dot-product
    (DP) execution units which will be bottlenecked and cause delays in thread waiting/*spinning
    at synchronization* barrier when *hyperthreading* is enabled - because using logical
    cores causes insufficient concurrency for all working threads as each logical
    thread *contends for the same core resources*. Instead, if we use 1 thread per
    physical core, we avoid this contention. So we generally recommend *avoiding logical
    cores* by setting CPU *thread affinity* to physical cores via *core pinning*.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-socket systems have *Non-Uniform Memory Access (NUMA)* which is a shared
    memory architecture that describes the placement of main memory modules with respect
    to processors. But if a process is not NUMA-aware, slow *remote memory* is frequently
    accessed when *threads migrate* cross socket via *Intel Ultra Path Interconnect
    (UPI)* during run time. We address this problem by setting CPU *thread affinity*
    to a specific socket via *core pinning*.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing these principles in mind, proper CPU runtime configuration can significantly
    boost out-of-box performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog, we’ll walk you through the important runtime configurations you
    should be aware of from [CPU Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-specific-optimizations),
    explain how they work, how to profile them and how to integrate them within a
    model serving framework like [TorchServe](https://github.com/pytorch/serve) via
    an easy to use [launch script](https://github.com/intel/intel-extension-for-pytorch/blob/master/docs/tutorials/performance_tuning/launch_script.md)
    which we’ve [integrated](https://github.com/pytorch/serve/pull/1354) ¹ natively.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll explain all of these ideas **visually** from **first principles** with
    lots of **profiles** and show you how we applied our learnings to make out of
    the box CPU performance on TorchServe better.
  prefs: []
  type: TYPE_NORMAL
- en: The feature has to be explicitly enabled by setting *cpu_launcher_enable=true*
    in *config.properties*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid logical cores for deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Avoiding logical cores for deep learning workloads generally improves performance.
    To understand this, let us take a step back to GEMM.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizing GEMM optimizes deep learning**'
  prefs: []
  type: TYPE_NORMAL
- en: The majority of time in deep learning training or inference is spent on millions
    of repeated operations of GEMM which is at the core of fully connected layers.
    Fully connected layers have been used for decades since multi-layer perceptrons
    (MLP) [proved to be a universal approximator of any continuous function](https://en.wikipedia.org/wiki/Universal_approximation_theorem).
    Any MLP can be entirely represented as GEMM. And even a convolution can be represented
    as a GEMM by using a [Toepliz matrix](https://en.wikipedia.org/wiki/Toeplitz_matrix).
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the original topic, most GEMM operators benefit from using non-hyperthreading,
    because the majority of time in deep learning training or inference is spent on
    millions of repeated operations of GEMM running on fused-multiply-add (FMA) or
    dot-product (DP) execution units shared by hyperthreading cores. With hyperthreading
    enabled, OpenMP threads will contend for the same GEMM execution units.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/1_.png](../Images/155a9f07e52c325ee9b974697797746c.png)](../_images/1_.png)'
  prefs: []
  type: TYPE_NORMAL
- en: And if 2 logical threads run GEMM at the same time, they will be sharing the
    same core resources causing front end bound, such that the overhead from this
    front end bound is greater than the gain from running both logical threads at
    the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore we generally recommend avoiding using logical cores for deep learning
    workloads to achieve good performance. The launch script by default uses physical
    cores only; however, users can easily experiment with logical vs. physical cores
    by simply toggling the `--use_logical_core` launch script knob.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the following example of feeding ResNet50 dummy tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Throughout the blog, we’ll use [Intel® VTune™ Profiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html#gs.v4egjg)
    to profile and verify optimizations. And we’ll run all exercises on a machine
    with two Intel(R) Xeon(R) Platinum 8180M CPUs. The CPU information is shown in
    Figure 2.1.
  prefs: []
  type: TYPE_NORMAL
- en: Environment variable `OMP_NUM_THREADS` is used to set the number of threads
    for parallel region. We’ll compare `OMP_NUM_THREADS=2` with (1) use of logical
    cores and (2) use of physical cores only.
  prefs: []
  type: TYPE_NORMAL
- en: Both OpenMP threads trying to utilize the same GEMM execution units shared by
    hyperthreading cores (0, 56)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can visualize this by running `htop` command on Linux as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/2.png](../Images/ea54206cf91398975d9ffa16edf04058.png)](../_images/2.png)[![../_images/3.png](../Images/ea77107db83563dd38651a1cd5831c9c.png)](../_images/3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We notice that the Spin Time is flagged, and Imbalance or Serial Spinning contributed
    to the majority of it - 4.980 seconds out of the 8.982 seconds total. The Imbalance
    or Serial Spinning when using logical cores is due to insufficient concurrency
    of working threads as each logical thread contends for the same core resources.
  prefs: []
  type: TYPE_NORMAL
- en: The Top Hotspots section of the execution summary indicates that `__kmp_fork_barrier`
    took 4.589 seconds of CPU time - during 9.33% of the CPU execution time, threads
    were just spinning at this barrier due to thread synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: Each OpenMP thread utilizing GEMM execution units in respective physical cores
    (0,1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/4.png](../Images/709b5ac62c0252784e8beaf785047853.png)](../_images/4.png)[![../_images/5.png](../Images/6803d67e46cc078fee10a753e7e95e0f.png)](../_images/5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We first note that the execution time dropped from 32 seconds to 23 seconds
    by avoiding logical cores. While there’s still some non-negligible Imbalance or
    Serial Spinning, we note relative improvement from 4.980 seconds to 3.887 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: By not using logical threads (instead, using 1 thread per physical core), we
    avoid logical threads contending for the same core resources. The Top Hotspots
    section also indicates relative improvement of `__kmp_fork_barrier` time from
    4.589 seconds to 3.530 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Local memory access is always faster than remote memory access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We generally recommend binding a process to a local socket such that the process
    does not migrate across sockets. Generally the goal of doing so is to utilize
    high speed cache on local memory and to avoid remote memory access which can be
    ~2x slower.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/6.png](../Images/4883ee88dea607f56c62f6bd09501713.png)](../_images/6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. Two-socket configuration
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. shows a typical two-socket configuration. Notice that each socket
    has its own local memory. Sockets are connected to each other via Intel Ultra
    Path Interconnect (UPI) which allows each socket to access the local memory of
    another socket called remote memory. Local memory access is always faster than
    remote memory access.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/7.png](../Images/9f8fe3672209c49d3625946847233f4b.png)](../_images/7.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1\. CPU information
  prefs: []
  type: TYPE_NORMAL
- en: Users can get their CPU information by running `lscpu` command on their Linux
    machine. Figure 2.1\. shows an example of `lscpu` execution on a machine with
    two Intel(R) Xeon(R) Platinum 8180M CPUs. Notice that there are 28 cores per socket,
    and 2 threads per core (i.e., hyperthreading is enabled). In other words, there
    are 28 logical cores in addition to 28 physical cores, giving a total of 56 cores
    per socket. And there are 2 sockets, giving a total of 112 cores (`Thread(s) per
    core` x `Core(s) per socket` x `Socket(s)`).
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/8.png](../Images/401010f7117d9febf2003c41ba5c4559.png)](../_images/8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2\. CPU information
  prefs: []
  type: TYPE_NORMAL
- en: The 2 sockets are mapped to 2 NUMA nodes (NUMA node 0, NUMA node 1) respectively.
    Physical cores are indexed prior to logical cores. As shown in Figure 2.2., the
    first 28 physical cores (0-27) and the first 28 logical cores (56-83) on the first
    socket are on NUMA node 0\. And the second 28 physical cores (28-55) and the second
    28 logical cores (84-111) on the second socket are on NUMA node 1\. Cores on the
    same socket share local memory and last level cache (LLC) which is much faster
    than cross-socket communication via Intel UPI.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand NUMA, cross-socket (UPI) traffic, local vs. remote memory
    access in multi-processor systems, let’s profile and verify our understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll reuse the ResNet50 example above.
  prefs: []
  type: TYPE_NORMAL
- en: As we did not pin threads to processor cores of a specific socket, the operating
    system periodically schedules threads on processor cores located in different
    sockets.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/9.gif](../Images/34e582c1c262c693f8472ce4254570ba.png)](../_images/9.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. CPU usage of non NUMA-aware application. 1 main worker thread was
    launched, then it launched a physical core number (56) of threads on all cores,
    including logical cores.
  prefs: []
  type: TYPE_NORMAL
- en: '(Aside: If the number of threads is not set by [torch.set_num_threads](https://pytorch.org/docs/stable/generated/torch.set_num_threads.html),
    the default number of threads is the number of physical cores in a hyperthreading
    enabled system. This can be verified by [torch.get_num_threads](https://pytorch.org/docs/stable/generated/torch.get_num_threads.html).
    Hence we see above about half of the cores busy running the example script.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/10.png](../Images/19d05986d8f0236e3463065f779359e1.png)](../_images/10.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4\. Non-Uniform Memory Access Analysis graph
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4\. compares local vs. remote memory access over time. We verify usage
    of remote memory which could result in sub-optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Set thread affinity to reduce remote memory access and cross-socket (UPI)
    traffic**'
  prefs: []
  type: TYPE_NORMAL
- en: Pinning threads to cores on the same socket helps maintain locality of memory
    access. In this example, we’ll pin to the physical cores on the first NUMA node
    (0-27). With the launch script, users can easily experiment with NUMA nodes configuration
    by simply toggling the `--node_id` launch script knob.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s visualize the CPU usage now.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/11.gif](../Images/7dad920dd5537d51cf51649c1e4da9d5.png)](../_images/11.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5\. CPU usage of NUMA-aware application
  prefs: []
  type: TYPE_NORMAL
- en: 1 main worker thread was launched, then it launched threads on all physical
    cores on the first numa node.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/12.png](../Images/cf04e2e8d9070f5a2606fb137b9e383f.png)](../_images/12.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6\. Non-Uniform Memory Access Analysis graph
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Figure 6., now almost all memory accesses are local accesses.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient CPU usage with core pinning for multi-worker inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When running multi-worker inference, cores are overlapped (or shared) between
    workers causing inefficient CPU usage. To address this problem, the launch script
    equally divides the number of available cores by the number of workers such that
    each worker is pinned to assigned cores during runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise with TorchServe**'
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, let’s apply the CPU performance tuning principles and recommendations
    that we have discussed so far to [TorchServe apache-bench benchmarking](https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-apache-bench).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use ResNet50 with 4 workers, concurrency 100, requests 10,000\. All other
    parameters (e.g., batch_size, input, etc) are the same as the [default parameters](https://github.com/pytorch/serve/blob/master/benchmarks/benchmark-ab.py#L18).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll compare the following three configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: default TorchServe setting (no core pinning)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[torch.set_num_threads](https://pytorch.org/docs/stable/generated/torch.set_num_threads.html)
    = `number of physical cores / number of workers` (no core pinning)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: core pinning via the launch script (Required Torchserve>=0.6.1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After this exercise, we’ll have verified that we prefer avoiding logical cores
    and prefer local memory access via core pinning with a real TorchServe use case.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Default TorchServe setting (no core pinning)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [base_handler](https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py)
    doesn’t explicitly set [torch.set_num_threads](https://pytorch.org/docs/stable/generated/torch.set_num_threads.html).
    Hence the default number of threads is the number of physical CPU cores as described
    [here](https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#runtime-api).
    Users can check the number of threads by [torch.get_num_threads](https://pytorch.org/docs/stable/generated/torch.get_num_threads.html)
    in the base_handler. Each of the 4 main worker threads launches a physical core
    number (56) of threads, launching a total of 56x4 = 224 threads, which is more
    than the total number of cores 112\. Therefore cores are guaranteed to be heavily
    overlapped with high logical core utilization- multiple workers using multiple
    cores at the same time. Furthermore, because threads are not affinitized to specific
    CPU cores, the operating system periodically schedules threads to cores located
    in different sockets.
  prefs: []
  type: TYPE_NORMAL
- en: CPU usage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/13.png](../Images/2b07d65eadf8e98357f803af1877fdcd.png)](../_images/13.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 4 main worker threads were launched, then each launched a physical core number
    (56) of threads on all cores, including logical cores.
  prefs: []
  type: TYPE_NORMAL
- en: Core Bound stalls
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/14.png](../Images/c748ce77da86cac08e077037f7461454.png)](../_images/14.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We observe a very high Core Bound stall of 88.4%, decreasing pipeline efficiency.
    Core Bound stalls indicate sub-optimal use of available execution units in the
    CPU. For example, several GEMM instructions in a row competing for fused-multiply-add
    (FMA) or dot-product (DP) execution units shared by hyperthreading cores could
    cause Core Bound stalls. And as described in the previous section, use of logical
    cores amplifies this problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/15.png](../Images/bdcd9079d5ed8138caf487ade7a083bc.png)](../_images/15.png)[![../_images/16.png](../Images/3db21e6b9c932a6387171481b4cc4d0f.png)](../_images/16.png)'
  prefs: []
  type: TYPE_NORMAL
- en: An empty pipeline slot not filled with micro-ops (uOps) is attributed to a stall.
    For example, without core pinning CPU usage may not effectively be on compute
    but on other operations like thread scheduling from Linux kernel. We see above
    that `__sched_yield` contributed to the majority of the Spin Time.
  prefs: []
  type: TYPE_NORMAL
- en: Thread Migration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Without core pinning, scheduler may migrate thread executing on a core to a
    different core. Thread migration can disassociate the thread from data that has
    already been fetched into the caches resulting in longer data access latencies.
    This problem is exacerbated in NUMA systems when thread migrates across sockets.
    Data that has been fetched to high speed cache on local memory now becomes remote
    memory, which is much slower.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/17.png](../Images/2a12b3c78a1e37b95144e879414c7bee.png)](../_images/17.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Generally the total number of threads should be less than or equal to the total
    number of threads supported by the core. In the above example, we notice a large
    number of threads executing on core_51 instead of the expected 2 threads (since
    hyperthreading is enabled in Intel(R) Xeon(R) Platinum 8180 CPUs) . This indicates
    thread migration.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/18.png](../Images/3b6d2c15f792114de5f771ac3eabe97e.png)](../_images/18.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, notice that thread (TID:97097) was executing on a large number
    of CPU cores, indicating CPU migration. For example, this thread was executing
    on cpu_81, then migrated to cpu_14, then migrated to cpu_5, and so on. Furthermore,
    note that this thread migrated cross socket back and forth many times, resulting
    in very inefficient memory access. For example, this thread executed on cpu_70
    (NUMA node 0), then migrated to cpu_100 (NUMA node 1), then migrated to cpu_24
    (NUMA node 0).
  prefs: []
  type: TYPE_NORMAL
- en: Non Uniform Memory Access Analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/19.png](../Images/eb535b6f0a49e85a2f6c69b3307eb58d.png)](../_images/19.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Compare local vs. remote memory access over time. We observe that about half,
    51.09%, of the memory accesses were remote accesses, indicating sub-optimal NUMA
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. torch.set_num_threads = `number of physical cores / number of workers` (no
    core pinning)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For an apple-to-apple comparison with launcher’s core pinning, we’ll set the
    number of threads to the number of cores divided by the number of workers (launcher
    does this internally). Add the following code snippet in the [base_handler](https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As before without core pinning, these threads are not affinitized to specific
    CPU cores, causing the operating system to periodically schedule threads on cores
    located in different sockets.
  prefs: []
  type: TYPE_NORMAL
- en: CPU usage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/20.gif](../Images/324a4abab610044d4447be1f203420e5.png)](../_images/20.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: 4 main worker threads were launched, then each launched a `num_physical_cores/num_workers`
    number (14) of threads on all cores, including logical cores.
  prefs: []
  type: TYPE_NORMAL
- en: Core Bound stalls
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/21.png](../Images/c8d610422cd75e7798fb7a63febc2cf1.png)](../_images/21.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Although the percentage of Core Bound stalls has decreased from 88.4% to 73.5%,
    the Core Bound is still very high.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/22.png](../Images/38f5ccd2ad70dfa738cd25d4795a2103.png)](../_images/22.png)[![../_images/23.png](../Images/27c04f2268b595a2bd5299b190141526.png)](../_images/23.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Thread Migration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/24.png](../Images/f5b0a04ed3cae12a3b77548c56420027.png)](../_images/24.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Similar as before, without core pinning thread (TID:94290) was executing on
    a large number of CPU cores, indicating CPU migration. We notice again cross-socket
    thread migration, resulting in very inefficient memory access. For example, this
    thread executed on cpu_78 (NUMA node 0), then migrated to cpu_108 (NUMA node 1).
  prefs: []
  type: TYPE_NORMAL
- en: Non Uniform Memory Access Analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/25.png](../Images/6d7d4b4277221a27dd70ad2125f77d31.png)](../_images/25.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Although an improvement from the original 51.09%, still 40.45% of memory access
    is remote, indicating sub-optimal NUMA configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. launcher core pinning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Launcher will internally equally distribute physical cores to workers, and bind
    them to each worker. As a reminder, launcher by default uses physical cores only.
    In this example, launcher will bind worker 0 to cores 0-13 (NUMA node 0), worker
    1 to cores 14-27 (NUMA node 0), worker 2 to cores 28-41 (NUMA node 1), and worker
    3 to cores 42-55 (NUMA node 1). Doing so ensures that cores are not overlapped
    among workers and avoids logical core usage.
  prefs: []
  type: TYPE_NORMAL
- en: CPU usage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/26.gif](../Images/cc975550a4d7909f0f03f76326e46cd3.png)](../_images/26.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: 4 main worker threads were launched, then each launched a `num_physical_cores/num_workers`
    number (14) of threads affinitized to the assigned physical cores.
  prefs: []
  type: TYPE_NORMAL
- en: Core Bound stalls
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/27.png](../Images/a577ea5b668f5f85e0da983e9c1a599e.png)](../_images/27.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Core Bound stalls has decreased significantly from the original 88.4% to 46.2%
    - almost a 2x improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/28.png](../Images/59c95d60c58a4d7c0419a746f14eabcb.png)](../_images/28.png)[![../_images/29.png](../Images/f2d4511f210f226a6c6e025502e82080.png)](../_images/29.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We verify that with core binding, most CPU time is effectively used on compute
    - Spin Time of 0.256s.
  prefs: []
  type: TYPE_NORMAL
- en: Thread Migration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/30.png](../Images/84fcb0c473094ff12e9703650eecaa18.png)](../_images/30.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We verify that OMP Primary Thread #0 was bound to assigned physical cores (42-55),
    and did not migrate cross-socket.'
  prefs: []
  type: TYPE_NORMAL
- en: Non Uniform Memory Access Analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[![../_images/31.png](../Images/9fb87b733977c7fb231442b0a54ea406.png)](../_images/31.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Now almost all, 89.52%, memory accesses are local accesses.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this blog, we’ve showcased that properly setting your CPU runtime configuration
    can significantly boost out-of-box CPU performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have walked through some general CPU performance tuning principles and recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: In a hyperthreading enabled system, avoid logical cores by setting thread affinity
    to physical cores only via core pinning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a multi-socket system with NUMA, avoid cross-socket remote memory access
    by setting thread affinity to a specific socket via core pinning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have visually explained these ideas from first principles and have verified
    the performance boost with profiling. And finally, we have applied all of our
    learnings to TorchServe to boost out-of-box TorchServe CPU performance.
  prefs: []
  type: TYPE_NORMAL
- en: These principles can be automatically configured via an easy to use launch script
    which has already been integrated into TorchServe.
  prefs: []
  type: TYPE_NORMAL
- en: 'For interested readers, please check out the following documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[CPU specific optimizations](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-specific-optimizations)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Maximize Performance of Intel® Software Optimization for PyTorch* on CPU](https://www.intel.com/content/www/us/en/developer/articles/technical/how-to-get-better-performance-on-pytorchcaffe2-with-intel-acceleration.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Performance Tuning Guide](https://intel.github.io/intel-extension-for-pytorch/tutorials/performance_tuning/tuning_guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Launch Script Usage Guide](https://intel.github.io/intel-extension-for-pytorch/tutorials/performance_tuning/launch_script.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top-down Microarchitecture Analysis Method](https://www.intel.com/content/www/us/en/develop/documentation/vtune-cookbook/top/methodologies/top-down-microarchitecture-analysis-method.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Configuring oneDNN for Benchmarking](https://oneapi-src.github.io/oneDNN/dev_guide_performance_settings.html#benchmarking-settings)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Intel® VTune™ Profiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html#gs.tcbgpa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Intel® VTune™ Profiler User Guide](https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/top.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And stay tuned for a follow-up posts on optimized kernels on CPU via [Intel®
    Extension for PyTorch*](https://github.com/intel/intel-extension-for-pytorch)
    and advanced launcher configurations such as memory allocator.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank Ashok Emani (Intel) and Jiong Gong (Intel) for their
    immense guidance and support, and thorough feedback and reviews throughout many
    steps of this blog. We would also like to thank Hamid Shojanazeri (Meta), Li Ning
    (AWS) and Jing Xu (Intel) for helpful feedback in code review. And Suraj Subramanian
    (Meta) and Geeta Chauhan (Meta) for helpful feedback on the blog.
  prefs: []
  type: TYPE_NORMAL
