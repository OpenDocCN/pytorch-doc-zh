# torchaudio.models

> 原文：[https://pytorch.org/audio/stable/models.html](https://pytorch.org/audio/stable/models.html)

`torchaudio.models`子包含有用于处理常见音频任务的模型定义。

注意

对于具有预训练参数的模型，请参考[`torchaudio.pipelines`](pipelines.html#module-torchaudio.pipelines "torchaudio.pipelines")模块。

模型定义负责构建计算图并执行它们。

一些模型具有复杂的结构和变体。对于这样的模型，提供了工厂函数。

| [`Conformer`](generated/torchaudio.models.Conformer.html#torchaudio.models.Conformer "torchaudio.models.Conformer") | Conformer架构介绍在*Conformer: Convolution-augmented Transformer for Speech Recognition* [[Gulati *et al.*, 2020](references.html#id21 "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: convolution-augmented transformer for speech recognition. 2020\. arXiv:2005.08100.")]. |
| --- | --- |
| [`ConvTasNet`](generated/torchaudio.models.ConvTasNet.html#torchaudio.models.ConvTasNet "torchaudio.models.ConvTasNet") | Conv-TasNet架构介绍在*Conv-TasNet: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation* [[Luo and Mesgarani, 2019](references.html#id22 "Yi Luo and Nima Mesgarani. Conv-tasnet: surpassing ideal time–frequency magnitude masking for speech separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(8):1256–1266, Aug 2019\. URL: http://dx.doi.org/10.1109/TASLP.2019.2915167, doi:10.1109/taslp.2019.2915167.")]. |
| [`DeepSpeech`](generated/torchaudio.models.DeepSpeech.html#torchaudio.models.DeepSpeech "torchaudio.models.DeepSpeech") | DeepSpeech架构介绍在*Deep Speech: Scaling up end-to-end speech recognition* [[Hannun *et al.*, 2014](references.html#id17 "Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y. Ng. Deep speech: scaling up end-to-end speech recognition. 2014\. arXiv:1412.5567.")]. |
| [`Emformer`](generated/torchaudio.models.Emformer.html#torchaudio.models.Emformer "torchaudio.models.Emformer") | Emformer架构介绍在*Emformer: Efficient Memory Transformer Based Acoustic Model for Low Latency Streaming Speech Recognition* [[Shi *et al.*, 2021](references.html#id30 "Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian Chan, Frank Zhang, Duc Le, and Mike Seltzer. Emformer: efficient memory transformer based acoustic model for low latency streaming speech recognition. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 6783-6787\. 2021.")]. |
| [`HDemucs`](generated/torchaudio.models.HDemucs.html#torchaudio.models.HDemucs "torchaudio.models.HDemucs") | 来自*Hybrid Spectrogram and Waveform Source Separation*的混合Demucs模型[[Défossez, 2021](references.html#id50 "Alexandre Défossez. Hybrid spectrogram and waveform source separation. In Proceedings of the ISMIR 2021 Workshop on Music Source Separation. 2021.")]. |
| [`HuBERTPretrainModel`](generated/torchaudio.models.HuBERTPretrainModel.html#torchaudio.models.HuBERTPretrainModel "torchaudio.models.HuBERTPretrainModel") | HuBERT模型用于*HuBERT*中的预训练 [[Hsu *et al.*, 2021](references.html#id16 "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: self-supervised speech representation learning by masked prediction of hidden units. 2021\. arXiv:2106.07447.")]. |
| [`RNNT`](generated/torchaudio.models.RNNT.html#torchaudio.models.RNNT "torchaudio.models.RNNT") | 循环神经网络转录器（RNN-T）模型。 |
| [`RNNTBeamSearch`](generated/torchaudio.models.RNNTBeamSearch.html#torchaudio.models.RNNTBeamSearch "torchaudio.models.RNNTBeamSearch") | RNN-T模型的束搜索解码器。 |
| [`SquimObjective`](generated/torchaudio.models.SquimObjective.html#torchaudio.models.SquimObjective "torchaudio.models.SquimObjective") | 预测语音增强的**客观**度量分数（例如，STOI、PESQ和SI-SDR）的语音质量和可懂度测量（SQUIM）模型。 |
| [`SquimSubjective`](generated/torchaudio.models.SquimSubjective.html#torchaudio.models.SquimSubjective "torchaudio.models.SquimSubjective") | 预测语音增强的**主观**度量分数（例如，平均意见分数（MOS））的语音质量和可懂度测量（SQUIM）模型。 |
| [`Tacotron2`](generated/torchaudio.models.Tacotron2.html#torchaudio.models.Tacotron2 "torchaudio.models.Tacotron2") | 基于《自然TTS合成：通过在Mel频谱图预测上对WaveNet进行条件化》[[Shen等，2018](references.html#id27 "Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan等。通过在mel频谱图预测上对WaveNet进行条件化的自然TTS合成。在2018年IEEE国际声学、语音和信号处理会议（ICASSP）上，4779-4783。IEEE，2018。")] 的Tacotron2模型，基于[Nvidia深度学习示例](https://github.com/NVIDIA/DeepLearningExamples/)的实现。 |
| [`Wav2Letter`](generated/torchaudio.models.Wav2Letter.html#torchaudio.models.Wav2Letter "torchaudio.models.Wav2Letter") | 来自《Wav2Letter：基于端到端ConvNet的语音识别系统》[[Collobert等，2016](references.html#id19 "Ronan Collobert, Christian Puhrsch, Gabriel Synnaeve。Wav2letter：基于端到端ConvNet的语音识别系统。2016。arXiv:1609.03193。")] 的Wav2Letter模型架构。 |
| [`Wav2Vec2Model`](generated/torchaudio.models.Wav2Vec2Model.html#torchaudio.models.Wav2Vec2Model "torchaudio.models.Wav2Vec2Model") | *wav2vec 2.0*中使用的声学模型[[Baevski等，2020](references.html#id15 "Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli。Wav2vec 2.0：自监督学习语音表示的框架。2020。arXiv:2006.11477。")]。 |
| [`WaveRNN`](generated/torchaudio.models.WaveRNN.html#torchaudio.models.WaveRNN "torchaudio.models.WaveRNN") | 基于《高效神经音频合成》[[Kalchbrenner等，2018](references.html#id3 "Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aäron van den Oord, Sander Dieleman, Koray Kavukcuoglu等。高效神经音频合成。CoRR，2018。URL：http://arxiv.org/abs/1802.08435，arXiv:1802.08435。")] 的WaveRNN模型，基于[fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)的实现。 |
