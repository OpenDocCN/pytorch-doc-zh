["```py\nclass LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        # 3 * state_size for input gate, output gate and candidate cell gate.\n        # input_features + state_size because we will multiply with [input, h].\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        old_h, old_cell = state\n        X = torch.cat([old_h, input], dim=1)\n\n        # Compute the input, output and candidate cell gates with one MM.\n        gate_weights = F.linear(X, self.weights, self.bias)\n        # Split the combined gate weight matrix into its components.\n        gates = gate_weights.chunk(3, dim=1)\n\n        input_gate = torch.sigmoid(gates[0])\n        output_gate = torch.sigmoid(gates[1])\n        # Here we use an ELU instead of the usual tanh.\n        candidate_cell = F.elu(gates[2])\n\n        # Compute the new cell state.\n        new_cell = old_cell + candidate_cell * input_gate\n        # Compute the new hidden state and output.\n        new_h = torch.tanh(new_cell) * output_gate\n\n        return new_h, new_cell \n```", "```py\nimport torch\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nnew_h, new_C = rnn(X, (h, C)) \n```", "```py\nfrom setuptools import setup, Extension\nfrom torch.utils import cpp_extension\n\nsetup(name='lltm_cpp',\n      ext_modules=[cpp_extension.CppExtension('lltm_cpp', ['lltm.cpp'])],\n      cmdclass={'build_ext': cpp_extension.BuildExtension}) \n```", "```py\nExtension(\n   name='lltm_cpp',\n   sources=['lltm.cpp'],\n   include_dirs=cpp_extension.include_paths(),\n   language='c++') \n```", "```py\n#include  <torch/extension.h>\n\n#include  <iostream>\n\ntorch::Tensor  d_sigmoid(torch::Tensor  z)  {\n  auto  s  =  torch::sigmoid(z);\n  return  (1  -  s)  *  s;\n} \n```", "```py\n#include  <ATen/ATen.h>\nat::Tensor  SigmoidAlphaBlendForwardCuda(....) \n```", "```py\n#include  <torch/extension.h>\ntorch::Tensor  SigmoidAlphaBlendForwardCuda(...) \n```", "```py\n#include  <vector>\n\nstd::vector<at::Tensor>  lltm_forward(\n  torch::Tensor  input,\n  torch::Tensor  weights,\n  torch::Tensor  bias,\n  torch::Tensor  old_h,\n  torch::Tensor  old_cell)  {\n  auto  X  =  torch::cat({old_h,  input},  /*dim=*/1);\n\n  auto  gate_weights  =  torch::addmm(bias,  X,  weights.transpose(0,  1));\n  auto  gates  =  gate_weights.chunk(3,  /*dim=*/1);\n\n  auto  input_gate  =  torch::sigmoid(gates[0]);\n  auto  output_gate  =  torch::sigmoid(gates[1]);\n  auto  candidate_cell  =  torch::elu(gates[2],  /*alpha=*/1.0);\n\n  auto  new_cell  =  old_cell  +  candidate_cell  *  input_gate;\n  auto  new_h  =  torch::tanh(new_cell)  *  output_gate;\n\n  return  {new_h,\n  new_cell,\n  input_gate,\n  output_gate,\n  candidate_cell,\n  X,\n  gate_weights};\n} \n```", "```py\n// tanh'(z) = 1 - tanh^2(z)\ntorch::Tensor  d_tanh(torch::Tensor  z)  {\n  return  1  -  z.tanh().pow(2);\n}\n\n// elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) < 0, else 0}\ntorch::Tensor  d_elu(torch::Tensor  z,  torch::Scalar  alpha  =  1.0)  {\n  auto  e  =  z.exp();\n  auto  mask  =  (alpha  *  (e  -  1))  <  0;\n  return  (z  >  0).type_as(z)  +  mask.type_as(z)  *  (alpha  *  e);\n}\n\nstd::vector<torch::Tensor>  lltm_backward(\n  torch::Tensor  grad_h,\n  torch::Tensor  grad_cell,\n  torch::Tensor  new_cell,\n  torch::Tensor  input_gate,\n  torch::Tensor  output_gate,\n  torch::Tensor  candidate_cell,\n  torch::Tensor  X,\n  torch::Tensor  gate_weights,\n  torch::Tensor  weights)  {\n  auto  d_output_gate  =  torch::tanh(new_cell)  *  grad_h;\n  auto  d_tanh_new_cell  =  output_gate  *  grad_h;\n  auto  d_new_cell  =  d_tanh(new_cell)  *  d_tanh_new_cell  +  grad_cell;\n\n  auto  d_old_cell  =  d_new_cell;\n  auto  d_candidate_cell  =  input_gate  *  d_new_cell;\n  auto  d_input_gate  =  candidate_cell  *  d_new_cell;\n\n  auto  gates  =  gate_weights.chunk(3,  /*dim=*/1);\n  d_input_gate  *=  d_sigmoid(gates[0]);\n  d_output_gate  *=  d_sigmoid(gates[1]);\n  d_candidate_cell  *=  d_elu(gates[2]);\n\n  auto  d_gates  =\n  torch::cat({d_input_gate,  d_output_gate,  d_candidate_cell},  /*dim=*/1);\n\n  auto  d_weights  =  d_gates.t().mm(X);\n  auto  d_bias  =  d_gates.sum(/*dim=*/0,  /*keepdim=*/true);\n\n  auto  d_X  =  d_gates.mm(weights);\n  const  auto  state_size  =  grad_h.size(1);\n  auto  d_old_h  =  d_X.slice(/*dim=*/1,  0,  state_size);\n  auto  d_input  =  d_X.slice(/*dim=*/1,  state_size);\n\n  return  {d_old_h,  d_input,  d_weights,  d_bias,  d_old_cell};\n} \n```", "```py\nPYBIND11_MODULE(TORCH_EXTENSION_NAME,  m)  {\n  m.def(\"forward\",  &lltm_forward,  \"LLTM forward\");\n  m.def(\"backward\",  &lltm_backward,  \"LLTM backward\");\n} \n```", "```py\npytorch/\n  lltm-extension/\n    lltm.cpp\n    setup.py \n```", "```py\nrunning install\nrunning bdist_egg\nrunning egg_info\ncreating lltm_cpp.egg-info\nwriting lltm_cpp.egg-info/PKG-INFO\nwriting dependency_links to lltm_cpp.egg-info/dependency_links.txt\nwriting top-level names to lltm_cpp.egg-info/top_level.txt\nwriting manifest file 'lltm_cpp.egg-info/SOURCES.txt'\nreading manifest file 'lltm_cpp.egg-info/SOURCES.txt'\nwriting manifest file 'lltm_cpp.egg-info/SOURCES.txt'\ninstalling library code to build/bdist.linux-x86_64/egg\nrunning install_lib\nrunning build_ext\nbuilding 'lltm_cpp' extension\ncreating build\ncreating build/temp.linux-x86_64-3.7\ngcc -pthread -B ~/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I~/local/miniconda/lib/python3.7/site-packages/torch/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/TH -I~/local/miniconda/lib/python3.7/site-packages/torch/include/THC -I~/local/miniconda/include/python3.7m -c lltm.cpp -o build/temp.linux-x86_64-3.7/lltm.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=lltm_cpp -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11\ncc1plus: warning: command line option \u2018-Wstrict-prototypes\u2019 is valid for C/ObjC but not for C++\ncreating build/lib.linux-x86_64-3.7\ng++ -pthread -shared -B ~/local/miniconda/compiler_compat -L~/local/miniconda/lib -Wl,-rpath=~/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/lltm.o -o build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so\ncreating build/bdist.linux-x86_64\ncreating build/bdist.linux-x86_64/egg\ncopying build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\ncreating stub loader for lltm_cpp.cpython-37m-x86_64-linux-gnu.so\nbyte-compiling build/bdist.linux-x86_64/egg/lltm_cpp.py to lltm_cpp.cpython-37.pyc\ncreating build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\nwriting build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\nzip_safe flag not set; analyzing archive contents...\n__pycache__.lltm_cpp.cpython-37: module references __file__\ncreating 'dist/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\nremoving 'build/bdist.linux-x86_64/egg' (and everything under it)\nProcessing lltm_cpp-0.0.0-py3.7-linux-x86_64.egg\nremoving '~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' (and everything under it)\ncreating ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg\nExtracting lltm_cpp-0.0.0-py3.7-linux-x86_64.egg to ~/local/miniconda/lib/python3.7/site-packages\nlltm-cpp 0.0.0 is already the active version in easy-install.pth\n\nInstalled ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg\nProcessing dependencies for lltm-cpp==0.0.0\nFinished processing dependencies for lltm-cpp==0.0.0 \n```", "```py\nIn [1]: import torch\nIn [2]: import lltm_cpp\nIn [3]: lltm_cpp.forward\nOut[3]: <function lltm.PyCapsule.forward> \n```", "```py\nIn[4] help(lltm_cpp.forward)\nforward(...) method of builtins.PyCapsule instance\n    forward(arg0: torch::Tensor, arg1: torch::Tensor, arg2: torch::Tensor, arg3: torch::Tensor, arg4: torch::Tensor) -> List[torch::Tensor]\n\n    LLTM forward \n```", "```py\nimport math\nimport torch\n\n# Our module!\nimport lltm_cpp\n\nclass LLTMFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, weights, bias, old_h, old_cell):\n        outputs = lltm_cpp.forward(input, weights, bias, old_h, old_cell)\n        new_h, new_cell = outputs[:2]\n        variables = outputs[1:] + [weights]\n        ctx.save_for_backward(*variables)\n\n        return new_h, new_cell\n\n    @staticmethod\n    def backward(ctx, grad_h, grad_cell):\n        outputs = lltm_cpp.backward(\n            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)\n        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs\n        return d_input, d_weights, d_bias, d_old_h, d_old_cell\n\nclass LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        return LLTMFunction.apply(input, self.weights, self.bias, *state) \n```", "```py\nimport time\n\nimport torch\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} s | Backward {:.3f} s'.format(forward, backward)) \n```", "```py\nForward: 506.480 us | Backward 444.694 us \n```", "```py\nForward: 349.335 us | Backward 443.523 us \n```", "```py\nimport torch\n\nassert torch.cuda.is_available()\ncuda_device = torch.device(\"cuda\")  # device object representing GPU\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\n# Note the device=cuda_device arguments here\nX = torch.randn(batch_size, input_features, device=cuda_device)\nh = torch.randn(batch_size, state_size, device=cuda_device)\nC = torch.randn(batch_size, state_size, device=cuda_device)\n\nrnn = LLTM(input_features, state_size).to(cuda_device)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    torch.cuda.synchronize()\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    torch.cuda.synchronize()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5)) \n```", "```py\nForward: 187.719 us | Backward 410.815 us \n```", "```py\nForward: 149.802 us | Backward 393.458 us \n```", "```py\nfrom torch.utils.cpp_extension import load\n\nlltm_cpp = load(name=\"lltm_cpp\", sources=[\"lltm.cpp\"]) \n```", "```py\nUsing /tmp/torch_extensions as PyTorch extensions root...\nEmitting ninja build file /tmp/torch_extensions/lltm_cpp/build.ninja...\nBuilding extension module lltm_cpp...\nLoading extension module lltm_cpp... \n```", "```py\n#include  <torch/extension.h>\n\n#include  <vector>\n\n// CUDA forward declarations\n\nstd::vector<torch::Tensor>  lltm_cuda_forward(\n  torch::Tensor  input,\n  torch::Tensor  weights,\n  torch::Tensor  bias,\n  torch::Tensor  old_h,\n  torch::Tensor  old_cell);\n\nstd::vector<torch::Tensor>  lltm_cuda_backward(\n  torch::Tensor  grad_h,\n  torch::Tensor  grad_cell,\n  torch::Tensor  new_cell,\n  torch::Tensor  input_gate,\n  torch::Tensor  output_gate,\n  torch::Tensor  candidate_cell,\n  torch::Tensor  X,\n  torch::Tensor  gate_weights,\n  torch::Tensor  weights);\n\n// C++ interface\n\n#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n\nstd::vector<torch::Tensor>  lltm_forward(\n  torch::Tensor  input,\n  torch::Tensor  weights,\n  torch::Tensor  bias,\n  torch::Tensor  old_h,\n  torch::Tensor  old_cell)  {\n  CHECK_INPUT(input);\n  CHECK_INPUT(weights);\n  CHECK_INPUT(bias);\n  CHECK_INPUT(old_h);\n  CHECK_INPUT(old_cell);\n\n  return  lltm_cuda_forward(input,  weights,  bias,  old_h,  old_cell);\n}\n\nstd::vector<torch::Tensor>  lltm_backward(\n  torch::Tensor  grad_h,\n  torch::Tensor  grad_cell,\n  torch::Tensor  new_cell,\n  torch::Tensor  input_gate,\n  torch::Tensor  output_gate,\n  torch::Tensor  candidate_cell,\n  torch::Tensor  X,\n  torch::Tensor  gate_weights,\n  torch::Tensor  weights)  {\n  CHECK_INPUT(grad_h);\n  CHECK_INPUT(grad_cell);\n  CHECK_INPUT(input_gate);\n  CHECK_INPUT(output_gate);\n  CHECK_INPUT(candidate_cell);\n  CHECK_INPUT(X);\n  CHECK_INPUT(gate_weights);\n  CHECK_INPUT(weights);\n\n  return  lltm_cuda_backward(\n  grad_h,\n  grad_cell,\n  new_cell,\n  input_gate,\n  output_gate,\n  candidate_cell,\n  X,\n  gate_weights,\n  weights);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME,  m)  {\n  m.def(\"forward\",  &lltm_forward,  \"LLTM forward (CUDA)\");\n  m.def(\"backward\",  &lltm_backward,  \"LLTM backward (CUDA)\");\n} \n```", "```py\n#include  <torch/extension.h>\n\n#include  <cuda.h>\n#include  <cuda_runtime.h>\n\n#include  <vector>\n\ntemplate  <typename  scalar_t>\n__device__  __forceinline__  scalar_t  sigmoid(scalar_t  z)  {\n  return  1.0  /  (1.0  +  exp(-z));\n} \n```", "```py\ntemplate  <typename  scalar_t>\n__device__  __forceinline__  scalar_t  d_sigmoid(scalar_t  z)  {\n  const  auto  s  =  sigmoid(z);\n  return  (1.0  -  s)  *  s;\n}\n\ntemplate  <typename  scalar_t>\n__device__  __forceinline__  scalar_t  d_tanh(scalar_t  z)  {\n  const  auto  t  =  tanh(z);\n  return  1  -  (t  *  t);\n}\n\ntemplate  <typename  scalar_t>\n__device__  __forceinline__  scalar_t  elu(scalar_t  z,  scalar_t  alpha  =  1.0)  {\n  return  fmax(0.0,  z)  +  fmin(0.0,  alpha  *  (exp(z)  -  1.0));\n}\n\ntemplate  <typename  scalar_t>\n__device__  __forceinline__  scalar_t  d_elu(scalar_t  z,  scalar_t  alpha  =  1.0)  {\n  const  auto  e  =  exp(z);\n  const  auto  d_relu  =  z  <  0.0  ?  0.0  :  1.0;\n  return  d_relu  +  (((alpha  *  (e  -  1.0))  <  0.0)  ?  (alpha  *  e)  :  0.0);\n} \n```", "```py\nstd::vector<torch::Tensor>  lltm_cuda_forward(\n  torch::Tensor  input,\n  torch::Tensor  weights,\n  torch::Tensor  bias,\n  torch::Tensor  old_h,\n  torch::Tensor  old_cell)  {\n  auto  X  =  torch::cat({old_h,  input},  /*dim=*/1);\n  auto  gates  =  torch::addmm(bias,  X,  weights.transpose(0,  1));\n\n  const  auto  batch_size  =  old_cell.size(0);\n  const  auto  state_size  =  old_cell.size(1);\n\n  auto  new_h  =  torch::zeros_like(old_cell);\n  auto  new_cell  =  torch::zeros_like(old_cell);\n  auto  input_gate  =  torch::zeros_like(old_cell);\n  auto  output_gate  =  torch::zeros_like(old_cell);\n  auto  candidate_cell  =  torch::zeros_like(old_cell);\n\n  const  int  threads  =  1024;\n  const  dim3  blocks((state_size  +  threads  -  1)  /  threads,  batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(gates.type(),  \"lltm_forward_cuda\",  ([&]  {\n  lltm_cuda_forward_kernel<scalar_t><<<blocks,  threads>>>(\n  gates.data<scalar_t>(),\n  old_cell.data<scalar_t>(),\n  new_h.data<scalar_t>(),\n  new_cell.data<scalar_t>(),\n  input_gate.data<scalar_t>(),\n  output_gate.data<scalar_t>(),\n  candidate_cell.data<scalar_t>(),\n  state_size);\n  }));\n\n  return  {new_h,  new_cell,  input_gate,  output_gate,  candidate_cell,  X,  gates};\n} \n```", "```py\nswitch  (tensor.type().scalarType())  {\n  case  torch::ScalarType::Double:\n  return  function<double>(tensor.data<double>());\n  case  torch::ScalarType::Float:\n  return  function<float>(tensor.data<float>());\n  ...\n} \n```", "```py\ntemplate  <typename  scalar_t>\n__global__  void  lltm_cuda_forward_kernel(\n  const  scalar_t*  __restrict__  gates,\n  const  scalar_t*  __restrict__  old_cell,\n  scalar_t*  __restrict__  new_h,\n  scalar_t*  __restrict__  new_cell,\n  scalar_t*  __restrict__  input_gate,\n  scalar_t*  __restrict__  output_gate,\n  scalar_t*  __restrict__  candidate_cell,\n  size_t  state_size)  {\n  const  int  column  =  blockIdx.x  *  blockDim.x  +  threadIdx.x;\n  const  int  index  =  blockIdx.y  *  state_size  +  column;\n  const  int  gates_row  =  blockIdx.y  *  (state_size  *  3);\n  if  (column  <  state_size)  {\n  input_gate[index]  =  sigmoid(gates[gates_row  +  column]);\n  output_gate[index]  =  sigmoid(gates[gates_row  +  state_size  +  column]);\n  candidate_cell[index]  =  elu(gates[gates_row  +  2  *  state_size  +  column]);\n  new_cell[index]  =\n  old_cell[index]  +  candidate_cell[index]  *  input_gate[index];\n  new_h[index]  =  tanh(new_cell[index])  *  output_gate[index];\n  }\n} \n```", "```py\ngates.data<scalar_t>()[n*3*state_size  +  row*state_size  +  column] \n```", "```py\ntorch::Tensor  foo  =  torch::rand({12,  12});\n\n// assert foo is 2-dimensional and holds floats.\nauto  foo_a  =  foo.accessor<float,2>();\nfloat  trace  =  0;\n\nfor(int  i  =  0;  i  <  foo_a.size(0);  i++)  {\n  // use the accessor foo_a to get tensor data.\n  trace  +=  foo_a[i][i];\n} \n```", "```py\n__global__  void  lltm_cuda_forward_kernel(\n  const  torch::PackedTensorAccessor32<scalar_t,3,torch::RestrictPtrTraits>  gates,\n  const  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  old_cell,\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  new_h,\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  new_cell,\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  input_gate,\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  output_gate,\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  candidate_cell) \n```", "```py\ntemplate  <typename  scalar_t>\n__global__  void  lltm_cuda_forward_kernel(\n  const  torch::PackedTensorAccessor32<scalar_t,3,torch::RestrictPtrTraits>  gates,\n  const  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  old_cell,\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  new_h,\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  new_cell,\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  input_gate,\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  output_gate,\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  candidate_cell)  {\n  //batch index\n  const  int  n  =  blockIdx.y;\n  // column index\n  const  int  c  =  blockIdx.x  *  blockDim.x  +  threadIdx.x;\n  if  (c  <  gates.size(2)){\n  input_gate[n][c]  =  sigmoid(gates[n][0][c]);\n  output_gate[n][c]  =  sigmoid(gates[n][1][c]);\n  candidate_cell[n][c]  =  elu(gates[n][2][c]);\n  new_cell[n][c]  =\n  old_cell[n][c]  +  candidate_cell[n][c]  *  input_gate[n][c];\n  new_h[n][c]  =  tanh(new_cell[n][c])  *  output_gate[n][c];\n  }\n} \n```", "```py\nstd::vector<torch::Tensor>  lltm_cuda_forward(\n  torch::Tensor  input,\n  torch::Tensor  weights,\n  torch::Tensor  bias,\n  torch::Tensor  old_h,\n  torch::Tensor  old_cell)  {\n  auto  X  =  torch::cat({old_h,  input},  /*dim=*/1);\n  auto  gate_weights  =  torch::addmm(bias,  X,  weights.transpose(0,  1));\n\n  const  auto  batch_size  =  old_cell.size(0);\n  const  auto  state_size  =  old_cell.size(1);\n\n  auto  gates  =  gate_weights.reshape({batch_size,  3,  state_size});\n  auto  new_h  =  torch::zeros_like(old_cell);\n  auto  new_cell  =  torch::zeros_like(old_cell);\n  auto  input_gate  =  torch::zeros_like(old_cell);\n  auto  output_gate  =  torch::zeros_like(old_cell);\n  auto  candidate_cell  =  torch::zeros_like(old_cell);\n\n  const  int  threads  =  1024;\n  const  dim3  blocks((state_size  +  threads  -  1)  /  threads,  batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(gates.type(),  \"lltm_forward_cuda\",  ([&]  {\n  lltm_cuda_forward_kernel<scalar_t><<<blocks,  threads>>>(\n  gates.packed_accessor32<scalar_t,3,torch::RestrictPtrTraits>(),\n  old_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  new_h.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  new_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  input_gate.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  output_gate.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  candidate_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>());\n  }));\n\n  return  {new_h,  new_cell,  input_gate,  output_gate,  candidate_cell,  X,  gates};\n} \n```", "```py\ntemplate  <typename  scalar_t>\n__global__  void  lltm_cuda_backward_kernel(\n  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  d_old_cell,\n  torch::PackedTensorAccessor32<scalar_t,3,torch::RestrictPtrTraits>  d_gates,\n  const  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  grad_h,\n  const  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  grad_cell,\n  const  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  new_cell,\n  const  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  input_gate,\n  const  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  output_gate,\n  const  torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits>  candidate_cell,\n  const  torch::PackedTensorAccessor32<scalar_t,3,torch::RestrictPtrTraits>  gate_weights)  {\n  //batch index\n  const  int  n  =  blockIdx.y;\n  // column index\n  const  int  c  =  blockIdx.x  *  blockDim.x  +  threadIdx.x;\n  if  (c  <  d_gates.size(2)){\n  const  auto  d_output_gate  =  tanh(new_cell[n][c])  *  grad_h[n][c];\n  const  auto  d_tanh_new_cell  =  output_gate[n][c]  *  grad_h[n][c];\n  const  auto  d_new_cell  =\n  d_tanh(new_cell[n][c])  *  d_tanh_new_cell  +  grad_cell[n][c];\n\n  d_old_cell[n][c]  =  d_new_cell;\n  const  auto  d_candidate_cell  =  input_gate[n][c]  *  d_new_cell;\n  const  auto  d_input_gate  =  candidate_cell[n][c]  *  d_new_cell;\n\n  d_gates[n][0][c]  =\n  d_input_gate  *  d_sigmoid(gate_weights[n][0][c]);\n  d_gates[n][1][c]  =\n  d_output_gate  *  d_sigmoid(gate_weights[n][1][c]);\n  d_gates[n][2][c]  =\n  d_candidate_cell  *  d_elu(gate_weights[n][2][c]);\n  }\n}\n\nstd::vector<torch::Tensor>  lltm_cuda_backward(\n  torch::Tensor  grad_h,\n  torch::Tensor  grad_cell,\n  torch::Tensor  new_cell,\n  torch::Tensor  input_gate,\n  torch::Tensor  output_gate,\n  torch::Tensor  candidate_cell,\n  torch::Tensor  X,\n  torch::Tensor  gates,\n  torch::Tensor  weights)  {\n  auto  d_old_cell  =  torch::zeros_like(new_cell);\n  auto  d_gates  =  torch::zeros_like(gates);\n\n  const  auto  batch_size  =  new_cell.size(0);\n  const  auto  state_size  =  new_cell.size(1);\n\n  const  int  threads  =  1024;\n  const  dim3  blocks((state_size  +  threads  -  1)  /  threads,  batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(X.type(),  \"lltm_backward_cuda\",  ([&]  {\n  lltm_cuda_backward_kernel<scalar_t><<<blocks,  threads>>>(\n  d_old_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  d_gates.packed_accessor32<scalar_t,3,torch::RestrictPtrTraits>(),\n  grad_h.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  grad_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  new_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  input_gate.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  output_gate.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  candidate_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n  gates.packed_accessor32<scalar_t,3,torch::RestrictPtrTraits>());\n  }));\n\n  auto  d_gate_weights  =  d_gates.reshape({batch_size,  3*state_size});\n  auto  d_weights  =  d_gate_weights.t().mm(X);\n  auto  d_bias  =  d_gate_weights.sum(/*dim=*/0,  /*keepdim=*/true);\n\n  auto  d_X  =  d_gate_weights.mm(weights);\n  auto  d_old_h  =  d_X.slice(/*dim=*/1,  0,  state_size);\n  auto  d_input  =  d_X.slice(/*dim=*/1,  state_size);\n\n  return  {d_old_h,  d_input,  d_weights,  d_bias,  d_old_cell,  d_gates};\n} \n```", "```py\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='lltm',\n    ext_modules=[\n        CUDAExtension('lltm_cuda', [\n            'lltm_cuda.cpp',\n            'lltm_cuda_kernel.cu',\n        ])\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    }) \n```", "```py\nfrom torch.utils.cpp_extension import load\n\nlltm = load(name='lltm', sources=['lltm_cuda.cpp', 'lltm_cuda_kernel.cu']) \n```", "```py\nForward: 149.802 us | Backward 393.458 us \n```", "```py\nForward: 129.431 us | Backward 304.641 us \n```"]