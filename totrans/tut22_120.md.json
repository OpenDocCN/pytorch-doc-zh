["```py\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Policy(nn.Module):\n\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=1) \n```", "```py\nimport argparse\nimport gym\nimport torch.distributed.rpc as rpc\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Reinforcement Learning Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument('--world_size', default=2, type=int, metavar='W',\n                    help='number of workers')\nparser.add_argument('--log_interval', type=int, default=10, metavar='N',\n                    help='interval between training status logs')\nparser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n                    help='how much to value future rewards')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed  for reproducibility')\nargs = parser.parse_args()\n\nclass Observer:\n\n    def __init__(self):\n        self.id = rpc.get_worker_info().id\n        self.env = gym.make('CartPole-v1')\n        self.env.seed(args.seed)\n\n    def run_episode(self, agent_rref):\n        state, ep_reward = self.env.reset(), 0\n        for _ in range(10000):\n            # send the state to the agent to get an action\n            action = agent_rref.rpc_sync().select_action(self.id, state)\n\n            # apply the action to the environment, and get the reward\n            state, reward, done, _ = self.env.step(action)\n\n            # report the reward to the agent for training purpose\n            agent_rref.rpc_sync().report_reward(self.id, reward)\n\n            # finishes after the number of self.env._max_episode_steps\n            if done:\n                break \n```", "```py\nimport gym\nimport numpy as np\n\nimport torch\nimport torch.distributed.rpc as rpc\nimport torch.optim as optim\nfrom torch.distributed.rpc import RRef, rpc_async, remote\nfrom torch.distributions import Categorical\n\nclass Agent:\n    def __init__(self, world_size):\n        self.ob_rrefs = []\n        self.agent_rref = RRef(self)\n        self.rewards = {}\n        self.saved_log_probs = {}\n        self.policy = Policy()\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n        self.eps = np.finfo(np.float32).eps.item()\n        self.running_reward = 0\n        self.reward_threshold = gym.make('CartPole-v1').spec.reward_threshold\n        for ob_rank in range(1, world_size):\n            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n            self.ob_rrefs.append(remote(ob_info, Observer))\n            self.rewards[ob_info.id] = []\n            self.saved_log_probs[ob_info.id] = [] \n```", "```py\nclass Agent:\n    ...\n    def select_action(self, ob_id, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        probs = self.policy(state)\n        m = Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs[ob_id].append(m.log_prob(action))\n        return action.item()\n\n    def report_reward(self, ob_id, reward):\n        self.rewards[ob_id].append(reward) \n```", "```py\nclass Agent:\n    ...\n    def run_episode(self):\n        futs = []\n        for ob_rref in self.ob_rrefs:\n            # make async RPC to kick off an episode on all observers\n            futs.append(\n                rpc_async(\n                    ob_rref.owner(),\n                    ob_rref.rpc_sync().run_episode,\n                    args=(self.agent_rref,)\n                )\n            )\n\n        # wait until all obervers have finished this episode\n        for fut in futs:\n            fut.wait() \n```", "```py\nclass Agent:\n    ...\n    def finish_episode(self):\n      # joins probs and rewards from different observers into lists\n      R, probs, rewards = 0, [], []\n      for ob_id in self.rewards:\n          probs.extend(self.saved_log_probs[ob_id])\n          rewards.extend(self.rewards[ob_id])\n\n      # use the minimum observer reward to calculate the running reward\n      min_reward = min([sum(self.rewards[ob_id]) for ob_id in self.rewards])\n      self.running_reward = 0.05 * min_reward + (1 - 0.05) * self.running_reward\n\n      # clear saved probs and rewards\n      for ob_id in self.rewards:\n          self.rewards[ob_id] = []\n          self.saved_log_probs[ob_id] = []\n\n      policy_loss, returns = [], []\n      for r in rewards[::-1]:\n          R = r + args.gamma * R\n          returns.insert(0, R)\n      returns = torch.tensor(returns)\n      returns = (returns - returns.mean()) / (returns.std() + self.eps)\n      for log_prob, R in zip(probs, returns):\n          policy_loss.append(-log_prob * R)\n      self.optimizer.zero_grad()\n      policy_loss = torch.cat(policy_loss).sum()\n      policy_loss.backward()\n      self.optimizer.step()\n      return min_reward \n```", "```py\nimport os\nfrom itertools import count\n\nimport torch.multiprocessing as mp\n\nAGENT_NAME = \"agent\"\nOBSERVER_NAME=\"obs{}\"\n\ndef run_worker(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 0:\n        # rank0 is the agent\n        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n\n        agent = Agent(world_size)\n        print(f\"This will run until reward threshold of {agent.reward_threshold}\"\n                \" is reached. Ctrl+C to exit.\")\n        for i_episode in count(1):\n            agent.run_episode()\n            last_reward = agent.finish_episode()\n\n            if i_episode % args.log_interval == 0:\n                print(f\"Episode {i_episode}\\tLast reward: {last_reward:.2f}\\tAverage reward: \"\n                    f\"{agent.running_reward:.2f}\")\n            if agent.running_reward > agent.reward_threshold:\n                print(f\"Solved! Running reward is now {agent.running_reward}!\")\n                break\n    else:\n        # other ranks are the observer\n        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n        # observers passively waiting for instructions from the agent\n\n    # block until all rpcs finish, and shutdown the RPC instance\n    rpc.shutdown()\n\nmp.spawn(\n    run_worker,\n    args=(args.world_size, ),\n    nprocs=args.world_size,\n    join=True\n) \n```", "```py\nThis will run until reward threshold of 475.0 is reached. Ctrl+C to exit.\nEpisode 10      Last reward: 26.00      Average reward: 10.01\nEpisode 20      Last reward: 16.00      Average reward: 11.27\nEpisode 30      Last reward: 49.00      Average reward: 18.62\nEpisode 40      Last reward: 45.00      Average reward: 26.09\nEpisode 50      Last reward: 44.00      Average reward: 30.03\nEpisode 60      Last reward: 111.00     Average reward: 42.23\nEpisode 70      Last reward: 131.00     Average reward: 70.11\nEpisode 80      Last reward: 87.00      Average reward: 76.51\nEpisode 90      Last reward: 86.00      Average reward: 95.93\nEpisode 100     Last reward: 13.00      Average reward: 123.93\nEpisode 110     Last reward: 33.00      Average reward: 91.39\nEpisode 120     Last reward: 73.00      Average reward: 76.38\nEpisode 130     Last reward: 137.00     Average reward: 88.08\nEpisode 140     Last reward: 89.00      Average reward: 104.96\nEpisode 150     Last reward: 97.00      Average reward: 98.74\nEpisode 160     Last reward: 150.00     Average reward: 100.87\nEpisode 170     Last reward: 126.00     Average reward: 104.38\nEpisode 180     Last reward: 500.00     Average reward: 213.74\nEpisode 190     Last reward: 322.00     Average reward: 300.22\nEpisode 200     Last reward: 165.00     Average reward: 272.71\nEpisode 210     Last reward: 168.00     Average reward: 233.11\nEpisode 220     Last reward: 184.00     Average reward: 195.02\nEpisode 230     Last reward: 284.00     Average reward: 208.32\nEpisode 240     Last reward: 395.00     Average reward: 247.37\nEpisode 250     Last reward: 500.00     Average reward: 335.42\nEpisode 260     Last reward: 500.00     Average reward: 386.30\nEpisode 270     Last reward: 500.00     Average reward: 405.29\nEpisode 280     Last reward: 500.00     Average reward: 443.29\nEpisode 290     Last reward: 500.00     Average reward: 464.65\nSolved! Running reward is now 475.3163778435275! \n```", "```py\nclass EmbeddingTable(nn.Module):\n  r\"\"\"\n Encoding layers of the RNNModel\n \"\"\"\n    def __init__(self, ntoken, ninp, dropout):\n        super(EmbeddingTable, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp).cuda()\n        self.encoder.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, input):\n        return self.drop(self.encoder(input.cuda()).cpu()\n\nclass Decoder(nn.Module):\n    def __init__(self, ntoken, nhid, dropout):\n        super(Decoder, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, output):\n        return self.decoder(self.drop(output)) \n```", "```py\nclass RNNModel(nn.Module):\n    def __init__(self, ps, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(RNNModel, self).__init__()\n\n        # setup embedding table remotely\n        self.emb_table_rref = rpc.remote(ps, EmbeddingTable, args=(ntoken, ninp, dropout))\n        # setup LSTM locally\n        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n        # setup decoder remotely\n        self.decoder_rref = rpc.remote(ps, Decoder, args=(ntoken, nhid, dropout))\n\n    def forward(self, input, hidden):\n        # pass input to the remote embedding table and fetch emb tensor back\n        emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)\n        output, hidden = self.rnn(emb, hidden)\n        # pass output to the rremote decoder and get the decoded output back\n        decoded = _remote_method(Decoder.forward, self.decoder_rref, output)\n        return decoded, hidden \n```", "```py\ndef _parameter_rrefs(module):\n    param_rrefs = []\n    for param in module.parameters():\n        param_rrefs.append(RRef(param))\n    return param_rrefs \n```", "```py\nclass RNNModel(nn.Module):\n    ...\n    def parameter_rrefs(self):\n        remote_params = []\n        # get RRefs of embedding table\n        remote_params.extend(_remote_method(_parameter_rrefs, self.emb_table_rref))\n        # create RRefs for local parameters\n        remote_params.extend(_parameter_rrefs(self.rnn))\n        # get RRefs of decoder\n        remote_params.extend(_remote_method(_parameter_rrefs, self.decoder_rref))\n        return remote_params \n```", "```py\ndef run_trainer():\n    batch = 5\n    ntoken = 10\n    ninp = 2\n\n    nhid = 3\n    nindices = 3\n    nlayers = 4\n    hidden = (\n        torch.randn(nlayers, nindices, nhid),\n        torch.randn(nlayers, nindices, nhid)\n    )\n\n    model = rnn.RNNModel('ps', ntoken, ninp, nhid, nlayers)\n\n    # setup distributed optimizer\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model.parameter_rrefs(),\n        lr=0.05,\n    )\n\n    criterion = torch.nn.CrossEntropyLoss()\n\n    def get_next_batch():\n        for _ in range(5):\n            data = torch.LongTensor(batch, nindices) % ntoken\n            target = torch.LongTensor(batch, ntoken) % nindices\n            yield data, target\n\n    # train for 10 iterations\n    for epoch in range(10):\n        for data, target in get_next_batch():\n            # create distributed autograd context\n            with dist_autograd.context() as context_id:\n                hidden[0].detach_()\n                hidden[1].detach_()\n                output, hidden = model(data, hidden)\n                loss = criterion(output, target)\n                # run distributed backward pass\n                dist_autograd.backward(context_id, [loss])\n                # run distributed optimizer\n                opt.step(context_id)\n                # not necessary to zero grads since they are\n                # accumulated into the distributed autograd context\n                # which is reset every iteration.\n        print(\"Training epoch {}\".format(epoch)) \n```", "```py\ndef run_worker(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 1:\n        rpc.init_rpc(\"trainer\", rank=rank, world_size=world_size)\n        _run_trainer()\n    else:\n        rpc.init_rpc(\"ps\", rank=rank, world_size=world_size)\n        # parameter server do nothing\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\nif __name__==\"__main__\":\n    world_size = 2\n    mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True) \n```"]