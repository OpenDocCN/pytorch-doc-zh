["```py\nat::Tensor  wrapper_Custom_Tensor_add(const  at::Tensor  &  self,  const  at::Tensor  &  other,  const  at::Scalar  &  alpha)  {\n  // Implementation of add kernel in new backend\n  ...\n}\n\nTORCH_LIBRARY_IMPL(aten,  PrivateUse1,  m)  {\n  ...\n  m.impl(\"add.Tensor\",  TORCH_FN(wrapper_Custom_Tensor_add));\n  ...\n}\n\nvoid  custom_cpu_fallback(const  c10::OperatorHandle&  op,  torch::jit::Stack*  stack)  {\n  // Add some hints about new devices that do not support and need to fall back to cpu\n  at::native::cpu_fallback(op,  stack);\n}\n\nTORCH_LIBRARY_IMPL(_,  PrivateUse1,  m)  {\n  m.fallback(torch::CppFunction::makeFromBoxedFunction<&custom_cpu_fallback>());\n} \n```", "```py\nclass  CumtomSeluFunction  :  public  torch::autograd::Function<CumtomSeluFunction>  {\n  // Implementation of selu kernel in new backend\n}\n\nat::Tensor  wrapper_AutogradCumstom__selu(const  at::Tensor  &  self)  {\n  return  CumtomSeluFunction::apply(self);\n}\n\nTORCH_LIBRARY_IMPL(aten,  AutogradPrivateUse1,  m)  {\n  ...\n  m.impl(\"selu\",  TORCH_FN(wrapper_AutogradCustom__selu));\n  ...\n} \n```", "```py\nTORCH_LIBRARY_IMPL(aten,  AutocastPrivateUse1,  m)  {\n  ...\n  KERNEL_PRIVATEUSEONE(<operator>,  <policy>)\n  ...\n}\n\nTORCH_LIBRARY_IMPL(_,  AutocastPrivateUse1,  m)  {\n  m.fallback(torch::CppFunction::makeFallthrough());\n} \n```", "```py\nstruct  CustomGeneratorImpl  :  public  c10::GeneratorImpl  {\n  // Implementation of generator in new backend\n}\n\nat::Generator  make_custom_generator(c10::DeviceIndex  device_index)  {\n  return  at::make_generator<CustomGeneratorImpl>(device_index);\n}\n\nREGISTER_GENERATOR_PRIVATEUSE1(make_cumstom_generator) \n```", "```py\nstruct  CustomGuardImpl  final  :  public  c10::impl::DeviceGuardImplInterface  {\n  // Implementation of guard in new backend\n}\n\nC10_REGISTER_GUARD_IMPL(PrivateUse1,  CustomGuardImpl); \n```", "```py\nstruct  CustomBackendMetadata  :  public  c10::BackendMeta  {\n  // Implementation of backend metadata in new backend\n}\n\nvoid  for_serialization(const  at::Tensor&  t,  std::unordered_map<std::string,  bool>&  m)  {\n  // Implementation of serialization\n}\n\nvoid  for_deserialization(const  at::Tensor&  t,  std::unordered_map<std::string,  bool>&  m)  {\n  // Implementation of deserialization\n}\n\nTensorBackendMetaRegistry(c10::DeviceType::PrivateUse1,  &for_serialization,  &for_deserialization); \n```", "```py\ntorch._register_device_module('npu', torch_npu.npu) \n```", "```py\ntorch.rand((2,2),device='npu:0')\ntorch.rand((2,2),device='privateuse1:0') \n```", "```py\ntorch.rename_privateuse1_backend(\"npu\") \n```", "```py\nc10::register_privateuse1_backend(\"npu\") \n```", "```py\ntorch.rename_privateuse1_backend(\"npu\")\nunsupported_dtype = [torch.quint8]\ntorch.utils.generate_methods_for_privateuse1_backend(for_tensor=True, for_module=True, for_storage=True, unsupported_dtype=unsupported_dtype) \n```", "```py\ntorch.Tensor.npu()\ntorch.Tensor.is_npu\ntorch.Storage.npu()\ntorch.Storage.is_npu\n... \n```"]