["```py\nimport torch\nimport torch.nn as nn\n\nprint(f\"torch version: {torch.__version__}\")\n\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nprint(f\"torch cuda available: {torch.cuda.is_available()}\")\n\nimport torch, torchtext\nfrom torchtext.models import RobertaClassificationHead\nfrom torchtext.functional import to_tensor\nxlmr_large = torchtext.models.XLMR_LARGE_ENCODER\nclassifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\nmodel = xlmr_large.get_model(head=classifier_head)\ntransform = xlmr_large.transform() \n```", "```py\nsmall_input_batch = [\n               \"Hello world\",\n               \"How are you!\"\n]\nbig_input_batch = [\n               \"Hello world\",\n               \"How are you!\",\n  \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\nBuonapartes. But I warn you, if you don't tell me that this means war,\nif you still try to defend the infamies and horrors perpetrated by\nthat Antichrist- I really believe he is Antichrist- I will have\nnothing more to do with you and you are no longer my friend, no longer\nmy 'faithful slave,' as you call yourself! But how do you do? I see\nI have frightened you- sit down and tell me all the news.`\n\nIt was in July, 1805, and the speaker was the well-known Anna\nPavlovna Scherer, maid of honor and favorite of the Empress Marya\nFedorovna. With these words she greeted Prince Vasili Kuragin, a man\nof high rank and importance, who was the first to arrive at her\nreception. Anna Pavlovna had had a cough for some days. She was, as\nshe said, suffering from la grippe; grippe being then a new word in\nSt. Petersburg, used only by the elite.\"\"\"\n] \n```", "```py\ninput_batch=big_input_batch\n\nmodel_input = to_tensor(transform(input_batch), padding_value=1)\noutput = model(model_input)\noutput.shape \n```", "```py\nITERATIONS=10 \n```", "```py\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof) \n```", "```py\nmodel.encoder.transformer.layers.enable_nested_tensor \n```", "```py\nmodel.encoder.transformer.layers.enable_nested_tensor=False \n```", "```py\nmodel.to(DEVICE)\nmodel_input = model_input.to(DEVICE)\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof) \n```", "```py\nmodel.encoder.transformer.layers.enable_nested_tensor = True \n```", "```py\nmodel.to(DEVICE)\nmodel_input = model_input.to(DEVICE)\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof) \n```"]