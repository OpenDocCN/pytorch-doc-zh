- en: Image Segmentation DeepLabV3 on Android
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/tutorials/beginner/deeplabv3_on_android.html](https://pytorch.org/tutorials/beginner/deeplabv3_on_android.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Author**: [Jeff Tang](https://github.com/jeffxtang)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reviewed by**: [Jeremiah Chung](https://github.com/jeremiahschung)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semantic image segmentation is a computer vision task that uses semantic labels
    to mark specific regions of an input image. The PyTorch semantic image segmentation
    [DeepLabV3 model](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101)
    can be used to label image regions with [20 semantic classes](http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2007/segexamples/index.html)
    including, for example, bicycle, bus, car, dog, and person. Image segmentation
    models can be very useful in applications such as autonomous driving and scene
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will provide a step-by-step guide on how to prepare and
    run the PyTorch DeepLabV3 model on Android, taking you from the beginning of having
    a model you may want to use on Android to the end of having a complete Android
    app using the model. We will also cover practical and general tips on how to check
    if your next favorable pretrained PyTorch models can run on Android, and how to
    avoid pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Before going through this tutorial, you should check out [PyTorch Mobile for
    Android](https://pytorch.org/mobile/android/) and give the PyTorch Android [Hello
    World](https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp)
    example app a quick try. This tutorial will go beyond the image classification
    model, usually the first kind of model deployed on mobile. The complete code for
    this tutorial is available [here](https://github.com/pytorch/android-demo-app/tree/master/ImageSegmentation).
  prefs: []
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this tutorial, you will learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the DeepLabV3 model for Android deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the output of the model for the example input image in Python and compare
    it to the output from the Android app.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a new Android app or reuse an Android example app to load the converted
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the input into the format that the model expects and process the model
    output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete the UI, refactor, build and run the app to see image segmentation in
    action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch 1.6 or 1.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: torchvision 0.7 or 0.8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Android Studio 3.5.1 or above with NDK installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. Convert the DeepLabV3 model for Android deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step to deploying a model on Android is to convert the model into
    the [TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Not all PyTorch models can be converted to TorchScript at this time because
    a model definition may use language features that are not in TorchScript, which
    is a subset of Python. See the [Script and Optimize Recipe](../recipes/script_optimized.html)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simply run the script below to generate the scripted model deeplabv3_scripted.pt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The size of the generated deeplabv3_scripted.pt model file should be around
    168MB. Ideally, a model should also be quantized for significant size reduction
    and faster inference before being deployed on an Android app. To have a general
    understanding of quantization, see the [Quantization Recipe](../recipes/quantization.html)
    and the resource links there. We will cover in detail how to correctly apply a
    quantization workflow called Post Training [Static Quantization](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)
    to the DeepLabV3 model in a future tutorial or recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Get example input and output of the model in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have a scripted PyTorch model, let’s test with some example inputs
    to make sure the model works correctly on Android. First, let’s write a Python
    script that uses the model to make inferences and examine inputs and outputs.
    For this example of the DeepLabV3 model, we can reuse the code in Step 1 and in
    the [DeepLabV3 model hub site](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101).
    Add the following code snippet to the code above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Download deeplab.jpg from [here](https://github.com/jeffxtang/android-demo-app/blob/new_demo_apps/ImageSegmentation/app/src/main/assets/deeplab.jpg),
    then run the script above and you will see the shapes of the input and output
    of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: So if you provide the same image input deeplab.jpg of size 400x400 to the model
    on Android, the output of the model should have the size [21, 400, 400]. You should
    also print out at least the beginning parts of the actual data of the input and
    output, to be used in Step 4 below to compare with the actual input and output
    of the model when running in the Android app.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Build a new Android app or reuse an example app and load the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, follow Step 3 of the [Model Preparation for Android recipe](../recipes/model_preparation_android.html#add-the-model-and-pytorch-library-on-android)
    to use our model in an Android Studio project with PyTorch Mobile enabled. Because
    both DeepLabV3 used in this tutorial and MobileNet v2 used in the PyTorch Hello
    World Android example are computer vision models, you can also get the [Hello
    World example repo](https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp)
    to make it easier to modify the code that loads the model and processes the input
    and output. The main goal in this step and Step 4 is to make sure the model deeplabv3_scripted.pt
    generated in Step 1 can indeed work correctly on Android.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s add deeplabv3_scripted.pt and deeplab.jpg used in Step 2 to the Android
    Studio project and modify the onCreate method in the MainActivity to resemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then set a breakpoint at the line finish() and build and run the app. If the
    app doesn’t stop at the breakpoint, it means that the scripted model in Step 1
    has been successfully loaded on Android.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Process the model input and output for model inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After the model loads in the previous step, let’s verify that it works with
    expected inputs and can generate expected outputs. As the model input for the
    DeepLabV3 model is an image the same as that of the MobileNet v2 in the Hello
    World example, we will reuse some of the code in the [MainActivity.java](https://github.com/pytorch/android-demo-app/blob/master/HelloWorldApp/app/src/main/java/org/pytorch/helloworld/MainActivity.java)
    file from Hello World for input processing. Replace the code snippet between [line
    50](https://github.com/pytorch/android-demo-app/blob/master/HelloWorldApp/app/src/main/java/org/pytorch/helloworld/MainActivity.java#L50)
    and 73 in MainActivity.java with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The model output is a dictionary for the DeepLabV3 model so we use toDictStringKey
    to correctly extract the result. For other models, the model output may also be
    a single tensor or a tuple of tensors, among other things.
  prefs: []
  type: TYPE_NORMAL
- en: With the code changes shown above, you can set breakpoints after final float[]
    inputs and final float[] outputs, which populate the input tensor and output tensor
    data to float arrays for easy debugging. Run the app and when it stops at the
    breakpoints, compare the numbers in inputs and outputs with the model input and
    output data you see in Step 2 to see if they match. For the same inputs to the
    models running on Android and Python, you should get the same outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: You may see different model outputs with the same image input when running on
    an Android emulator due to some Android emulator’s floating point implementation
    issue. So it is best to test the app on a real Android device.
  prefs: []
  type: TYPE_NORMAL
- en: All we have done so far is to confirm that the model of our interest can be
    scripted and run correctly in our Android app as in Python. The steps we walked
    through so far for using a model in an iOS app consumes the bulk, if not most,
    of our app development time, similar to how data preprocessing is the heaviest
    lift for a typical machine learning project.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Complete the UI, refactor, build and run the app
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we are ready to complete the app and the UI to actually see the processed
    result as a new image. The output processing code should be like this, added to
    the end of the code snippet in Step 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The constants used in the code above are defined in the beginning of the class
    MainActivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The implementation here is based on the understanding of the DeepLabV3 model
    which outputs a tensor of size [21, width, height] for an input image of width*height.
    Each element in the width*height output array is a value between 0 and 20 (for
    a total of 21 semantic labels described in Introduction) and the value is used
    to set a specific color. Color coding of the segmentation here is based on the
    class with the highest probability, and you can extend the color coding for all
    classes in your own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the output processing, you will also need to call the code below to render
    the RGB intValues array to a bitmap instance outputBitmap before displaying it
    on an ImageView:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The UI for this app is also similar to that for Hello World, except that you
    do not need the TextView to show the image classification result. You can also
    add two buttons Segment and Restart as shown in the code repository to run the
    model inference and to show back the original image after the segmentation result
    is shown.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now when you run the app on an Android emulator or preferably an actual device,
    you will see screens like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![../_images/deeplabv3_android.png](../Images/1b0ecd17a6617abde8eb2e7e3409bbd0.png)](../_images/deeplabv3_android.png)
    [![../_images/deeplabv3_android2.png](../Images/01e9b7b7725f15ac40b77b270306d4f8.png)](../_images/deeplabv3_android2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Recap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we described what it takes to convert a pretrained PyTorch
    DeepLabV3 model for Android and how to make sure the model can run successfully
    on Android. Our focus was to help you understand the process of confirming that
    a model can indeed run on Android. The complete code repository is available [here](https://github.com/pytorch/android-demo-app/tree/master/ImageSegmentation).
  prefs: []
  type: TYPE_NORMAL
- en: More advanced topics such as quantization and using models via transfer learning
    or of your own on Android will be covered soon in future demo apps and tutorials.
  prefs: []
  type: TYPE_NORMAL
- en: Learn More
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch Mobile site](https://pytorch.org/mobile)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DeepLabV3 model](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DeepLabV3 paper](https://arxiv.org/pdf/1706.05587.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
