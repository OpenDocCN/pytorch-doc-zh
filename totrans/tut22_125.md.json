["```py\nimport sys\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, [TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer \"torch.nn.TransformerEncoderLayer\")\n\nif sys.platform == 'win32':\n    print('Windows platform is not supported for pipeline parallelism')\n    sys.exit(0)\nif [torch.cuda.device_count](https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count \"torch.cuda.device_count\")() < 2:\n    print('Need at least two GPU devices for this tutorial')\n    sys.exit(0)\n\nclass Encoder([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, ntoken, ninp, dropout=0.5):\n        super([Encoder](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.pos_encoder = [PositionalEncoding](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(ninp, dropout)\n        self.encoder = [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding \"torch.nn.Embedding\")(ntoken, ninp)\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        # Need (S, N) format for encoder.\n        src = src.t()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        return self.pos_encoder(src)\n\nclass Decoder([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self, ntoken, ninp):\n        super([Decoder](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.decoder = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(ninp, ntoken)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, inp):\n        # Need batch dimension first for output of pipeline.\n        return self.decoder(inp).permute(1, 0, 2) \n```", "```py\nclass PositionalEncoding([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super([PositionalEncoding](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        self.dropout = [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout \"torch.nn.Dropout\")(p=dropout)\n\n        pe = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros \"torch.zeros\")(max_len, d_model)\n        position = [torch.arange](https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange \"torch.arange\")(0, max_len, dtype=[torch.float](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")).unsqueeze(1)\n        div_term = [torch.exp](https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp \"torch.exp\")([torch.arange](https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange \"torch.arange\")(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = [torch.sin](https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin \"torch.sin\")(position * div_term)\n        pe[:, 1::2] = [torch.cos](https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos \"torch.cos\")(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x) \n```", "```py\nimport torch\nfrom torchtext.datasets import [WikiText2](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2 \"torchtext.datasets.WikiText2\")\nfrom torchtext.data.utils import [get_tokenizer](https://pytorch.org/text/stable/data_utils.html#torchtext.data.utils.get_tokenizer \"torchtext.data.utils.get_tokenizer\")\nfrom torchtext.vocab import [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")\n\ntrain_iter = [WikiText2](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2 \"torchtext.datasets.WikiText2\")(split='train')\ntokenizer = [get_tokenizer](https://pytorch.org/text/stable/data_utils.html#torchtext.data.utils.get_tokenizer \"torchtext.data.utils.get_tokenizer\")('basic_english')\n[vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\") = [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator \"torchtext.vocab.build_vocab_from_iterator\")(map(tokenizer, train_iter), specials=[\"<unk>\"])\n[vocab.set_default_index](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.set_default_index \"torchtext.vocab.Vocab.set_default_index\")([vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\")[\"<unk>\"])\n\ndef data_process(raw_text_iter):\n  data = [[torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor \"torch.tensor\")([vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\")(tokenizer(item)), dtype=[torch.long](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype \"torch.dtype\")) for item in raw_text_iter]\n  return [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat \"torch.cat\")(tuple(filter(lambda t: t.numel() > 0, data)))\n\ntrain_iter, val_iter, test_iter = [WikiText2](https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2 \"torchtext.datasets.WikiText2\")()\n[train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = data_process(train_iter)\n[val_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = data_process(val_iter)\n[test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = data_process(test_iter)\n\ndevice = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")(\"cuda\")\n\ndef batchify(data, bsz):\n    # Divide the dataset into ``bsz`` parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the ``bsz` batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\n[train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = batchify([train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), batch_size)\n[val_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = batchify([val_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), eval_batch_size)\n[test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\") = batchify([test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), eval_batch_size) \n```", "```py\nbptt = 25\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    # Need batch dimension first for pipeline parallelism.\n    return data.t(), target \n```", "```py\nntokens = len([vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\")) # the size of vocabulary\nemsize = 4096 # embedding dimension\nnhid = 4096 # the dimension of the feedforward network model in ``nn.TransformerEncoder``\nnlayers = 12 # the number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\nnhead = 16 # the number of heads in the Multihead Attention models\ndropout = 0.2 # the dropout value\n\nfrom torch.distributed import rpc\ntmpfile = tempfile.NamedTemporaryFile()\n[rpc.init_rpc](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.init_rpc \"torch.distributed.rpc.init_rpc\")(\n    name=\"worker\",\n    rank=0,\n    world_size=1,\n    rpc_backend_options=[rpc.TensorPipeRpcBackendOptions](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions \"torch.distributed.rpc.TensorPipeRpcBackendOptions\")(\n        init_method=\"file://{}\".format(tmpfile.name),\n        # Specifying _transports and _channels is a workaround and we no longer\n        # will have to specify _transports and _channels for PyTorch\n        # versions >= 1.8.1\n        _transports=[\"ibv\", \"uv\"],\n        _channels=[\"cuda_ipc\", \"cuda_basic\"],\n    )\n)\n\nnum_gpus = 2\npartition_len = ((nlayers - 1) // num_gpus) + 1\n\n# Add encoder in the beginning.\ntmp_list = [[Encoder](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(ntokens, emsize, dropout).cuda(0)]\nmodule_list = []\n\n# Add all the necessary transformer blocks.\nfor i in range(nlayers):\n    [transformer_block](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer \"torch.nn.TransformerEncoderLayer\") = [TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer \"torch.nn.TransformerEncoderLayer\")(emsize, nhead, nhid, dropout)\n    if i != 0 and i % (partition_len) == 0:\n        module_list.append([nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(*tmp_list))\n        tmp_list = []\n    device = i // (partition_len)\n    tmp_list.append([transformer_block.to](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to \"torch.nn.Module.to\")(device))\n\n# Add decoder in the end.\ntmp_list.append([Decoder](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")(ntokens, emsize).cuda(num_gpus - 1))\nmodule_list.append([nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(*tmp_list))\n\nfrom torch.distributed.pipeline.sync import [Pipe](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\")\n\n# Build the pipeline.\nchunks = 8\n[model](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\") = [Pipe](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\")([torch.nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential \"torch.nn.Sequential\")(*module_list), chunks = chunks)\n\ndef get_total_params(module: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    total_params = 0\n    for param in module.parameters():\n        total_params += param.numel()\n    return total_params\n\nprint ('Total parameters in model: {:,}'.format(get_total_params([model](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\")))) \n```", "```py\nTotal parameters in model: 1,444,261,998 \n```", "```py\n[criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\") = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")()\nlr = 5.0 # learning rate\n[optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\") = [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")(), lr=lr)\n[scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\") = [torch.optim.lr_scheduler.StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\")([optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\"), 1.0, gamma=0.95)\n\nimport time\ndef train():\n    [model.train](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train \"torch.nn.Module.train\")() # Turn on the train mode\n    total_loss = 0.\n    start_time = time.time()\n    ntokens = len([vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\"))\n\n    # Train only for 50 batches to keep script execution time low.\n    nbatches = min(50 * bptt, [train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\").size(0) - 1)\n\n    for batch, i in enumerate(range(0, nbatches, bptt)):\n        data, targets = get_batch([train_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"), i)\n        [optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.zero_grad \"torch.optim.SGD.zero_grad\")()\n        # Since the Pipe is only within a single host and process the ``RRef``\n        # returned by forward method is local to this node and can simply\n        # retrieved via ``RRef.local_value()``.\n        output = [model](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\")(data).local_value()\n        # Need to move targets to the device where the output of the\n        # pipeline resides.\n        loss = [criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")(output.view(-1, ntokens), targets.cuda(1))\n        loss.backward()\n        [torch.nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_ \"torch.nn.utils.clip_grad_norm_\")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters \"torch.nn.Module.parameters\")(), 0.5)\n        [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD \"torch.optim.SGD\").step()\n\n        total_loss += loss.item()\n        log_interval = 10\n        if batch % log_interval == 0 and batch > 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n                  'lr {:02.2f} | ms/batch {:5.2f} | '\n                  'loss {:5.2f} | ppl {:8.2f}'.format(\n                    epoch, batch, nbatches // bptt, [scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\").get_lr()[0],\n                    elapsed * 1000 / log_interval,\n                    cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(eval_model, data_source):\n    eval_model.eval() # Turn on the evaluation mode\n    total_loss = 0.\n    ntokens = len([vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab \"torchtext.vocab.Vocab\"))\n    # Evaluate only for 50 batches to keep script execution time low.\n    nbatches = min(50 * bptt, data_source.size(0) - 1)\n    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad \"torch.no_grad\")():\n        for i in range(0, nbatches, bptt):\n            data, targets = get_batch(data_source, i)\n            output = eval_model(data).local_value()\n            output_flat = output.view(-1, ntokens)\n            # Need to move targets to the device where the output of the\n            # pipeline resides.\n            total_loss += len(data) * [criterion](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss \"torch.nn.CrossEntropyLoss\")(output_flat, targets.cuda(1)).item()\n    return total_loss / (len(data_source) - 1) \n```", "```py\nbest_val_loss = float(\"inf\")\nepochs = 3 # The number of epochs\n[best_model](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\") = None\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train()\n    val_loss = evaluate([model](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\"), [val_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\n    print('-' * 89)\n    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                     val_loss, math.exp(val_loss)))\n    print('-' * 89)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        [best_model](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\") = [model](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\")\n\n    [scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR \"torch.optim.lr_scheduler.StepLR\").step() \n```", "```py\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:402: UserWarning:\n\nTo get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n\n| epoch   1 |    10/   50 batches | lr 5.00 | ms/batch 2955.60 | loss 51.97 | ppl 37278238304344674926592.00\n| epoch   1 |    20/   50 batches | lr 5.00 | ms/batch 2626.09 | loss 39.16 | ppl 101468412802272112.00\n| epoch   1 |    30/   50 batches | lr 5.00 | ms/batch 2627.16 | loss 45.74 | ppl 73373605537851539456.00\n| epoch   1 |    40/   50 batches | lr 5.00 | ms/batch 2632.18 | loss 39.05 | ppl 90831844662671120.00\n-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 148.93s | valid loss  1.59 | valid ppl     4.92\n-----------------------------------------------------------------------------------------\n| epoch   2 |    10/   50 batches | lr 4.51 | ms/batch 2894.00 | loss 38.92 | ppl 79792098193225456.00\n| epoch   2 |    20/   50 batches | lr 4.51 | ms/batch 2632.71 | loss 33.86 | ppl 508484255367480.44\n| epoch   2 |    30/   50 batches | lr 4.51 | ms/batch 2630.00 | loss 29.47 | ppl 6267626426289.98\n| epoch   2 |    40/   50 batches | lr 4.51 | ms/batch 2630.24 | loss 20.07 | ppl 521065165.54\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time: 148.40s | valid loss  0.54 | valid ppl     1.71\n-----------------------------------------------------------------------------------------\n| epoch   3 |    10/   50 batches | lr 4.29 | ms/batch 2891.16 | loss 13.75 | ppl 935925.21\n| epoch   3 |    20/   50 batches | lr 4.29 | ms/batch 2629.50 | loss 10.74 | ppl 46322.74\n| epoch   3 |    30/   50 batches | lr 4.29 | ms/batch 2629.95 | loss 10.97 | ppl 58152.80\n| epoch   3 |    40/   50 batches | lr 4.29 | ms/batch 2629.52 | loss 11.29 | ppl 80130.60\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time: 148.36s | valid loss  0.24 | valid ppl     1.27\n----------------------------------------------------------------------------------------- \n```", "```py\ntest_loss = evaluate([best_model](https://pytorch.org/docs/stable/pipeline.html#torch.distributed.pipeline.sync.Pipe \"torch.distributed.pipeline.sync.Pipe\"), [test_data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\"))\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89) \n```", "```py\n=========================================================================================\n| End of training | test loss  0.21 | test ppl     1.23\n========================================================================================= \n```"]