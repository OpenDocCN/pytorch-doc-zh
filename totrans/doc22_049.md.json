["```py\nclass torch.distributed.fsdp.FullyShardedDataParallel(module, process_group=None, sharding_strategy=None, cpu_offload=None, auto_wrap_policy=None, backward_prefetch=BackwardPrefetch.BACKWARD_PRE, mixed_precision=None, ignored_modules=None, param_init_fn=None, device_id=None, sync_module_states=False, forward_prefetch=False, limit_all_gathers=True, use_orig_params=False, ignored_states=None, device_mesh=None)\u00b6\n```", "```py\n>>> import torch\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> torch.cuda.set_device(device_id)\n>>> sharded_module = FSDP(my_module)\n>>> optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)\n>>> x = sharded_module(x, y=3, z=torch.Tensor([1]))\n>>> loss = x.sum()\n>>> loss.backward()\n>>> optim.step() \n```", "```py\n    >>> def custom_auto_wrap_policy(\n    >>>     module: nn.Module,\n    >>>     recurse: bool,\n    >>>     nonwrapped_numel: int,\n    >>>     # Additional custom arguments\n    >>>     min_num_params: int = int(1e8),\n    >>> ) -> bool:\n    >>>     return nonwrapped_numel >= min_num_params\n    >>> # Configure a custom `min_num_params`\n    >>> my_auto_wrap_policy = functools.partial(custom_auto_wrap_policy, min_num_params=int(1e5)) \n    ```", "```py\n    >>> module = MyModule(device=\"meta\")\n    >>> def my_init_fn(module: nn.Module):\n    >>>     # E.g. initialize depending on the module type\n    >>>     ...\n    >>> fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)\n    >>> print(next(fsdp_model.parameters()).device) # current CUDA device\n    >>> # With torchdistX\n    >>> module = deferred_init.deferred_init(MyModule, device=\"cuda\")\n    >>> # Will initialize via deferred_init.materialize_module().\n    >>> fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy) \n    ```", "```py\napply(fn)\u00b6\n```", "```py\ncheck_is_root()\u00b6\n```", "```py\nclip_grad_norm_(max_norm, norm_type=2.0)\u00b6\n```", "```py\nstatic flatten_sharded_optim_state_dict(sharded_optim_state_dict, model, optim)\u00b6\n```", "```py\nforward(*args, **kwargs)\u00b6\n```", "```py\nstatic fsdp_modules(module, root_only=False)\u00b6\n```", "```py\nstatic full_optim_state_dict(model, optim, optim_input=None, rank0_only=True, group=None)\u00b6\n```", "```py\nstatic get_state_dict_type(module)\u00b6\n```", "```py\nproperty module: Module\u00b6\n```", "```py\nnamed_buffers(*args, **kwargs)\u00b6\n```", "```py\nnamed_parameters(*args, **kwargs)\u00b6\n```", "```py\nno_sync()\u00b6\n```", "```py\nstatic optim_state_dict(model, optim, optim_state_dict=None, group=None)\u00b6\n```", "```py\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     optim_state_dict, model, optim\n>>> )\n>>> optim.load_state_dict(optim_state_dict) \n```", "```py\nstatic optim_state_dict_to_load(model, optim, optim_state_dict, is_named_optimizer=False, load_directly=False, group=None)\u00b6\n```", "```py\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> original_osd = optim.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(\n>>>     model,\n>>>     optim,\n>>>     optim_state_dict=original_osd\n>>> )\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     model, optim, optim_state_dict\n>>> )\n>>> optim.load_state_dict(optim_state_dict) \n```", "```py\nregister_comm_hook(state, hook)\u00b6\n```", "```py\nstatic rekey_optim_state_dict(optim_state_dict, optim_state_key_type, model, optim_input=None, optim=None)\u00b6\n```", "```py\n>>> wrapped_model, wrapped_optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)\n>>> nonwrapped_model, nonwrapped_optim = ...\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)\n>>> nonwrapped_optim.load_state_dict(rekeyed_osd) \n```", "```py\n>>> nonwrapped_model, nonwrapped_optim = ...\n>>> osd = nonwrapped_optim.state_dict()\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)\n>>> wrapped_model, wrapped_optim = ...\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)\n>>> wrapped_optim.load_state_dict(sharded_osd) \n```", "```py\nstatic scatter_full_optim_state_dict(full_optim_state_dict, model, optim_input=None, optim=None, group=None)\u00b6\n```", "```py\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)  # only non-empty on rank 0\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim, new_group = ...\n>>> sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)\n>>> new_optim.load_state_dict(sharded_osd) \n```", "```py\nstatic set_state_dict_type(module, state_dict_type, state_dict_config=None, optim_state_dict_config=None)\u00b6\n```", "```py\n>>> model = DDP(FSDP(...))\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>>     state_dict_config = ShardedStateDictConfig(offload_to_cpu=True),\n>>>     optim_state_dict_config = OptimStateDictConfig(offload_to_cpu=True),\n>>> )\n>>> param_state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim) \n```", "```py\nstatic shard_full_optim_state_dict(full_optim_state_dict, model, optim_input=None, optim=None)\u00b6\n```", "```py\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)\n>>> torch.save(full_osd, PATH)\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim = ...\n>>> full_osd = torch.load(PATH)\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)\n>>> new_optim.load_state_dict(sharded_osd) \n```", "```py\nstatic sharded_optim_state_dict(model, optim, group=None)\u00b6\n```", "```py\nstatic state_dict_type(module, state_dict_type, state_dict_config=None, optim_state_dict_config=None)\u00b6\n```", "```py\n>>> model = DDP(FSDP(...))\n>>> with FSDP.state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>> ):\n>>>     checkpoint = model.state_dict() \n```", "```py\nstatic summon_full_params(module, recurse=True, writeback=True, rank0_only=False, offload_to_cpu=False, with_grads=False)\u00b6\n```", "```py\nclass torch.distributed.fsdp.BackwardPrefetch(value)\u00b6\n```", "```py\nclass torch.distributed.fsdp.ShardingStrategy(value)\u00b6\n```", "```py\nclass torch.distributed.fsdp.MixedPrecision(param_dtype=None, reduce_dtype=None, buffer_dtype=None, keep_low_precision_grads=False, cast_forward_inputs=False, cast_root_forward_inputs=True, _module_classes_to_ignore=(<class 'torch.nn.modules.batchnorm._BatchNorm'>, ))\u00b6\n```", "```py\n>>> model = nn.Sequential(nn.Linear(3, 3), nn.Linear(3, 3))\n>>> model[1] = FSDP(\n>>>     model[1],\n>>>     mixed_precision=MixedPrecision(param_dtype=torch.float16, cast_forward_inputs=True),\n>>> )\n>>> model = FSDP(\n>>>     model,\n>>>     mixed_precision=MixedPrecision(param_dtype=torch.bfloat16, cast_forward_inputs=True),\n>>> ) \n```", "```py\nclass torch.distributed.fsdp.CPUOffload(offload_params=False)\u00b6\n```", "```py\nclass torch.distributed.fsdp.StateDictConfig(offload_to_cpu=False)\u00b6\n```", "```py\nclass torch.distributed.fsdp.FullStateDictConfig(offload_to_cpu=False, rank0_only=False)\u00b6\n```", "```py\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> fsdp = FSDP(model, auto_wrap_policy=...)\n>>> cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n>>> with FSDP.state_dict_type(fsdp, StateDictType.FULL_STATE_DICT, cfg):\n>>>     state = fsdp.state_dict()\n>>>     # `state` will be empty on non rank 0 and contain CPU tensors on rank 0.\n>>> # To reload checkpoint for inference, finetuning, transfer learning, etc:\n>>> model = model_fn() # Initialize model in preparation for wrapping with FSDP\n>>> if dist.get_rank() == 0:\n>>>     # Load checkpoint only on rank 0 to avoid memory redundancy\n>>>     state_dict = torch.load(\"my_checkpoint.pt\")\n>>>     model.load_state_dict(state_dict)\n>>> # All ranks initialize FSDP module as usual. `sync_module_states` argument\n>>> # communicates loaded checkpoint states from rank 0 to rest of the world.\n>>> fsdp = FSDP(model, device_id=torch.cuda.current_device(), auto_wrap_policy=..., sync_module_states=True)\n>>> # After this point, all ranks have FSDP model with loaded checkpoint. \n```", "```py\nclass torch.distributed.fsdp.ShardedStateDictConfig(offload_to_cpu=False, _use_dtensor=False)\u00b6\n```", "```py\nclass torch.distributed.fsdp.LocalStateDictConfig(offload_to_cpu: bool = False)\u00b6\n```", "```py\nclass torch.distributed.fsdp.OptimStateDictConfig(offload_to_cpu=True)\u00b6\n```", "```py\nclass torch.distributed.fsdp.FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False)\u00b6\n```", "```py\nclass torch.distributed.fsdp.ShardedOptimStateDictConfig(offload_to_cpu=True, _use_dtensor=False)\u00b6\n```", "```py\nclass torch.distributed.fsdp.LocalOptimStateDictConfig(offload_to_cpu: bool = False)\u00b6\n```", "```py\nclass torch.distributed.fsdp.StateDictSettings(state_dict_type: torch.distributed.fsdp.api.StateDictType, state_dict_config: torch.distributed.fsdp.api.StateDictConfig, optim_state_dict_config: torch.distributed.fsdp.api.OptimStateDictConfig)\u00b6\n```"]