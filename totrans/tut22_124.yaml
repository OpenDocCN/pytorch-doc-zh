- en: Combining Distributed DataParallel with Distributed RPC Framework
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将分布式DataParallel与分布式RPC框架结合起来
- en: 原文：[https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html](https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html](https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html)
- en: '**Authors**: [Pritam Damania](https://github.com/pritamdamania87) and [Yi Wang](https://github.com/wayi1)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：[Pritam Damania](https://github.com/pritamdamania87) 和 [Yi Wang](https://github.com/wayi1)'
- en: Note
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '![edit](../Images/a8aa37bcc5edbf2ba5fcf18dba1e55f9.png) View and edit this
    tutorial in [github](https://github.com/pytorch/tutorials/blob/main/advanced_source/rpc_ddp_tutorial.rst).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '![edit](../Images/a8aa37bcc5edbf2ba5fcf18dba1e55f9.png) 在[github](https://github.com/pytorch/tutorials/blob/main/advanced_source/rpc_ddp_tutorial.rst)中查看和编辑本教程。'
- en: This tutorial uses a simple example to demonstrate how you can combine [DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)
    (DDP) with the [Distributed RPC framework](https://pytorch.org/docs/master/rpc.html)
    to combine distributed data parallelism with distributed model parallelism to
    train a simple model. Source code of the example can be found [here](https://github.com/pytorch/examples/tree/master/distributed/rpc/ddp_rpc).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程使用一个简单的示例来演示如何将[DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)（DDP）与[Distributed
    RPC framework](https://pytorch.org/docs/master/rpc.html)结合起来，以将分布式数据并行与分布式模型并行结合起来训练一个简单的模型。示例的源代码可以在[这里](https://github.com/pytorch/examples/tree/master/distributed/rpc/ddp_rpc)找到。
- en: 'Previous tutorials, [Getting Started With Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
    and [Getting Started with Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html),
    described how to perform distributed data parallel and distributed model parallel
    training respectively. Although, there are several training paradigms where you
    might want to combine these two techniques. For example:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的教程，[Getting Started With Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)和[Getting
    Started with Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)，分别描述了如何执行分布式数据并行和分布式模型并行训练。尽管如此，有几种训练范式可能需要结合这两种技术。例如：
- en: If we have a model with a sparse part (large embedding table) and a dense part
    (FC layers), we might want to put the embedding table on a parameter server and
    replicate the FC layer across multiple trainers using [DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel).
    The [Distributed RPC framework](https://pytorch.org/docs/master/rpc.html) can
    be used to perform embedding lookups on the parameter server.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们的模型有一个稀疏部分（大型嵌入表）和一个稠密部分（FC层），我们可能希望将嵌入表放在参数服务器上，并使用[DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)将FC层复制到多个训练器上。[Distributed
    RPC framework](https://pytorch.org/docs/master/rpc.html)可用于在参数服务器上执行嵌入查找。
- en: Enable hybrid parallelism as described in the [PipeDream](https://arxiv.org/abs/1806.03377)
    paper. We can use the [Distributed RPC framework](https://pytorch.org/docs/master/rpc.html)
    to pipeline stages of the model across multiple workers and replicate each stage
    (if needed) using [DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel).
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用混合并行，如[PipeDream](https://arxiv.org/abs/1806.03377)论文中所述。我们可以使用[Distributed
    RPC framework](https://pytorch.org/docs/master/rpc.html)将模型的阶段在多个工作节点上进行流水线处理，并使用[DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)复制每个阶段（如果需要）。
- en: 'In this tutorial we will cover case 1 mentioned above. We have a total of 4
    workers in our setup as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将涵盖上述第1种情况。在我们的设置中，总共有4个工作节点：
- en: 1 Master, which is responsible for creating an embedding table (nn.EmbeddingBag)
    on the parameter server. The master also drives the training loop on the two trainers.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1个主节点，负责在参数服务器上创建一个嵌入表（nn.EmbeddingBag）。主节点还驱动两个训练器的训练循环。
- en: 1 Parameter Server, which basically holds the embedding table in memory and
    responds to RPCs from the Master and Trainers.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1个参数服务器，基本上在内存中保存嵌入表，并响应来自主节点和训练器的RPC。
- en: 2 Trainers, which store an FC layer (nn.Linear) which is replicated amongst
    themselves using [DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel).
    The trainers are also responsible for executing the forward pass, backward pass
    and optimizer step.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2个训练器，它们存储一个在它们之间复制的FC层（nn.Linear），使用[DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)。这些训练器还负责执行前向传播、反向传播和优化器步骤。
- en: 'The entire training process is executed as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 整个训练过程如下执行：
- en: The master creates a [RemoteModule](https://pytorch.org/docs/master/rpc.html#remotemodule)
    that holds an embedding table on the Parameter Server.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主节点创建一个[RemoteModule](https://pytorch.org/docs/master/rpc.html#remotemodule)，在参数服务器上保存一个嵌入表。
- en: The master, then kicks off the training loop on the trainers and passes the
    remote module to the trainers.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后主节点启动训练循环，并将远程模块传递给训练器。
- en: The trainers create a `HybridModel` which first performs an embedding lookup
    using the remote module provided by the master and then executes the FC layer
    which is wrapped inside DDP.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练器创建一个`HybridModel`，首先使用主节点提供的远程模块进行嵌入查找，然后执行包含在DDP中的FC层。
- en: The trainer executes the forward pass of the model and uses the loss to execute
    the backward pass using [Distributed Autograd](https://pytorch.org/docs/master/rpc.html#distributed-autograd-framework).
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练器执行模型的前向传播，并使用损失执行反向传播，使用[Distributed Autograd](https://pytorch.org/docs/master/rpc.html#distributed-autograd-framework)。
- en: As part of the backward pass, the gradients for the FC layer are computed first
    and synced to all trainers via allreduce in DDP.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在反向传播的过程中，首先计算FC层的梯度，然后通过DDP中的allreduce同步到所有训练器。
- en: Next, Distributed Autograd propagates the gradients to the parameter server,
    where the gradients for the embedding table are updated.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，Distributed Autograd将梯度传播到参数服务器，更新嵌入表的梯度。
- en: Finally, the [Distributed Optimizer](https://pytorch.org/docs/master/rpc.html#module-torch.distributed.optim)
    is used to update all the parameters.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用[Distributed Optimizer](https://pytorch.org/docs/master/rpc.html#module-torch.distributed.optim)来更新所有参数。
- en: Attention
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You should always use [Distributed Autograd](https://pytorch.org/docs/master/rpc.html#distributed-autograd-framework)
    for the backward pass if you’re combining DDP and RPC.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果结合DDP和RPC，应始终使用[Distributed Autograd](https://pytorch.org/docs/master/rpc.html#distributed-autograd-framework)进行反向传播。
- en: Now, let’s go through each part in detail. Firstly, we need to setup all of
    our workers before we can perform any training. We create 4 processes such that
    ranks 0 and 1 are our trainers, rank 2 is the master and rank 3 is the parameter
    server.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐个详细介绍每个部分。首先，我们需要在进行任何训练之前设置所有的worker。我们创建4个进程，其中rank 0和1是我们的Trainer，rank
    2是主节点，rank 3是参数服务器。
- en: We initialize the RPC framework on all 4 workers using the TCP init_method.
    Once RPC initialization is done, the master creates a remote module that holds
    an [EmbeddingBag](https://pytorch.org/docs/master/generated/torch.nn.EmbeddingBag.html)
    layer on the Parameter Server using [RemoteModule](https://pytorch.org/docs/master/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule).
    The master then loops through each trainer and kicks off the training loop by
    calling `_run_trainer` on each trainer using [rpc_async](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.rpc_async).
    Finally, the master waits for all training to finish before exiting.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用TCP init_method在所有4个worker上初始化RPC框架。一旦RPC初始化完成，主节点会创建一个远程模块，该模块在参数服务器上保存了一个[EmbeddingBag](https://pytorch.org/docs/master/generated/torch.nn.EmbeddingBag.html)层，使用[RemoteModule](https://pytorch.org/docs/master/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule)。然后主节点循环遍历每个Trainer，并通过调用[rpc_async](https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.rpc_async)在每个Trainer上调用`_run_trainer`来启动训练循环。最后，主节点在退出之前等待所有训练完成。
- en: The trainers first initialize a `ProcessGroup` for DDP with world_size=2 (for
    two trainers) using [init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group).
    Next, they initialize the RPC framework using the TCP init_method. Note that the
    ports are different in RPC initialization and ProcessGroup initialization. This
    is to avoid port conflicts between initialization of both frameworks. Once the
    initialization is done, the trainers just wait for the `_run_trainer` RPC from
    the master.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer首先为DDP初始化一个world_size=2（两个Trainer）的`ProcessGroup`，使用[init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)。接下来，他们使用TCP
    init_method初始化RPC框架。请注意，RPC初始化和ProcessGroup初始化中的端口是不同的。这是为了避免两个框架初始化之间的端口冲突。初始化完成后，Trainer只需等待来自主节点的`_run_trainer`
    RPC。
- en: The parameter server just initializes the RPC framework and waits for RPCs from
    the trainers and master.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 参数服务器只是初始化RPC框架并等待来自Trainer和主节点的RPC。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Before we discuss details of the Trainer, let’s introduce the `HybridModel`
    that the trainer uses. As described below, the `HybridModel` is initialized using
    a remote module that holds an embedding table (`remote_emb_module`) on the parameter
    server and the `device` to use for DDP. The initialization of the model wraps
    an [nn.Linear](https://pytorch.org/docs/master/generated/torch.nn.Linear.html)
    layer inside DDP to replicate and synchronize this layer across all trainers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论Trainer的细节之前，让我们先介绍一下Trainer使用的`HybridModel`。如下所述，`HybridModel`是使用一个远程模块进行初始化的，该远程模块在参数服务器上保存了一个嵌入表（`remote_emb_module`）和用于DDP的`device`。模型的初始化将一个[nn.Linear](https://pytorch.org/docs/master/generated/torch.nn.Linear.html)层包装在DDP中，以便在所有Trainer之间复制和同步这个层。
- en: The forward method of the model is pretty straightforward. It performs an embedding
    lookup on the parameter server using RemoteModule’s `forward` and passes its output
    onto the FC layer.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的前向方法非常简单。它使用RemoteModule的`forward`在参数服务器上进行嵌入查找，并将其输出传递给FC层。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, let’s look at the setup on the Trainer. The trainer first creates the
    `HybridModel` described above using a remote module that holds the embedding table
    on the parameter server and its own rank.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下Trainer的设置。Trainer首先使用一个远程模块创建上述`HybridModel`，该远程模块在参数服务器上保存了嵌入表和自己的rank。
- en: Now, we need to retrieve a list of RRefs to all the parameters that we would
    like to optimize with [DistributedOptimizer](https://pytorch.org/docs/master/rpc.html#module-torch.distributed.optim).
    To retrieve the parameters for the embedding table from the parameter server,
    we can call RemoteModule’s [remote_parameters](https://pytorch.org/docs/master/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters),
    which basically walks through all the parameters for the embedding table and returns
    a list of RRefs. The trainer calls this method on the parameter server via RPC
    to receive a list of RRefs to the desired parameters. Since the DistributedOptimizer
    always takes a list of RRefs to parameters that need to be optimized, we need
    to create RRefs even for the local parameters for our FC layers. This is done
    by walking `model.fc.parameters()`, creating an RRef for each parameter and appending
    it to the list returned from `remote_parameters()`. Note that we cannnot use `model.parameters()`,
    because it will recursively call `model.remote_emb_module.parameters()`, which
    is not supported by `RemoteModule`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要获取一个RRefs列表，其中包含我们想要使用[DistributedOptimizer](https://pytorch.org/docs/master/rpc.html#module-torch.distributed.optim)进行优化的所有参数。为了从参数服务器检索嵌入表的参数，我们可以调用RemoteModule的[remote_parameters](https://pytorch.org/docs/master/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters)，这个方法基本上遍历了嵌入表的所有参数，并返回一个RRefs列表。Trainer通过RPC在参数服务器上调用这个方法，以接收到所需参数的RRefs列表。由于DistributedOptimizer始终需要一个要优化的参数的RRefs列表，我们需要为FC层的本地参数创建RRefs。这是通过遍历`model.fc.parameters()`，为每个参数创建一个RRef，并将其附加到从`remote_parameters()`返回的列表中完成的。请注意，我们不能使用`model.parameters()`，因为它会递归调用`model.remote_emb_module.parameters()`，这是`RemoteModule`不支持的。
- en: Finally, we create our DistributedOptimizer using all the RRefs and define a
    CrossEntropyLoss function.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用所有的RRefs创建我们的DistributedOptimizer，并定义一个CrossEntropyLoss函数。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we’re ready to introduce the main training loop that is run on each trainer.
    `get_next_batch` is just a helper function to generate random inputs and targets
    for training. We run the training loop for multiple epochs and for each batch:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备介绍在每个训练器上运行的主要训练循环。`get_next_batch`只是一个辅助函数，用于生成训练的随机输入和目标。我们对多个epochs和每个batch运行训练循环：
- en: Setup a [Distributed Autograd Context](https://pytorch.org/docs/master/rpc.html#torch.distributed.autograd.context)
    for Distributed Autograd.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为分布式自动求导设置[Distributed Autograd Context](https://pytorch.org/docs/master/rpc.html#torch.distributed.autograd.context)。
- en: Run the forward pass of the model and retrieve its output.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行模型的前向传播并检索其输出。
- en: Compute the loss based on our outputs and targets using the loss function.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用损失函数基于我们的输出和目标计算损失。
- en: Use Distributed Autograd to execute a distributed backward pass using the loss.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分布式自动求导来执行使用损失函数的分布式反向传播。
- en: Finally, run a Distributed Optimizer step to optimize all the parameters.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，运行一个分布式优化器步骤来优化所有参数。
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Source code for the entire example can be found [here](https://github.com/pytorch/examples/tree/master/distributed/rpc/ddp_rpc).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 整个示例的源代码可以在[这里](https://github.com/pytorch/examples/tree/master/distributed/rpc/ddp_rpc)找到。
