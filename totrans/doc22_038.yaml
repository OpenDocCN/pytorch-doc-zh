- en: Automatic differentiation package - torch.autograd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/autograd.html](https://pytorch.org/docs/stable/autograd.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`torch.autograd` provides classes and functions implementing automatic differentiation
    of arbitrary scalar valued functions. It requires minimal changes to the existing
    code - you only need to declare `Tensor` s for which gradients should be computed
    with the `requires_grad=True` keyword. As of now, we only support autograd for
    floating point `Tensor` types ( half, float, double and bfloat16) and complex
    `Tensor` types (cfloat, cdouble).'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`backward`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") | Computes the sum of gradients of given tensors with
    respect to graph leaves. |'
  prefs: []
  type: TYPE_TB
- en: '| [`grad`](generated/torch.autograd.grad.html#torch.autograd.grad "torch.autograd.grad")
    | Computes and returns the sum of gradients of outputs with respect to the inputs.
    |'
  prefs: []
  type: TYPE_TB
- en: '## Forward-mode Automatic Differentiation[](#forward-mode-automatic-differentiation
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This API is in beta. Even though the function signatures are very unlikely to
    change, improved operator coverage is planned before we consider this stable.
  prefs: []
  type: TYPE_NORMAL
- en: Please see the [forward-mode AD tutorial](https://pytorch.org/tutorials/intermediate/forward_ad_usage.html)
    for detailed steps on how to use this API.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`forward_ad.dual_level`](generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level
    "torch.autograd.forward_ad.dual_level") | Context-manager for forward AD, where
    all forward AD computation must occur within the `dual_level` context. |'
  prefs: []
  type: TYPE_TB
- en: '| [`forward_ad.make_dual`](generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual
    "torch.autograd.forward_ad.make_dual") | Associate a tensor value with its tangent
    to create a "dual tensor" for forward AD gradient computation. |'
  prefs: []
  type: TYPE_TB
- en: '| [`forward_ad.unpack_dual`](generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual
    "torch.autograd.forward_ad.unpack_dual") | Unpack a "dual tensor" to get both
    its Tensor value and its forward AD gradient. |  ## Functional higher level API[](#functional-higher-level-api
    "Permalink to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This API is in beta. Even though the function signatures are very unlikely to
    change, major improvements to performances are planned before we consider this
    stable.
  prefs: []
  type: TYPE_NORMAL
- en: This section contains the higher level API for the autograd that builds on the
    basic API above and allows you to compute jacobians, hessians, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'This API works with user-provided functions that take only Tensors as input
    and return only Tensors. If your function takes other arguments that are not Tensors
    or Tensors that don’t have requires_grad set, you can use a lambda to capture
    them. For example, for a function `f` that takes three inputs, a Tensor for which
    we want the jacobian, another tensor that should be considered constant and a
    boolean flag as `f(input, constant, flag=flag)` you can use it as `functional.jacobian(lambda
    x: f(x, constant, flag=flag), input)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`functional.jacobian`](generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian
    "torch.autograd.functional.jacobian") | Compute the Jacobian of a given function.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`functional.hessian`](generated/torch.autograd.functional.hessian.html#torch.autograd.functional.hessian
    "torch.autograd.functional.hessian") | Compute the Hessian of a given scalar function.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`functional.vjp`](generated/torch.autograd.functional.vjp.html#torch.autograd.functional.vjp
    "torch.autograd.functional.vjp") | Compute the dot product between a vector `v`
    and the Jacobian of the given function at the point given by the inputs. |'
  prefs: []
  type: TYPE_TB
- en: '| [`functional.jvp`](generated/torch.autograd.functional.jvp.html#torch.autograd.functional.jvp
    "torch.autograd.functional.jvp") | Compute the dot product between the Jacobian
    of the given function at the point given by the inputs and a vector `v`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`functional.vhp`](generated/torch.autograd.functional.vhp.html#torch.autograd.functional.vhp
    "torch.autograd.functional.vhp") | Compute the dot product between vector `v`
    and Hessian of a given scalar function at a specified point. |'
  prefs: []
  type: TYPE_TB
- en: '| [`functional.hvp`](generated/torch.autograd.functional.hvp.html#torch.autograd.functional.hvp
    "torch.autograd.functional.hvp") | Compute the dot product between the scalar
    function''s Hessian and a vector `v` at a specified point. |  ## Locally disabling
    gradient computation[](#locally-disabling-gradient-computation "Permalink to
    this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: 'See [Locally disabling gradient computation](notes/autograd.html#locally-disable-grad-doc)
    for more information on the differences between no-grad and inference mode as
    well as other related mechanisms that may be confused with the two. Also see [Locally
    disabling gradient computation](torch.html#torch-rst-local-disable-grad) for a
    list of functions that can be used to locally disable gradients.  ## Default gradient
    layouts'
  prefs: []
  type: TYPE_NORMAL
- en: When a non-sparse `param` receives a non-sparse gradient during [`torch.autograd.backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") or [`torch.Tensor.backward()`](generated/torch.Tensor.backward.html#torch.Tensor.backward
    "torch.Tensor.backward") `param.grad` is accumulated as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `param.grad` is initially `None`:'
  prefs: []
  type: TYPE_NORMAL
- en: If `param`’s memory is non-overlapping and dense, `.grad` is created with strides
    matching `param` (thus matching `param`’s layout).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, `.grad` is created with rowmajor-contiguous strides.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If `param` already has a non-sparse `.grad` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: If `create_graph=False`, `backward()` accumulates into `.grad` in-place, which
    preserves its strides.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If `create_graph=True`, `backward()` replaces `.grad` with a new tensor `.grad
    + new grad`, which attempts (but does not guarantee) matching the preexisting
    `.grad`’s strides.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The default behavior (letting `.grad`s be `None` before the first `backward()`,
    such that their layout is created according to 1 or 2, and retained over time
    according to 3 or 4) is recommended for best performance. Calls to `model.zero_grad()`
    or `optimizer.zero_grad()` will not affect `.grad` layouts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, resetting all `.grad`s to `None` before each accumulation phase, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: such that they’re recreated according to 1 or 2 every time, is a valid alternative
    to `model.zero_grad()` or `optimizer.zero_grad()` that may improve performance
    for some networks.
  prefs: []
  type: TYPE_NORMAL
- en: Manual gradient layouts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you need manual control over `.grad`’s strides, assign `param.grad =` a zeroed
    tensor with desired strides before the first `backward()`, and never reset it
    to `None`. 3 guarantees your layout is preserved as long as `create_graph=False`.
    4 indicates your layout is *likely* preserved even if `create_graph=True`.
  prefs: []
  type: TYPE_NORMAL
- en: In-place operations on Tensors[](#in-place-operations-on-tensors "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supporting in-place operations in autograd is a hard matter, and we discourage
    their use in most cases. Autograd’s aggressive buffer freeing and reuse makes
    it very efficient and there are very few occasions when in-place operations actually
    lower memory usage by any significant amount. Unless you’re operating under heavy
    memory pressure, you might never need to use them.
  prefs: []
  type: TYPE_NORMAL
- en: In-place correctness checks[](#in-place-correctness-checks "Permalink to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All `Tensor` s keep track of in-place operations applied to them, and if the
    implementation detects that a tensor was saved for backward in one of the functions,
    but it was modified in-place afterwards, an error will be raised once backward
    pass is started. This ensures that if you’re using in-place functions and not
    seeing any errors, you can be sure that the computed gradients are correct.
  prefs: []
  type: TYPE_NORMAL
- en: Variable (deprecated)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'The Variable API has been deprecated: Variables are no longer necessary to
    use autograd with tensors. Autograd automatically supports Tensors with `requires_grad`
    set to `True`. Below please find a quick guide on what has changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Variable(tensor)` and `Variable(tensor, requires_grad)` still work as expected,
    but they return Tensors instead of Variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`var.data` is the same thing as `tensor.data`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods such as `var.backward(), var.detach(), var.register_hook()` now work
    on tensors with the same method names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, one can now create tensors with `requires_grad=True` using factory
    methods such as [`torch.randn()`](generated/torch.randn.html#torch.randn "torch.randn"),
    [`torch.zeros()`](generated/torch.zeros.html#torch.zeros "torch.zeros"), [`torch.ones()`](generated/torch.ones.html#torch.ones
    "torch.ones"), and others like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)`'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor autograd functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| `torch.Tensor.grad` | This attribute is `None` by default and becomes a Tensor
    the first time a call to [`backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") computes gradients for `self`. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.Tensor.requires_grad` | Is `True` if gradients need to be computed
    for this Tensor, `False` otherwise. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.Tensor.is_leaf` | All Tensors that have `requires_grad` which is `False`
    will be leaf Tensors by convention. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.Tensor.backward`([gradient, ...]) | Computes the gradient of current
    tensor wrt graph leaves. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.Tensor.detach` | Returns a new Tensor, detached from the current graph.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.Tensor.detach_` | Detaches the Tensor from the graph that created
    it, making it a leaf. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.Tensor.register_hook`(hook) | Registers a backward hook. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.Tensor.register_post_accumulate_grad_hook`(hook) | Registers a backward
    hook that runs after grad accumulation. |'
  prefs: []
  type: TYPE_TB
- en: '| `torch.Tensor.retain_grad`() | Enables this Tensor to have their [`grad`](generated/torch.autograd.grad.html#torch.autograd.grad
    "torch.autograd.grad") populated during [`backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward"). |'
  prefs: []
  type: TYPE_TB
- en: Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Base class to create custom autograd.Function.
  prefs: []
  type: TYPE_NORMAL
- en: To create a custom autograd.Function, subclass this class and implement the
    [`forward()`](generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") and [`backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") static methods. Then, to use your custom op in the
    forward pass, call the class method `apply`. Do not call [`forward()`](generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") directly.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure correctness and best performance, make sure you are calling the correct
    methods on `ctx` and validating your backward function using [`torch.autograd.gradcheck()`](#module-torch.autograd.gradcheck
    "torch.autograd.gradcheck").
  prefs: []
  type: TYPE_NORMAL
- en: See [Extending torch.autograd](notes/extending.html#extending-autograd) for
    more details on how to use this class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '| [`Function.forward`](generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") | Define the forward of the custom autograd
    Function. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Function.backward`](generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward") | Define a formula for differentiating the
    operation with backward mode automatic differentiation. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Function.jvp`](generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") | Define a formula for differentiating the operation
    with forward mode automatic differentiation. |'
  prefs: []
  type: TYPE_TB
- en: '| [`Function.vmap`](generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap") | Define the behavior for this autograd.Function
    underneath [`torch.vmap()`](generated/torch.vmap.html#torch.vmap "torch.vmap").
    |'
  prefs: []
  type: TYPE_TB
- en: Context method mixins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When creating a new [`Function`](#torch.autograd.Function "torch.autograd.Function"),
    the following methods are available to ctx.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`function.FunctionCtx.mark_dirty`](generated/torch.autograd.function.FunctionCtx.mark_dirty.html#torch.autograd.function.FunctionCtx.mark_dirty
    "torch.autograd.function.FunctionCtx.mark_dirty") | Mark given tensors as modified
    in an in-place operation. |'
  prefs: []
  type: TYPE_TB
- en: '| [`function.FunctionCtx.mark_non_differentiable`](generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html#torch.autograd.function.FunctionCtx.mark_non_differentiable
    "torch.autograd.function.FunctionCtx.mark_non_differentiable") | Mark outputs
    as non-differentiable. |'
  prefs: []
  type: TYPE_TB
- en: '| [`function.FunctionCtx.save_for_backward`](generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward
    "torch.autograd.function.FunctionCtx.save_for_backward") | Save given tensors
    for a future call to [`backward()`](generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`function.FunctionCtx.set_materialize_grads`](generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html#torch.autograd.function.FunctionCtx.set_materialize_grads
    "torch.autograd.function.FunctionCtx.set_materialize_grads") | Set whether to
    materialize grad tensors. |'
  prefs: []
  type: TYPE_TB
- en: '## Numerical gradient checking[](#module-torch.autograd.gradcheck "Permalink
    to this heading")'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`gradcheck`](generated/torch.autograd.gradcheck.gradcheck.html#torch.autograd.gradcheck.gradcheck
    "torch.autograd.gradcheck.gradcheck") | Check gradients computed via small finite
    differences against analytical gradients wrt tensors in `inputs` that are of floating
    point or complex type and with `requires_grad=True`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`gradgradcheck`](generated/torch.autograd.gradcheck.gradgradcheck.html#torch.autograd.gradcheck.gradgradcheck
    "torch.autograd.gradcheck.gradgradcheck") | Check gradients of gradients computed
    via small finite differences against analytical gradients wrt tensors in `inputs`
    and `grad_outputs` that are of floating point or complex type and with `requires_grad=True`.
    |'
  prefs: []
  type: TYPE_TB
- en: Profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autograd includes a profiler that lets you inspect the cost of different operators
    inside your model - both on the CPU and GPU. There are three modes implemented
    at the moment - CPU-only using [`profile`](#torch.autograd.profiler.profile "torch.autograd.profiler.profile").
    nvprof based (registers both CPU and GPU activity) using [`emit_nvtx`](#torch.autograd.profiler.emit_nvtx
    "torch.autograd.profiler.emit_nvtx"). and vtune profiler based using [`emit_itt`](#torch.autograd.profiler.emit_itt
    "torch.autograd.profiler.emit_itt").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Context manager that manages autograd profiler state and holds a summary of
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood it just records events of functions being executed in C++ and
    exposes those events to Python. You can wrap any code into it and it will only
    report runtime of PyTorch functions. Note: profiler is thread local and is automatically
    propagated into the async tasks'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**enabled** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Setting this to False makes this context
    manager a no-op.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cuda** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Enables timing of CUDA events as well using
    the cudaEvent API. Adds approximately 4us of overhead to each tensor operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**record_shapes** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If shapes recording is set, information
    about input dimensions will be collected. This allows one to see which dimensions
    have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True).
    Please note that shape recording might skew your profiling data. It is recommended
    to use separate runs with and without shape recording to validate the timing.
    Most likely the skew will be negligible for bottom most events (in a case of nested
    function calls). But for higher level functions the total self cpu time might
    be artificially increased because of the shape collection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**with_flops** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If with_flops is set, the profiler will
    estimate the FLOPs (floating point operations) value using the operator’s input
    shape. This allows one to estimate the hardware performance. Currently, this option
    only works for the matrix multiplication and 2D convolution operators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**profile_memory** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – track tensor memory allocation/deallocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**with_stack** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – record source information (file and line
    number) for the ops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**with_modules** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – record module hierarchy (including function names) corresponding
    to the callstack of the op. e.g. If module A’s forward call’s module B’s forward
    which contains an aten::add op, then aten::add’s module hierarchy is A.B Note
    that this support exist, at the moment, only for TorchScript models and not eager
    mode models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_kineto** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – experimental, enable profiling with Kineto
    profiler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cpu** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – profile CPU events; setting to `False` requires
    `use_kineto=True` and can be used to lower the overhead for GPU-only profiling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**experimental_config** (*_ExperimentalConfig*) – A set of experimental options
    used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '| [`profiler.profile.export_chrome_trace`](generated/torch.autograd.profiler.profile.export_chrome_trace.html#torch.autograd.profiler.profile.export_chrome_trace
    "torch.autograd.profiler.profile.export_chrome_trace") | Export an EventList as
    a Chrome tracing tools file. |'
  prefs: []
  type: TYPE_TB
- en: '| [`profiler.profile.key_averages`](generated/torch.autograd.profiler.profile.key_averages.html#torch.autograd.profiler.profile.key_averages
    "torch.autograd.profiler.profile.key_averages") | Averages all function events
    over their keys. |'
  prefs: []
  type: TYPE_TB
- en: '| [`profiler.profile.self_cpu_time_total`](generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total
    "torch.autograd.profiler.profile.self_cpu_time_total") | Returns total time spent
    on CPU. |'
  prefs: []
  type: TYPE_TB
- en: '| [`profiler.profile.total_average`](generated/torch.autograd.profiler.profile.total_average.html#torch.autograd.profiler.profile.total_average
    "torch.autograd.profiler.profile.total_average") | Averages all events. |'
  prefs: []
  type: TYPE_TB
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Context manager that makes every autograd operation emit an NVTX range.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is useful when running the program under nvprof:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, there’s no way to force nvprof to flush the data it collected
    to disk, so for CUDA profiling one has to use this context manager to annotate
    nvprof traces and wait for the process to exit before inspecting them. Then, either
    NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or [`torch.autograd.profiler.load_nvprof()`](generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof
    "torch.autograd.profiler.load_nvprof") can load the results for inspection e.g.
    in Python REPL.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**enabled** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Setting `enabled=False` makes this context
    manager a no-op. Default: `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**record_shapes** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If `record_shapes=True`, the nvtx range
    wrapping each autograd op will append information about the sizes of Tensor arguments
    received by that op, in the following format: `[[arg0.size(0), arg0.size(1), ...],
    [arg1.size(0), arg1.size(1), ...], ...]` Non-tensor arguments will be represented
    by `[]`. Arguments will be listed in the order they are received by the backend
    op. Please note that this order may not match the order in which those arguments
    were passed on the Python side. Also note that shape recording may increase the
    overhead of nvtx range creation. Default: `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Forward-backward correlation**'
  prefs: []
  type: TYPE_NORMAL
- en: When viewing a profile created using [`emit_nvtx`](#torch.autograd.profiler.emit_nvtx
    "torch.autograd.profiler.emit_nvtx") in the Nvidia Visual Profiler, correlating
    each backward-pass op with the corresponding forward-pass op can be difficult.
    To ease this task, [`emit_nvtx`](#torch.autograd.profiler.emit_nvtx "torch.autograd.profiler.emit_nvtx")
    appends sequence number information to the ranges it generates.
  prefs: []
  type: TYPE_NORMAL
- en: During the forward pass, each function range is decorated with `seq=<N>`. `seq`
    is a running counter, incremented each time a new backward Function object is
    created and stashed for backward. Thus, the `seq=<N>` annotation associated with
    each forward function range tells you that if a backward Function object is created
    by this forward function, the backward object will receive sequence number N.
    During the backward pass, the top-level range wrapping each C++ backward Function’s
    `apply()` call is decorated with `stashed seq=<M>`. `M` is the sequence number
    that the backward object was created with. By comparing `stashed seq` numbers
    in backward with `seq` numbers in forward, you can track down which forward op
    created each backward Function.
  prefs: []
  type: TYPE_NORMAL
- en: Any functions executed during the backward pass are also decorated with `seq=<N>`.
    During default backward (with `create_graph=False`) this information is irrelevant,
    and in fact, `N` may simply be 0 for all such functions. Only the top-level ranges
    associated with backward Function objects’ `apply()` methods are useful, as a
    way to correlate these Function objects with the earlier forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: '**Double-backward**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If, on the other hand, a backward pass with `create_graph=True` is underway
    (in other words, if you are setting up for a double-backward), each function’s
    execution during backward is given a nonzero, useful `seq=<N>`. Those functions
    may themselves create Function objects to be executed later during double-backward,
    just as the original functions in the forward pass did. The relationship between
    backward and double-backward is conceptually the same as the relationship between
    forward and backward: The functions still emit current-sequence-number-tagged
    ranges, the Function objects they create still stash those sequence numbers, and
    during the eventual double-backward, the Function objects’ `apply()` ranges are
    still tagged with `stashed seq` numbers, which can be compared to seq numbers
    from the backward pass.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Context manager that makes every autograd operation emit an ITT range.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is useful when running the program under Intel(R) VTune Profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The Instrumentation and Tracing Technology (ITT) API enables your application
    to generate and control the collection of trace data during its execution across
    different Intel tools. This context manager is to annotate Intel(R) VTune Profiling
    trace. With help of this context manager, you will be able to see labled ranges
    in Intel(R) VTune Profiler GUI.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**enabled** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Setting `enabled=False` makes this context
    manager a no-op. Default: `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**record_shapes** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If `record_shapes=True`, the itt range wrapping
    each autograd op will append information about the sizes of Tensor arguments received
    by that op, in the following format: `[[arg0.size(0), arg0.size(1), ...], [arg1.size(0),
    arg1.size(1), ...], ...]` Non-tensor arguments will be represented by `[]`. Arguments
    will be listed in the order they are received by the backend op. Please note that
    this order may not match the order in which those arguments were passed on the
    Python side. Also note that shape recording may increase the overhead of itt range
    creation. Default: `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '| [`profiler.load_nvprof`](generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof
    "torch.autograd.profiler.load_nvprof") | Open an nvprof trace file and parses
    autograd annotations. |'
  prefs: []
  type: TYPE_TB
- en: Anomaly detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Context-manager that enable anomaly detection for the autograd engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This does two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Running the forward pass with detection enabled will allow the backward pass
    to print the traceback of the forward operation that created the failing backward
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `check_nan` is `True`, any backward computation that generate “nan” value
    will raise an error. Default `True`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: This mode should be enabled only for debugging as the different tests will slow
    down your program execution.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Context-manager that sets the anomaly detection for the autograd engine on or
    off.
  prefs: []
  type: TYPE_NORMAL
- en: '`set_detect_anomaly` will enable or disable the autograd anomaly detection
    based on its argument `mode`. It can be used as a context-manager or as a function.'
  prefs: []
  type: TYPE_NORMAL
- en: See `detect_anomaly` above for details of the anomaly detection behaviour.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**mode** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in
    Python v3.12)")) – Flag whether to enable anomaly detection (`True`), or disable
    (`False`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**check_nan** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Flag whether to raise an error when the backward generate
    “nan”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autograd graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autograd exposes methods that allow one to inspect the graph and interpose behavior
    during the backward pass.
  prefs: []
  type: TYPE_NORMAL
- en: The `grad_fn` attribute of a [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor")
    holds a `torch.autograd.graph.Node` if the tensor is the output of a operation
    that was recorded by autograd (i.e., grad_mode is enabled and at least one of
    the inputs required gradients), or `None` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '| [`graph.Node.name`](generated/torch.autograd.graph.Node.name.html#torch.autograd.graph.Node.name
    "torch.autograd.graph.Node.name") | Return the name. |'
  prefs: []
  type: TYPE_TB
- en: '| [`graph.Node.metadata`](generated/torch.autograd.graph.Node.metadata.html#torch.autograd.graph.Node.metadata
    "torch.autograd.graph.Node.metadata") | Return the metadata. |'
  prefs: []
  type: TYPE_TB
- en: '| [`graph.Node.next_functions`](generated/torch.autograd.graph.Node.next_functions.html#torch.autograd.graph.Node.next_functions
    "torch.autograd.graph.Node.next_functions") |  |'
  prefs: []
  type: TYPE_TB
- en: '| [`graph.Node.register_hook`](generated/torch.autograd.graph.Node.register_hook.html#torch.autograd.graph.Node.register_hook
    "torch.autograd.graph.Node.register_hook") | Register a backward hook. |'
  prefs: []
  type: TYPE_TB
- en: '| [`graph.Node.register_prehook`](generated/torch.autograd.graph.Node.register_prehook.html#torch.autograd.graph.Node.register_prehook
    "torch.autograd.graph.Node.register_prehook") | Register a backward pre-hook.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Some operations need intermediary results to be saved during the forward pass
    in order to execute the backward pass. These intermediary results are saved as
    attributes on the `grad_fn` and can be accessed. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can also define how these saved tensors should be packed / unpacked using
    hooks. A common application is to trade compute for memory by saving those intermediary
    results to disk or to CPU instead of leaving them on the GPU. This is especially
    useful if you notice your model fits on GPU during evaluation, but not training.
    Also see [Hooks for saved tensors](notes/autograd.html#saved-tensors-hooks-doc).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Context-manager that sets a pair of pack / unpack hooks for saved tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Use this context-manager to define how intermediary results of an operation
    should be packed before saving, and unpacked on retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: In that context, the `pack_hook` function will be called everytime an operation
    saves a tensor for backward (this includes intermediary results saved using `save_for_backward()`
    but also those recorded by a PyTorch-defined operation). The output of `pack_hook`
    is then stored in the computation graph instead of the original tensor.
  prefs: []
  type: TYPE_NORMAL
- en: The `unpack_hook` is called when the saved tensor needs to be accessed, namely
    when executing [`torch.Tensor.backward()`](generated/torch.Tensor.backward.html#torch.Tensor.backward
    "torch.Tensor.backward") or [`torch.autograd.grad()`](generated/torch.autograd.grad.html#torch.autograd.grad
    "torch.autograd.grad"). It takes as argument the *packed* object returned by `pack_hook`
    and should return a tensor which has the same content as the original tensor (passed
    as input to the corresponding `pack_hook`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The hooks should have the following signatures:'
  prefs: []
  type: TYPE_NORMAL
- en: 'pack_hook(tensor: Tensor) -> Any'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: unpack_hook(Any) -> Tensor
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: where the return value of `pack_hook` is a valid input to `unpack_hook`.
  prefs: []
  type: TYPE_NORMAL
- en: In general, you want `unpack_hook(pack_hook(t))` to be equal to `t` in terms
    of value, size, dtype and device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Performing an inplace operation on the input to either hooks may lead to undefined
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Only one pair of hooks is allowed at a time. When recursively nesting this context-manager,
    only the inner-most pair of hooks will be applied.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Context manager under which tensors saved by the forward pass will be stored
    on cpu, then retrieved for backward.
  prefs: []
  type: TYPE_NORMAL
- en: When performing operations within this context manager, intermediary results
    saved in the graph during the forward pass will be moved to CPU, then copied back
    to the original device when needed for the backward pass. If the graph was already
    on CPU, no tensor copy is performed.
  prefs: []
  type: TYPE_NORMAL
- en: Use this context-manager to trade compute for GPU memory usage (e.g. when your
    model doesn’t fit in GPU memory during training).
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pin_memory** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True` tensors will be saved to CPU pinned memory during
    packing and copied to GPU asynchronously during unpacking. Defaults to `False`.
    Also see [Use pinned memory buffers](notes/cuda.html#cuda-memory-pinning).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Context-manager that disables the saved tensors default hooks feature.
  prefs: []
  type: TYPE_NORMAL
- en: Useful for if you are creating a feature that does not work with saved tensors
    default hooks.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**error_message** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – When saved tensors default hooks are used when they have
    been are disabled, a RuntimeError with this error message gets raised.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Register a multi-grad backward hook.
  prefs: []
  type: TYPE_NORMAL
- en: The hook will be called after gradients with respect to every tensor in `tensors`
    have been computed. If a tensor is in `tensors` but is not part of the graph,
    or if a tensor is not needed to compute the gradients for any `inputs` specified
    for the current `.backward()` or `.grad()` call, this tensor will be ignored and
    the hook will not wait for its gradient to be computed.
  prefs: []
  type: TYPE_NORMAL
- en: After every non-ignored tensor’s gradient has been computed, `fn` will be called
    with those gradients. `None` will be passed for tensors that did not have their
    gradients computed.
  prefs: []
  type: TYPE_NORMAL
- en: The hook should not modify its arguments.
  prefs: []
  type: TYPE_NORMAL
- en: This function returns a handle with a method `handle.remove()` that removes
    the hook.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: See [Backward Hooks execution](notes/autograd.html#backward-hooks-execution)
    for more information on how when this hook is executed, and how its execution
    is ordered relative to other hooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Context manager under which mutating tensors saved for backward is allowed.
  prefs: []
  type: TYPE_NORMAL
- en: Under this context manager, tensors saved for backward are cloned on mutation,
    so the original version can still be used during backward. Normally, mutating
    a tensor saved for backward will result in an error raised when it’s used during
    backward.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure the correct behavior, both the forward and backward should be run
    under the same context manager.
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: An _AllowMutationOnSavedContext object storing the state managed by this context
    manager. This object can be useful for debugging purposes. The state managed by
    the context manager is automatically cleared upon exiting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Object representing a given gradient edge within the autograd graph. To get
    the gradient edge where a given Tensor gradient will be computed, you can do `edge
    = autograd.graph.get_gradient_edge(tensor)`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Get the gradient edge for computing the gradient of the given Tensor.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, it is equivalent to call `g = autograd.grad(loss, input)` and
    `g = autograd.grad(loss, get_gradient_edge(input))`.
  prefs: []
  type: TYPE_NORMAL
