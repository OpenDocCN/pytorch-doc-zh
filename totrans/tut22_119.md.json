["```py\n// file name: dummy.hpp\n#include  <torch/python.h>\n\n#include  <torch/csrc/distributed/c10d/Backend.hpp>\n#include  <torch/csrc/distributed/c10d/Work.hpp>\n#include  <torch/csrc/distributed/c10d/Store.hpp>\n#include  <torch/csrc/distributed/c10d/Types.hpp>\n#include  <torch/csrc/distributed/c10d/Utils.hpp>\n\n#include  <pybind11/chrono.h>\n\nnamespace  c10d  {\n\nclass  BackendDummy  :  public  Backend  {\n  public:\n  BackendDummy(int  rank,  int  size);\n\n  c10::intrusive_ptr<Work>  allgather(\n  std::vector<std::vector<at::Tensor>>&  outputTensors,\n  std::vector<at::Tensor>&  inputTensors,\n  const  AllgatherOptions&  opts  =  AllgatherOptions())  override;\n\n  c10::intrusive_ptr<Work>  allreduce(\n  std::vector<at::Tensor>&  tensors,\n  const  AllreduceOptions&  opts  =  AllreduceOptions())  override;\n\n  // The collective communication APIs without a custom implementation\n  // will error out if invoked by application code.\n};\n\nclass  WorkDummy  :  public  Work  {\n  public:\n  WorkDummy(\n  OpType  opType,\n  c10::intrusive_ptr<c10::ivalue::Future>  future)  // future of the output\n  :  Work(\n  -1,  // rank, only used by recvAnySource, irrelevant in this demo\n  opType),\n  future_(std::move(future))  {}\n  bool  isCompleted()  override;\n  bool  isSuccess()  const  override;\n  bool  wait(std::chrono::milliseconds  timeout  =  kUnsetTimeout)  override;\n  virtual  c10::intrusive_ptr<c10::ivalue::Future>  getFuture()  override;\n\n  private:\n  c10::intrusive_ptr<c10::ivalue::Future>  future_;\n};\n}  // namespace c10d \n```", "```py\n// file name: dummy.cpp\n#include  \"dummy.hpp\"\n\nnamespace  c10d  {\n\n// This is a dummy allgather that sets all output tensors to zero\n// Modify the implementation to conduct real communication asynchronously\nc10::intrusive_ptr<Work>  BackendDummy::allgather(\n  std::vector<std::vector<at::Tensor>>&  outputTensors,\n  std::vector<at::Tensor>&  inputTensors,\n  const  AllgatherOptions&  /* unused */)  {\n  for  (auto&  outputTensorVec  :  outputTensors)  {\n  for  (auto&  outputTensor  :  outputTensorVec)  {\n  outputTensor.zero_();\n  }\n  }\n\n  auto  future  =  c10::make_intrusive<c10::ivalue::Future>(\n  c10::ListType::create(c10::ListType::create(c10::TensorType::get())));\n  future->markCompleted(c10::IValue(outputTensors));\n  return  c10::make_intrusive<WorkDummy>(OpType::ALLGATHER,  std::move(future));\n}\n\n// This is a dummy allreduce that sets all output tensors to zero\n// Modify the implementation to conduct real communication asynchronously\nc10::intrusive_ptr<Work>  BackendDummy::allreduce(\n  std::vector<at::Tensor>&  tensors,\n  const  AllreduceOptions&  opts)  {\n  for  (auto&  tensor  :  tensors)  {\n  tensor.zero_();\n  }\n\n  auto  future  =  c10::make_intrusive<c10::ivalue::Future>(\n  c10::ListType::create(c10::TensorType::get()));\n  future->markCompleted(c10::IValue(tensors));\n  return  c10::make_intrusive<WorkDummy>(OpType::ALLGATHER,  std::move(future));\n}\n}  // namespace c10d \n```", "```py\n// file name: dummy.hpp\nclass  BackendDummy  :  public  Backend  {\n  ...\n  <Step  1  code>\n  ...\n\n  static  c10::intrusive_ptr<Backend>  createBackendDummy(\n  const  c10::intrusive_ptr<::c10d::Store>&  store,\n  int  rank,\n  int  size,\n  const  std::chrono::duration<float>&  timeout);\n\n  static  void  BackendDummyConstructor()  __attribute__((constructor))  {\n  py::object  module  =  py::module::import(\"torch.distributed\");\n  py::object  register_backend  =\n  module.attr(\"Backend\").attr(\"register_backend\");\n  // torch.distributed.Backend.register_backend will add `dummy` as a\n  // new valid backend.\n  register_backend(\"dummy\",  py::cpp_function(createBackendDummy));\n  }\n} \n```", "```py\n// file name: dummy.cpp\nc10::intrusive_ptr<Backend>  BackendDummy::createBackendDummy(\n  const  c10::intrusive_ptr<::c10d::Store>&  /* unused */,\n  int  rank,\n  int  size,\n  const  std::chrono::duration<float>&  /* unused */)  {\n  return  c10::make_intrusive<BackendDummy>(rank,  size);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME,  m)  {\n  m.def(\"createBackendDummy\",  &BackendDummy::createBackendDummy);\n} \n```", "```py\n# file name: setup.py\nimport os\nimport sys\nimport torch\nfrom setuptools import setup\nfrom torch.utils import cpp_extension\n\nsources = [\"src/dummy.cpp\"]\ninclude_dirs = [f\"{os.path.dirname(os.path.abspath(__file__))}/include/\"]\n\nif torch.cuda.is_available():\n    module = cpp_extension.CUDAExtension(\n        name = \"dummy_collectives\",\n        sources = sources,\n        include_dirs = include_dirs,\n    )\nelse:\n    module = cpp_extension.CppExtension(\n        name = \"dummy_collectives\",\n        sources = sources,\n        include_dirs = include_dirs,\n    )\n\nsetup(\n    name = \"Dummy-Collectives\",\n    version = \"0.0.1\",\n    ext_modules = [module],\n    cmdclass={'build_ext': cpp_extension.BuildExtension}\n) \n```", "```py\nimport os\n\nimport torch\n# importing dummy_collectives makes torch.distributed recognize `dummy`\n# as a valid backend.\nimport dummy_collectives\n\nimport torch.distributed as dist\n\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '29500'\n\n# Alternatively:\n# dist.init_process_group(\"dummy\", rank=0, world_size=1)\ndist.init_process_group(\"cpu:gloo,cuda:dummy\", rank=0, world_size=1)\n\n# this goes through gloo\nx = torch.ones(6)\ndist.all_reduce(x)\nprint(f\"cpu allreduce: {x}\")\n\n# this goes through dummy\nif torch.cuda.is_available():\n    y = x.cuda()\n    dist.all_reduce(y)\n    print(f\"cuda allreduce: {y}\")\n\n    try:\n        dist.broadcast(y, 0)\n    except RuntimeError:\n        print(\"got RuntimeError when calling broadcast\") \n```"]