- en: torch.nn.functional
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pytorch.org/docs/stable/nn.functional.html](https://pytorch.org/docs/stable/nn.functional.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Convolution functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`conv1d`](generated/torch.nn.functional.conv1d.html#torch.nn.functional.conv1d
    "torch.nn.functional.conv1d") | Applies a 1D convolution over an input signal
    composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`conv2d`](generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d
    "torch.nn.functional.conv2d") | Applies a 2D convolution over an input image composed
    of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`conv3d`](generated/torch.nn.functional.conv3d.html#torch.nn.functional.conv3d
    "torch.nn.functional.conv3d") | Applies a 3D convolution over an input image composed
    of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`conv_transpose1d`](generated/torch.nn.functional.conv_transpose1d.html#torch.nn.functional.conv_transpose1d
    "torch.nn.functional.conv_transpose1d") | Applies a 1D transposed convolution
    operator over an input signal composed of several input planes, sometimes also
    called "deconvolution". |'
  prefs: []
  type: TYPE_TB
- en: '| [`conv_transpose2d`](generated/torch.nn.functional.conv_transpose2d.html#torch.nn.functional.conv_transpose2d
    "torch.nn.functional.conv_transpose2d") | Applies a 2D transposed convolution
    operator over an input image composed of several input planes, sometimes also
    called "deconvolution". |'
  prefs: []
  type: TYPE_TB
- en: '| [`conv_transpose3d`](generated/torch.nn.functional.conv_transpose3d.html#torch.nn.functional.conv_transpose3d
    "torch.nn.functional.conv_transpose3d") | Applies a 3D transposed convolution
    operator over an input image composed of several input planes, sometimes also
    called "deconvolution" |'
  prefs: []
  type: TYPE_TB
- en: '| [`unfold`](generated/torch.nn.functional.unfold.html#torch.nn.functional.unfold
    "torch.nn.functional.unfold") | Extract sliding local blocks from a batched input
    tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| [`fold`](generated/torch.nn.functional.fold.html#torch.nn.functional.fold
    "torch.nn.functional.fold") | Combine an array of sliding local blocks into a
    large containing tensor. |'
  prefs: []
  type: TYPE_TB
- en: Pooling functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`avg_pool1d`](generated/torch.nn.functional.avg_pool1d.html#torch.nn.functional.avg_pool1d
    "torch.nn.functional.avg_pool1d") | Applies a 1D average pooling over an input
    signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`avg_pool2d`](generated/torch.nn.functional.avg_pool2d.html#torch.nn.functional.avg_pool2d
    "torch.nn.functional.avg_pool2d") | Applies 2D average-pooling operation in <math><semantics><mrow><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation
    encoding="application/x-tex">kH \times kW</annotation></semantics></math>kH×kW
    regions by step size <math><semantics><mrow><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation
    encoding="application/x-tex">sH \times sW</annotation></semantics></math>sH×sW
    steps. |'
  prefs: []
  type: TYPE_TB
- en: '| [`avg_pool3d`](generated/torch.nn.functional.avg_pool3d.html#torch.nn.functional.avg_pool3d
    "torch.nn.functional.avg_pool3d") | Applies 3D average-pooling operation in <math><semantics><mrow><mi>k</mi><mi>T</mi><mo>×</mo><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation
    encoding="application/x-tex">kT \times kH \times kW</annotation></semantics></math>kT×kH×kW
    regions by step size <math><semantics><mrow><mi>s</mi><mi>T</mi><mo>×</mo><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation
    encoding="application/x-tex">sT \times sH \times sW</annotation></semantics></math>sT×sH×sW
    steps. |'
  prefs: []
  type: TYPE_TB
- en: '| [`max_pool1d`](generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d
    "torch.nn.functional.max_pool1d") | Applies a 1D max pooling over an input signal
    composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`max_pool2d`](generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d
    "torch.nn.functional.max_pool2d") | Applies a 2D max pooling over an input signal
    composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`max_pool3d`](generated/torch.nn.functional.max_pool3d.html#torch.nn.functional.max_pool3d
    "torch.nn.functional.max_pool3d") | Applies a 3D max pooling over an input signal
    composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`max_unpool1d`](generated/torch.nn.functional.max_unpool1d.html#torch.nn.functional.max_unpool1d
    "torch.nn.functional.max_unpool1d") | Compute a partial inverse of `MaxPool1d`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`max_unpool2d`](generated/torch.nn.functional.max_unpool2d.html#torch.nn.functional.max_unpool2d
    "torch.nn.functional.max_unpool2d") | Compute a partial inverse of `MaxPool2d`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`max_unpool3d`](generated/torch.nn.functional.max_unpool3d.html#torch.nn.functional.max_unpool3d
    "torch.nn.functional.max_unpool3d") | Compute a partial inverse of `MaxPool3d`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`lp_pool1d`](generated/torch.nn.functional.lp_pool1d.html#torch.nn.functional.lp_pool1d
    "torch.nn.functional.lp_pool1d") | Apply a 1D power-average pooling over an input
    signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`lp_pool2d`](generated/torch.nn.functional.lp_pool2d.html#torch.nn.functional.lp_pool2d
    "torch.nn.functional.lp_pool2d") | Apply a 2D power-average pooling over an input
    signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`adaptive_max_pool1d`](generated/torch.nn.functional.adaptive_max_pool1d.html#torch.nn.functional.adaptive_max_pool1d
    "torch.nn.functional.adaptive_max_pool1d") | Applies a 1D adaptive max pooling
    over an input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`adaptive_max_pool2d`](generated/torch.nn.functional.adaptive_max_pool2d.html#torch.nn.functional.adaptive_max_pool2d
    "torch.nn.functional.adaptive_max_pool2d") | Applies a 2D adaptive max pooling
    over an input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`adaptive_max_pool3d`](generated/torch.nn.functional.adaptive_max_pool3d.html#torch.nn.functional.adaptive_max_pool3d
    "torch.nn.functional.adaptive_max_pool3d") | Applies a 3D adaptive max pooling
    over an input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`adaptive_avg_pool1d`](generated/torch.nn.functional.adaptive_avg_pool1d.html#torch.nn.functional.adaptive_avg_pool1d
    "torch.nn.functional.adaptive_avg_pool1d") | Applies a 1D adaptive average pooling
    over an input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`adaptive_avg_pool2d`](generated/torch.nn.functional.adaptive_avg_pool2d.html#torch.nn.functional.adaptive_avg_pool2d
    "torch.nn.functional.adaptive_avg_pool2d") | Apply a 2D adaptive average pooling
    over an input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`adaptive_avg_pool3d`](generated/torch.nn.functional.adaptive_avg_pool3d.html#torch.nn.functional.adaptive_avg_pool3d
    "torch.nn.functional.adaptive_avg_pool3d") | Apply a 3D adaptive average pooling
    over an input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`fractional_max_pool2d`](generated/torch.nn.functional.fractional_max_pool2d.html#torch.nn.functional.fractional_max_pool2d
    "torch.nn.functional.fractional_max_pool2d") | Applies 2D fractional max pooling
    over an input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: '| [`fractional_max_pool3d`](generated/torch.nn.functional.fractional_max_pool3d.html#torch.nn.functional.fractional_max_pool3d
    "torch.nn.functional.fractional_max_pool3d") | Applies 3D fractional max pooling
    over an input signal composed of several input planes. |'
  prefs: []
  type: TYPE_TB
- en: Attention Mechanisms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`scaled_dot_product_attention`](generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention
    "torch.nn.functional.scaled_dot_product_attention") | Computes scaled dot product
    attention on query, key and value tensors, using an optional attention mask if
    passed, and applying dropout if a probability greater than 0.0 is specified. |'
  prefs: []
  type: TYPE_TB
- en: Non-linear activation functions[](#non-linear-activation-functions "Permalink
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`threshold`](generated/torch.nn.functional.threshold.html#torch.nn.functional.threshold
    "torch.nn.functional.threshold") | Apply a threshold to each element of the input
    Tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| [`threshold_`](generated/torch.nn.functional.threshold_.html#torch.nn.functional.threshold_
    "torch.nn.functional.threshold_") | In-place version of [`threshold()`](generated/torch.nn.functional.threshold.html#torch.nn.functional.threshold
    "torch.nn.functional.threshold"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`relu`](generated/torch.nn.functional.relu.html#torch.nn.functional.relu
    "torch.nn.functional.relu") | Applies the rectified linear unit function element-wise.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`relu_`](generated/torch.nn.functional.relu_.html#torch.nn.functional.relu_
    "torch.nn.functional.relu_") | In-place version of [`relu()`](generated/torch.nn.functional.relu.html#torch.nn.functional.relu
    "torch.nn.functional.relu"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`hardtanh`](generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh
    "torch.nn.functional.hardtanh") | Applies the HardTanh function element-wise.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`hardtanh_`](generated/torch.nn.functional.hardtanh_.html#torch.nn.functional.hardtanh_
    "torch.nn.functional.hardtanh_") | In-place version of [`hardtanh()`](generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh
    "torch.nn.functional.hardtanh"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`hardswish`](generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish
    "torch.nn.functional.hardswish") | Apply hardswish function, element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`relu6`](generated/torch.nn.functional.relu6.html#torch.nn.functional.relu6
    "torch.nn.functional.relu6") | Applies the element-wise function <math><semantics><mrow><mtext>ReLU6</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo
    separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mn>6</mn><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU6}(x)
    = \min(\max(0,x), 6)</annotation></semantics></math>ReLU6(x)=min(max(0,x),6).
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`elu`](generated/torch.nn.functional.elu.html#torch.nn.functional.elu "torch.nn.functional.elu")
    | Apply the Exponential Linear Unit (ELU) function element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`elu_`](generated/torch.nn.functional.elu_.html#torch.nn.functional.elu_
    "torch.nn.functional.elu_") | In-place version of [`elu()`](generated/torch.nn.functional.elu.html#torch.nn.functional.elu
    "torch.nn.functional.elu"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`selu`](generated/torch.nn.functional.selu.html#torch.nn.functional.selu
    "torch.nn.functional.selu") | Applies element-wise, <math><semantics><mrow><mtext>SELU</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo>∗</mo><mo
    stretchy="false">(</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo
    separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>min</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>α</mi><mo>∗</mo><mo
    stretchy="false">(</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo
    stretchy="false">)</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{SELU}(x)
    = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</annotation></semantics></math>SELU(x)=scale∗(max(0,x)+min(0,α∗(exp(x)−1))),
    with <math><semantics><mrow><mi>α</mi><mo>=</mo><mn>1.6732632423543772848170429916717</mn></mrow><annotation
    encoding="application/x-tex">\alpha=1.6732632423543772848170429916717</annotation></semantics></math>α=1.6732632423543772848170429916717
    and <math><semantics><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo>=</mo><mn>1.0507009873554804934193349852946</mn></mrow><annotation
    encoding="application/x-tex">scale=1.0507009873554804934193349852946</annotation></semantics></math>scale=1.0507009873554804934193349852946.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`celu`](generated/torch.nn.functional.celu.html#torch.nn.functional.celu
    "torch.nn.functional.celu") | Applies element-wise, <math><semantics><mrow><mtext>CELU</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>min</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>α</mi><mo>∗</mo><mo
    stretchy="false">(</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mi
    mathvariant="normal">/</mi><mi>α</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn><mo
    stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{CELU}(x)
    = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</annotation></semantics></math>CELU(x)=max(0,x)+min(0,α∗(exp(x/α)−1)).
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`leaky_relu`](generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu
    "torch.nn.functional.leaky_relu") | Applies element-wise, <math><semantics><mrow><mtext>LeakyReLU</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mtext>negative_slope</mtext><mo>∗</mo><mi>min</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope}
    * \min(0, x)</annotation></semantics></math>LeakyReLU(x)=max(0,x)+negative_slope∗min(0,x)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`leaky_relu_`](generated/torch.nn.functional.leaky_relu_.html#torch.nn.functional.leaky_relu_
    "torch.nn.functional.leaky_relu_") | In-place version of [`leaky_relu()`](generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu
    "torch.nn.functional.leaky_relu"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`prelu`](generated/torch.nn.functional.prelu.html#torch.nn.functional.prelu
    "torch.nn.functional.prelu") | Applies element-wise the function <math><semantics><mrow><mtext>PReLU</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mtext>weight</mtext><mo>∗</mo><mi>min</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)</annotation></semantics></math>PReLU(x)=max(0,x)+weight∗min(0,x)
    where weight is a learnable parameter. |'
  prefs: []
  type: TYPE_TB
- en: '| [`rrelu`](generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu
    "torch.nn.functional.rrelu") | Randomized leaky ReLU. |'
  prefs: []
  type: TYPE_TB
- en: '| [`rrelu_`](generated/torch.nn.functional.rrelu_.html#torch.nn.functional.rrelu_
    "torch.nn.functional.rrelu_") | In-place version of [`rrelu()`](generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu
    "torch.nn.functional.rrelu"). |'
  prefs: []
  type: TYPE_TB
- en: '| [`glu`](generated/torch.nn.functional.glu.html#torch.nn.functional.glu "torch.nn.functional.glu")
    | The gated linear unit. |'
  prefs: []
  type: TYPE_TB
- en: '| [`gelu`](generated/torch.nn.functional.gelu.html#torch.nn.functional.gelu
    "torch.nn.functional.gelu") | When the approximate argument is ''none'', it applies
    element-wise the function <math><semantics><mrow><mtext>GELU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo
    stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>∗</mo><mi mathvariant="normal">Φ</mi><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\text{GELU}(x) = x * \Phi(x)</annotation></semantics></math>GELU(x)=x∗Φ(x)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`logsigmoid`](generated/torch.nn.functional.logsigmoid.html#torch.nn.functional.logsigmoid
    "torch.nn.functional.logsigmoid") | Applies element-wise <math><semantics><mrow><mtext>LogSigmoid</mtext><mo
    stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo
    fence="true">(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mo>−</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo
    fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{LogSigmoid}(x_i)
    = \log \left(\frac{1}{1 + \exp(-x_i)}\right)</annotation></semantics></math>LogSigmoid(xi​)=log(1+exp(−xi​)1​)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`hardshrink`](generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink
    "torch.nn.functional.hardshrink") | Applies the hard shrinkage function element-wise
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`tanhshrink`](generated/torch.nn.functional.tanhshrink.html#torch.nn.functional.tanhshrink
    "torch.nn.functional.tanhshrink") | Applies element-wise, <math><semantics><mrow><mtext>Tanhshrink</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>−</mo><mtext>Tanh</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\text{Tanhshrink}(x) = x - \text{Tanh}(x)</annotation></semantics></math>Tanhshrink(x)=x−Tanh(x)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`softsign`](generated/torch.nn.functional.softsign.html#torch.nn.functional.softsign
    "torch.nn.functional.softsign") | Applies element-wise, the function <math><semantics><mrow><mtext>SoftSign</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>x</mi><mrow><mn>1</mn><mo>+</mo><mi
    mathvariant="normal">∣</mi><mi>x</mi><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\text{SoftSign}(x) = \frac{x}{1 + &#124;x&#124;}</annotation></semantics></math>SoftSign(x)=1+∣x∣x​
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`softplus`](generated/torch.nn.functional.softplus.html#torch.nn.functional.softplus
    "torch.nn.functional.softplus") | Applies element-wise, the function <math><semantics><mrow><mtext>Softplus</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>β</mi></mfrac><mo>∗</mo><mi>log</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>β</mi><mo>∗</mo><mi>x</mi><mo
    stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Softplus}(x)
    = \frac{1}{\beta} * \log(1 + \exp(\beta * x))</annotation></semantics></math>Softplus(x)=β1​∗log(1+exp(β∗x)).
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`softmin`](generated/torch.nn.functional.softmin.html#torch.nn.functional.softmin
    "torch.nn.functional.softmin") | Apply a softmin function. |'
  prefs: []
  type: TYPE_TB
- en: '| [`softmax`](generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax
    "torch.nn.functional.softmax") | Apply a softmax function. |'
  prefs: []
  type: TYPE_TB
- en: '| [`softshrink`](generated/torch.nn.functional.softshrink.html#torch.nn.functional.softshrink
    "torch.nn.functional.softshrink") | Applies the soft shrinkage function elementwise
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`gumbel_softmax`](generated/torch.nn.functional.gumbel_softmax.html#torch.nn.functional.gumbel_softmax
    "torch.nn.functional.gumbel_softmax") | Sample from the Gumbel-Softmax distribution
    ([Link 1](https://arxiv.org/abs/1611.00712) [Link 2](https://arxiv.org/abs/1611.01144))
    and optionally discretize. |'
  prefs: []
  type: TYPE_TB
- en: '| [`log_softmax`](generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax
    "torch.nn.functional.log_softmax") | Apply a softmax followed by a logarithm.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`tanh`](generated/torch.nn.functional.tanh.html#torch.nn.functional.tanh
    "torch.nn.functional.tanh") | Applies element-wise, <math><semantics><mrow><mtext>Tanh</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>exp</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>exp</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x)
    + \exp(-x)}</annotation></semantics></math>Tanh(x)=tanh(x)=exp(x)+exp(−x)exp(x)−exp(−x)​
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`sigmoid`](generated/torch.nn.functional.sigmoid.html#torch.nn.functional.sigmoid
    "torch.nn.functional.sigmoid") | Applies the element-wise function <math><semantics><mrow><mtext>Sigmoid</mtext><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo
    stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}</annotation></semantics></math>Sigmoid(x)=1+exp(−x)1​
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`hardsigmoid`](generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid
    "torch.nn.functional.hardsigmoid") | Apply the Hardsigmoid function element-wise.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`silu`](generated/torch.nn.functional.silu.html#torch.nn.functional.silu
    "torch.nn.functional.silu") | Apply the Sigmoid Linear Unit (SiLU) function, element-wise.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`mish`](generated/torch.nn.functional.mish.html#torch.nn.functional.mish
    "torch.nn.functional.mish") | Apply the Mish function, element-wise. |'
  prefs: []
  type: TYPE_TB
- en: '| [`batch_norm`](generated/torch.nn.functional.batch_norm.html#torch.nn.functional.batch_norm
    "torch.nn.functional.batch_norm") | Apply Batch Normalization for each channel
    across a batch of data. |'
  prefs: []
  type: TYPE_TB
- en: '| [`group_norm`](generated/torch.nn.functional.group_norm.html#torch.nn.functional.group_norm
    "torch.nn.functional.group_norm") | Apply Group Normalization for last certain
    number of dimensions. |'
  prefs: []
  type: TYPE_TB
- en: '| [`instance_norm`](generated/torch.nn.functional.instance_norm.html#torch.nn.functional.instance_norm
    "torch.nn.functional.instance_norm") | Apply Instance Normalization independently
    for each channel in every data sample within a batch. |'
  prefs: []
  type: TYPE_TB
- en: '| [`layer_norm`](generated/torch.nn.functional.layer_norm.html#torch.nn.functional.layer_norm
    "torch.nn.functional.layer_norm") | Apply Layer Normalization for last certain
    number of dimensions. |'
  prefs: []
  type: TYPE_TB
- en: '| [`local_response_norm`](generated/torch.nn.functional.local_response_norm.html#torch.nn.functional.local_response_norm
    "torch.nn.functional.local_response_norm") | Apply local response normalization
    over an input signal. |'
  prefs: []
  type: TYPE_TB
- en: '| [`normalize`](generated/torch.nn.functional.normalize.html#torch.nn.functional.normalize
    "torch.nn.functional.normalize") | Perform <math><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation
    encoding="application/x-tex">L_p</annotation></semantics></math>Lp​ normalization
    of inputs over specified dimension. |'
  prefs: []
  type: TYPE_TB
- en: Linear functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`linear`](generated/torch.nn.functional.linear.html#torch.nn.functional.linear
    "torch.nn.functional.linear") | Applies a linear transformation to the incoming
    data: <math><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation
    encoding="application/x-tex">y = xA^T + b</annotation></semantics></math>y=xAT+b.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`bilinear`](generated/torch.nn.functional.bilinear.html#torch.nn.functional.bilinear
    "torch.nn.functional.bilinear") | Applies a bilinear transformation to the incoming
    data: <math><semantics><mrow><mi>y</mi><mo>=</mo><msubsup><mi>x</mi><mn>1</mn><mi>T</mi></msubsup><mi>A</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi></mrow><annotation
    encoding="application/x-tex">y = x_1^T A x_2 + b</annotation></semantics></math>y=x1T​Ax2​+b
    |'
  prefs: []
  type: TYPE_TB
- en: Dropout functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`dropout`](generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout
    "torch.nn.functional.dropout") | During training, randomly zeroes some elements
    of the input tensor with probability `p`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`alpha_dropout`](generated/torch.nn.functional.alpha_dropout.html#torch.nn.functional.alpha_dropout
    "torch.nn.functional.alpha_dropout") | Apply alpha dropout to the input. |'
  prefs: []
  type: TYPE_TB
- en: '| [`feature_alpha_dropout`](generated/torch.nn.functional.feature_alpha_dropout.html#torch.nn.functional.feature_alpha_dropout
    "torch.nn.functional.feature_alpha_dropout") | Randomly masks out entire channels
    (a channel is a feature map). |'
  prefs: []
  type: TYPE_TB
- en: '| [`dropout1d`](generated/torch.nn.functional.dropout1d.html#torch.nn.functional.dropout1d
    "torch.nn.functional.dropout1d") | Randomly zero out entire channels (a channel
    is a 1D feature map). |'
  prefs: []
  type: TYPE_TB
- en: '| [`dropout2d`](generated/torch.nn.functional.dropout2d.html#torch.nn.functional.dropout2d
    "torch.nn.functional.dropout2d") | Randomly zero out entire channels (a channel
    is a 2D feature map). |'
  prefs: []
  type: TYPE_TB
- en: '| [`dropout3d`](generated/torch.nn.functional.dropout3d.html#torch.nn.functional.dropout3d
    "torch.nn.functional.dropout3d") | Randomly zero out entire channels (a channel
    is a 3D feature map). |'
  prefs: []
  type: TYPE_TB
- en: Sparse functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`embedding`](generated/torch.nn.functional.embedding.html#torch.nn.functional.embedding
    "torch.nn.functional.embedding") | Generate a simple lookup table that looks up
    embeddings in a fixed dictionary and size. |'
  prefs: []
  type: TYPE_TB
- en: '| [`embedding_bag`](generated/torch.nn.functional.embedding_bag.html#torch.nn.functional.embedding_bag
    "torch.nn.functional.embedding_bag") | Compute sums, means or maxes of bags of
    embeddings. |'
  prefs: []
  type: TYPE_TB
- en: '| [`one_hot`](generated/torch.nn.functional.one_hot.html#torch.nn.functional.one_hot
    "torch.nn.functional.one_hot") | Takes LongTensor with index values of shape `(*)`
    and returns a tensor of shape `(*, num_classes)` that have zeros everywhere except
    where the index of last dimension matches the corresponding value of the input
    tensor, in which case it will be 1. |'
  prefs: []
  type: TYPE_TB
- en: Distance functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`pairwise_distance`](generated/torch.nn.functional.pairwise_distance.html#torch.nn.functional.pairwise_distance
    "torch.nn.functional.pairwise_distance") | See [`torch.nn.PairwiseDistance`](generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance
    "torch.nn.PairwiseDistance") for details |'
  prefs: []
  type: TYPE_TB
- en: '| [`cosine_similarity`](generated/torch.nn.functional.cosine_similarity.html#torch.nn.functional.cosine_similarity
    "torch.nn.functional.cosine_similarity") | Returns cosine similarity between `x1`
    and `x2`, computed along dim. |'
  prefs: []
  type: TYPE_TB
- en: '| [`pdist`](generated/torch.nn.functional.pdist.html#torch.nn.functional.pdist
    "torch.nn.functional.pdist") | Computes the p-norm distance between every pair
    of row vectors in the input. |'
  prefs: []
  type: TYPE_TB
- en: Loss functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`binary_cross_entropy`](generated/torch.nn.functional.binary_cross_entropy.html#torch.nn.functional.binary_cross_entropy
    "torch.nn.functional.binary_cross_entropy") | Measure Binary Cross Entropy between
    the target and input probabilities. |'
  prefs: []
  type: TYPE_TB
- en: '| [`binary_cross_entropy_with_logits`](generated/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits
    "torch.nn.functional.binary_cross_entropy_with_logits") | Calculate Binary Cross
    Entropy between target and input logits. |'
  prefs: []
  type: TYPE_TB
- en: '| [`poisson_nll_loss`](generated/torch.nn.functional.poisson_nll_loss.html#torch.nn.functional.poisson_nll_loss
    "torch.nn.functional.poisson_nll_loss") | Poisson negative log likelihood loss.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`cosine_embedding_loss`](generated/torch.nn.functional.cosine_embedding_loss.html#torch.nn.functional.cosine_embedding_loss
    "torch.nn.functional.cosine_embedding_loss") | See [`CosineEmbeddingLoss`](generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss
    "torch.nn.CosineEmbeddingLoss") for details. |'
  prefs: []
  type: TYPE_TB
- en: '| [`cross_entropy`](generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy
    "torch.nn.functional.cross_entropy") | Compute the cross entropy loss between
    input logits and target. |'
  prefs: []
  type: TYPE_TB
- en: '| [`ctc_loss`](generated/torch.nn.functional.ctc_loss.html#torch.nn.functional.ctc_loss
    "torch.nn.functional.ctc_loss") | Apply the Connectionist Temporal Classification
    loss. |'
  prefs: []
  type: TYPE_TB
- en: '| [`gaussian_nll_loss`](generated/torch.nn.functional.gaussian_nll_loss.html#torch.nn.functional.gaussian_nll_loss
    "torch.nn.functional.gaussian_nll_loss") | Gaussian negative log likelihood loss.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`hinge_embedding_loss`](generated/torch.nn.functional.hinge_embedding_loss.html#torch.nn.functional.hinge_embedding_loss
    "torch.nn.functional.hinge_embedding_loss") | See [`HingeEmbeddingLoss`](generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss
    "torch.nn.HingeEmbeddingLoss") for details. |'
  prefs: []
  type: TYPE_TB
- en: '| [`kl_div`](generated/torch.nn.functional.kl_div.html#torch.nn.functional.kl_div
    "torch.nn.functional.kl_div") | Compute the KL Divergence loss. |'
  prefs: []
  type: TYPE_TB
- en: '| [`l1_loss`](generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss
    "torch.nn.functional.l1_loss") | Function that takes the mean element-wise absolute
    value difference. |'
  prefs: []
  type: TYPE_TB
- en: '| [`mse_loss`](generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss
    "torch.nn.functional.mse_loss") | Measures the element-wise mean squared error.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`margin_ranking_loss`](generated/torch.nn.functional.margin_ranking_loss.html#torch.nn.functional.margin_ranking_loss
    "torch.nn.functional.margin_ranking_loss") | See [`MarginRankingLoss`](generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss
    "torch.nn.MarginRankingLoss") for details. |'
  prefs: []
  type: TYPE_TB
- en: '| [`multilabel_margin_loss`](generated/torch.nn.functional.multilabel_margin_loss.html#torch.nn.functional.multilabel_margin_loss
    "torch.nn.functional.multilabel_margin_loss") | See [`MultiLabelMarginLoss`](generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss
    "torch.nn.MultiLabelMarginLoss") for details. |'
  prefs: []
  type: TYPE_TB
- en: '| [`multilabel_soft_margin_loss`](generated/torch.nn.functional.multilabel_soft_margin_loss.html#torch.nn.functional.multilabel_soft_margin_loss
    "torch.nn.functional.multilabel_soft_margin_loss") | See [`MultiLabelSoftMarginLoss`](generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss
    "torch.nn.MultiLabelSoftMarginLoss") for details. |'
  prefs: []
  type: TYPE_TB
- en: '| [`multi_margin_loss`](generated/torch.nn.functional.multi_margin_loss.html#torch.nn.functional.multi_margin_loss
    "torch.nn.functional.multi_margin_loss") | See [`MultiMarginLoss`](generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss
    "torch.nn.MultiMarginLoss") for details. |'
  prefs: []
  type: TYPE_TB
- en: '| [`nll_loss`](generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss
    "torch.nn.functional.nll_loss") | Compute the negative log likelihood loss. |'
  prefs: []
  type: TYPE_TB
- en: '| [`huber_loss`](generated/torch.nn.functional.huber_loss.html#torch.nn.functional.huber_loss
    "torch.nn.functional.huber_loss") | Compute the Huber loss. |'
  prefs: []
  type: TYPE_TB
- en: '| [`smooth_l1_loss`](generated/torch.nn.functional.smooth_l1_loss.html#torch.nn.functional.smooth_l1_loss
    "torch.nn.functional.smooth_l1_loss") | Compute the Smooth L1 loss. |'
  prefs: []
  type: TYPE_TB
- en: '| [`soft_margin_loss`](generated/torch.nn.functional.soft_margin_loss.html#torch.nn.functional.soft_margin_loss
    "torch.nn.functional.soft_margin_loss") | See [`SoftMarginLoss`](generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss
    "torch.nn.SoftMarginLoss") for details. |'
  prefs: []
  type: TYPE_TB
- en: '| [`triplet_margin_loss`](generated/torch.nn.functional.triplet_margin_loss.html#torch.nn.functional.triplet_margin_loss
    "torch.nn.functional.triplet_margin_loss") | Compute the triplet loss between
    given input tensors and a margin greater than 0. |'
  prefs: []
  type: TYPE_TB
- en: '| [`triplet_margin_with_distance_loss`](generated/torch.nn.functional.triplet_margin_with_distance_loss.html#torch.nn.functional.triplet_margin_with_distance_loss
    "torch.nn.functional.triplet_margin_with_distance_loss") | Compute the triplet
    margin loss for input tensors using a custom distance function. |'
  prefs: []
  type: TYPE_TB
- en: Vision functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| [`pixel_shuffle`](generated/torch.nn.functional.pixel_shuffle.html#torch.nn.functional.pixel_shuffle
    "torch.nn.functional.pixel_shuffle") | Rearranges elements in a tensor of shape
    <math><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo>×</mo><msup><mi>r</mi><mn>2</mn></msup><mo
    separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">(*, C \times r^2, H, W)</annotation></semantics></math>(∗,C×r2,H,W)
    to a tensor of shape <math><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo
    separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo>×</mo><mi>r</mi><mo
    separator="true">,</mo><mi>W</mi><mo>×</mo><mi>r</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">(*, C, H \times r, W \times r)</annotation></semantics></math>(∗,C,H×r,W×r),
    where r is the `upscale_factor`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`pixel_unshuffle`](generated/torch.nn.functional.pixel_unshuffle.html#torch.nn.functional.pixel_unshuffle
    "torch.nn.functional.pixel_unshuffle") | Reverses the [`PixelShuffle`](generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle
    "torch.nn.PixelShuffle") operation by rearranging elements in a tensor of shape
    <math><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo
    separator="true">,</mo><mi>H</mi><mo>×</mo><mi>r</mi><mo separator="true">,</mo><mi>W</mi><mo>×</mo><mi>r</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, C,
    H \times r, W \times r)</annotation></semantics></math>(∗,C,H×r,W×r) to a tensor
    of shape <math><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo>×</mo><msup><mi>r</mi><mn>2</mn></msup><mo
    separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">(*, C \times r^2, H, W)</annotation></semantics></math>(∗,C×r2,H,W),
    where r is the `downscale_factor`. |'
  prefs: []
  type: TYPE_TB
- en: '| [`pad`](generated/torch.nn.functional.pad.html#torch.nn.functional.pad "torch.nn.functional.pad")
    | Pads tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| [`interpolate`](generated/torch.nn.functional.interpolate.html#torch.nn.functional.interpolate
    "torch.nn.functional.interpolate") | Down/up samples the input. |'
  prefs: []
  type: TYPE_TB
- en: '| [`upsample`](generated/torch.nn.functional.upsample.html#torch.nn.functional.upsample
    "torch.nn.functional.upsample") | Upsample input. |'
  prefs: []
  type: TYPE_TB
- en: '| [`upsample_nearest`](generated/torch.nn.functional.upsample_nearest.html#torch.nn.functional.upsample_nearest
    "torch.nn.functional.upsample_nearest") | Upsamples the input, using nearest neighbours''
    pixel values. |'
  prefs: []
  type: TYPE_TB
- en: '| [`upsample_bilinear`](generated/torch.nn.functional.upsample_bilinear.html#torch.nn.functional.upsample_bilinear
    "torch.nn.functional.upsample_bilinear") | Upsamples the input, using bilinear
    upsampling. |'
  prefs: []
  type: TYPE_TB
- en: '| [`grid_sample`](generated/torch.nn.functional.grid_sample.html#torch.nn.functional.grid_sample
    "torch.nn.functional.grid_sample") | Compute grid sample. |'
  prefs: []
  type: TYPE_TB
- en: '| [`affine_grid`](generated/torch.nn.functional.affine_grid.html#torch.nn.functional.affine_grid
    "torch.nn.functional.affine_grid") | Generate 2D or 3D flow field (sampling grid),
    given a batch of affine matrices `theta`. |'
  prefs: []
  type: TYPE_TB
- en: DataParallel functions (multi-GPU, distributed)[](#dataparallel-functions-multi-gpu-distributed
    "Permalink to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: data_parallel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| `torch.nn.parallel.data_parallel` | Evaluate module(input) in parallel across
    the GPUs given in device_ids. |'
  prefs: []
  type: TYPE_TB
