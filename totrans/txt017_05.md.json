["```py\nclass torchtext.nn.MultiheadAttentionContainer(nhead, in_proj_container, attention_layer, out_proj, batch_first=False)\u00b6\n```", "```py\n__init__(nhead, in_proj_container, attention_layer, out_proj, batch_first=False) \u2192 None\u00b6\n```", "```py\n>>> import torch\n>>> from torchtext.nn import MultiheadAttentionContainer, InProjContainer, ScaledDotProduct\n>>> embed_dim, num_heads, bsz = 10, 5, 64\n>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\n torch.nn.Linear(embed_dim, embed_dim),\n torch.nn.Linear(embed_dim, embed_dim))\n>>> MHA = MultiheadAttentionContainer(num_heads,\n in_proj_container,\n ScaledDotProduct(),\n torch.nn.Linear(embed_dim, embed_dim))\n>>> query = torch.rand((21, bsz, embed_dim))\n>>> key = value = torch.rand((16, bsz, embed_dim))\n>>> attn_output, attn_weights = MHA(query, key, value)\n>>> print(attn_output.shape)\n>>> torch.Size([21, 64, 10]) \n```", "```py\nforward(query: Tensor, key: Tensor, value: Tensor, attn_mask: Optional[Tensor] = None, bias_k: Optional[Tensor] = None, bias_v: Optional[Tensor] = None) \u2192 Tuple[Tensor, Tensor]\u00b6\n```", "```py\nclass torchtext.nn.InProjContainer(query_proj, key_proj, value_proj)\u00b6\n```", "```py\n__init__(query_proj, key_proj, value_proj) \u2192 None\u00b6\n```", "```py\nforward(query: Tensor, key: Tensor, value: Tensor) \u2192 Tuple[Tensor, Tensor, Tensor]\u00b6\n```", "```py\n>>> import torch\n>>> from torchtext.nn import InProjContainer\n>>> embed_dim, bsz = 10, 64\n>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\n torch.nn.Linear(embed_dim, embed_dim),\n torch.nn.Linear(embed_dim, embed_dim))\n>>> q = torch.rand((5, bsz, embed_dim))\n>>> k = v = torch.rand((6, bsz, embed_dim))\n>>> q, k, v = in_proj_container(q, k, v) \n```", "```py\nclass torchtext.nn.ScaledDotProduct(dropout=0.0, batch_first=False)\u00b6\n```", "```py\n__init__(dropout=0.0, batch_first=False) \u2192 None\u00b6\n```", "```py\n>>> import torch, torchtext\n>>> SDP = torchtext.nn.ScaledDotProduct(dropout=0.1)\n>>> q = torch.randn(21, 256, 3)\n>>> k = v = torch.randn(21, 256, 3)\n>>> attn_output, attn_weights = SDP(q, k, v)\n>>> print(attn_output.shape, attn_weights.shape)\ntorch.Size([21, 256, 3]) torch.Size([256, 21, 21]) \n```", "```py\nforward(query: Tensor, key: Tensor, value: Tensor, attn_mask: Optional[Tensor] = None, bias_k: Optional[Tensor] = None, bias_v: Optional[Tensor] = None) \u2192 Tuple[Tensor, Tensor]\u00b6\n```"]