["```py\nimport torch\nfrom torch import nn\nimport torch.nn.utils.prune as prune\nimport torch.nn.functional as F \n```", "```py\n[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")(\"cuda\" if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available \"torch.cuda.is_available\")() else \"cpu\")\n\nclass LeNet([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")):\n    def __init__(self):\n        super([LeNet](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\"), self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n        self.conv1 = [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(1, 6, 5)\n        self.conv2 = [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")(6, 16, 5)\n        self.fc1 = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(16 * 5 * 5, 120)  # 5x5 image dimension\n        self.fc2 = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(120, 84)\n        self.fc3 = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")(84, 10)\n\n    def forward(self, x):\n        x = [F.max_pool2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d \"torch.nn.functional.max_pool2d\")([F.relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu \"torch.nn.functional.relu\")(self.conv1(x)), (2, 2))\n        x = [F.max_pool2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d \"torch.nn.functional.max_pool2d\")([F.relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu \"torch.nn.functional.relu\")(self.conv2(x)), 2)\n        x = x.view(-1, int(x.nelement() / x.shape[0]))\n        x = [F.relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu \"torch.nn.functional.relu\")(self.fc1(x))\n        x = [F.relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu \"torch.nn.functional.relu\")(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = [LeNet](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")().to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device \"torch.device\")) \n```", "```py\n[module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\") = [model.conv1](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")\nprint(list([module.named_parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters \"torch.nn.Module.named_parameters\")())) \n```", "```py\n[('weight', Parameter containing:\ntensor([[[[ 0.1529,  0.1660, -0.0469,  0.1837, -0.0438],\n          [ 0.0404, -0.0974,  0.1175,  0.1763, -0.1467],\n          [ 0.1738,  0.0374,  0.1478,  0.0271,  0.0964],\n          [-0.0282,  0.1542,  0.0296, -0.0934,  0.0510],\n          [-0.0921, -0.0235, -0.0812,  0.1327, -0.1579]]],\n\n        [[[-0.0922, -0.0565, -0.1203,  0.0189, -0.1975],\n          [ 0.1806, -0.1699,  0.1544,  0.0333, -0.0649],\n          [ 0.1236,  0.0312,  0.1616,  0.0219, -0.0631],\n          [ 0.0537, -0.0542,  0.0842,  0.1786,  0.1156],\n          [-0.0874,  0.1155,  0.0358,  0.1016, -0.1219]]],\n\n        [[[-0.1980, -0.0773, -0.1534,  0.1641,  0.0576],\n          [ 0.0828,  0.0633, -0.0035,  0.1565, -0.1421],\n          [ 0.0126, -0.1365,  0.0617, -0.0689,  0.0613],\n          [-0.0417,  0.1659, -0.1185, -0.1193, -0.1193],\n          [ 0.1799,  0.0667,  0.1925, -0.1651, -0.1984]]],\n\n        [[[-0.1565, -0.1345,  0.0810,  0.0716,  0.1662],\n          [-0.1033, -0.1363,  0.1061, -0.0808,  0.1214],\n          [-0.0475,  0.1144, -0.1554, -0.1009,  0.0610],\n          [ 0.0423, -0.0510,  0.1192,  0.1360, -0.1450],\n          [-0.1068,  0.1831, -0.0675, -0.0709, -0.1935]]],\n\n        [[[-0.1145,  0.0500, -0.0264, -0.1452,  0.0047],\n          [-0.1366, -0.1697, -0.1101, -0.1750, -0.1273],\n          [ 0.1999,  0.0378,  0.0616, -0.1865, -0.1314],\n          [-0.0666,  0.0313, -0.1760, -0.0862, -0.1197],\n          [ 0.0006, -0.0744, -0.0139, -0.1355, -0.1373]]],\n\n        [[[-0.1167, -0.0685, -0.1579,  0.1677, -0.0397],\n          [ 0.1721,  0.0623, -0.1694,  0.1384, -0.0550],\n          [-0.0767, -0.1660, -0.1988,  0.0572, -0.0437],\n          [ 0.0779, -0.1641,  0.1485, -0.1468, -0.0345],\n          [ 0.0418,  0.1033,  0.1615,  0.1822, -0.1586]]]], device='cuda:0',\n       requires_grad=True)), ('bias', Parameter containing:\ntensor([ 0.0503, -0.0860, -0.0219, -0.1497,  0.1822, -0.1468], device='cuda:0',\n       requires_grad=True))] \n```", "```py\nprint(list([module.named_buffers](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers \"torch.nn.Module.named_buffers\")())) \n```", "```py\n[] \n```", "```py\n[prune.random_unstructured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured \"torch.nn.utils.prune.random_unstructured\")([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), name=\"weight\", amount=0.3) \n```", "```py\nConv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) \n```", "```py\nprint(list([module.named_parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters \"torch.nn.Module.named_parameters\")())) \n```", "```py\n[('bias', Parameter containing:\ntensor([ 0.0503, -0.0860, -0.0219, -0.1497,  0.1822, -0.1468], device='cuda:0',\n       requires_grad=True)), ('weight_orig', Parameter containing:\ntensor([[[[ 0.1529,  0.1660, -0.0469,  0.1837, -0.0438],\n          [ 0.0404, -0.0974,  0.1175,  0.1763, -0.1467],\n          [ 0.1738,  0.0374,  0.1478,  0.0271,  0.0964],\n          [-0.0282,  0.1542,  0.0296, -0.0934,  0.0510],\n          [-0.0921, -0.0235, -0.0812,  0.1327, -0.1579]]],\n\n        [[[-0.0922, -0.0565, -0.1203,  0.0189, -0.1975],\n          [ 0.1806, -0.1699,  0.1544,  0.0333, -0.0649],\n          [ 0.1236,  0.0312,  0.1616,  0.0219, -0.0631],\n          [ 0.0537, -0.0542,  0.0842,  0.1786,  0.1156],\n          [-0.0874,  0.1155,  0.0358,  0.1016, -0.1219]]],\n\n        [[[-0.1980, -0.0773, -0.1534,  0.1641,  0.0576],\n          [ 0.0828,  0.0633, -0.0035,  0.1565, -0.1421],\n          [ 0.0126, -0.1365,  0.0617, -0.0689,  0.0613],\n          [-0.0417,  0.1659, -0.1185, -0.1193, -0.1193],\n          [ 0.1799,  0.0667,  0.1925, -0.1651, -0.1984]]],\n\n        [[[-0.1565, -0.1345,  0.0810,  0.0716,  0.1662],\n          [-0.1033, -0.1363,  0.1061, -0.0808,  0.1214],\n          [-0.0475,  0.1144, -0.1554, -0.1009,  0.0610],\n          [ 0.0423, -0.0510,  0.1192,  0.1360, -0.1450],\n          [-0.1068,  0.1831, -0.0675, -0.0709, -0.1935]]],\n\n        [[[-0.1145,  0.0500, -0.0264, -0.1452,  0.0047],\n          [-0.1366, -0.1697, -0.1101, -0.1750, -0.1273],\n          [ 0.1999,  0.0378,  0.0616, -0.1865, -0.1314],\n          [-0.0666,  0.0313, -0.1760, -0.0862, -0.1197],\n          [ 0.0006, -0.0744, -0.0139, -0.1355, -0.1373]]],\n\n        [[[-0.1167, -0.0685, -0.1579,  0.1677, -0.0397],\n          [ 0.1721,  0.0623, -0.1694,  0.1384, -0.0550],\n          [-0.0767, -0.1660, -0.1988,  0.0572, -0.0437],\n          [ 0.0779, -0.1641,  0.1485, -0.1468, -0.0345],\n          [ 0.0418,  0.1033,  0.1615,  0.1822, -0.1586]]]], device='cuda:0',\n       requires_grad=True))] \n```", "```py\nprint(list([module.named_buffers](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers \"torch.nn.Module.named_buffers\")())) \n```", "```py\n[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n          [1., 0., 1., 1., 1.],\n          [1., 0., 0., 1., 1.],\n          [1., 0., 1., 1., 1.],\n          [1., 0., 0., 1., 1.]]],\n\n        [[[1., 1., 1., 0., 1.],\n          [1., 1., 1., 1., 1.],\n          [0., 1., 1., 1., 0.],\n          [1., 1., 0., 1., 0.],\n          [0., 1., 0., 1., 1.]]],\n\n        [[[1., 0., 0., 0., 1.],\n          [1., 0., 1., 1., 0.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 0., 1., 1., 0.]]],\n\n        [[[1., 1., 1., 1., 1.],\n          [0., 1., 1., 1., 0.],\n          [1., 1., 1., 0., 1.],\n          [0., 0., 1., 1., 1.],\n          [1., 1., 0., 1., 1.]]],\n\n        [[[1., 0., 1., 1., 1.],\n          [1., 1., 0., 0., 0.],\n          [1., 1., 0., 0., 0.],\n          [0., 1., 1., 0., 1.],\n          [1., 0., 0., 0., 1.]]],\n\n        [[[1., 0., 1., 0., 1.],\n          [0., 1., 1., 1., 1.],\n          [1., 1., 0., 1., 0.],\n          [1., 1., 1., 1., 1.],\n          [1., 0., 0., 1., 1.]]]], device='cuda:0'))] \n```", "```py\nprint([module.weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) \n```", "```py\ntensor([[[[ 0.1529,  0.1660, -0.0469,  0.1837, -0.0438],\n          [ 0.0404, -0.0000,  0.1175,  0.1763, -0.1467],\n          [ 0.1738,  0.0000,  0.0000,  0.0271,  0.0964],\n          [-0.0282,  0.0000,  0.0296, -0.0934,  0.0510],\n          [-0.0921, -0.0000, -0.0000,  0.1327, -0.1579]]],\n\n        [[[-0.0922, -0.0565, -0.1203,  0.0000, -0.1975],\n          [ 0.1806, -0.1699,  0.1544,  0.0333, -0.0649],\n          [ 0.0000,  0.0312,  0.1616,  0.0219, -0.0000],\n          [ 0.0537, -0.0542,  0.0000,  0.1786,  0.0000],\n          [-0.0000,  0.1155,  0.0000,  0.1016, -0.1219]]],\n\n        [[[-0.1980, -0.0000, -0.0000,  0.0000,  0.0576],\n          [ 0.0828,  0.0000, -0.0035,  0.1565, -0.0000],\n          [ 0.0126, -0.1365,  0.0617, -0.0689,  0.0613],\n          [-0.0417,  0.1659, -0.1185, -0.1193, -0.1193],\n          [ 0.1799,  0.0000,  0.1925, -0.1651, -0.0000]]],\n\n        [[[-0.1565, -0.1345,  0.0810,  0.0716,  0.1662],\n          [-0.0000, -0.1363,  0.1061, -0.0808,  0.0000],\n          [-0.0475,  0.1144, -0.1554, -0.0000,  0.0610],\n          [ 0.0000, -0.0000,  0.1192,  0.1360, -0.1450],\n          [-0.1068,  0.1831, -0.0000, -0.0709, -0.1935]]],\n\n        [[[-0.1145,  0.0000, -0.0264, -0.1452,  0.0047],\n          [-0.1366, -0.1697, -0.0000, -0.0000, -0.0000],\n          [ 0.1999,  0.0378,  0.0000, -0.0000, -0.0000],\n          [-0.0000,  0.0313, -0.1760, -0.0000, -0.1197],\n          [ 0.0006, -0.0000, -0.0000, -0.0000, -0.1373]]],\n\n        [[[-0.1167, -0.0000, -0.1579,  0.0000, -0.0397],\n          [ 0.0000,  0.0623, -0.1694,  0.1384, -0.0550],\n          [-0.0767, -0.1660, -0.0000,  0.0572, -0.0000],\n          [ 0.0779, -0.1641,  0.1485, -0.1468, -0.0345],\n          [ 0.0418,  0.0000,  0.0000,  0.1822, -0.1586]]]], device='cuda:0',\n       grad_fn=<MulBackward0>) \n```", "```py\nprint([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")._forward_pre_hooks) \n```", "```py\nOrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7f4c36e10e50>)]) \n```", "```py\n[prune.l1_unstructured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured \"torch.nn.utils.prune.l1_unstructured\")([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), name=\"bias\", amount=3) \n```", "```py\nConv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) \n```", "```py\nprint(list([module.named_parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters \"torch.nn.Module.named_parameters\")())) \n```", "```py\n[('weight_orig', Parameter containing:\ntensor([[[[ 0.1529,  0.1660, -0.0469,  0.1837, -0.0438],\n          [ 0.0404, -0.0974,  0.1175,  0.1763, -0.1467],\n          [ 0.1738,  0.0374,  0.1478,  0.0271,  0.0964],\n          [-0.0282,  0.1542,  0.0296, -0.0934,  0.0510],\n          [-0.0921, -0.0235, -0.0812,  0.1327, -0.1579]]],\n\n        [[[-0.0922, -0.0565, -0.1203,  0.0189, -0.1975],\n          [ 0.1806, -0.1699,  0.1544,  0.0333, -0.0649],\n          [ 0.1236,  0.0312,  0.1616,  0.0219, -0.0631],\n          [ 0.0537, -0.0542,  0.0842,  0.1786,  0.1156],\n          [-0.0874,  0.1155,  0.0358,  0.1016, -0.1219]]],\n\n        [[[-0.1980, -0.0773, -0.1534,  0.1641,  0.0576],\n          [ 0.0828,  0.0633, -0.0035,  0.1565, -0.1421],\n          [ 0.0126, -0.1365,  0.0617, -0.0689,  0.0613],\n          [-0.0417,  0.1659, -0.1185, -0.1193, -0.1193],\n          [ 0.1799,  0.0667,  0.1925, -0.1651, -0.1984]]],\n\n        [[[-0.1565, -0.1345,  0.0810,  0.0716,  0.1662],\n          [-0.1033, -0.1363,  0.1061, -0.0808,  0.1214],\n          [-0.0475,  0.1144, -0.1554, -0.1009,  0.0610],\n          [ 0.0423, -0.0510,  0.1192,  0.1360, -0.1450],\n          [-0.1068,  0.1831, -0.0675, -0.0709, -0.1935]]],\n\n        [[[-0.1145,  0.0500, -0.0264, -0.1452,  0.0047],\n          [-0.1366, -0.1697, -0.1101, -0.1750, -0.1273],\n          [ 0.1999,  0.0378,  0.0616, -0.1865, -0.1314],\n          [-0.0666,  0.0313, -0.1760, -0.0862, -0.1197],\n          [ 0.0006, -0.0744, -0.0139, -0.1355, -0.1373]]],\n\n        [[[-0.1167, -0.0685, -0.1579,  0.1677, -0.0397],\n          [ 0.1721,  0.0623, -0.1694,  0.1384, -0.0550],\n          [-0.0767, -0.1660, -0.1988,  0.0572, -0.0437],\n          [ 0.0779, -0.1641,  0.1485, -0.1468, -0.0345],\n          [ 0.0418,  0.1033,  0.1615,  0.1822, -0.1586]]]], device='cuda:0',\n       requires_grad=True)), ('bias_orig', Parameter containing:\ntensor([ 0.0503, -0.0860, -0.0219, -0.1497,  0.1822, -0.1468], device='cuda:0',\n       requires_grad=True))] \n```", "```py\nprint(list([module.named_buffers](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers \"torch.nn.Module.named_buffers\")())) \n```", "```py\n[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n          [1., 0., 1., 1., 1.],\n          [1., 0., 0., 1., 1.],\n          [1., 0., 1., 1., 1.],\n          [1., 0., 0., 1., 1.]]],\n\n        [[[1., 1., 1., 0., 1.],\n          [1., 1., 1., 1., 1.],\n          [0., 1., 1., 1., 0.],\n          [1., 1., 0., 1., 0.],\n          [0., 1., 0., 1., 1.]]],\n\n        [[[1., 0., 0., 0., 1.],\n          [1., 0., 1., 1., 0.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 0., 1., 1., 0.]]],\n\n        [[[1., 1., 1., 1., 1.],\n          [0., 1., 1., 1., 0.],\n          [1., 1., 1., 0., 1.],\n          [0., 0., 1., 1., 1.],\n          [1., 1., 0., 1., 1.]]],\n\n        [[[1., 0., 1., 1., 1.],\n          [1., 1., 0., 0., 0.],\n          [1., 1., 0., 0., 0.],\n          [0., 1., 1., 0., 1.],\n          [1., 0., 0., 0., 1.]]],\n\n        [[[1., 0., 1., 0., 1.],\n          [0., 1., 1., 1., 1.],\n          [1., 1., 0., 1., 0.],\n          [1., 1., 1., 1., 1.],\n          [1., 0., 0., 1., 1.]]]], device='cuda:0')), ('bias_mask', tensor([0., 0., 0., 1., 1., 1.], device='cuda:0'))] \n```", "```py\nprint([module.bias](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\")) \n```", "```py\ntensor([ 0.0000, -0.0000, -0.0000, -0.1497,  0.1822, -0.1468], device='cuda:0',\n       grad_fn=<MulBackward0>) \n```", "```py\nprint([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")._forward_pre_hooks) \n```", "```py\nOrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7f4c36e10e50>), (1, <torch.nn.utils.prune.L1Unstructured object at 0x7f4c632815d0>)]) \n```", "```py\n[prune.ln_structured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.ln_structured.html#torch.nn.utils.prune.ln_structured \"torch.nn.utils.prune.ln_structured\")([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), name=\"weight\", amount=0.5, n=2, dim=0)\n\n# As we can verify, this will zero out all the connections corresponding to\n# 50% (3 out of 6) of the channels, while preserving the action of the\n# previous mask.\nprint([module.weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) \n```", "```py\ntensor([[[[ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000]]],\n\n        [[[-0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000]]],\n\n        [[[-0.1980, -0.0000, -0.0000,  0.0000,  0.0576],\n          [ 0.0828,  0.0000, -0.0035,  0.1565, -0.0000],\n          [ 0.0126, -0.1365,  0.0617, -0.0689,  0.0613],\n          [-0.0417,  0.1659, -0.1185, -0.1193, -0.1193],\n          [ 0.1799,  0.0000,  0.1925, -0.1651, -0.0000]]],\n\n        [[[-0.1565, -0.1345,  0.0810,  0.0716,  0.1662],\n          [-0.0000, -0.1363,  0.1061, -0.0808,  0.0000],\n          [-0.0475,  0.1144, -0.1554, -0.0000,  0.0610],\n          [ 0.0000, -0.0000,  0.1192,  0.1360, -0.1450],\n          [-0.1068,  0.1831, -0.0000, -0.0709, -0.1935]]],\n\n        [[[-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n          [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000]]],\n\n        [[[-0.1167, -0.0000, -0.1579,  0.0000, -0.0397],\n          [ 0.0000,  0.0623, -0.1694,  0.1384, -0.0550],\n          [-0.0767, -0.1660, -0.0000,  0.0572, -0.0000],\n          [ 0.0779, -0.1641,  0.1485, -0.1468, -0.0345],\n          [ 0.0418,  0.0000,  0.0000,  0.1822, -0.1586]]]], device='cuda:0',\n       grad_fn=<MulBackward0>) \n```", "```py\nfor [hook](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer \"torch.nn.utils.prune.PruningContainer\") in [module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")._forward_pre_hooks.values():\n    if [hook](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer \"torch.nn.utils.prune.PruningContainer\")._tensor_name == \"weight\":  # select out the correct hook\n        break\n\nprint(list([hook](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer \"torch.nn.utils.prune.PruningContainer\")))  # pruning history in the container \n```", "```py\n[<torch.nn.utils.prune.RandomUnstructured object at 0x7f4c36e10e50>, <torch.nn.utils.prune.LnStructured object at 0x7f4c63282c50>] \n```", "```py\nprint([model.state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict \"torch.nn.Module.state_dict\")().keys()) \n```", "```py\nodict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']) \n```", "```py\nprint(list([module.named_parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters \"torch.nn.Module.named_parameters\")())) \n```", "```py\n[('weight_orig', Parameter containing:\ntensor([[[[ 0.1529,  0.1660, -0.0469,  0.1837, -0.0438],\n          [ 0.0404, -0.0974,  0.1175,  0.1763, -0.1467],\n          [ 0.1738,  0.0374,  0.1478,  0.0271,  0.0964],\n          [-0.0282,  0.1542,  0.0296, -0.0934,  0.0510],\n          [-0.0921, -0.0235, -0.0812,  0.1327, -0.1579]]],\n\n        [[[-0.0922, -0.0565, -0.1203,  0.0189, -0.1975],\n          [ 0.1806, -0.1699,  0.1544,  0.0333, -0.0649],\n          [ 0.1236,  0.0312,  0.1616,  0.0219, -0.0631],\n          [ 0.0537, -0.0542,  0.0842,  0.1786,  0.1156],\n          [-0.0874,  0.1155,  0.0358,  0.1016, -0.1219]]],\n\n        [[[-0.1980, -0.0773, -0.1534,  0.1641,  0.0576],\n          [ 0.0828,  0.0633, -0.0035,  0.1565, -0.1421],\n          [ 0.0126, -0.1365,  0.0617, -0.0689,  0.0613],\n          [-0.0417,  0.1659, -0.1185, -0.1193, -0.1193],\n          [ 0.1799,  0.0667,  0.1925, -0.1651, -0.1984]]],\n\n        [[[-0.1565, -0.1345,  0.0810,  0.0716,  0.1662],\n          [-0.1033, -0.1363,  0.1061, -0.0808,  0.1214],\n          [-0.0475,  0.1144, -0.1554, -0.1009,  0.0610],\n          [ 0.0423, -0.0510,  0.1192,  0.1360, -0.1450],\n          [-0.1068,  0.1831, -0.0675, -0.0709, -0.1935]]],\n\n        [[[-0.1145,  0.0500, -0.0264, -0.1452,  0.0047],\n          [-0.1366, -0.1697, -0.1101, -0.1750, -0.1273],\n          [ 0.1999,  0.0378,  0.0616, -0.1865, -0.1314],\n          [-0.0666,  0.0313, -0.1760, -0.0862, -0.1197],\n          [ 0.0006, -0.0744, -0.0139, -0.1355, -0.1373]]],\n\n        [[[-0.1167, -0.0685, -0.1579,  0.1677, -0.0397],\n          [ 0.1721,  0.0623, -0.1694,  0.1384, -0.0550],\n          [-0.0767, -0.1660, -0.1988,  0.0572, -0.0437],\n          [ 0.0779, -0.1641,  0.1485, -0.1468, -0.0345],\n          [ 0.0418,  0.1033,  0.1615,  0.1822, -0.1586]]]], device='cuda:0',\n       requires_grad=True)), ('bias_orig', Parameter containing:\ntensor([ 0.0503, -0.0860, -0.0219, -0.1497,  0.1822, -0.1468], device='cuda:0',\n       requires_grad=True))] \n```", "```py\nprint(list([module.named_buffers](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers \"torch.nn.Module.named_buffers\")())) \n```", "```py\n[('weight_mask', tensor([[[[0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.]]],\n\n        [[[0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.]]],\n\n        [[[1., 0., 0., 0., 1.],\n          [1., 0., 1., 1., 0.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 0., 1., 1., 0.]]],\n\n        [[[1., 1., 1., 1., 1.],\n          [0., 1., 1., 1., 0.],\n          [1., 1., 1., 0., 1.],\n          [0., 0., 1., 1., 1.],\n          [1., 1., 0., 1., 1.]]],\n\n        [[[0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.]]],\n\n        [[[1., 0., 1., 0., 1.],\n          [0., 1., 1., 1., 1.],\n          [1., 1., 0., 1., 0.],\n          [1., 1., 1., 1., 1.],\n          [1., 0., 0., 1., 1.]]]], device='cuda:0')), ('bias_mask', tensor([0., 0., 0., 1., 1., 1.], device='cuda:0'))] \n```", "```py\nprint([module.weight](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) \n```", "```py\ntensor([[[[ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000]]],\n\n        [[[-0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000]]],\n\n        [[[-0.1980, -0.0000, -0.0000,  0.0000,  0.0576],\n          [ 0.0828,  0.0000, -0.0035,  0.1565, -0.0000],\n          [ 0.0126, -0.1365,  0.0617, -0.0689,  0.0613],\n          [-0.0417,  0.1659, -0.1185, -0.1193, -0.1193],\n          [ 0.1799,  0.0000,  0.1925, -0.1651, -0.0000]]],\n\n        [[[-0.1565, -0.1345,  0.0810,  0.0716,  0.1662],\n          [-0.0000, -0.1363,  0.1061, -0.0808,  0.0000],\n          [-0.0475,  0.1144, -0.1554, -0.0000,  0.0610],\n          [ 0.0000, -0.0000,  0.1192,  0.1360, -0.1450],\n          [-0.1068,  0.1831, -0.0000, -0.0709, -0.1935]]],\n\n        [[[-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n          [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000]]],\n\n        [[[-0.1167, -0.0000, -0.1579,  0.0000, -0.0397],\n          [ 0.0000,  0.0623, -0.1694,  0.1384, -0.0550],\n          [-0.0767, -0.1660, -0.0000,  0.0572, -0.0000],\n          [ 0.0779, -0.1641,  0.1485, -0.1468, -0.0345],\n          [ 0.0418,  0.0000,  0.0000,  0.1822, -0.1586]]]], device='cuda:0',\n       grad_fn=<MulBackward0>) \n```", "```py\n[prune.remove](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.remove.html#torch.nn.utils.prune.remove \"torch.nn.utils.prune.remove\")([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), 'weight')\nprint(list([module.named_parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters \"torch.nn.Module.named_parameters\")())) \n```", "```py\n[('bias_orig', Parameter containing:\ntensor([ 0.0503, -0.0860, -0.0219, -0.1497,  0.1822, -0.1468], device='cuda:0',\n       requires_grad=True)), ('weight', Parameter containing:\ntensor([[[[ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000]]],\n\n        [[[-0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000]]],\n\n        [[[-0.1980, -0.0000, -0.0000,  0.0000,  0.0576],\n          [ 0.0828,  0.0000, -0.0035,  0.1565, -0.0000],\n          [ 0.0126, -0.1365,  0.0617, -0.0689,  0.0613],\n          [-0.0417,  0.1659, -0.1185, -0.1193, -0.1193],\n          [ 0.1799,  0.0000,  0.1925, -0.1651, -0.0000]]],\n\n        [[[-0.1565, -0.1345,  0.0810,  0.0716,  0.1662],\n          [-0.0000, -0.1363,  0.1061, -0.0808,  0.0000],\n          [-0.0475,  0.1144, -0.1554, -0.0000,  0.0610],\n          [ 0.0000, -0.0000,  0.1192,  0.1360, -0.1450],\n          [-0.1068,  0.1831, -0.0000, -0.0709, -0.1935]]],\n\n        [[[-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n          [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000]]],\n\n        [[[-0.1167, -0.0000, -0.1579,  0.0000, -0.0397],\n          [ 0.0000,  0.0623, -0.1694,  0.1384, -0.0550],\n          [-0.0767, -0.1660, -0.0000,  0.0572, -0.0000],\n          [ 0.0779, -0.1641,  0.1485, -0.1468, -0.0345],\n          [ 0.0418,  0.0000,  0.0000,  0.1822, -0.1586]]]], device='cuda:0',\n       requires_grad=True))] \n```", "```py\nprint(list([module.named_buffers](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers \"torch.nn.Module.named_buffers\")())) \n```", "```py\n[('bias_mask', tensor([0., 0., 0., 1., 1., 1.], device='cuda:0'))] \n```", "```py\nnew_model = [LeNet](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")()\nfor name, [module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\") in [new_model.named_modules](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_modules \"torch.nn.Module.named_modules\")():\n    # prune 20% of connections in all 2D-conv layers\n    if isinstance([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\")):\n        [prune.l1_unstructured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured \"torch.nn.utils.prune.l1_unstructured\")([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), name='weight', amount=0.2)\n    # prune 40% of connections in all linear layers\n    elif isinstance([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\")):\n        [prune.l1_unstructured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured \"torch.nn.utils.prune.l1_unstructured\")([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), name='weight', amount=0.4)\n\nprint(dict([new_model.named_buffers](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers \"torch.nn.Module.named_buffers\")()).keys())  # to verify that all masks exist \n```", "```py\ndict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask']) \n```", "```py\nmodel = [LeNet](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")()\n\nparameters_to_prune = (\n    ([model.conv1](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\"), 'weight'),\n    ([model.conv2](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d \"torch.nn.Conv2d\"), 'weight'),\n    ([model.fc1](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), 'weight'),\n    ([model.fc2](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), 'weight'),\n    ([model.fc3](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), 'weight'),\n)\n\n[prune.global_unstructured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured \"torch.nn.utils.prune.global_unstructured\")(\n    parameters_to_prune,\n    pruning_method=[prune.L1Unstructured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured \"torch.nn.utils.prune.L1Unstructured\"),\n    amount=0.2,\n) \n```", "```py\nprint(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float([torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")([model.conv1.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") == 0))\n        / float([model.conv1.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\").nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float([torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")([model.conv2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") == 0))\n        / float([model.conv2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\").nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float([torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")([model.fc1.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") == 0))\n        / float([model.fc1.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\").nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float([torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")([model.fc2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") == 0))\n        / float([model.fc2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\").nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float([torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")([model.fc3.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") == 0))\n        / float([model.fc3.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\").nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")([model.conv1.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") == 0)\n            + [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")([model.conv2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") == 0)\n            + [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")([model.fc1.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") == 0)\n            + [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")([model.fc2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") == 0)\n            + [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum \"torch.sum\")([model.fc3.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\") == 0)\n        )\n        / float(\n            [model.conv1.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\").nelement()\n            + [model.conv2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\").nelement()\n            + [model.fc1.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\").nelement()\n            + [model.fc2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\").nelement()\n            + [model.fc3.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter \"torch.nn.parameter.Parameter\").nelement()\n        )\n    )\n) \n```", "```py\nSparsity in conv1.weight: 4.67%\nSparsity in conv2.weight: 13.92%\nSparsity in fc1.weight: 22.16%\nSparsity in fc2.weight: 12.10%\nSparsity in fc3.weight: 11.31%\nGlobal sparsity: 20.00% \n```", "```py\nclass FooBarPruningMethod([prune.BasePruningMethod](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod \"torch.nn.utils.prune.BasePruningMethod\")):\n  \"\"\"Prune every other entry in a tensor\n \"\"\"\n    PRUNING_TYPE = 'unstructured'\n\n    def compute_mask(self, t, default_mask):\n        mask = default_mask.clone()\n        mask.view(-1)[::2] = 0\n        return mask \n```", "```py\ndef foobar_unstructured([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), name):\n  \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n by removing every other entry in the tensors.\n Modifies module in place (and also return the modified module)\n by:\n 1) adding a named buffer called `name+'_mask'` corresponding to the\n binary mask applied to the parameter `name` by the pruning method.\n The parameter `name` is replaced by its pruned version, while the\n original (unpruned) parameter is stored in a new parameter named\n `name+'_orig'`.\n\n Args:\n module (nn.Module): module containing the tensor to prune\n name (string): parameter name within `module` on which pruning\n will act.\n\n Returns:\n module (nn.Module): modified (i.e. pruned) version of the input\n module\n\n Examples:\n >>> m = nn.Linear(3, 4)\n >>> foobar_unstructured(m, name='bias')\n \"\"\"\n    [FooBarPruningMethod.apply](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.apply \"torch.nn.utils.prune.BasePruningMethod.apply\")([module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), name)\n    return [module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\") \n```", "```py\nmodel = [LeNet](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\")()\nfoobar_unstructured([model.fc3](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear \"torch.nn.Linear\"), name='bias')\n\nprint([model.fc3.bias_mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor \"torch.Tensor\")) \n```", "```py\ntensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.]) \n```"]