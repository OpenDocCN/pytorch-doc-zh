["```py\n// This header is all you need to do the C++ portions of this\n// tutorial\n#include  <torch/script.h>\n// This header is what defines the custom class registration\n// behavior specifically. script.h already includes this, but\n// we include it here so you know it exists in case you want\n// to look at the API or implementation.\n#include  <torch/custom_class.h>\n\n#include  <string>\n#include  <vector>\n\ntemplate  <class  T>\nstruct  MyStackClass  :  torch::CustomClassHolder  {\n  std::vector<T>  stack_;\n  MyStackClass(std::vector<T>  init)  :  stack_(init.begin(),  init.end())  {}\n\n  void  push(T  x)  {\n  stack_.push_back(x);\n  }\n  T  pop()  {\n  auto  val  =  stack_.back();\n  stack_.pop_back();\n  return  val;\n  }\n\n  c10::intrusive_ptr<MyStackClass>  clone()  const  {\n  return  c10::make_intrusive<MyStackClass>(stack_);\n  }\n\n  void  merge(const  c10::intrusive_ptr<MyStackClass>&  c)  {\n  for  (auto&  elem  :  c->stack_)  {\n  push(elem);\n  }\n  }\n}; \n```", "```py\n// Notice a few things:\n// - We pass the class to be registered as a template parameter to\n//   `torch::class_`. In this instance, we've passed the\n//   specialization of the MyStackClass class ``MyStackClass<std::string>``.\n//   In general, you cannot register a non-specialized template\n//   class. For non-templated classes, you can just pass the\n//   class name directly as the template parameter.\n// - The arguments passed to the constructor make up the \"qualified name\"\n//   of the class. In this case, the registered class will appear in\n//   Python and C++ as `torch.classes.my_classes.MyStackClass`. We call\n//   the first argument the \"namespace\" and the second argument the\n//   actual class name.\nTORCH_LIBRARY(my_classes,  m)  {\n  m.class_<MyStackClass<std::string>>(\"MyStackClass\")\n  // The following line registers the contructor of our MyStackClass\n  // class that takes a single `std::vector<std::string>` argument,\n  // i.e. it exposes the C++ method `MyStackClass(std::vector<T> init)`.\n  // Currently, we do not support registering overloaded\n  // constructors, so for now you can only `def()` one instance of\n  // `torch::init`.\n  .def(torch::init<std::vector<std::string>>())\n  // The next line registers a stateless (i.e. no captures) C++ lambda\n  // function as a method. Note that a lambda function must take a\n  // `c10::intrusive_ptr<YourClass>` (or some const/ref version of that)\n  // as the first argument. Other arguments can be whatever you want.\n  .def(\"top\",  [](const  c10::intrusive_ptr<MyStackClass<std::string>>&  self)  {\n  return  self->stack_.back();\n  })\n  // The following four lines expose methods of the MyStackClass<std::string>\n  // class as-is. `torch::class_` will automatically examine the\n  // argument and return types of the passed-in method pointers and\n  // expose these to Python and TorchScript accordingly. Finally, notice\n  // that we must take the *address* of the fully-qualified method name,\n  // i.e. use the unary `&` operator, due to C++ typing rules.\n  .def(\"push\",  &MyStackClass<std::string>::push)\n  .def(\"pop\",  &MyStackClass<std::string>::pop)\n  .def(\"clone\",  &MyStackClass<std::string>::clone)\n  .def(\"merge\",  &MyStackClass<std::string>::merge)\n  ;\n} \n```", "```py\ncmake_minimum_required(VERSION  3.1  FATAL_ERROR)\nproject(custom_class)\n\nfind_package(Torch  REQUIRED)\n\n# Define our library target\nadd_library(custom_class  SHARED  class.cpp)\nset(CMAKE_CXX_STANDARD  14)\n# Link against LibTorch\ntarget_link_libraries(custom_class  \"${TORCH_LIBRARIES}\") \n```", "```py\ncustom_class_project/\n  class.cpp\n  CMakeLists.txt\n  build/ \n```", "```py\n$  cd  build\n$  cmake  -DCMAKE_PREFIX_PATH=\"$(python  -c  'import torch.utils; print(torch.utils.cmake_prefix_path)')\"  ..\n  --  The  C  compiler  identification  is  GNU  7.3.1\n  --  The  CXX  compiler  identification  is  GNU  7.3.1\n  --  Check  for  working  C  compiler:  /opt/rh/devtoolset-7/root/usr/bin/cc\n  --  Check  for  working  C  compiler:  /opt/rh/devtoolset-7/root/usr/bin/cc  --  works\n  --  Detecting  C  compiler  ABI  info\n  --  Detecting  C  compiler  ABI  info  -  done\n  --  Detecting  C  compile  features\n  --  Detecting  C  compile  features  -  done\n  --  Check  for  working  CXX  compiler:  /opt/rh/devtoolset-7/root/usr/bin/c++\n  --  Check  for  working  CXX  compiler:  /opt/rh/devtoolset-7/root/usr/bin/c++  --  works\n  --  Detecting  CXX  compiler  ABI  info\n  --  Detecting  CXX  compiler  ABI  info  -  done\n  --  Detecting  CXX  compile  features\n  --  Detecting  CXX  compile  features  -  done\n  --  Looking  for  pthread.h\n  --  Looking  for  pthread.h  -  found\n  --  Looking  for  pthread_create\n  --  Looking  for  pthread_create  -  not  found\n  --  Looking  for  pthread_create  in  pthreads\n  --  Looking  for  pthread_create  in  pthreads  -  not  found\n  --  Looking  for  pthread_create  in  pthread\n  --  Looking  for  pthread_create  in  pthread  -  found\n  --  Found  Threads:  TRUE\n  --  Found  torch:  /torchbind_tutorial/libtorch/lib/libtorch.so\n  --  Configuring  done\n  --  Generating  done\n  --  Build  files  have  been  written  to:  /torchbind_tutorial/build\n$  make  -j\n  Scanning  dependencies  of  target  custom_class\n  [  50%]  Building  CXX  object  CMakeFiles/custom_class.dir/class.cpp.o\n  [100%]  Linking  CXX  shared  library  libcustom_class.so\n  [100%]  Built  target  custom_class \n```", "```py\ncustom_class_project/\n  class.cpp\n  CMakeLists.txt\n  build/\n    libcustom_class.so \n```", "```py\nimport torch\n\n# `torch.classes.load_library()` allows you to pass the path to your .so file\n# to load it in and make the custom C++ classes available to both Python and\n# TorchScript\ntorch.classes.load_library(\"build/libcustom_class.so\")\n# You can query the loaded libraries like this:\nprint(torch.classes.loaded_libraries)\n# prints {'/custom_class_project/build/libcustom_class.so'}\n\n# We can find and instantiate our custom C++ class in python by using the\n# `torch.classes` namespace:\n#\n# This instantiation will invoke the MyStackClass(std::vector<T> init)\n# constructor we registered earlier\ns = torch.classes.my_classes.MyStackClass([\"foo\", \"bar\"])\n\n# We can call methods in Python\ns.push(\"pushed\")\nassert s.pop() == \"pushed\"\n\n# Test custom operator\ns.push(\"pushed\")\ntorch.ops.my_classes.manipulate_instance(s)  # acting as s.pop()\nassert s.top() == \"bar\" \n\n# Returning and passing instances of custom classes works as you'd expect\ns2 = s.clone()\ns.merge(s2)\nfor expected in [\"bar\", \"foo\", \"bar\", \"foo\"]:\n    assert s.pop() == expected\n\n# We can also use the class in TorchScript\n# For now, we need to assign the class's type to a local in order to\n# annotate the type on the TorchScript function. This may change\n# in the future.\nMyStackClass = torch.classes.my_classes.MyStackClass\n\n@torch.jit.script\ndef do_stacks(s: MyStackClass):  # We can pass a custom class instance\n    # We can instantiate the class\n    s2 = torch.classes.my_classes.MyStackClass([\"hi\", \"mom\"])\n    s2.merge(s)  # We can call a method on the class\n    # We can also return instances of the class\n    # from TorchScript function/methods\n    return s2.clone(), s2.top()\n\nstack, top = do_stacks(torch.classes.my_classes.MyStackClass([\"wow\"]))\nassert top == \"wow\"\nfor expected in [\"wow\", \"mom\", \"hi\"]:\n    assert stack.pop() == expected \n```", "```py\nimport torch\n\ntorch.classes.load_library('build/libcustom_class.so')\n\nclass Foo(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, s: str) -> str:\n        stack = torch.classes.my_classes.MyStackClass([\"hi\", \"mom\"])\n        return stack.pop() + s\n\nscripted_foo = torch.jit.script(Foo())\nprint(scripted_foo.graph)\n\nscripted_foo.save('foo.pt') \n```", "```py\ncpp_inference_example/\n  infer.cpp\n  CMakeLists.txt\n  foo.pt\n  build/\n  custom_class_project/\n    class.cpp\n    CMakeLists.txt\n    build/ \n```", "```py\n#include  <torch/script.h>\n\n#include  <iostream>\n#include  <memory>\n\nint  main(int  argc,  const  char*  argv[])  {\n  torch::jit::Module  module;\n  try  {\n  // Deserialize the ScriptModule from a file using torch::jit::load().\n  module  =  torch::jit::load(\"foo.pt\");\n  }\n  catch  (const  c10::Error&  e)  {\n  std::cerr  <<  \"error loading the model\\n\";\n  return  -1;\n  }\n\n  std::vector<c10::IValue>  inputs  =  {\"foobarbaz\"};\n  auto  output  =  module.forward(inputs).toString();\n  std::cout  <<  output->string()  <<  std::endl;\n} \n```", "```py\ncmake_minimum_required(VERSION  3.1  FATAL_ERROR)\nproject(infer)\n\nfind_package(Torch  REQUIRED)\n\nadd_subdirectory(custom_class_project)\n\n# Define our library target\nadd_executable(infer  infer.cpp)\nset(CMAKE_CXX_STANDARD  14)\n# Link against LibTorch\ntarget_link_libraries(infer  \"${TORCH_LIBRARIES}\")\n# This is where we link in our libcustom_class code, making our\n# custom class available in our binary.\ntarget_link_libraries(infer  -Wl,--no-as-needed  custom_class) \n```", "```py\n$  cd  build\n$  cmake  -DCMAKE_PREFIX_PATH=\"$(python  -c  'import torch.utils; print(torch.utils.cmake_prefix_path)')\"  ..\n  --  The  C  compiler  identification  is  GNU  7.3.1\n  --  The  CXX  compiler  identification  is  GNU  7.3.1\n  --  Check  for  working  C  compiler:  /opt/rh/devtoolset-7/root/usr/bin/cc\n  --  Check  for  working  C  compiler:  /opt/rh/devtoolset-7/root/usr/bin/cc  --  works\n  --  Detecting  C  compiler  ABI  info\n  --  Detecting  C  compiler  ABI  info  -  done\n  --  Detecting  C  compile  features\n  --  Detecting  C  compile  features  -  done\n  --  Check  for  working  CXX  compiler:  /opt/rh/devtoolset-7/root/usr/bin/c++\n  --  Check  for  working  CXX  compiler:  /opt/rh/devtoolset-7/root/usr/bin/c++  --  works\n  --  Detecting  CXX  compiler  ABI  info\n  --  Detecting  CXX  compiler  ABI  info  -  done\n  --  Detecting  CXX  compile  features\n  --  Detecting  CXX  compile  features  -  done\n  --  Looking  for  pthread.h\n  --  Looking  for  pthread.h  -  found\n  --  Looking  for  pthread_create\n  --  Looking  for  pthread_create  -  not  found\n  --  Looking  for  pthread_create  in  pthreads\n  --  Looking  for  pthread_create  in  pthreads  -  not  found\n  --  Looking  for  pthread_create  in  pthread\n  --  Looking  for  pthread_create  in  pthread  -  found\n  --  Found  Threads:  TRUE\n  --  Found  torch:  /local/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so\n  --  Configuring  done\n  --  Generating  done\n  --  Build  files  have  been  written  to:  /cpp_inference_example/build\n$  make  -j\n  Scanning  dependencies  of  target  custom_class\n  [  25%]  Building  CXX  object  custom_class_project/CMakeFiles/custom_class.dir/class.cpp.o\n  [  50%]  Linking  CXX  shared  library  libcustom_class.so\n  [  50%]  Built  target  custom_class\n  Scanning  dependencies  of  target  infer\n  [  75%]  Building  CXX  object  CMakeFiles/infer.dir/infer.cpp.o\n  [100%]  Linking  CXX  executable  infer\n  [100%]  Built  target  infer \n```", "```py\n$  ./infer\n  momfoobarbaz \n```", "```py\n# export_attr.py\nimport torch\n\ntorch.classes.load_library('build/libcustom_class.so')\n\nclass Foo(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.stack = torch.classes.my_classes.MyStackClass([\"just\", \"testing\"])\n\n    def forward(self, s: str) -> str:\n        return self.stack.pop() + s\n\nscripted_foo = torch.jit.script(Foo())\n\nscripted_foo.save('foo.pt')\nloaded = torch.jit.load('foo.pt')\n\nprint(loaded.stack.pop()) \n```", "```py\n$  python  export_attr.py\nRuntimeError:  Cannot  serialize  custom  bound  C++  class  __torch__.torch.classes.my_classes.MyStackClass.  Please  define  serialization  methods  via  def_pickle  for  this  class.  (pushIValueImpl  at  ../torch/csrc/jit/pickler.cpp:128) \n```", "```py\n // class_<>::def_pickle allows you to define the serialization\n  // and deserialization methods for your C++ class.\n  // Currently, we only support passing stateless lambda functions\n  // as arguments to def_pickle\n  .def_pickle(\n  // __getstate__\n  // This function defines what data structure should be produced\n  // when we serialize an instance of this class. The function\n  // must take a single `self` argument, which is an intrusive_ptr\n  // to the instance of the object. The function can return\n  // any type that is supported as a return value of the TorchScript\n  // custom operator API. In this instance, we've chosen to return\n  // a std::vector<std::string> as the salient data to preserve\n  // from the class.\n  [](const  c10::intrusive_ptr<MyStackClass<std::string>>&  self)\n  ->  std::vector<std::string>  {\n  return  self->stack_;\n  },\n  // __setstate__\n  // This function defines how to create a new instance of the C++\n  // class when we are deserializing. The function must take a\n  // single argument of the same type as the return value of\n  // `__getstate__`. The function must return an intrusive_ptr\n  // to a new instance of the C++ class, initialized however\n  // you would like given the serialized state.\n  [](std::vector<std::string>  state)\n  ->  c10::intrusive_ptr<MyStackClass<std::string>>  {\n  // A convenient way to instantiate an object and get an\n  // intrusive_ptr to it is via `make_intrusive`. We use\n  // that here to allocate an instance of MyStackClass<std::string>\n  // and call the single-argument std::vector<std::string>\n  // constructor with the serialized state.\n  return  c10::make_intrusive<MyStackClass<std::string>>(std::move(state));\n  }); \n```", "```py\n$  python  ../export_attr.py\ntesting \n```", "```py\nc10::intrusive_ptr<MyStackClass<std::string>>  manipulate_instance(const  c10::intrusive_ptr<MyStackClass<std::string>>&  instance)  {\n  instance->pop();\n  return  instance;\n} \n```", "```py\n m.def(\n  \"manipulate_instance(__torch__.torch.classes.my_classes.MyStackClass x) -> __torch__.torch.classes.my_classes.MyStackClass Y\",\n  manipulate_instance\n  ); \n```", "```py\nclass TryCustomOp(torch.nn.Module):\n    def __init__(self):\n        super(TryCustomOp, self).__init__()\n        self.f = torch.classes.my_classes.MyStackClass([\"foo\", \"bar\"])\n\n    def forward(self):\n        return torch.ops.my_classes.manipulate_instance(self.f) \n```"]