# 大规模部署的功能

> 原文：[`pytorch.org/docs/stable/notes/large_scale_deployments.html`](https://pytorch.org/docs/stable/notes/large_scale_deployments.html)

+   全局操作符分析

+   API 使用日志记录

+   将元数据附加到保存的 TorchScript 模型

+   构建环境注意事项

+   常见扩展点

本说明介绍了在更大系统中运行 PyTorch 或在更大组织中操作使用 PyTorch 的多个系统时可能有用的几个扩展点和技巧。

这并不涵盖将模型部署到生产环境的主题。请查看 `torch.jit` 或其中一个相应的教程。

该说明假定您要么在组织中从源代码构建 PyTorch，要么具有在 PyTorch 使用时加载附加代码的静态链接能力。因此，许多钩子都公开为可以在集中位置触发一次的 C++ API，例如在静态初始化代码中。

## 全局操作符分析[](#fleet-wide-operator-profiling "跳转到此标题")

PyTorch 自带 `torch.autograd.profiler`，能够按需测量各个操作符所花费的时间。可以使用相同的机制对运行 PyTorch 的任何进程进行“始终开启”测量。这对于收集在给定进程或整个机器集上运行的 PyTorch 工作负载信息可能很有用。

可以使用 `torch::addGlobalCallback` 为任何操作符调用添加新的回调。钩子将使用描述调用上下文（例如名称）的 `torch::RecordFunction` 结构进行调用。如果启用，`RecordFunction::inputs()` 包含作为 `torch::IValue` 变体类型表示的函数参数。请注意，输入日志记录相对昂贵，因此必须显式启用。

操作符回调还可以访问 `c10::ThreadLocalDebugInfo::get()` 接口，该接口返回一个持有调试信息的结构体指针。可以使用 `at::DebugInfoGuard` 对象提前设置调试信息。调试信息会通过前向（包括异步 `fork` 任务）和反向传递传播，并且对于从应用程序的更高层向操作符回调传递一些额外信息（例如模型 ID）可能很有用。

调用回调会增加一些开销，因此通常最好随机抽样操作符调用。可以通过将可选的抽样率传递给 `torch::addGlobalCallback` 来在每个回调基础上启用此功能。

请注意，`addGlobalCallback` 不是线程安全的，只能在没有运行 PyTorch 操作符时调用。通常，在初始化期间调用它们一次是个好主意。

以下是一个示例：

```py
// Called somewhere in the program beginning
void  init()  {
  // Sample one in a hundred operator runs randomly
  addGlobalCallback(
  RecordFunctionCallback(
  &onFunctionEnter,
  &onFunctionExit)
  .needsInputs(true)
  .samplingProb(0.01)
  );
  // Note, to enable observers in the model calling thread,
  // call enableRecordFunction() in the thread before running a model
}

void  onFunctionEnter(const  RecordFunction&  fn)  {
  std::cerr  <<  "Before function "  <<  fn.name()
  <<  " with "  <<  fn.inputs().size()  <<  " inputs"  <<  std::endl;
}

void  onFunctionExit(const  RecordFunction&  fn)  {
  std::cerr  <<  "After function "  <<  fn.name();
} 
```

## API 使用日志记录

在更广泛的生态系统中运行时，例如在托管作业调度程序中，跟踪调用特定 PyTorch API 的二进制文件通常很有用。在几个重要的 API 点注入了简单的仪器，触发给定的回调。因为通常 PyTorch 在一次性的 Python 脚本中被调用，所以对于每个 API，回调只会在给定进程中触发一次。

`c10::SetAPIUsageHandler` 可用于注册 API 使用仪器处理程序。传递的参数将是用于标识使用点的“api key”，例如 PyTorch 扩展导入的 `python.import` 或如果触发了 TorchScript 编译，则为 `torch.script.compile`。 

```py
SetAPIUsageLogger([](const  std::string&  event_name)  {
  std::cerr  <<  "API was used: "  <<  event_name  <<  std::endl;
}); 
```

开发者注意：可以在 C++ 代码中使用 `C10_LOG_API_USAGE_ONCE("my_api")` 或在 Python 中使用 `torch._C._log_api_usage_once("my.api")` 来添加新的 API 触发点。

## 将元数据附加到保存的 TorchScript 模型[](#attaching-metadata-to-saved-torchscript-models "跳转到此标题")

TorchScript 模块可以保存为捆绑序列化参数和模块代码的存档文件，作为 TorchScript（参见`torch.jit.save()`）。通常方便将附加信息与模型一起捆绑，例如，模型生产者的描述或辅助工件。

可以通过将`_extra_files`参数传递给`torch.jit.save()`和`torch::jit::load`来在保存过程中存储和检索任意二进制数据块。由于 TorchScript 文件是常规 ZIP 存档，额外信息被存储为存档的`extra/`目录中的常规文件。

还有一个全局钩子，允许在当前进程中的任何 TorchScript 存档上附加额外文件。类似于数字相机生成的 JPEG 元数据，可能有助于使用生产者元数据标记模型。示例用法可能如下：

```py
SetExportModuleExtraFilesHook([](const  Module&)  {
  ExtraFilesMap  files;
  files["producer_info.json"]  =  "{\"user\": \""  +  getenv("USER")  +  "\"}";
  return  files;
}); 
```

## 构建环境考虑[](#build-environment-considerations "跳转到此标题")

TorchScript 的编译需要访问原始的 Python 文件，因为它使用 Python 的`inspect.getsource`调用。在某些生产环境中，可能需要显式部署`.py`文件以及预编译的`.pyc`文件。

## 常见扩展点[](#common-extension-points "跳转到此标题")

PyTorch 的 API 通常松散耦合，很容易用专门版本替换组件。常见的扩展点包括：

+   在 C++中实现的自定义运算符 - 详细信息请参阅[教程](https://pytorch.org/tutorials/advanced/cpp_extension.html)。

+   通常可以通过调用相应的 Python 库直接集成自定义数据读取。通过扩展`Dataset`或`IterableDataset`，可以利用`torch.utils.data`的现有功能。
