# torchtext.nn

> 原文：[https://pytorch.org/text/stable/nn_modules.html](https://pytorch.org/text/stable/nn_modules.html)

## MultiheadAttentionContainer[](#multiheadattentioncontainer "跳转到此标题")

```py
class torchtext.nn.MultiheadAttentionContainer(nhead, in_proj_container, attention_layer, out_proj, batch_first=False)¶
```

```py
__init__(nhead, in_proj_container, attention_layer, out_proj, batch_first=False) → None¶
```

一个多头注意力容器

参数：

+   **nhead** - 多头注意力模型中的头数

+   **in_proj_container** - 多头in-projection线性层的容器（又名nn.Linear）。

+   **attention_layer** - 自定义注意力层。从MHA容器发送到注意力层的输入的形状为（...，L，N * H，E / H）用于查询和（...，S，N * H，E / H）用于键/值，而注意力层的输出形状预计为（...，L，N * H，E / H）。如果用户希望整体MultiheadAttentionContainer具有广播，则attention_layer需要支持广播。

+   **out_proj** - 多头输出投影层（又名nn.Linear）。

+   **batch_first** - 如果为`True`，则输入和输出张量将提供为（...，N，L，E）。默认值：`False`

示例::

```py
>>> import torch
>>> from torchtext.nn import MultiheadAttentionContainer, InProjContainer, ScaledDotProduct
>>> embed_dim, num_heads, bsz = 10, 5, 64
>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),
 torch.nn.Linear(embed_dim, embed_dim),
 torch.nn.Linear(embed_dim, embed_dim))
>>> MHA = MultiheadAttentionContainer(num_heads,
 in_proj_container,
 ScaledDotProduct(),
 torch.nn.Linear(embed_dim, embed_dim))
>>> query = torch.rand((21, bsz, embed_dim))
>>> key = value = torch.rand((16, bsz, embed_dim))
>>> attn_output, attn_weights = MHA(query, key, value)
>>> print(attn_output.shape)
>>> torch.Size([21, 64, 10]) 
```

```py
forward(query: Tensor, key: Tensor, value: Tensor, attn_mask: Optional[Tensor] = None, bias_k: Optional[Tensor] = None, bias_v: Optional[Tensor] = None) → Tuple[Tensor, Tensor]¶
```

参数：

+   **查询**（*张量*） - 注意力函数的查询。有关更多详细信息，请参阅“注意力就是一切”。

+   **key**（*张量*） - 注意力函数的键。有关更多详细信息，请参阅“注意力就是一切”。

+   **value**（*张量*） - 注意力函数的值。有关更多详细信息，请参阅“注意力就是一切”。

+   **attn_mask**（*BoolTensor*，*可选*） - 防止注意力集中在某些位置的3D掩码。

+   **bias_k**（*张量*，*可选*） - 要添加到键的一个以上的键和值序列，以在序列维度（dim=-3）上进行增量解码。用户应提供`bias_v`。

+   **bias_v**（*张量*，*可选*） - 要添加到值的一个以上的键和值序列，以在序列维度（dim=-3）上进行增量解码。用户还应提供`bias_k`。

形状：

> +   输入：
> +   
>     > +   查询：\((...，L，N，E)\)
>     > +   
>     > +   键：\((...，S，N，E)\)
>     > +   
>     > +   值：\((...，S，N，E)\)
>     > +   
>     > +   attn_mask，bias_k和bias_v：与注意力层中相应参数的形状相同。
>     > +   
> +   输出：
> +   
>     > +   attn_output：\((...，L，N，E)\)
>     > +   
>     > +   attn_output_weights：\((N * H，L，S)\)
>     > +   
> 注意：具有超过三个维度的查询/键/值输入是可选的（用于广播目的）。MultiheadAttentionContainer模块将在最后三个维度上操作。
> 
> 其中L是目标长度，S是序列长度，H是注意力头数，N是批量大小，E是嵌入维度。

## InProjContainer[](#inprojcontainer "跳转到此标题")

```py
class torchtext.nn.InProjContainer(query_proj, key_proj, value_proj)¶
```

```py
__init__(query_proj, key_proj, value_proj) → None¶
```

在MultiheadAttention中投影查询/键/值的in-proj容器。此模块在将投影的查询/键/值重新整形为多个头之前发生。请参阅“注意力就是一切”论文中图2中的Multi-head Attention底部的线性层。还可以在torchtext.nn.MultiheadAttentionContainer中查看用法示例。

参数：

+   **query_proj** - 用于查询的投影层。典型的投影层是torch.nn.Linear。

+   **key_proj** - 用于键的投影层。典型的投影层是torch.nn.Linear。

+   **value_proj** - 用于值的投影层。典型的投影层是torch.nn.Linear。

```py
forward(query: Tensor, key: Tensor, value: Tensor) → Tuple[Tensor, Tensor, Tensor]¶
```

使用in-proj层对输入序列进行投影。查询/键/值分别简单地传递给查询/键/值_proj的前向函数。

参数：

+   **查询**（*张量*） - 要投影的查询。

+   **key**（*张量*） - 要投影的键。

+   **value**（*张量*） - 要投影的值。

示例::

```py
>>> import torch
>>> from torchtext.nn import InProjContainer
>>> embed_dim, bsz = 10, 64
>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),
 torch.nn.Linear(embed_dim, embed_dim),
 torch.nn.Linear(embed_dim, embed_dim))
>>> q = torch.rand((5, bsz, embed_dim))
>>> k = v = torch.rand((6, bsz, embed_dim))
>>> q, k, v = in_proj_container(q, k, v) 
```

## ScaledDotProduct[](#scaleddotproduct "跳转到此标题")

```py
class torchtext.nn.ScaledDotProduct(dropout=0.0, batch_first=False)¶
```

```py
__init__(dropout=0.0, batch_first=False) → None¶
```

处理投影的查询和键值对以应用缩放的点积注意力。

参数：

+   **辍学**（[*float*](https://docs.python.org/3/library/functions.html#float "(在Python v3.12中)")) - 放弃注意力权重的概率。

+   **batch_first** - 如果为`True`，则输入和输出张量将提供为（批次，序列，特征）。默认值：`False`

示例::

```py
>>> import torch, torchtext
>>> SDP = torchtext.nn.ScaledDotProduct(dropout=0.1)
>>> q = torch.randn(21, 256, 3)
>>> k = v = torch.randn(21, 256, 3)
>>> attn_output, attn_weights = SDP(q, k, v)
>>> print(attn_output.shape, attn_weights.shape)
torch.Size([21, 256, 3]) torch.Size([256, 21, 21]) 
```

```py
forward(query: Tensor, key: Tensor, value: Tensor, attn_mask: Optional[Tensor] = None, bias_k: Optional[Tensor] = None, bias_v: Optional[Tensor] = None) → Tuple[Tensor, Tensor]¶
```

使用投影的键值对进行缩放的点积以更新投影的查询。

参数：

+   **query**（*Tensor*）- 投影的查询

+   **key**（*Tensor*）- 投影的键

+   **value**（*Tensor*）- 投影的值

+   **attn_mask**（*BoolTensor*，*可选*）- 3D掩码，防止注意力集中在某些位置。

+   **attn_mask** - 3D掩码，防止注意力集中在某些位置。

+   **bias_k**（*Tensor*，*可选*）- 一个额外的键和值序列，将添加到键的序列维度（dim=-3）。这些用于增量解码。用户应提供`bias_v`。

+   **bias_v**（*Tensor*，*可选*）- 一个额外的键和值序列，将添加到值的序列维度（dim=-3）。这些用于增量解码。用户还应提供`bias_k`。

形状:

+   query：\((..., L, N * H, E / H)\)

+   键：\((..., S, N * H, E / H)\)

+   value：\((..., S, N * H, E / H)\)

+   attn_mask：\((N * H, L, S)\)，具有`True`的位置不允许参与

    当`False`值将保持不变。

+   bias_k和bias_v：bias：\((1, N * H, E / H)\)

+   输出：\((..., L, N * H, E / H)\)，\((N * H, L, S)\)

注意：具有超过三个维度的查询/键/值输入是可选的（用于广播目的）。

ScaledDotProduct模块将在最后三个维度上操作。

其中L是目标长度，S是源长度，H是注意力头的数量，N是批量大小，E是嵌入维度。
