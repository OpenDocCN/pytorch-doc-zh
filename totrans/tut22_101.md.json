["```py\nimport os\nimport sys\nimport time\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\n# Set up warnings\nimport warnings\nwarnings.filterwarnings(\n    action='ignore',\n    category=DeprecationWarning,\n    module=r'.*'\n)\nwarnings.filterwarnings(\n    action='default',\n    module=r'torch.ao.quantization'\n)\n\n# Specify random seed for repeatable results\ntorch.manual_seed(191009) \n```", "```py\nfrom torch.ao.quantization import QuantStub, DeQuantStub\n\ndef _make_divisible(v, divisor, min_value=None):\n  \"\"\"\n This function is taken from the original tf repo.\n It ensures that all layers have a channel number that is divisible by 8\n It can be seen here:\n https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n :param v:\n :param divisor:\n :param min_value:\n :return:\n \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes, momentum=0.1),\n            # Replace with ReLU\n            nn.ReLU(inplace=False)\n        )\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup, momentum=0.1),\n        ])\n        self.conv = nn.Sequential(*layers)\n        # Replace torch.add with floatfunctional\n        self.skip_add = nn.quantized.FloatFunctional()\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return self.skip_add.add(x, self.conv(x))\n        else:\n            return self.conv(x)\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n  \"\"\"\n MobileNet V2 main class\n Args:\n num_classes (int): Number of classes\n width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n inverted_residual_setting: Network structure\n round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n Set to 1 to turn off rounding\n \"\"\"\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s\n                [1, 16, 1, 1],\n                [6, 24, 2, 2],\n                [6, 32, 3, 2],\n                [6, 64, 4, 2],\n                [6, 96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n            raise ValueError(\"inverted_residual_setting should be non-empty \"\n                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n\n        # building first layer\n        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n        features = [ConvBNReLU(3, input_channel, stride=2)]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c * width_mult, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n        self.quant = QuantStub()\n        self.dequant = DeQuantStub()\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.features(x)\n        x = x.mean([2, 3])\n        x = self.classifier(x)\n        x = self.dequant(x)\n        return x\n\n    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n    # This operation does not change the numerics\n    def fuse_model(self, is_qat=False):\n        fuse_modules = torch.ao.quantization.fuse_modules_qat if is_qat else torch.ao.quantization.fuse_modules\n        for m in self.modules():\n            if type(m) == ConvBNReLU:\n                fuse_modules(m, ['0', '1', '2'], inplace=True)\n            if type(m) == InvertedResidual:\n                for idx in range(len(m.conv)):\n                    if type(m.conv[idx]) == nn.Conv2d:\n                        fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True) \n```", "```py\nclass AverageMeter(object):\n  \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\ndef accuracy(output, target, topk=(1,)):\n  \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\ndef evaluate(model, criterion, data_loader, neval_batches):\n    model.eval()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    cnt = 0\n    with torch.no_grad():\n        for image, target in data_loader:\n            output = model(image)\n            loss = criterion(output, target)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            print('.', end = '')\n            top1.update(acc1[0], image.size(0))\n            top5.update(acc5[0], image.size(0))\n            if cnt >= neval_batches:\n                 return top1, top5\n\n    return top1, top5\n\ndef load_model(model_file):\n    model = MobileNetV2()\n    state_dict = torch.load(model_file)\n    model.load_state_dict(state_dict)\n    model.to('cpu')\n    return model\n\ndef print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p') \n```", "```py\ndef prepare_data_loaders(data_path):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    dataset = torchvision.datasets.ImageNet(\n        data_path, split=\"train\", transform=transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n    dataset_test = torchvision.datasets.ImageNet(\n        data_path, split=\"val\", transform=transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    train_sampler = torch.utils.data.RandomSampler(dataset)\n    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=train_batch_size,\n        sampler=train_sampler)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=eval_batch_size,\n        sampler=test_sampler)\n\n    return data_loader, data_loader_test \n```", "```py\ndata_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv) \n```", "```py\nnum_eval_batches = 1000\n\nprint(\"Size of baseline model\")\nprint_size_of_model(float_model)\n\ntop1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file) \n```", "```py\nnum_calibration_batches = 32\n\nmyModel = load_model(saved_model_dir + float_model_file).to('cpu')\nmyModel.eval()\n\n# Fuse Conv, bn and relu\nmyModel.fuse_model()\n\n# Specify quantization configuration\n# Start with simple min/max range estimation and per-tensor quantization of weights\nmyModel.qconfig = torch.ao.quantization.default_qconfig\nprint(myModel.qconfig)\ntorch.ao.quantization.prepare(myModel, inplace=True)\n\n# Calibrate first\nprint('Post Training Quantization Prepare: Inserting Observers')\nprint('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)\n\n# Calibrate with the training set\nevaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\nprint('Post Training Quantization: Calibration done')\n\n# Convert to quantized model\ntorch.ao.quantization.convert(myModel, inplace=True)\n# You may see a user warning about needing to calibrate the model. This warning can be safely ignored.\n# This warning occurs because not all modules are run in each model runs, so some\n# modules may not be calibrated.\nprint('Post Training Quantization: Convert done')\nprint('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n\nprint(\"Size of model after quantization\")\nprint_size_of_model(myModel)\n\ntop1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg)) \n```", "```py\nper_channel_quantized_model = load_model(saved_model_dir + float_model_file)\nper_channel_quantized_model.eval()\nper_channel_quantized_model.fuse_model()\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\nprint(per_channel_quantized_model.qconfig)\n\ntorch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\nevaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\ntorch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\ntop1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file) \n```", "```py\ndef train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n    model.train()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    avgloss = AverageMeter('Loss', '1.5f')\n\n    cnt = 0\n    for image, target in data_loader:\n        start_time = time.time()\n        print('.', end = '')\n        cnt += 1\n        image, target = image.to(device), target.to(device)\n        output = model(image)\n        loss = criterion(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        top1.update(acc1[0], image.size(0))\n        top5.update(acc5[0], image.size(0))\n        avgloss.update(loss, image.size(0))\n        if cnt >= ntrain_batches:\n            print('Loss', avgloss.avg)\n\n            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n                  .format(top1=top1, top5=top5))\n            return\n\n    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n          .format(top1=top1, top5=top5))\n    return \n```", "```py\nqat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model(is_qat=True)\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86') \n```", "```py\ntorch.ao.quantization.prepare_qat(qat_model, inplace=True)\nprint('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv) \n```", "```py\nnum_train_batches = 20\n\n# QAT takes time and one needs to train over a few epochs.\n# Train and check accuracy after each epoch\nfor nepoch in range(8):\n    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n    if nepoch > 3:\n        # Freeze quantizer parameters\n        qat_model.apply(torch.ao.quantization.disable_observer)\n    if nepoch > 2:\n        # Freeze batch norm mean and variance estimates\n        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n\n    # Check the accuracy after each epoch\n    quantized_model = torch.ao.quantization.convert(qat_model.eval(), inplace=False)\n    quantized_model.eval()\n    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)\n    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg)) \n```", "```py\ndef run_benchmark(model_file, img_loader):\n    elapsed = 0\n    model = torch.jit.load(model_file)\n    model.eval()\n    num_batches = 5\n    # Run the scripted model on a few batches of images\n    for i, (images, target) in enumerate(img_loader):\n        if i < num_batches:\n            start = time.time()\n            output = model(images)\n            end = time.time()\n            elapsed = elapsed + (end-start)\n        else:\n            break\n    num_images = images.size()[0] * num_batches\n\n    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n    return elapsed\n\nrun_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n\nrun_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test) \n```"]