# torchtext.data.utils

> 原文：[`pytorch.org/text/stable/data_utils.html`](https://pytorch.org/text/stable/data_utils.html)

## get_tokenizer[](#get-tokenizer "Permalink to this heading")

```py
torchtext.data.utils.get_tokenizer(tokenizer, language='en')¶
```

为字符串句子生成分词器函数。

参数：

+   **tokenizer** - 分词器函数的名称。如果为 None，则返回 split()函数，该函数通过空格拆分字符串句子。如果为 basic_english，则返回 _basic_english_normalize()函数，该函数首先对字符串进行规范化，然后按空格拆分。如果为可调用函数，则返回该函数。如果为分词器库（例如 spacy、moses、toktok、revtok、subword），则返回相应的库。

+   **language** - 默认为 en

示例

```py
>>> import torchtext
>>> from torchtext.data import get_tokenizer
>>> tokenizer = get_tokenizer("basic_english")
>>> tokens = tokenizer("You can now install TorchText using pip!")
>>> tokens
>>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!'] 
```

## ngrams_iterator[](#ngrams-iterator "Permalink to this heading")

```py
torchtext.data.utils.ngrams_iterator(token_list, ngrams)¶
```

返回一个迭代器，产生给定的标记和它们的 ngrams。

参数：

+   **token_list** - 一个标记列表

+   **ngrams** - ngrams 的数量。

示例

```py
>>> token_list = ['here', 'we', 'are']
>>> list(ngrams_iterator(token_list, 2))
>>> ['here', 'here we', 'we', 'we are', 'are'] 
```
