# torchtext.models

> 原文：[`pytorch.org/text/stable/models.html`](https://pytorch.org/text/stable/models.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

## RobertaBundle

```py
class torchtext.models.RobertaBundle(_params: torchtext.models.RobertaEncoderParams, _path: Optional[str] = None, _head: Optional[torch.nn.Module] = None, transform: Optional[Callable] = None)
```

示例 - 预训练基本 xlmr 编码器

```py
>>> import torch, torchtext
>>> from torchtext.functional import to_tensor
>>> xlmr_base = torchtext.models.XLMR_BASE_ENCODER
>>> model = xlmr_base.get_model()
>>> transform = xlmr_base.transform()
>>> input_batch = ["Hello world", "How are you!"]
>>> model_input = to_tensor(transform(input_batch), padding_value=1)
>>> output = model(model_input)
>>> output.shape
torch.Size([2, 6, 768]) 
```

示例 - 预训练的大型 xlmr 编码器附加到未初始化的分类头部

```py
>>> import torch, torchtext
>>> from torchtext.models import RobertaClassificationHead
>>> from torchtext.functional import to_tensor
>>> xlmr_large = torchtext.models.XLMR_LARGE_ENCODER
>>> classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)
>>> model = xlmr_large.get_model(head=classifier_head)
>>> transform = xlmr_large.transform()
>>> input_batch = ["Hello world", "How are you!"]
>>> model_input = to_tensor(transform(input_batch), padding_value=1)
>>> output = model(model_input)
>>> output.shape
torch.Size([1, 2]) 
```

示例 - 用户指定的配置和检查点

```py
>>> from torchtext.models import RobertaEncoderConf, RobertaBundle, RobertaClassificationHead
>>> model_weights_path = "https://download.pytorch.org/models/text/xlmr.base.encoder.pt"
>>> encoder_conf = RobertaEncoderConf(vocab_size=250002)
>>> classifier_head = RobertaClassificationHead(num_classes=2, input_dim=768)
>>> model = RobertaBundle.build_model(encoder_conf=encoder_conf, head=classifier_head, checkpoint=model_weights_path) 
```

```py
get_model(head: Optional[torch.nn.Module] = None, load_weights: bool = True, freeze_encoder: bool = False, *, dl_kwargs=None) → torchtext.models.RobertaModel
```

参数：

+   **head**（*nn.Module*）- 一个要附加到编码器上以执行特定任务的模块。如果提供，它将替换默认成员头（默认值：`None`）

+   **load_weights**（[*bool*](https://docs.python.org/3/library/functions.html#bool "(在 Python v3.12 中)")）- 指示是否加载可用的权重。（默认值：`True`）

+   **freeze_encoder**（[*bool*](https://docs.python.org/3/library/functions.html#bool "(在 Python v3.12 中)")）- 指示是否冻结编码器权重。（默认值：`False`）

+   **dl_kwargs**（*关键字参数的字典*）- 传递给[`torch.hub.load_state_dict_from_url()`](https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url "(在 PyTorch v2.1 中)")。（默认值：`None`）

## XLMR_BASE_ENCODER

```py
torchtext.models.XLMR_BASE_ENCODER
```

带有基本配置的 XLM-R 编码器

XLM-RoBERTa 模型是在规模上进行无监督跨语言表示学习提出的<https://arxiv.org/abs/1911.02116>。它是一个大型多语言语言模型，训练于 2.5TB 的经过筛选的 CommonCrawl 数据，并基于 RoBERTa 模型架构。

最初由 XLM-RoBERTa 的作者根据 MIT 许可证发布，并以相同的许可证重新分发。[[许可证](https://github.com/pytorch/fairseq/blob/main/LICENSE)，[来源](https://github.com/pytorch/fairseq/tree/main/examples/xlmr#pre-trained-models)]

请参考`torchtext.models.RobertaBundle()`以获取用法。

## XLMR_LARGE_ENCODER

```py
torchtext.models.XLMR_LARGE_ENCODER
```

带有大型配置的 XLM-R 编码器

XLM-RoBERTa 模型是在规模上进行无监督跨语言表示学习提出的<https://arxiv.org/abs/1911.02116>。它是一个大型多语言语言模型，训练于 2.5TB 的经过筛选的 CommonCrawl 数据，并基于 RoBERTa 模型架构。

最初由 XLM-RoBERTa 的作者根据 MIT 许可证发布，并以相同的许可证重新分发。[[许可证](https://github.com/pytorch/fairseq/blob/main/LICENSE)，[来源](https://github.com/pytorch/fairseq/tree/main/examples/xlmr#pre-trained-models)]

请参考`torchtext.models.RobertaBundle()`以获取用法。

## ROBERTA_BASE_ENCODER

```py
torchtext.models.ROBERTA_BASE_ENCODER
```

带有基本配置的 Roberta 编码器

RoBERTa 在 BERT 的预训练过程中进行迭代，包括更长时间地训练模型，使用更大的批次处理更多数据；移除下一个句子预测目标；在更长的序列上进行训练；并动态地改变应用于训练数据的掩码模式。

RoBERTa 模型是在五个数据集的汇总上进行预训练的：BookCorpus，英文维基百科，CC-News，OpenWebText 和 STORIES。这些数据集一起包含超过 160GB 的文本。

最初由 RoBERTa 的作者根据 MIT 许可证发布，并以相同的许可证重新分发。[[许可证](https://github.com/pytorch/fairseq/blob/main/LICENSE)，[来源](https://github.com/pytorch/fairseq/tree/main/examples/roberta#pre-trained-models)]

请参考`torchtext.models.RobertaBundle()`以获取用法。

## ROBERTA_LARGE_ENCODER

```py
torchtext.models.ROBERTA_LARGE_ENCODER
```

带有大型配置的 Roberta 编码器

RoBERTa 在 BERT 的预训练过程上进行了迭代，包括更长时间地训练模型，使用更大的批次处理更多数据；移除下一个句子预测目标；在更长的序列上进行训练；以及动态地改变应用于训练数据的掩码模式。

RoBERTa 模型是在五个数据集的基础上进行预训练的：BookCorpus、英文维基百科、CC-News、OpenWebText 和 STORIES。这些数据集总共包含超过 160GB 的文本。

最初由 RoBERTa 的作者在 MIT 许可下发布，并以相同许可重新分发。[[许可证](https://github.com/pytorch/fairseq/blob/main/LICENSE), [来源](https://github.com/pytorch/fairseq/tree/main/examples/roberta#pre-trained-models)]

请参考`torchtext.models.RobertaBundle()`进行使用。
