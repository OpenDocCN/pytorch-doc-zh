# torch.utils.bottleneck

`torch.utils.bottleneck`是一个可以用作调试程序瓶颈的初始步骤的工具。它汇总了`Python`分析器和`PyTorch`的`autograd`分析器脚本的运行情况。

用命令行运行它

```py
python -m torch.utils.bottleneck /path/to/source/script.py [args] 
```

`[args]`是`script.py`中的任意参数，也可以运行如下代码获取更多使用说明。

```py
python -m torch.utils.bottleneck -h 
```

> 警告 由于您的脚本将被分析，请确保它在有限的时间内退出。
> 
> 警告 由于 CUDA 内核的异步特性，在针对 CUDA 代码运行时，cProfile 输出和 CPU 模式 autograd 分析器可能无法显示以正确的顺序显示：报告的 CPU 时间报告启动内核所用的时间量，但不包括时间内核花在 GPU 上执行，除非操作进行同步。在常规 CPU 模式分析器下，进行同步的 Ops 似乎非常昂贵。在这些情况下，时序不正确，CUDA 模式 autograd 分析器可能会有所帮助。

* * *

> 注意 要决定要查看哪个（纯 CPU 模式或 CUDA 模式）autograd 分析器输出，应首先检查脚本是否受 CPU 限制（“CPU 总时间远远超过 CUDA 总时间”）。如果它受到 CPU 限制，查看 CPU 模式 autograd 分析器的结果将有所帮助。另一方面，如果脚本大部分时间都在 GPU 上执行，那么开始在 CUDA 模式 autograd 分析器的输出中查找负责任的 CUDA 操作符是有意义的。 当然，实际情况要复杂得多，根据你正在评估的模型部分，你的脚本可能不会处于这两个极端之一。如果探查器输出没有帮助，你可以尝试寻找的结果 torch.autograd.profiler.emit_nvtx()用 nvprof。但是，请注意 NVTX 的开销非常高，并且经常会产生严重偏斜的时间表。

* * *

> 警告 如果您正在分析 CUDA 代码，那么 bottleneck 运行的第一个分析器（cProfile）将在其时间报告中包含 CUDA 启动时间（CUDA 缓冲区分配成本）。如果您的瓶颈导致代码比 CUDA 启动时间慢得多，这应该没有关系。

有关分析器的更复杂用途（如多`GPU`情况下），请参阅[https://docs.python.org/3/library/profile.html](https://docs.python.org/3/library/profile.html) 或使用`torch.autograd.profiler.profile()`获取更多信息。

### 译者署名

| 用户名 | 头像 | 职能 | 签名 |
| --- | --- | --- | --- |
| [Song](https://ptorch.com) | ![](img/2018033000352689884.jpeg) | 翻译 | 人生总要追求点什么 |