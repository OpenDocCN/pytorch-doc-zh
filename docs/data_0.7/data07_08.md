# DataPipe 教程

> 原文：[`pytorch.org/data/beta/dp_tutorial.html`](https://pytorch.org/data/beta/dp_tutorial.html)

## 使用 DataPipes[](#using-datapipes "跳转到此标题")

假设我们想要从 CSV 文件中加载数据，以下是步骤：

+   列出目录中的所有 CSV 文件

+   加载 CSV 文件

+   解析 CSV 文件并产生行

+   将数据集分割为训练集和验证集

有一些内置的 DataPipes 可以帮助我们进行上述操作。

+   `FileLister` - 列出目录中的文件

+   `Filter` - 根据给定函数过滤 DataPipe 中的元素

+   `FileOpener` - 消耗文件路径并返回打开的文件流

+   `CSVParser` - 消耗文件流，解析 CSV 内容，并逐行返回解析后的内容

+   `RandomSplitter` - 从源 DataPipe 中随机分割样本为组

例如，`CSVParser`的源代码看起来像这样：

```py
@functional_datapipe("parse_csv")
class CSVParserIterDataPipe(IterDataPipe):
    def __init__(self, dp, **fmtparams) -> None:
        self.dp = dp
        self.fmtparams = fmtparams

    def __iter__(self) -> Iterator[Union[Str_Or_Bytes, Tuple[str, Str_Or_Bytes]]]:
        for path, file in self.source_datapipe:
            stream = self._helper.skip_lines(file)
            stream = self._helper.strip_newline(stream)
            stream = self._helper.decode(stream)
            yield from self._helper.return_path(stream, path=path)  # Returns 1 line at a time as List[str or bytes] 
```

如在不同部分中提到的，DataPipes 可以使用它们的函数形式（推荐）或它们的类构造函数来调用。可以组装一个管道如下：

```py
import torchdata.datapipes as dp

FOLDER = 'path/2/csv/folder'
datapipe = dp.iter.FileLister([FOLDER]).filter(filter_fn=lambda filename: filename.endswith('.csv'))
datapipe = dp.iter.FileOpener(datapipe, mode='rt')
datapipe = datapipe.parse_csv(delimiter=',')
N_ROWS = 10000  # total number of rows of data
train, valid = datapipe.random_split(total_length=N_ROWS, weights={"train": 0.5, "valid": 0.5}, seed=0)

for x in train:  # Iterating through the training dataset
    pass

for y in valid:  # Iterating through the validation dataset
    pass 
```

您可以在这里找到所有内置的 IterDataPipes 和 MapDataPipes。

## 使用 DataLoader[](#working-with-dataloader "跳转到此标题")

在本节中，我们将演示如何使用`DataPipe`与`DataLoader`。大部分情况下，您只需将`dataset=datapipe`作为输入参数传递给`DataLoader`即可使用。有关与`DataLoader`相关的详细文档，请访问[此 PyTorch Core 页面](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)。

请参考此页面关于如何使用`DataPipe`与`DataLoader2`。

对于这个例子，我们首先会有一个帮助函数，生成一些带有随机标签和数据的 CSV 文件。

```py
import csv
import random

def generate_csv(file_label, num_rows: int = 5000, num_features: int = 20) -> None:
    fieldnames = ['label'] + [f'c{i}' for i in range(num_features)]
    writer = csv.DictWriter(open(f"sample_data{file_label}.csv", "w", newline=''), fieldnames=fieldnames)
    writer.writeheader()
    for i in range(num_rows):
        row_data = {col: random.random() for col in fieldnames}
        row_data['label'] = random.randint(0, 9)
        writer.writerow(row_data) 
```

接下来，我们将构建我们的 DataPipes 来读取和解析生成的 CSV 文件。请注意，我们更喜欢将定义的函数传递给 DataPipes，而不是 lambda 函数，因为前者可以与 pickle 序列化。

```py
import numpy as np
import torchdata.datapipes as dp

def filter_for_data(filename):
    return "sample_data" in filename and filename.endswith(".csv")

def row_processor(row):
    return {"label": np.array(row[0], np.int32), "data": np.array(row[1:], dtype=np.float64)}

def build_datapipes(root_dir="."):
    datapipe = dp.iter.FileLister(root_dir)
    datapipe = datapipe.filter(filter_fn=filter_for_data)
    datapipe = datapipe.open_files(mode='rt')
    datapipe = datapipe.parse_csv(delimiter=",", skip_lines=1)
    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader
    datapipe = datapipe.shuffle()
    datapipe = datapipe.map(row_processor)
    return datapipe 
```

最后，我们将把所有内容放在`'__main__'`中，并将 DataPipe 传递给 DataLoader。请注意，如果您选择在 DataLoader 中设置`batch_size > 1`时使用`Batcher`，则您的样本将被分批多次。您应该选择其中一个。

```py
from torch.utils.data import DataLoader

if __name__ == '__main__':
    num_files_to_generate = 3
    for i in range(num_files_to_generate):
        generate_csv(file_label=i, num_rows=10, num_features=3)
    datapipe = build_datapipes()
    dl = DataLoader(dataset=datapipe, batch_size=5, num_workers=2)
    first = next(iter(dl))
    labels, features = first['label'], first['data']
    print(f"Labels batch shape: {labels.size()}")
    print(f"Feature batch shape: {features.size()}")
    print(f"{labels  = }\n{features  = }")
    n_sample = 0
    for row in iter(dl):
        n_sample += 1
    print(f"{n_sample  = }") 
```

以下语句将被打印出来，显示单个批次的标签和特征的形状。

```py
Labels batch shape: torch.Size([5])
Feature batch shape: torch.Size([5, 3])
labels = tensor([8, 9, 5, 9, 7], dtype=torch.int32)
features = tensor([[0.2867, 0.5973, 0.0730],
        [0.7890, 0.9279, 0.7392],
        [0.8930, 0.7434, 0.0780],
        [0.8225, 0.4047, 0.0800],
        [0.1655, 0.0323, 0.5561]], dtype=torch.float64)
n_sample = 12 
```

`n_sample = 12`的原因是因为没有使用`ShardingFilter`（`datapipe.sharding_filter()`），因此每个工作进程将独立返回所有样本。在这种情况下，每个文件有 10 行，共 3 个文件，批量大小为 5，这给我们每个工作进程 6 个批次。有 2 个工作进程，我们从`DataLoader`中得到 12 个总批次。

为了使 DataPipe 分片与`DataLoader`一起工作，我们需要添加以下内容。

```py
def build_datapipes(root_dir="."):
    datapipe = ...
    # Add the following line to `build_datapipes`
    # Note that it is somewhere after `Shuffler` in the DataPipe line, but before expensive operations
    datapipe = datapipe.sharding_filter()
    return datapipe 
```

当我们重新运行时，我们将得到：

```py
...
n_sample = 6 
```

注意：

+   尽量在管道中尽早放置`ShardingFilter`（`datapipe.sharding_filter`），特别是在解码等昂贵操作之前，以避免在工作进程/分布式进程中重复执行这些昂贵操作。

+   对于需要分片的数据源，关键是在`ShardingFilter`之前添加`Shuffler`，以确保数据在分成片之前进行全局洗牌。否则，每个工作进程将始终处理相同的数据片段进行所有时期的训练。这意味着每个批次只包含来自同一数据片段的数据，这会导致训练时准确性较低。然而，对于已经为每个多/分布式进程分片的数据源，不再需要在管道中出现`ShardingFilter`。

+   在某些情况下，将`Shuffler`放在管道中较早的位置可能会导致性能变差，因为某些操作（例如解压缩）在顺序读取时速度更快。在这种情况下，我们建议在洗牌之前解压缩文件（可能在任何数据加载之前）。

您可以在此页面找到各种研究领域的更多 DataPipe 实现示例。

## 实现自定义 DataPipe[](#implementing-a-custom-datapipe "跳转到此标题")

目前，我们已经拥有大量内置的 DataPipes，并且我们希望它们能够涵盖大多数必要的数据处理操作。如果没有一个支持您的需求，您可以创建自己的自定义 DataPipe。

作为一个指导示例，让我们实现一个将可调用函数应用于输入迭代器的`IterDataPipe`。对于`MapDataPipe`，请查看[map](https://github.com/pytorch/pytorch/tree/master/torch/utils/data/datapipes/map)文件夹中的示例，并按照下面的步骤为`__getitem__`方法而不是`__iter__`方法。

### 命名[](#naming "跳转到此标题")

`DataPipe`的命名约定是“操作”-er，后跟`IterDataPipe`或`MapDataPipe`，因为每个 DataPipe 本质上是一个容器，用于将操作应用于从源`DataPipe`中产生的数据。为了简洁起见，在**init**文件中我们将其别名为“Operation-er”。对于我们的`IterDataPipe`示例，我们将模块命名为`MapperIterDataPipe`，并在`torchdata.datapipes`下将其别名为`iter.Mapper`。

对于功能方法的命名约定是`datapipe.<operation>`。例如，`Mapper`的功能方法名称是`map`，因此可以通过`datapipe.map(...)`来调用它。

### 构造函数[](#constructor "跳转到此标题")

数据集现在通常构建为`DataPipes`堆叠，因此每个`DataPipe`通常将源`DataPipe`作为其第一个参数。以下是 Mapper 的简化版本示例：

```py
from torchdata.datapipes.iter import IterDataPipe

class MapperIterDataPipe(IterDataPipe):
    def __init__(self, source_dp: IterDataPipe, fn) -> None:
        super().__init__()
        self.source_dp = source_dp
        self.fn = fn 
```

注意：

+   避免在`__init__`函数中从源 DataPipe 加载数据，以支持延迟数据加载并节省内存。

+   如果`IterDataPipe`实例在内存中保存数据，请注意数据的原地修改。当从实例创建第二个迭代器时，数据可能已经发生了变化。请参考`IterableWrapper`[类](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/datapipes/iter/utils.py)来为每个迭代器`deepcopy`数据。

+   避免使用现有 DataPipes 的功能名称作为变量名。例如，`.filter`是可以用来调用`FilterIterDataPipe`的功能名称。在另一个`IterDataPipe`中有一个名为`filter`的变量可能会导致混淆。

### 迭代器[](#iterator "跳转到此标题")

对于`IterDataPipes`，需要一个`__iter__`函数来从源`IterDataPipe`中消耗数据，然后在`yield`之前对数据应用操作。

```py
class MapperIterDataPipe(IterDataPipe):
    # ... See __init__() defined above

    def __iter__(self):
        for d in self.dp:
            yield self.fn(d) 
```

### 长度[](#length "跳转到此标题")

在许多情况下，就像我们的`MapperIterDataPipe`示例一样，DataPipe 的`__len__`方法返回源 DataPipe 的长度。

```py
class MapperIterDataPipe(IterDataPipe):
    # ... See __iter__() defined above

    def __len__(self):
        return len(self.dp) 
```

但请注意，对于`IterDataPipe`，`__len__`是可选的，通常不建议使用。在下面的 DataPipes 部分中，对于`CSVParserIterDataPipe`，`__len__`未实现，因为在加载之前无法确定每个文件中的行数。在某些特殊情况下，`__len__`可以被设置为返回整数或根据输入引发错误。在这些情况下，错误必须是`TypeError`，以支持 Python 的内置函数如`list(dp)`。

### 使用功能 API 注册 DataPipes[](#registering-datapipes-with-the-functional-api "跳转到此标题的永久链接")

每个 DataPipe 都可以注册以支持使用装饰器`functional_datapipe`进行功能调用。

```py
@functional_datapipe("map")
class MapperIterDataPipe(IterDataPipe):
   # ... 
```

然后，可以使用它们的功能形式（推荐）或类构造函数构建 DataPipes 堆栈：

```py
import torchdata.datapipes as dp

# Using functional form (recommended)
datapipes1 = dp.iter.FileOpener(['a.file', 'b.file']).map(fn=decoder).shuffle().batch(2)
# Using class constructors
datapipes2 = dp.iter.FileOpener(['a.file', 'b.file'])
datapipes2 = dp.iter.Mapper(datapipes2, fn=decoder)
datapipes2 = dp.iter.Shuffler(datapipes2)
datapipes2 = dp.iter.Batcher(datapipes2, 2) 
```

在上面的示例中，`datapipes1`和`datapipes2`代表完全相同的`IterDataPipe`堆栈。我们建议使用 DataPipes 的功能形式。

## 与云存储提供商合作[](#working-with-cloud-storage-providers "跳转到此标题的永久链接")

在本节中，我们展示了使用内置`fsspec` DataPipes 访问 AWS S3、Google Cloud Storage 和 Azure Cloud Storage 的示例。尽管这里只讨论了这两个提供商，但使用其他库，`fsspec` DataPipes 也应该允许您连接到其他存储系统（[已知实现列表](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)）。

如果您对其他云存储提供商的支持有任何请求，或者有代码示例要与社区分享，请在 GitHub 上告诉我们。

### 使用`fsspec` DataPipes 访问 AWS S3[](#accessing-aws-s3-with-fsspec-datapipes "跳转到此标题的永久链接")

这需要安装库`fsspec`（[文档](https://filesystem-spec.readthedocs.io/en/latest/)）和`s3fs`（[s3fs GitHub 仓库](https://github.com/fsspec/s3fs)）。

您可以通过将以`s3://BUCKET_NAME`开头的路径传递给 FSSpecFileLister（`.list_files_by_fsspec(...)`）来列出 S3 存储桶目录中的文件。

```py
from torchdata.datapipes.iter import IterableWrapper

dp = IterableWrapper(["s3://BUCKET_NAME"]).list_files_by_fsspec() 
```

您还可以使用 FSSpecFileOpener（`.open_files_by_fsspec(...)`)打开文件并流式传输（如果文件格式支持）。

请注意，您还可以通过参数`kwargs_for_open`提供额外的参数。这对于访问特定存储桶版本等目的可能很有用，您可以通过传入`{version_id: 'SOMEVERSIONID'}`来实现（更多关于 S3 存储桶版本感知的详细信息，请参阅`s3fs`的[文档](https://s3fs.readthedocs.io/en/latest/#bucket-version-awareness)）。支持的参数取决于您正在访问的（云）文件系统。

在下面的示例中，我们通过使用 TarArchiveLoader（`.load_from_tar(mode="r|")`）来流式传输存档，与通常的`mode="r:"`相反。这使我们能够在将整个存档下载到内存之前开始处理存档中的数据。

```py
from torchdata.datapipes.iter import IterableWrapper
dp = IterableWrapper(["s3://BUCKET_NAME/DIRECTORY/1.tar"])
dp = dp.open_files_by_fsspec(mode="rb", anon=True).load_from_tar(mode="r|") # Streaming version
# The rest of data processing logic goes here 
```

最后，FSSpecFileSaver 也可用于将数据写入云端。

### 使用`fsspec` DataPipes 访问 Google Cloud Storage（GCS）[](#accessing-google-cloud-storage-gcs-with-fsspec-datapipes "跳转到此标题的永久链接")

这需要安装库`fsspec`（[文档](https://filesystem-spec.readthedocs.io/en/latest/)）和`gcsfs`（[gcsfs GitHub 仓库](https://github.com/fsspec/gcsfs)）。

您可以通过指定以`"gcs://BUCKET_NAME"`开头的路径来列出 GCS 存储桶目录中的文件。下面示例中的存储桶名称是`uspto-pair`。

```py
from torchdata.datapipes.iter import IterableWrapper

dp = IterableWrapper(["gcs://uspto-pair/"]).list_files_by_fsspec()
print(list(dp))
# ['gcs://uspto-pair/applications', 'gcs://uspto-pair/docs', 'gcs://uspto-pair/prosecution-history-docs'] 
```

以下是从名为`uspto-pair`的存储桶中的`applications`目录加载`05900035.zip`文件的示例。

```py
from torchdata.datapipes.iter import IterableWrapper

dp = IterableWrapper(["gcs://uspto-pair/applications/05900035.zip"]) \
        .open_files_by_fsspec(mode="rb") \
        .load_from_zip()
# Logic to process those archive files comes after
for path, filestream in dp:
    print(path, filestream)
# gcs:/uspto-pair/applications/05900035.zip/05900035/README.txt, StreamWrapper<...>
# gcs:/uspto-pair/applications/05900035.zip/05900035/05900035-address_and_attorney_agent.tsv, StreamWrapper<...>
# gcs:/uspto-pair/applications/05900035.zip/05900035/05900035-application_data.tsv, StreamWrapper<...>
# gcs:/uspto-pair/applications/05900035.zip/05900035/05900035-continuity_data.tsv, StreamWrapper<...>
# gcs:/uspto-pair/applications/05900035.zip/05900035/05900035-transaction_history.tsv, StreamWrapper<...> 
```

### 使用`fsspec` DataPipes 访问 Azure Blob 存储[](#accessing-azure-blob-storage-with-fsspec-datapipes "跳转到此标题")

这需要安装库`fsspec`（[文档](https://filesystem-spec.readthedocs.io/en/latest/)）和`adlfs`（[adlfs GitHub 仓库](https://github.com/fsspec/adlfs)）。您可以通过提供以`abfs://`开头的 URI 来访问 Azure Data Lake Storage Gen2。例如，FSSpecFileLister（`.list_files_by_fsspec(...)`）可用于列出容器中目录中的文件：

```py
from torchdata.datapipes.iter import IterableWrapper

storage_options={'account_name': ACCOUNT_NAME, 'account_key': ACCOUNT_KEY}
dp = IterableWrapper(['abfs://CONTAINER/DIRECTORY']).list_files_by_fsspec(**storage_options)
print(list(dp))
# ['abfs://container/directory/file1.txt', 'abfs://container/directory/file2.txt', ...] 
```

您还可以使用 FSSpecFileOpener（`.open_files_by_fsspec(...)`）打开文件并流式传输（如果文件格式支持）。

这里是一个从属于账户`pandemicdatalake`的公共容器内的目录`curated/covid-19/ecdc_cases/latest`中加载 CSV 文件`ecdc_cases.csv`的示例。

```py
from torchdata.datapipes.iter import IterableWrapper
dp = IterableWrapper(['abfs://public/curated/covid-19/ecdc_cases/latest/ecdc_cases.csv']) \
        .open_files_by_fsspec(account_name='pandemicdatalake') \
        .parse_csv()
print(list(dp)[:3])
# [['date_rep', 'day', ..., 'iso_country', 'daterep'],
# ['2020-12-14', '14', ..., 'AF', '2020-12-14'],
# ['2020-12-13', '13', ..., 'AF', '2020-12-13']] 
```

如有必要，您还可以通过使用以`adl://`和`abfs://`开头的 URI 来访问 Azure Data Lake Storage Gen1，如[adlfs 仓库的 README](https://github.com/fsspec/adlfs/blob/main/README.md)中所述。

### 使用`fsspec` DataPipes 访问 Azure ML 数据存储[](#accessing-azure-ml-datastores-with-fsspec-datapipes "跳转到此标题")

Azure ML 数据存储是对 Azure 上现有存储账户的*引用*。创建和使用 Azure ML 数据存储的主要优势是：

+   一个通用且易于使用的 API，用于与 Azure 中的不同存储类型（Blob/Files/<datastore>）进行交互。

+   团队合作时更容易发现有用的数据存储。

+   身份验证会自动处理 - 支持基于凭据的访问（服务主体/SAS/密钥）和基于身份的访问（Azure Active Directory/托管标识）。使用基于凭据的身份验证时，您无需在代码中暴露密钥。

这需要安装库`azureml-fsspec`（[文档](https://learn.microsoft.com/python/api/azureml-fsspec/?view=azure-ml-py)）。

通过提供以`azureml://`开头的 URI，您可以访问 Azure ML 数据存储。例如，FSSpecFileLister（`.list_files_by_fsspec(...)`）可用于列出容器中目录中的文件：

```py
from torchdata.datapipes.iter import IterableWrapper

# set the subscription_id, resource_group, and AzureML workspace_name
subscription_id = "<subscription_id>"
resource_group = "<resource_group>"
workspace_name = "<workspace_name>"

# set the datastore name and path on the datastore
datastore_name = "<datastore_name>"
path_on_datastore = "<path_on_datastore>"

uri = f"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace_name}/datastores/{datastore_name}/paths/{path_on_datastore}"

dp = IterableWrapper([uri]).list_files_by_fsspec()
print(list(dp))
# ['azureml:///<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/<folder>/file1.txt',
# 'azureml:///<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/<folder>/file2.txt', ...] 
```

您还可以使用 FSSpecFileOpener（`.open_files_by_fsspec(...)`）打开文件并流式传输（如果文件格式支持）。

这里是一个从默认的 Azure ML 数据存储`workspaceblobstore`中加载 tar 文件的示例，路径为`/cifar-10-python.tar.gz`（顶层文件夹）。

```py
from torchdata.datapipes.iter import IterableWrapper

# set the subscription_id, resource_group, and AzureML workspace_name
subscription_id = "<subscription_id>"
resource_group = "<resource_group>"
workspace_name = "<workspace_name>"

# set the datastore name and path on the datastore
datastore_name = "workspaceblobstore"
path_on_datastore = "cifar-10-python.tar.gz"

uri = f"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace_name}/datastores/{datastore_name}/paths/{path_on_datastore}"

dp = IterableWrapper([uri]) \
        .open_files_by_fsspec(mode="rb") \
        .load_from_tar()

for path, filestream in dp:
    print(path)
# ['azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/data_batch_4',
#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/readme.html',
#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/test_batch',
#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/data_batch_3',
#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/batches.meta',
#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/data_batch_2',
#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/data_batch_5',
#   'azureml:/subscriptions/<sub_id>/resourcegroups/<rg_name>/workspaces/<ws_name>/datastores/<datastore>/paths/cifar-10-python.tar.gz/cifar-10-batches-py/data_batch_1] 
```

这里是一个加载 CSV 文件的示例 - 著名的泰坦尼克号数据集（[下载](https://raw.githubusercontent.com/Azure/azureml-examples/main/cli/assets/data/sample-data/titanic.csv)）- 从 Azure ML 数据存储`workspaceblobstore`中加载，路径为`/titanic.csv`（顶层文件夹）。

```py
from torchdata.datapipes.iter import IterableWrapper

# set the subscription_id, resource_group, and AzureML workspace_name
subscription_id = "<subscription_id>"
resource_group = "<resource_group>"
workspace_name = "<workspace_name>"

# set the datastore name and path on the datastore
datastore_name = "workspaceblobstore"
path_on_datastore = "titanic.csv"

uri = f"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace_name}/datastores/{datastore_name}/paths/{path_on_datastore}"

def row_processer(row):
    # return the label and data (the class and age of the passenger)
    # if missing age, set to 50
    if row[5] == "":
        row[5] = 50.0
    return {"label": np.array(row[1], np.int32), "data": np.array([row[2],row[5]], dtype=np.float32)}

dp = IterableWrapper([uri]) \
        .open_files_by_fsspec() \
        .parse_csv(delimiter=",", skip_lines=1) \
        .map(row_processer)

print(list(dp)[:3])
# [{'label': array(0, dtype=int32), 'data': array([ 3., 22.], dtype=float32)},
#  {'label': array(1, dtype=int32), 'data': array([ 1., 38.], dtype=float32)},
#  {'label': array(1, dtype=int32), 'data': array([ 3., 26.], dtype=float32)}] 
```
