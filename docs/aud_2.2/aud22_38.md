# 使用Emformer RNN-T的设备ASR

> 原文：[https://pytorch.org/audio/stable/tutorials/device_asr.html](https://pytorch.org/audio/stable/tutorials/device_asr.html)

注意

点击[这里](#sphx-glr-download-tutorials-device-asr-py)下载完整示例代码

**作者**：[Moto Hira](mailto:moto%40meta.com), [Jeff Hwang](mailto:jeffhwang%40meta.com)。

本教程展示了如何使用Emformer RNN-T和流式API在流式设备输入上执行语音识别，即笔记本电脑上的麦克风。

注意

本教程需要FFmpeg库。请参考[FFmpeg依赖](../installation.html#ffmpeg-dependency)获取详细信息。

注意

本教程在MacBook Pro和安装了Windows 10的Dynabook上进行了测试。

本教程在Google Colab上不起作用，因为运行本教程的服务器没有可以与之交谈的麦克风。

## 1\. 概述[](#overview "Permalink to this heading")

我们使用流式API逐块从音频设备（麦克风）获取音频，然后使用Emformer RNN-T进行推理。

有关流式API和Emformer RNN-T的基本用法，请参考[StreamReader基本用法](./streamreader_basic_tutorial.html)和[使用Emformer RNN-T进行在线ASR](./online_asr_tutorial.html)。

## 2\. 检查支持的设备[](#checking-the-supported-devices "Permalink to this heading")

首先，我们需要检查流式API可以访问的设备，并找出我们需要传递给[`StreamReader()`](../generated/torchaudio.io.StreamReader.html#torchaudio.io.StreamReader "torchaudio.io.StreamReader")类的参数（`src`和`format`）。

我们使用`ffmpeg`命令来实现。`ffmpeg`抽象了底层硬件实现的差异，但`format`的预期值在不同操作系统上有所不同，每个`format`定义了不同的`src`语法。

有关支持的`format`值和`src`语法的详细信息，请参考[https://ffmpeg.org/ffmpeg-devices.html](https://ffmpeg.org/ffmpeg-devices.html)。

对于macOS，以下命令将列出可用设备。

```py
$ ffmpeg -f avfoundation -list_devices true -i dummy
...
[AVFoundation indev @ 0x126e049d0] AVFoundation video devices:
[AVFoundation indev @ 0x126e049d0] [0] FaceTime HD Camera
[AVFoundation indev @ 0x126e049d0] [1] Capture screen 0
[AVFoundation indev @ 0x126e049d0] AVFoundation audio devices:
[AVFoundation indev @ 0x126e049d0] [0] ZoomAudioDevice
[AVFoundation indev @ 0x126e049d0] [1] MacBook Pro Microphone 
```

我们将使用以下值进行流式API。

```py
StreamReader(
    src = ":1",  # no video, audio from device 1, "MacBook Pro Microphone"
    format = "avfoundation",
) 
```

对于Windows，`dshow`设备应该可以工作。

```py
> ffmpeg -f dshow -list_devices true -i dummy
...
[dshow @ 000001adcabb02c0] DirectShow video devices (some may be both video and audio devices)
[dshow @ 000001adcabb02c0]  "TOSHIBA Web Camera - FHD"
[dshow @ 000001adcabb02c0]     Alternative name "@device_pnp_\\?\usb#vid_10f1&pid_1a42&mi_00#7&27d916e6&0&0000#{65e8773d-8f56-11d0-a3b9-00a0c9223196}\global"
[dshow @ 000001adcabb02c0] DirectShow audio devices
[dshow @ 000001adcabb02c0]  "... (Realtek High Definition Audio)"
[dshow @ 000001adcabb02c0]     Alternative name "@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\wave_{BF2B8AE1-10B8-4CA4-A0DC-D02E18A56177}" 
```

在上述情况下，可以使用以下值从麦克风进行流式传输。

```py
StreamReader(
    src = "audio=@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\wave_{BF2B8AE1-10B8-4CA4-A0DC-D02E18A56177}",
    format = "dshow",
) 
```

## 3\. 数据采集[](#data-acquisition "Permalink to this heading")

从麦克风输入流式音频需要正确计时数据采集。如果未能这样做，可能会导致数据流中出现不连续性。

因此，我们将在子进程中运行数据采集。

首先，我们创建一个封装在子进程中执行的整个过程的辅助函数。

此函数初始化流式API，获取数据然后将其放入队列，主进程正在监视该队列。

```py
import torch
import torchaudio

# The data acquisition process will stop after this number of steps.
# This eliminates the need of process synchronization and makes this
# tutorial simple.
NUM_ITER = 100

def stream(q, format, src, segment_length, sample_rate):
    from torchaudio.io import StreamReader

    print("Building StreamReader...")
    streamer = StreamReader(src, format=format)
    streamer.add_basic_audio_stream(frames_per_chunk=segment_length, sample_rate=sample_rate)

    print(streamer.get_src_stream_info(0))
    print(streamer.get_out_stream_info(0))

    print("Streaming...")
    print()
    stream_iterator = streamer.stream(timeout=-1, backoff=1.0)
    for _ in range(NUM_ITER):
        (chunk,) = next(stream_iterator)
        q.put(chunk) 
```

与非设备流式的显着区别在于，我们为`stream`方法提供了`timeout`和`backoff`参数。

在获取数据时，如果获取请求的速率高于硬件准备数据的速率，则底层实现会报告特殊的错误代码，并期望客户端代码重试。

精确的时序是流畅流媒体的关键。从低级实现报告此错误一直返回到Python层，在重试之前会增加不必要的开销。因此，重试行为是在C++层实现的，`timeout`和`backoff`参数允许客户端代码控制行为。

有关`timeout`和`backoff`参数的详细信息，请参考`stream()`方法的文档。

注意

`backoff`的适当值取决于系统配置。检查`backoff`值是否合适的一种方法是将获取的一系列块保存为连续音频并进行听取。如果`backoff`值太大，则数据流是不连续的。生成的音频听起来加快了。如果`backoff`值太小或为零，则音频流正常，但数据采集过程进入忙等待状态，这会增加CPU消耗。

## 4\. 构建推理流程[](#building-inference-pipeline "跳转到此标题")

接下来的步骤是创建推理所需的组件。

这与[使用Emformer RNN-T进行在线ASR](./online_asr_tutorial.html)是相同的流程。

```py
class Pipeline:
  """Build inference pipeline from RNNTBundle.

 Args:
 bundle (torchaudio.pipelines.RNNTBundle): Bundle object
 beam_width (int): Beam size of beam search decoder.
 """

    def __init__(self, bundle: torchaudio.pipelines.RNNTBundle, beam_width: int = 10):
        self.bundle = bundle
        self.feature_extractor = bundle.get_streaming_feature_extractor()
        self.decoder = bundle.get_decoder()
        self.token_processor = bundle.get_token_processor()

        self.beam_width = beam_width

        self.state = None
        self.hypotheses = None

    def infer(self, segment: [torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) -> str:
  """Perform streaming inference"""
        features, length = self.feature_extractor(segment)
        self.hypotheses, self.state = self.decoder.infer(
            features, length, self.beam_width, state=self.state, hypothesis=self.hypotheses
        )
        transcript = self.token_processor(self.hypotheses[0][0], lstrip=False)
        return transcript 
```

```py
class ContextCacher:
  """Cache the end of input data and prepend the next input data with it.

 Args:
 segment_length (int): The size of main segment.
 If the incoming segment is shorter, then the segment is padded.
 context_length (int): The size of the context, cached and appended.
 """

    def __init__(self, segment_length: int, context_length: int):
        self.segment_length = segment_length
        self.context_length = context_length
        self.context = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros "torch.zeros")([context_length])

    def __call__(self, chunk: [torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
        if chunk.size(0) < self.segment_length:
            chunk = [torch.nn.functional.pad](https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html#torch.nn.functional.pad "torch.nn.functional.pad")(chunk, (0, self.segment_length - chunk.size(0)))
        chunk_with_context = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat "torch.cat")((self.context, chunk))
        self.context = chunk[-self.context_length :]
        return chunk_with_context 
```

## 5\. 主要流程[](#the-main-process "跳转到此标题")

主进程的执行流程如下：

1.  初始化推理流程。

1.  启动数据获取子进程。

1.  运行推理。

1.  清理

注意

由于数据获取子进程将使用“spawn”方法启动，全局范围的所有代码也将在子进程中执行。

我们希望只在主进程中实例化流程，因此我们将它们放在一个函数中，并在`__name__ == "__main__"`保护内调用它。

```py
def main(device, src, bundle):
    print(torch.__version__)
    print(torchaudio.__version__)

    print("Building pipeline...")
    pipeline = Pipeline(bundle)

    sample_rate = bundle.sample_rate
    segment_length = bundle.segment_length * bundle.hop_length
    context_length = bundle.right_context_length * bundle.hop_length

    print(f"Sample rate: {sample_rate}")
    print(f"Main segment: {segment_length} frames ({segment_length  /  sample_rate} seconds)")
    print(f"Right context: {context_length} frames ({context_length  /  sample_rate} seconds)")

    cacher = ContextCacher(segment_length, context_length)

    @torch.inference_mode()
    def infer():
        for _ in range(NUM_ITER):
            chunk = q.get()
            segment = cacher(chunk[:, 0])
            transcript = pipeline.infer(segment)
            print(transcript, end="\r", flush=True)

    import torch.multiprocessing as mp

    ctx = mp.get_context("spawn")
    q = ctx.Queue()
    p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))
    p.start()
    infer()
    p.join()

if __name__ == "__main__":
    main(
        device="avfoundation",
        src=":1",
        bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH,
    ) 
```

```py
Building pipeline...
Sample rate: 16000
Main segment: 2560 frames (0.16 seconds)
Right context: 640 frames (0.04 seconds)
Building StreamReader...
SourceAudioStream(media_type='audio', codec='pcm_f32le', codec_long_name='PCM 32-bit floating point little-endian', format='flt', bit_rate=1536000, sample_rate=48000.0, num_channels=1)
OutputStream(source_index=0, filter_description='aresample=16000,aformat=sample_fmts=fltp')
Streaming...

hello world 
```

标签：[`torchaudio.io`](../io.html#module-torchaudio.io "torchaudio.io")

**脚本的总运行时间：**（0分钟0.000秒）

[`下载Python源代码：device_asr.py`](../_downloads/8009eae2a3a1a322f175ecc138597775/device_asr.py)

[`下载Jupyter笔记本：device_asr.ipynb`](../_downloads/c8265c298ed19ff44b504d5c3aa72563/device_asr.ipynb)

[Sphinx-Gallery生成的画廊](https://sphinx-gallery.github.io)
