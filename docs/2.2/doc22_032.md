# torch.nn

> 原文：[`pytorch.org/docs/stable/nn.html`](https://pytorch.org/docs/stable/nn.html)> 
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)


这些是图的基本构建块：

torch.nn

+   容器

+   卷积层

+   池化层

+   填充层

+   非线性激活（加权和，非线性）

+   非线性激活（其他）

+   归一化层

+   循环层

+   变换器层

+   线性层

+   丢弃层

+   稀疏层

+   距离函数

+   损失函数

+   视觉层

+   洗牌层

+   DataParallel 层（多 GPU，分布式）

+   实用工具

+   量化函数

+   延迟模块初始化

| `Parameter` | 一种被视为模块参数的张量。 |
| --- | --- |
| `UninitializedParameter` | 一个未初始化的参数。 |
| `UninitializedBuffer` | 一个未初始化的缓冲区。 |

## 容器

| `Module` | 所有神经网络模块的基类。 |
| --- | --- |
| `Sequential` | 一个顺序容器。 |
| `ModuleList` | 在列表中保存子模块。 |
| `ModuleDict` | 在字典中保存子模块。 |
| `ParameterList` | 在列表中保存参数。 |
| `ParameterDict` | 在字典中保存参数。 |

模块的全局钩子

| `register_module_forward_pre_hook` | 注册一个对所有模块通用的前向预钩子。 |
| --- | --- |
| `register_module_forward_hook` | 为所有模块注册一个全局前向钩子。 |
| `register_module_backward_hook` | 注册一个对所有模块通用的反向钩子。 |
| `register_module_full_backward_pre_hook` | 注册一个对所有模块通用的反向预钩子。 |
| `register_module_full_backward_hook` | 注册一个对所有模块通用的反向钩子。 |
| `register_module_buffer_registration_hook` | 注册一个适用于所有模块的缓冲区注册钩子。 |
| `register_module_module_registration_hook` | 注册一个适用于所有模块的模块注册钩子。 |
| `register_module_parameter_registration_hook` | 注册一个适用于所有模块的参数注册钩子。 |

## 卷积层

| `nn.Conv1d` | 对由多个输入平面组成的输入信号应用 1D 卷积。 |
| --- | --- |
| `nn.Conv2d` | 对由多个输入平面组成的输入信号应用 2D 卷积。 |
| `nn.Conv3d` | 对由多个输入平面组成的输入信号应用 3D 卷积。 |
| `nn.ConvTranspose1d` | 对由多个输入平面组成的输入图像应用 1D 转置卷积运算符。 |
| `nn.ConvTranspose2d` | 对由多个输入平面组成的输入图像应用 2D 转置卷积运算符。 |
| `nn.ConvTranspose3d` | 对由多个输入平面组成的输入图像应用 3D 转置卷积运算符。 |
| `nn.LazyConv1d` | 一个带有`in_channels`参数延迟初始化的`torch.nn.Conv1d`模块。 |
| `nn.LazyConv2d` | 一个带有`in_channels`参数延迟初始化的`torch.nn.Conv2d`模块。 |
| `nn.LazyConv3d` | 一个带有`in_channels`参数延迟初始化的`torch.nn.Conv3d`模块。 |
| `nn.LazyConvTranspose1d` | 一个带有`in_channels`参数延迟初始化的`torch.nn.ConvTranspose1d`模块。 |
| `nn.LazyConvTranspose2d` | 一个带有`in_channels`参数延迟初始化的`torch.nn.ConvTranspose2d`模块。 |
| `nn.LazyConvTranspose3d` | 一个带有`in_channels`参数延迟初始化的`torch.nn.ConvTranspose3d`模块。 |
| `nn.Unfold` | 从批量输入张量中提取滑动局部块。 |
| `nn.Fold` | 将一组滑动局部块组合成一个大的包含张量。 |

## 池化层

| `nn.MaxPool1d` | 对由多个输入平面组成的输入信号应用 1D 最大池化。 |
| --- | --- |
| `nn.MaxPool2d` | 对由多个输入平面组成的输入信号应用 2D 最大池化。 |
| `nn.MaxPool3d` | 对由多个输入平面组成的输入信号应用 3D 最大池化。 |
| `nn.MaxUnpool1d` | 计算`MaxPool1d`的部分逆操作。 |
| `nn.MaxUnpool2d` | 计算`MaxPool2d`的部分逆操作。 |
| `nn.MaxUnpool3d` | 计算`MaxPool3d`的部分逆操作。 |
| `nn.AvgPool1d` | 对由多个输入平面组成的输入信号应用 1D 平均池化。 |
| `nn.AvgPool2d` | 对由多个输入平面组成的输入信号应用 2D 平均池化。 |
| `nn.AvgPool3d` | 对由多个输入平面组成的输入信号应用 3D 平均池化。 |
| `nn.FractionalMaxPool2d` | 对由多个输入平面组成的输入信号应用 2D 分数最大池化。 |
| `nn.FractionalMaxPool3d` | 对由多个输入平面组成的输入信号应用 3D 分数最大池化。 |
| `nn.LPPool1d` | 对由多个输入平面组成的输入信号应用 1D 幂平均池化。 |
| `nn.LPPool2d` | 对由多个输入平面组成的输入信号应用 2D 幂平均池化。 |
| `nn.AdaptiveMaxPool1d` | 对由多个输入平面组成的输入信号应用 1D 自适应最大池化。 |
| `nn.AdaptiveMaxPool2d` | 对由多个输入平面组成的输入信号应用 2D 自适应最大池化。 |
| `nn.AdaptiveMaxPool3d` | 对由多个输入平面组成的输入信号应用 3D 自适应最大池化。 |
| `nn.AdaptiveAvgPool1d` | 对由多个输入平面组成的输入信号应用 1D 自适应平均池化。 |
| `nn.AdaptiveAvgPool2d` | 对由多个输入平面组成的输入信号应用 2D 自适应平均池化。 |
| `nn.AdaptiveAvgPool3d` | 对由多个输入平面组成的输入信号应用 3D 自适应平均池化。 |

## 填充层

| `nn.ReflectionPad1d` | 使用输入边界的反射来填充输入张量。 |
| --- | --- |
| `nn.ReflectionPad2d` | 使用输入边界的反射来填充输入张量。 |
| `nn.ReflectionPad3d` | 使用输入边界的反射来填充输入张量。 |
| `nn.ReplicationPad1d` | 使用输入边界的复制来填充输入张量。 |
| `nn.ReplicationPad2d` | 使用输入边界的复制来填充输入张量。 |
| `nn.ReplicationPad3d` | 使用输入边界的复制来填充输入张量。 |
| `nn.ZeroPad1d` | 使用零值填充输入张量的边界。 |
| `nn.ZeroPad2d` | 使用零值填充输入张量的边界。 |
| `nn.ZeroPad3d` | 使用零值填充输入张量的边界。 |
| `nn.ConstantPad1d` | 使用常数值填充输入张量的边界。 |
| `nn.ConstantPad2d` | 使用常数值填充输入张量的边界。 |
| `nn.ConstantPad3d` | 使用常数值填充输入张量的边界。 |

## 非线性激活函数（加权和，非线性）

| `nn.ELU` | 对每个元素应用指数线性单元（ELU）函数，如论文中所述：[通过指数线性单元（ELUs）实现快速准确的深度网络学习](https://arxiv.org/abs/1511.07289)。 |
| --- | --- |
| `nn.Hardshrink` | 对每个元素应用硬收缩（Hardshrink）函数。 |
| `nn.Hardsigmoid` | 对每个元素应用硬 Sigmoid 函数。 |
| `nn.Hardtanh` | 对每个元素应用 HardTanh 函数。 |
| `nn.Hardswish` | 对每个元素应用 Hardswish 函数，如论文中所述：[搜索 MobileNetV3](https://arxiv.org/abs/1905.02244)。 |
| `nn.LeakyReLU` | 应用逐元素函数： |
| `nn.LogSigmoid` | 应用逐元素函数： |
| `nn.MultiheadAttention` | 允许模型同时关注来自不同表示子空间的信息，如论文中所述：[注意力机制是你所需要的一切](https://arxiv.org/abs/1706.03762)。 |
| `nn.PReLU` | 应用逐元素函数： |
| `nn.ReLU` | 逐元素应用修正线性单元函数： |
| `nn.ReLU6` | 应用逐元素函数： |
| `nn.RReLU` | 应用随机泄漏修正线性单元函数，逐元素地，如论文中所述： |
| `nn.SELU` | 逐元素应用，如： |
| `nn.CELU` | 应用逐元素函数： |
| `nn.GELU` | 应用高斯误差线性单元函数： |
| `nn.Sigmoid` | 应用逐元素函数： |
| `nn.SiLU` | 应用 Sigmoid 线性单元（SiLU）函数，逐元素。 |
| `nn.Mish` | 应用 Mish 函数，逐元素。 |
| `nn.Softplus` | 逐元素应用 Softplus 函数$\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))$。 |
| `nn.Softshrink` | 应用软阈值函数，逐元素： |
| `nn.Softsign` | 应用逐元素函数： |
| `nn.Tanh` | 逐元素应用双曲正切（Tanh）函数。 |
| `nn.Tanhshrink` | 应用逐元素函数： |
| `nn.Threshold` | 对输入张量的每个元素进行阈值处理。 |
| `nn.GLU` | 应用门控线性单元函数${GLU}(a, b)= a \otimes \sigma(b)$，其中$a$是输入矩阵的前一半，$b$是后一半。 |

## 非线性激活函数（其他）

| `nn.Softmin` | 对 n 维输入张量应用 Softmin 函数，重新缩放它们，使得 n 维输出张量的元素位于范围[0, 1]并总和为 1。 |
| --- | --- |
| `nn.Softmax` | 对 n 维输入张量应用 Softmax 函数，重新缩放它们，使得 n 维输出张量的元素位于范围[0,1]并总和为 1。 |
| `nn.Softmax2d` | 对每个空间位置的特征应用 SoftMax。 |
| `nn.LogSoftmax` | 对 n 维输入张量应用$\log(\text{Softmax}(x))$函数。 |
| `nn.AdaptiveLogSoftmaxWithLoss` | 高效的 softmax 近似。 |

## 归一化层

| `nn.BatchNorm1d` | 对 2D 或 3D 输入应用批量归一化，如论文[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)中描述。 |
| --- | --- |
| `nn.BatchNorm2d` | 对 4D 输入（带有额外通道维度的 2D 输入的小批量）应用批归一化，如论文 [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) 中所述。 |
| `nn.BatchNorm3d` | 对 5D 输入（带有额外通道维度的 3D 输入的小批量）应用批归一化，如论文 [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) 中所述。 |
| `nn.LazyBatchNorm1d` | 具有延迟初始化的 `num_features` 参数的 `torch.nn.BatchNorm1d` 模块，该参数从 `input.size(1)` 推断而来。 |
| `nn.LazyBatchNorm2d` | 具有延迟初始化的 `num_features` 参数的 `torch.nn.BatchNorm2d` 模块，该参数从 `input.size(1)` 推断而来。 |
| `nn.LazyBatchNorm3d` | 具有延迟初始化的 `num_features` 参数的 `torch.nn.BatchNorm3d` 模块，该参数从 `input.size(1)` 推断而来。 |
| `nn.GroupNorm` | 对输入的一个小批量应用组归一化。 |
| `nn.SyncBatchNorm` | 对 N 维输入（带有额外通道维度的小批量 [N-2]D 输入）应用批归一化，如论文 [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) 中所述。 |
| `nn.InstanceNorm1d` | 应用实例归一化。 |
| `nn.InstanceNorm2d` | 应用实例归一化。 |
| `nn.InstanceNorm3d` | 应用实例归一化。 |
| `nn.LazyInstanceNorm1d` | 具有延迟初始化的 `num_features` 参数的 `torch.nn.InstanceNorm1d` 模块。 |
| `nn.LazyInstanceNorm2d` | 具有延迟初始化的 `num_features` 参数的 `torch.nn.InstanceNorm2d` 模块。 |
| `nn.LazyInstanceNorm3d` | 具有 `num_features` 参数的延迟初始化的 `torch.nn.InstanceNorm3d` 模块。 |
| `nn.LayerNorm` | 对输入的一个小批量应用层归一化。 |
| `nn.LocalResponseNorm` | 对输入信号应用局部响应归一化。 |

## Recurrent Layers

| `nn.RNNBase` | RNN 模块（RNN、LSTM、GRU）的基类。 |
| --- | --- |
| `nn.RNN` | 对输入序列应用多层 Elman RNN，使用 $\tanh$ 或 $\text{ReLU}$ 非线性。 |
| `nn.LSTM` | 对输入序列应用多层长短期记忆（LSTM）RNN。 |
| `nn.GRU` | 对输入序列应用多层门控循环单元（GRU）RNN。 |
| `nn.RNNCell` | 一个具有 tanh 或 ReLU 非线性的 Elman RNN 单元。 |
| `nn.LSTMCell` | 长短期记忆（LSTM）单元。 |
| `nn.GRUCell` | 门控循环单元（GRU）单元。 |

## Transformer Layers

| `nn.Transformer` | 一个 transformer 模型。 |
| --- | --- |
| `nn.TransformerEncoder` | TransformerEncoder 是 N 个编码器层的堆叠。 |
| `nn.TransformerDecoder` | TransformerDecoder 是 N 个解码器层的堆叠。 |
| `nn.TransformerEncoderLayer` | TransformerEncoderLayer 由 self-attn 和前馈网络组成。 |
| `nn.TransformerDecoderLayer` | TransformerDecoderLayer 由 self-attn、multi-head-attn 和前馈网络组成。 |

## Linear Layers

| `nn.Identity` | 一个占位符身份运算符，不受参数影响。 |
| --- | --- |
| `nn.Linear` | 对传入数据应用线性变换：$y = xA^T + b$。 |
| `nn.Bilinear` | 对传入数据应用双线性变换：$y = x_1^T A x_2 + b$。 |
| `nn.LazyLinear` | 一个 `torch.nn.Linear` 模块，其中的 in_features 是推断出来的。 |

## Dropout Layers

| `nn.Dropout` | 在训练期间，以概率 `p` 随机将输入张量的一些元素置零。 |
| --- | --- |
| `nn.Dropout1d` | 随机将整个通道置零。 |
| `nn.Dropout2d` | 随机将整个通道置零。 |
| `nn.Dropout3d` | 随机将整个通道置零。 |
| `nn.AlphaDropout` | 对输入应用 Alpha Dropout。 |
| `nn.FeatureAlphaDropout` | 随机屏蔽整个通道。 |

## Sparse Layers

| `nn.Embedding` | 存储固定字典和大小的嵌入的简单查找表。 |
| --- | --- |
| `nn.EmbeddingBag` | 计算嵌入“bags”的和或均值，而不实例化中间嵌入。 |

## 距离函数

| `nn.CosineSimilarity` | 返回 $x_1$ 和 $x_2$ 之间的余弦相似度，沿着维度计算。 |
| --- | --- |
| `nn.PairwiseDistance` | 计算输入向量之间的成对距离，或输入矩阵的列之间的距离。 |

## 损失函数

| `nn.L1Loss` | 创建一个标准，衡量输入 $x$ 和目标 $y$ 中每个元素的平均绝对误差（MAE）。 |
| --- | --- |
| `nn.MSELoss` | 创建一个标准，衡量输入 $x$ 和目标 $y$ 中每个元素的均方误差（平方 L2 范数）。 |
| `nn.CrossEntropyLoss` | 此标准计算输入 logits 和目标之间的交叉熵损失。 |
| `nn.CTCLoss` | 连接主义时间分类损失。 |
| `nn.NLLLoss` | 负对数似然损失。 |
| `nn.PoissonNLLLoss` | 具有泊松分布目标的负对数似然损失。 |
| `nn.GaussianNLLLoss` | 高斯负对数似然损失。 |
| `nn.KLDivLoss` | Kullback-Leibler 散度损失。 |
| `nn.BCELoss` | 创建一个衡量目标和输入概率之间的二元交叉熵的标准： |
| `nn.BCEWithLogitsLoss` | 此损失将 Sigmoid 层和 BCELoss 结合在一个单一类中。 |
| `nn.MarginRankingLoss` | 创建一个标准，衡量给定输入 $x1$、$x2$，两个 1D mini-batch 或 0D 张量，以及标签 1D mini-batch 或 0D 张量 $y$（包含 1 或-1）的损失。 |
| `nn.HingeEmbeddingLoss` | 给定输入张量 $x$ 和标签张量 $y$（包含 1 或-1），衡量损失。 |
| `nn.MultiLabelMarginLoss` | 创建一个标准，优化输入 $x$（一个 2D mini-batch 张量）和输出 $y$（目标类别索引的 2D 张量）之间的多类多分类铰链损失（基于边缘的损失）。 |
| `nn.HuberLoss` | 创建一个标准，如果绝对逐元素误差低于 delta，则使用平方项，否则使用 delta-scaled L1 项。 |
| `nn.SmoothL1Loss` | 创建一个标准，如果绝对逐元素误差低于 beta，则使用平方项，否则使用 L1 项。 |
| `nn.SoftMarginLoss` | 创建一个标准，优化输入张量$x$和目标张量$y$（包含 1 或-1）之间的两类分类逻辑损失。 |
| `nn.MultiLabelSoftMarginLoss` | 创建一个标准，基于最大熵，优化输入$x$和大小为$(N, C)$的目标$y$的多标签一对所有损失。 |
| `nn.CosineEmbeddingLoss` | 创建一个标准，根据输入张量$x_1$、$x_2$和一个值为 1 或-1 的张量标签$y$来测量损失。 |
| `nn.MultiMarginLoss` | 创建一个标准，优化输入$x$(一个 2D 小批量张量)和输出$y$(一个目标类别索引的 1D 张量，$0 \leq y \leq \text{x.size}(1)-1$)之间的多类分类铰链损失（基于边缘的损失）： |
| `nn.TripletMarginLoss` | 创建一个标准，根据大于$0$的边距值，测量给定输入张量$x_1$、$x_2$、$x_3$的三元组损失。 |
| `nn.TripletMarginWithDistanceLoss` | 创建一个标准，根据输入张量$a$、$p$、$n$（分别表示锚点、正例和负例）和用于计算锚点和正例之间关系（“正距离”）以及锚点和负例之间关系（“负距离”）的非负实值函数（“距离函数”）。 |

## 视觉层

| `nn.PixelShuffle` | 根据放大因子重新排列张量中的元素。 |
| --- | --- |
| `nn.PixelUnshuffle` | 反转 PixelShuffle 操作。 |
| `nn.Upsample` | 上采样给定的多通道 1D（时间）、2D（空间）或 3D（体积）数据。 |
| `nn.UpsamplingNearest2d` | 对由多个输入通道组成的输入信号应用 2D 最近邻上采样。 |
| `nn.UpsamplingBilinear2d` | 对由多个输入通道组成的输入信号应用 2D 双线性上采样。 |

## 洗牌层

| `nn.ChannelShuffle` | 分割并重新排列张量中的通道。 |
| --- | --- |

## 数据并行层（多 GPU，分布式）

| `nn.DataParallel` | 在模块级别实现数据并行。 |
| --- | --- |

| `nn.parallel.DistributedDataParallel` | 在模块级别基于`torch.distributed`实现分布式数据并行。 |  ## 实用工具

来自`torch.nn.utils`模块：

用于裁剪参数梯度的实用函数。

| `clip_grad_norm_` | 对参数的可迭代对象剪裁梯度范数。 |
| --- | --- |
| `clip_grad_norm` | 对参数的可迭代对象剪裁梯度范数。 |
| `clip_grad_value_` | 将参数的梯度剪裁到指定值。 |

将模块参数展平和展开为单个向量的实用函数。

| `parameters_to_vector` | 将参数的可迭代对象展平为单个向量。 |
| --- | --- |
| `vector_to_parameters` | 将向量的切片复制到参数的可迭代对象中。 |

用于融合带有 BatchNorm 模块的模块的实用函数。

| `fuse_conv_bn_eval` | 将卷积模块和 BatchNorm 模块融合为单个新的卷积模块。 |
| --- | --- |
| `fuse_conv_bn_weights` | 将卷积模块参数和 BatchNorm 模块参数融合为新的卷积模块参数。 |
| `fuse_linear_bn_eval` | 将线性模块和 BatchNorm 模块融合为单个新的线性模块。 |
| `fuse_linear_bn_weights` | 将线性模块参数和 BatchNorm 模块参数融合为新的线性模块参数。 |

用于转换模块参数内存格式的实用函数。

| `convert_conv2d_weight_memory_format` | 将`nn.Conv2d.weight`的`memory_format`转换为`memory_format`。 |
| --- | --- |

应用和移除模块参数中的权重归一化的实用函数。

| `weight_norm` | 对给定模块中的参数应用权重归一化。 |
| --- | --- |
| `remove_weight_norm` | 从模块中移除权重归一化重新参数化。 |
| `spectral_norm` | 对给定模块中的参数应用谱归一化。 |
| `remove_spectral_norm` | 从模块中移除谱归一化重新参数化。 |

用于初始化模块参数的实用函数。

| `skip_init` | 给定模块类对象和参数/关键字参数，实例化模块而不初始化参数/缓冲区。 |
| --- | --- |

用于修剪模块参数的实用类和函数。

| `prune.BasePruningMethod` | 用于创建新修剪技术的抽象基类。 |
| --- | --- |
| `prune.PruningContainer` | 包含一系列迭代剪枝方法的容器。 |
| `prune.Identity` | 不剪枝任何单元，但生成具有全为 1 的掩码的剪枝参数化的实用剪枝方法。 |
| `prune.RandomUnstructured` | 随机剪枝张量中的（当前未剪枝）单元。 |
| `prune.L1Unstructured` | 通过将具有最低 L1-范数的单元置零来剪枝张量中的（当前未剪枝）单元。 |
| `prune.RandomStructured` | 随机剪枝张量中的整个（当前未剪枝）通道。 |
| `prune.LnStructured` | 基于它们的 L`n`-范数剪枝张量中整个（当前未剪枝）通道。 |
| `prune.CustomFromMask` |  |
| `prune.identity` | 应用剪枝重新参数化，而不剪枝任何单元。 |
| `prune.random_unstructured` | 通过移除随机（当前未剪枝）单元来剪枝张量。 |
| `prune.l1_unstructured` | 通过移除具有最低 L1-范数的单元来剪枝张量。 |
| `prune.random_structured` | 通过沿指定维度移除随机通道来剪枝张量。 |
| `prune.ln_structured` | 通过沿指定维度移除具有最低 L`n`-范数的通道来剪枝张量。 |
| `prune.global_unstructured` | 通过应用指定的 `pruning_method` 全局剪枝与 `parameters` 中所有参数对应的张量。 |
| `prune.custom_from_mask` | 通过应用 `mask` 中的预先计算的掩码来剪枝与 `module` 中名为 `name` 的参数对应的张量。 |
| `prune.remove` | 从模块中移除剪枝重新参数化，并从前向钩子中移除剪枝方法。 |
| `prune.is_pruned` | 通过查找剪枝预钩子来检查模块是否被剪枝。 |

使用 `torch.nn.utils.parameterize.register_parametrization()` 中的新参数化功能实现的参数化。

| `parametrizations.orthogonal` | 对矩阵或一批矩阵应用正交或酉参数化。 |
| --- | --- |
| `parametrizations.weight_norm` | 将权重归一化应用于给定模块中的参数。 |
| `parametrizations.spectral_norm` | 对给定模块中的参数应用谱归一化。 |

用于在现有模块上对张量进行参数化的实用函数。请注意，这些函数可用于对给定的参数或缓冲区进行参数化，给定特定函数从输入空间映射到参数化空间。它们不是将对象转换为参数的参数化。有关如何实现自己的参数化的更多信息，请参阅[参数化教程](https://pytorch.org/tutorials/intermediate/parametrizations.html)。

| `parametrize.register_parametrization` | 在模块中的张量上注册参数化。 |
| --- | --- |
| `parametrize.remove_parametrizations` | 在模块中的张量上移除参数化。 |
| `parametrize.cached` | 启用与`register_parametrization()`注册的参数化中的缓存系统的上下文管理器。 |
| `parametrize.is_parametrized` | 确定模块是否具有参数化。 |
| `parametrize.ParametrizationList` | 一个顺序容器，保存和管理参数化`torch.nn.Module`的原始参数或缓冲区。 |

以无状态方式调用给定模块的实用函数。

| `stateless.functional_call` | 通过用提供的参数替换模块的参数和缓冲区来对模块执行功能调用。 |
| --- | --- |

其他模块中的实用函数

| `nn.utils.rnn.PackedSequence` | 包含打包序列的数据和`batch_sizes`列表。 |
| --- | --- |
| `nn.utils.rnn.pack_padded_sequence` | 将包含变长填充序列的张量打包。 |
| `nn.utils.rnn.pad_packed_sequence` | 填充变长序列的打包批次。 |
| `nn.utils.rnn.pad_sequence` | 使用`padding_value`填充变长张量列表。 |
| `nn.utils.rnn.pack_sequence` | 打包长度可变的张量列表。 |
| `nn.utils.rnn.unpack_sequence` | 将 PackedSequence 解包成长度可变的张量列表。 |
| `nn.utils.rnn.unpad_sequence` | 将填充的张量解除填充为长度可变的张量列表。 |
| `nn.Flatten` | 将连续的维度范围展平为张量。 |
| `nn.Unflatten` | 将张量展平，将其扩展为所需的形状。 |

## 量化函数

量化是指在比浮点精度更低的比特宽度上执行计算和存储张量的技术。PyTorch 支持每个张量和每个通道的非对称线性量化。要了解如何在 PyTorch 中使用量化函数，请参阅量化文档。

## 延迟模块初始化

| `nn.modules.lazy.LazyModuleMixin` | 用于延迟初始化参数的模块混合，也称为“延迟模块”。 |
| --- | --- |
