# PyTorch张量介绍

> [原文](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)

注意

点击[这里](#sphx-glr-download-beginner-introyt-tensors-deeper-tutorial-py)下载完整的示例代码

[介绍](introyt1_tutorial.html) || **张量** || [自动求导](autogradyt_tutorial.html) || [构建模型](modelsyt_tutorial.html) || [TensorBoard支持](tensorboardyt_tutorial.html) || [训练模型](trainingyt.html) || [模型理解](captumyt.html)

请跟随下面的视频或[YouTube](https://www.youtube.com/watch?v=r7QDUPb2dCM)。

[https://www.youtube.com/embed/r7QDUPb2dCM](https://www.youtube.com/embed/r7QDUPb2dCM)

张量是PyTorch中的中心数据抽象。这个交互式笔记本提供了对`torch.Tensor`类的深入介绍。

首先，让我们导入PyTorch模块。我们还将添加Python的math模块以便于一些示例。

```py
import torch
import math 
```

## 创建张量

创建张量的最简单方法是使用`torch.empty()`调用：

```py
[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.empty](https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty "torch.empty")(3, 4)
print(type([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))
print([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
<class 'torch.Tensor'>
tensor([[6.0864e-06, 0.0000e+00, 1.9431e-19, 1.1024e+24],
        [8.8221e-04, 0.0000e+00, 3.9172e-05, 0.0000e+00],
        [1.1978e-35, 0.0000e+00, 7.7463e-37, 0.0000e+00]]) 
```

让我们解开刚才做的事情：

+   我们使用`torch`模块附加的众多工厂方法之一创建了一个张量。

+   张量本身是二维的，有3行和4列。

+   返回对象的类型是`torch.Tensor`，它是`torch.FloatTensor`的别名；默认情况下，PyTorch张量由32位浮点数填充。（有关数据类型的更多信息请参见下文。）

+   打印张量时，您可能会看到一些看起来随机的值。`torch.empty()`调用为张量分配内存，但不会用任何值初始化它 - 所以您看到的是在分配时内存中的内容。

关于张量及其维度和术语的简要说明：

+   有时您会看到一个称为*向量*的一维张量。

+   同样，一个二维张量通常被称为*矩阵*。

+   超过两个维度的任何内容通常都被称为张量。

往往您会希望用某个值初始化张量。常见情况是全零、全一或随机值，`torch`模块为所有这些情况提供了工厂方法：

```py
[zeros](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros "torch.zeros")(2, 3)
print([zeros](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[ones](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 3)
print([ones](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed "torch.manual_seed")(1729)
[random](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 3)
print([random](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]]) 
```

工厂方法都只是做您期望的事情 - 我们有一个全是零的张量，另一个全是一的张量，还有一个介于0和1之间的随机值的张量。

### 随机张量和种子

说到随机张量，您是否注意到在其之前立即调用了`torch.manual_seed()`？使用随机值初始化张量，例如模型的学习权重，是常见的，但有时 - 尤其是在研究环境中 - 您会希望确保结果的可重现性。手动设置随机数生成器的种子是实现这一点的方法。让我们更仔细地看一下：

```py
[torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed "torch.manual_seed")(1729)
[random1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 3)
print([random1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[random2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 3)
print([random2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed "torch.manual_seed")(1729)
[random3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 3)
print([random3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[random4](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 3)
print([random4](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]])
tensor([[0.2332, 0.4047, 0.2162],
        [0.9927, 0.4128, 0.5938]])
tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]])
tensor([[0.2332, 0.4047, 0.2162],
        [0.9927, 0.4128, 0.5938]]) 
```

您应该看到`random1`和`random3`携带相同的值，`random2`和`random4`也是如此。手动设置RNG的种子会重置它，因此在大多数情况下，依赖随机数的相同计算应该提供相同的结果。

有关更多信息，请参阅[PyTorch关于可重现性的文档](https://pytorch.org/docs/stable/notes/randomness.html)。

### 张量形状

通常，当您对两个或更多张量执行操作时，它们需要具有相同的*形状* - 即，具有相同数量的维度和每个维度中相同数量的单元格。为此，我们有`torch.*_like()`方法：

```py
[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.empty](https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty "torch.empty")(2, 2, 3)
print([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)
print([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[empty_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.empty_like](https://pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like "torch.empty_like")([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([empty_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)
print([empty_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[zeros_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.zeros_like](https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like "torch.zeros_like")([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([zeros_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)
print([zeros_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[ones_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones_like](https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like "torch.ones_like")([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([ones_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)
print([ones_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[rand_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand_like](https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like "torch.rand_like")([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([rand_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)
print([rand_like_x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
torch.Size([2, 2, 3])
tensor([[[ 8.7595e-05,  0.0000e+00,  1.4013e-45],
         [ 0.0000e+00,  7.7463e-37,  0.0000e+00]],

        [[ 0.0000e+00,  0.0000e+00,  8.6286e-05],
         [ 0.0000e+00, -1.7707e+28,  4.5849e-41]]])
torch.Size([2, 2, 3])
tensor([[[ 0.0000e+00,  0.0000e+00,  1.4013e-45],
         [ 0.0000e+00,  7.7463e-37,  0.0000e+00]],

        [[ 0.0000e+00,  0.0000e+00,  8.6408e-05],
         [ 0.0000e+00, -1.7707e+28,  4.5849e-41]]])
torch.Size([2, 2, 3])
tensor([[[0., 0., 0.],
         [0., 0., 0.]],

        [[0., 0., 0.],
         [0., 0., 0.]]])
torch.Size([2, 2, 3])
tensor([[[1., 1., 1.],
         [1., 1., 1.]],

        [[1., 1., 1.],
         [1., 1., 1.]]])
torch.Size([2, 2, 3])
tensor([[[0.6128, 0.1519, 0.0453],
         [0.5035, 0.9978, 0.3884]],

        [[0.6929, 0.1703, 0.1384],
         [0.4759, 0.7481, 0.0361]]]) 
```

上面代码单元格中的第一个新内容是在张量上使用`.shape`属性。此属性包含张量每个维度的范围列表 - 在我们的情况下，`x`是一个形状为2 x 2 x 3的三维张量。

在下面，我们调用`.empty_like()`、`.zeros_like()`、`.ones_like()`和`.rand_like()`方法。使用`.shape`属性，我们可以验证每个方法返回的张量具有相同的维度和范围。

创建张量的最后一种方式是直接从PyTorch集合中指定其数据：

```py
[some_constants](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([[3.1415926, 2.71828], [1.61803, 0.0072897]])
print([some_constants](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[some_integers](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")((2, 3, 5, 7, 11, 13, 17, 19))
print([some_integers](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[more_integers](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")(((2, 4, 6), [3, 6, 9]))
print([more_integers](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[3.1416, 2.7183],
        [1.6180, 0.0073]])
tensor([ 2,  3,  5,  7, 11, 13, 17, 19])
tensor([[2, 4, 6],
        [3, 6, 9]]) 
```

如果您已经有一个Python元组或列表中的数据，使用`torch.tensor()`是创建张量的最简单方式。如上所示，嵌套集合将导致一个多维张量。

注意

`torch.tensor()`会创建数据的副本。

### 张量数据类型

设置张量的数据类型有几种方式：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")((2, 3), dtype=[torch.int16](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype"))
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")((2, 3), dtype=[torch.float64](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype")) * 20.
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").to([torch.int32](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype"))
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[1, 1, 1],
        [1, 1, 1]], dtype=torch.int16)
tensor([[ 0.9956,  1.4148,  5.8364],
        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)
tensor([[ 0,  1,  5],
        [11, 11, 11]], dtype=torch.int32) 
```

设置张量的基础数据类型的最简单方式是在创建时使用可选参数。在上面单元格的第一行中，我们为张量`a`设置了`dtype=torch.int16`。当我们打印`a`时，我们可以看到它充满了`1`而不是`1.` - Python微妙地暗示这是一个整数类型而不是浮点数。

关于打印`a`的另一件事是，与我们将`dtype`保留为默认值(32位浮点数)时不同，打印张量还会指定其`dtype`。

您可能还注意到，我们从将张量的形状指定为一系列整数参数开始，到将这些参数分组在一个元组中。这并不是严格必要的 - PyTorch将一系列初始的、未标记的整数参数作为张量形状 - 但是当添加可选参数时，可以使您的意图更易读。

设置数据类型的另一种方式是使用`.to()`方法。在上面的单元格中，我们按照通常的方式创建了一个随机浮点张量`b`。在那之后，我们通过使用`.to()`方法将`b`转换为32位整数来创建`c`。请注意，`c`包含与`b`相同的所有值，但被截断为整数。

可用的数据类型包括：

+   `torch.bool`

+   `torch.int8`

+   `torch.uint8`

+   `torch.int16`

+   `torch.int32`

+   `torch.int64`

+   `torch.half`

+   `torch.float`

+   `torch.double`

+   `torch.bfloat`

## 使用PyTorch张量进行数学和逻辑运算

现在您已经了解了一些创建张量的方式...您可以用它们做什么？

让我们先看一下基本的算术运算，以及张量如何与简单标量交互：

```py
[ones](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros "torch.zeros")(2, 2) + 1
[twos](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 2) * 2
[threes](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ([torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 2) * 7 - 1) / 2
[fours](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [twos](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") ** 2
[sqrt2s](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [twos](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") ** 0.5

print([ones](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([twos](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([threes](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([fours](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([sqrt2s](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[1., 1.],
        [1., 1.]])
tensor([[2., 2.],
        [2., 2.]])
tensor([[3., 3.],
        [3., 3.]])
tensor([[4., 4.],
        [4., 4.]])
tensor([[1.4142, 1.4142],
        [1.4142, 1.4142]]) 
```

正如您在上面看到的，张量和标量之间的算术运算，如加法、减法、乘法、除法和指数运算，会分布在张量的每个元素上。因为这样的操作的输出将是一个张量，您可以按照通常的运算符优先规则将它们链接在一起，就像我们创建`threes`的那一行一样。

两个张量之间的类似操作也会像您直觉地期望的那样：

```py
[powers2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [twos](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") ** [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([[1, 2], [3, 4]])
print([powers2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[fives](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [ones](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") + [fours](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")
print([fives](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[dozens](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [threes](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * [fours](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")
print([dozens](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[ 2.,  4.],
        [ 8., 16.]])
tensor([[5., 5.],
        [5., 5.]])
tensor([[12., 12.],
        [12., 12.]]) 
```

这里需要注意的是，前一个代码单元中的所有张量的形状都是相同的。当我们尝试在形状不同的张量上执行二进制操作时会发生什么？

注意

以下单元格会抛出运行时错误。这是故意的。

```py
a  =  torch.rand(2,  3)
b  =  torch.rand(3,  2)

print(a  *  b) 
```

在一般情况下，您不能以这种方式操作形状不同的张量，即使在上面的单元格中，张量具有相同数量的元素。

### 简而言之：张量广播

注意

如果您熟悉NumPy ndarrays中的广播语义，您会发现这里也适用相同的规则。

与相同形状规则相悖的是*张量广播*。这里是一个例子：

```py
[rand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 4)
[doubled](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [rand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * ([torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(1, 4) * 2)

print([rand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([doubled](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[0.6146, 0.5999, 0.5013, 0.9397],
        [0.8656, 0.5207, 0.6865, 0.3614]])
tensor([[1.2291, 1.1998, 1.0026, 1.8793],
        [1.7312, 1.0413, 1.3730, 0.7228]]) 
```

这里的诀窍是什么？我们是如何将一个2x4的张量乘以一个1x4的张量的？

广播是一种在形状相似的张量之间执行操作的方式。在上面的例子中，一行四列的张量与两行四列的张量的*每一行*相乘。

这是深度学习中的一个重要操作。一个常见的例子是将学习权重的张量乘以一个*批量*的输入张量，将操作应用于批量中的每个实例，并返回一个形状相同的张量 - 就像我们上面的(2, 4) * (1, 4)的例子返回了一个形状为(2, 4)的张量。

广播的规则是：

+   每个张量必须至少有一个维度 - 不能是空张量。

+   比较两个张量的维度大小，*从最后到第一个维度：*

    +   每个维度必须相等，*或*

    +   其中一个维度必须为1，*或*

    +   一个张量中不存在的维度

当然，形状相同的张量是可以“广播”的，就像您之前看到的那样。

以下是遵守上述规则并允许广播的一些示例情况：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") =     [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(4, 3, 2)

[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(   3, 1) # 3rd dim = 1, 2nd dim identical to a
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(   1, 2) # 3rd dim identical to a, 2nd dim = 1
print([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]],

        [[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]],

        [[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]],

        [[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]]])
tensor([[[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]],

        [[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]],

        [[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]],

        [[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]]])
tensor([[[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]],

        [[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]],

        [[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]],

        [[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]]]) 
```

仔细观察上面每个张量的值：

+   创建`b`的乘法操作在`a`的每个“层”上进行了广播。

+   对于`c`，操作在`a`的每一层和行上进行了广播 - 每个3元素列都是相同的。

+   对于`d`，我们将其调整了一下 - 现在每个*行*在层和列之间都是相同的。

有关广播的更多信息，请参阅[PyTorch文档](https://pytorch.org/docs/stable/notes/broadcasting.html)。

以下是一些尝试广播但将失败的示例：

注意

以下单元格会引发运行时错误。这是故意的。

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") =     [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(4, 3, 2)

[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(4, 3)    # dimensions must match last-to-first

[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(   2, 3) # both 3rd & 2nd dims different

[d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")((0, ))   # can't broadcast with an empty tensor 
```

### 更多张量数学

PyTorch张量有三百多个可以对其执行的操作。

以下是一些主要类别操作的小样本：

```py
# common functions
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 4) * 2 - 1
print('Common functions:')
print([torch.abs](https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs "torch.abs")([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))
print([torch.ceil](https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil "torch.ceil")([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))
print([torch.floor](https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor "torch.floor")([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))
print([torch.clamp](https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp "torch.clamp")([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), -0.5, 0.5))

# trigonometric functions and their inverses
[angles](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])
[sines](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.sin](https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin "torch.sin")([angles](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[inverses](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.asin](https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin "torch.asin")([sines](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print('\nSine and arcsine:')
print([angles](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([sines](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([inverses](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

# bitwise operations
print('\nBitwise XOR:')
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([1, 5, 11])
[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([2, 7, 10])
print([torch.bitwise_xor](https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor "torch.bitwise_xor")([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))

# comparisons:
print('\nBroadcasted, element-wise equality comparison:')
[d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([[1., 2.], [3., 4.]])
[e](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(1, 2)  # many comparison ops support broadcasting!
print([torch.eq](https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq "torch.eq")([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [e](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))) # returns a tensor of type bool

# reductions:
print('\nReduction ops:')
print([torch.max](https://pytorch.org/docs/stable/generated/torch.max.html#torch.max "torch.max")([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))        # returns a single-element tensor
print([torch.max](https://pytorch.org/docs/stable/generated/torch.max.html#torch.max "torch.max")([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")).item()) # extracts the value from the returned tensor
print([torch.mean](https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean "torch.mean")([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))       # average
print([torch.std](https://pytorch.org/docs/stable/generated/torch.std.html#torch.std "torch.std")([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))        # standard deviation
print([torch.prod](https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod "torch.prod")([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))       # product of all numbers
print([torch.unique](https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique "torch.unique")([torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([1, 2, 1, 2, 1, 2]))) # filter unique elements

# vector and linear algebra operations
[v1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([1., 0., 0.])         # x unit vector
[v2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([0., 1., 0.])         # y unit vector
[m1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2)                   # random matrix
[m2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([[3., 0.], [0., 3.]]) # three times identity matrix

print('\nVectors & Matrices:')
print([torch.cross](https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross "torch.cross")([v2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [v1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))) # negative of z unit vector (v1 x v2 == -v2 x v1)
print([m1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[m3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul "torch.matmul")([m1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [m2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([m3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))                  # 3 times m1
print([torch.svd](https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd "torch.svd")([m3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))       # singular value decomposition 
```

```py
Common functions:
tensor([[0.9238, 0.5724, 0.0791, 0.2629],
        [0.1986, 0.4439, 0.6434, 0.4776]])
tensor([[-0., -0., 1., -0.],
        [-0., 1., 1., -0.]])
tensor([[-1., -1.,  0., -1.],
        [-1.,  0.,  0., -1.]])
tensor([[-0.5000, -0.5000,  0.0791, -0.2629],
        [-0.1986,  0.4439,  0.5000, -0.4776]])

Sine and arcsine:
tensor([0.0000, 0.7854, 1.5708, 2.3562])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
tensor([0.0000, 0.7854, 1.5708, 0.7854])

Bitwise XOR:
tensor([3, 2, 1])

Broadcasted, element-wise equality comparison:
tensor([[ True, False],
        [False, False]])

Reduction ops:
tensor(4.)
4.0
tensor(2.5000)
tensor(1.2910)
tensor(24.)
tensor([1, 2])

Vectors & Matrices:
/var/lib/jenkins/workspace/beginner_source/introyt/tensors_deeper_tutorial.py:462: UserWarning:

Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:63.)

tensor([ 0.,  0., -1.])
tensor([[0.7375, 0.8328],
        [0.8444, 0.2941]])
tensor([[2.2125, 2.4985],
        [2.5332, 0.8822]])
torch.return_types.svd(
U=tensor([[-0.7889, -0.6145],
        [-0.6145,  0.7889]]),
S=tensor([4.1498, 1.0548]),
V=tensor([[-0.7957,  0.6056],
        [-0.6056, -0.7957]])) 
```

这只是一小部分操作。有关更多详细信息和数学函数的完整清单，请查看[文档](https://pytorch.org/docs/stable/torch.html#math-operations)。

### 就地更改张量

大多数张量上的二进制操作将返回第三个新张量。当我们说`c = a * b`（其中`a`和`b`是张量）时，新张量`c`将占用与其他张量不同的内存区域。

但是，有时您可能希望就地更改张量 - 例如，如果您正在进行可以丢弃中间值的逐元素计算。为此，大多数数学函数都有一个附加下划线（`_`）的版本，可以就地更改张量。

例如：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])
print('a:')
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([torch.sin](https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin "torch.sin")([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))   # this operation creates a new tensor in memory
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))              # a has not changed

[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])
print('\nb:')
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print(torch.sin_([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))  # note the underscore
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))              # b has changed 
```

```py
a:
tensor([0.0000, 0.7854, 1.5708, 2.3562])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
tensor([0.0000, 0.7854, 1.5708, 2.3562])

b:
tensor([0.0000, 0.7854, 1.5708, 2.3562])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
tensor([0.0000, 0.7071, 1.0000, 0.7071]) 
```

对于算术操作，有类似的函数行为：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 2)
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2)

print('Before:')
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print('\nAfter adding:')
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").add_([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print('\nAfter multiplying')
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").mul_([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
Before:
tensor([[1., 1.],
        [1., 1.]])
tensor([[0.3788, 0.4567],
        [0.0649, 0.6677]])

After adding:
tensor([[1.3788, 1.4567],
        [1.0649, 1.6677]])
tensor([[1.3788, 1.4567],
        [1.0649, 1.6677]])
tensor([[0.3788, 0.4567],
        [0.0649, 0.6677]])

After multiplying
tensor([[0.1435, 0.2086],
        [0.0042, 0.4459]])
tensor([[0.1435, 0.2086],
        [0.0042, 0.4459]]) 
```

请注意，这些就地算术函数是`torch.Tensor`对象上的方法，而不像许多其他函数（例如`torch.sin()`）附加到`torch`模块。正如您从`a.add_(b)`中看到的那样，*调用张量是在原地更改的*。

还有另一种选项，可以将计算结果放入现有的分配张量中。我们迄今为止看到的许多方法和函数 - 包括创建方法！ - 都有一个`out`参数，让您指定一个张量来接收输出。如果`out`张量具有正确的形状和`dtype`，则可以在不进行新的内存分配的情况下完成：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2)
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2)
[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros "torch.zeros")(2, 2)
old_id = id([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul "torch.matmul")([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), out=[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))                # contents of c have changed

assert [c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") is [d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")           # test c & d are same object, not just containing equal values
assert id([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) == old_id  # make sure that our new c is the same object as the old one

[torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2, out=[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) # works for creation too!
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))                # c has changed again
assert id([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) == old_id  # still the same object! 
```

```py
tensor([[0., 0.],
        [0., 0.]])
tensor([[0.3653, 0.8699],
        [0.2364, 0.3604]])
tensor([[0.0776, 0.4004],
        [0.9877, 0.0352]]) 
```

## 复制张量

与Python中的任何对象一样，将张量分配给变量会使变量成为张量的*标签*，而不是复制它。例如：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 2)
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[0][1] = 561  # we change a...
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))       # ...and b is also altered 
```

```py
tensor([[  1., 561.],
        [  1.,   1.]]) 
```

但是如果您想要一个单独的数据副本进行操作呢？`clone()`方法就是为您准备的：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 2)
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").clone()

assert [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") is not [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")      # different objects in memory...
print([torch.eq](https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq "torch.eq")([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))  # ...but still with the same contents!

[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[0][1] = 561          # a changes...
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))               # ...but b is still all ones 
```

```py
tensor([[True, True],
        [True, True]])
tensor([[1., 1.],
        [1., 1.]]) 
```

**在使用``clone()``时有一件重要的事情需要注意。**如果您的源张量启用了自动求导，那么克隆张量也会启用。**这将在自动求导的视频中更深入地介绍，但如果您想要简要了解详情，请继续阅读。

*在许多情况下，这可能是您想要的。*例如，如果您的模型在其`forward()`方法中具有多个计算路径，并且*原始张量和其克隆都对模型的输出有贡献*，那么为了启用模型学习，您希望为两个张量启用自动求导。如果您的源张量已启用自动求导（如果它是一组学习权重或从涉及权重的计算派生而来，则通常会启用），那么您将获得所需的结果。

另一方面，如果您进行的计算*既不需要原始张量也不需要其克隆跟踪梯度*，那么只要源张量关闭了自动求导，您就可以继续进行。

*然而，还有第三种情况:* 想象一下，你正在模型的`forward()`函数中执行计算，其中默认情况下为所有内容打开梯度，但你想要在中间提取一些值以生成一些指标。在这种情况下，你*不*希望克隆源张量跟踪梯度 - 关闭自动求导历史记录跟踪可以提高性能。为此，你可以在源张量上使用`.detach()`方法：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2, requires_grad=True) # turn on autograd
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").clone()
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").detach().clone()
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]], requires_grad=True)
tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]], grad_fn=<CloneBackward0>)
tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]])
tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]], requires_grad=True) 
```

这里发生了什么？

+   我们使用`requires_grad=True`创建`a`。**我们还没有涵盖这个可选参数，但在自动求导单元中会讨论。**

+   当我们打印`a`时，它告诉我们属性`requires_grad=True` - 这意味着自动求导和计算历史跟踪已打开。

+   我们克隆`a`并将其标记为`b`。当我们打印`b`时，我们可以看到它正在跟踪其计算历史 - 它继承了`a`的自动求导设置，并添加到了计算历史中。

+   我们将`a`克隆到`c`，但首先调用`detach()`。

+   打印`c`，我们看不到计算历史，也没有`requires_grad=True`。

`detach()`方法*将张量与其计算历史分离*。它表示，“接下来的操作就好像自动求导已关闭一样。” 它在*不*更改`a`的情况下执行此操作 - 当我们最后再次打印`a`时，你会看到它保留了`requires_grad=True`属性。

## 转移到GPU

PyTorch的一个主要优势是其在CUDA兼容的Nvidia GPU上的强大加速。 （“CUDA”代表*Compute Unified Device Architecture*，这是Nvidia用于并行计算的平台。）到目前为止，我们所做的一切都是在CPU上进行的。我们如何转移到更快的硬件？

首先，我们应该检查GPU是否可用，使用`is_available()`方法。

注意

如果你没有CUDA兼容的GPU和安装了CUDA驱动程序，本节中的可执行单元格将不会执行任何与GPU相关的代码。

```py
if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available "torch.cuda.is_available")():
    print('We have a GPU!')
else:
    print('Sorry, CPU only.') 
```

```py
We have a GPU! 
```

一旦我们确定有一个或多个GPU可用，我们需要将数据放在GPU可以看到的地方。你的CPU在计算时使用计算机RAM中的数据。你的GPU有专用内存附加在上面。每当你想在设备上执行计算时，你必须将进行该计算所需的*所有*数据移动到该设备可访问的内存中。 （口语上，“将数据移动到GPU可访问的内存”缩写为“将数据移动到GPU”。）

有多种方法可以将数据放在目标设备上。你可以在创建时执行：

```py
if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available "torch.cuda.is_available")():
    [gpu_rand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2, device='cuda')
    print([gpu_rand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
else:
    print('Sorry, CPU only.') 
```

```py
tensor([[0.3344, 0.2640],
        [0.2119, 0.0582]], device='cuda:0') 
```

默认情况下，新张量是在CPU上创建的，因此我们必须在想要使用可选`device`参数在GPU上创建张量时指定。当我们打印新张量时，你可以看到PyTorch告诉我们它在哪个设备上（如果不在CPU上）。

你可以使用`torch.cuda.device_count()`查询GPU的数量。如果你有多个GPU，你可以通过索引指定它们：`device='cuda:0'`，`device='cuda:1'`等。

作为编码实践，在所有地方使用字符串常量指定设备是相当脆弱的。在理想情况下，无论你是在CPU还是GPU硬件上，你的代码都应该表现稳健。你可以通过创建一个设备句柄来实现这一点，该句柄可以传递给你的张量，而不是一个字符串：

```py
if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available "torch.cuda.is_available")():
    [my_device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")('cuda')
else:
    [my_device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")('cpu')
print('Device: {}'.format([my_device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")))

[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2, device=[my_device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"))
print([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
Device: cuda
tensor([[0.0024, 0.6778],
        [0.2441, 0.6812]], device='cuda:0') 
```

如果你有一个现有张量存在于一个设备上，你可以使用`to()`方法将其移动到另一个设备上。以下代码行在CPU上创建一个张量，并将其移动到你在前一个单元格中获取的设备句柄。

```py
[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2)
[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").to([my_device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")) 
```

重要的是要知道，为了进行涉及两个或更多张量的计算，*所有张量必须在同一设备上*。无论你是否有GPU设备可用，以下代码都会抛出运行时错误：

```py
[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2)
[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2, device='gpu')
z = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") + [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")  # exception will be thrown 
```

## 操作张量形状

有时，你需要改变张量的形状。下面，我们将看几种常见情况以及如何处理它们。

### 改变维度数量

有一种情况可能需要改变维度的数量，就是向模型传递单个输入实例。PyTorch模型通常期望输入的*批量*。

例如，想象一个模型处理3 x 226 x 226的图像 - 一个有3个颜色通道的226像素正方形。当你加载和转换它时，你会得到一个形状为`(3, 226, 226)`的张量。然而，你的模型期望的输入形状是`(N, 3, 226, 226)`，其中`N`是批量中图像的数量。那么如何制作一个批量为1的批次？

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(3, 226, 226)
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").unsqueeze(0)

print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape) 
```

```py
torch.Size([3, 226, 226])
torch.Size([1, 3, 226, 226]) 
```

`unsqueeze()`方法添加一个长度为1的维度。`unsqueeze(0)`将其添加为一个新的第零维 - 现在你有一个批量为1的张量！

那么如果是*挤压*呢？我们所说的挤压是什么意思？我们利用了一个事实，即任何维度的长度为1 *不会*改变张量中的元素数量。

```py
[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(1, 1, 1, 1, 1)
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[[[[0.2347]]]]]) 
```

继续上面的例子，假设模型的输出对于每个输入是一个20元素向量。那么你期望输出的形状是`(N, 20)`，其中`N`是输入批次中的实例数。这意味着对于我们的单输入批次，我们将得到一个形状为`(1, 20)`的输出。

如果你想对这个输出进行一些*非批量*计算 - 比如只期望一个20元素的向量，怎么办？

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(1, 20)
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").squeeze(0)
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 2)
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)

[d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").squeeze(0)
print([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape) 
```

```py
torch.Size([1, 20])
tensor([[0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367,
         0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769,
         0.2792, 0.3277]])
torch.Size([20])
tensor([0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367,
        0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769,
        0.2792, 0.3277])
torch.Size([2, 2])
torch.Size([2, 2]) 
```

从形状可以看出，我们的二维张量现在是一维的，如果你仔细看上面单元格的输出，你会发现打印`a`会显示一个“额外”的方括号`[]`，因为有一个额外的维度。

你只能`squeeze()`长度为1的维度。看上面我们尝试在`c`中挤压一个大小为2的维度，最终得到的形状与开始时相同。调用`squeeze()`和`unsqueeze()`只能作用于长度为1的维度，因为否则会改变张量中的元素数量。

另一个可能使用`unsqueeze()`的地方是为了简化广播。回想一下我们之前的代码示例：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(4, 3, 2)

[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(   3, 1) # 3rd dim = 1, 2nd dim identical to a
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

这样做的净效果是在维度0和2上广播操作，导致随机的3 x 1张量与`a`中的每个3元素列进行逐元素相乘。

如果随机向量只是一个3元素向量怎么办？我们将失去进行广播的能力，因为最终的维度不会根据广播规则匹配。`unsqueeze()`来拯救：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(4, 3, 2)
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(   3)     # trying to multiply a * b will give a runtime error
[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * [c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))             # broadcasting works again! 
```

```py
torch.Size([3, 1])
tensor([[[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]],

        [[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]],

        [[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]],

        [[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]]]) 
```

`squeeze()`和`unsqueeze()`方法也有原地版本，`squeeze_()`和`unsqueeze_()`：

```py
[batch_me](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(3, 226, 226)
print([batch_me](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)
[batch_me](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").unsqueeze_(0)
print([batch_me](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape) 
```

```py
torch.Size([3, 226, 226])
torch.Size([1, 3, 226, 226]) 
```

有时候你会想要更彻底地改变张量的形状，同时仍保留元素数量和内容。一个这种情况是在模型的卷积层和线性层之间的接口处 - 这在图像分类模型中很常见。卷积核会产生一个形状为*特征 x 宽 x 高*的输出张量，但接下来的线性层期望一个一维输入。`reshape()`会为你做这个，只要你请求的维度产生的元素数量与输入张量相同即可：

```py
[output3d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(6, 20, 20)
print([output3d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)

[input1d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [output3d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").reshape(6 * 20 * 20)
print([input1d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape)

# can also call it as a method on the torch module:
print([torch.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape "torch.reshape")([output3d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), (6 * 20 * 20,)).shape) 
```

```py
torch.Size([6, 20, 20])
torch.Size([2400])
torch.Size([2400]) 
```

注意

上面单元格最后一行的`(6 * 20 * 20,)`参数是因为PyTorch在指定张量形状时期望一个**元组** - 但当形状是方法的第一个参数时，它允许我们欺骗并只使用一系列整数。在这里，我们必须添加括号和逗号来说服方法，让它相信这实际上是一个单元素元组。

在可以的情况下，`reshape()`会返回一个*视图*，即一个查看相同底层内存区域的独立张量对象以进行更改。*这很重要：*这意味着对源张量进行的任何更改都会反映在该张量的视图中，除非你使用`clone()`。

在这个介绍范围之外，`reshape()`有时必须返回一个携带数据副本的张量。更多信息请参阅[文档](https://pytorch.org/docs/stable/torch.html#torch.reshape)。

## NumPy桥接

在上面关于广播的部分中提到，PyTorch的广播语义与NumPy的兼容 - 但PyTorch和NumPy之间的关系甚至比这更深。

如果您有现有的ML或科学代码，并且数据存储在NumPy的ndarrays中，您可能希望将相同的数据表示为PyTorch张量，无论是为了利用PyTorch的GPU加速，还是为了利用其构建ML模型的高效抽象。在ndarrays和PyTorch张量之间轻松切换：

```py
import numpy as np

numpy_array = np.[ones](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")((2, 3))
print(numpy_array)

[pytorch_tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.from_numpy](https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy "torch.from_numpy")(numpy_array)
print([pytorch_tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
[[1\. 1\. 1.]
 [1\. 1\. 1.]]
tensor([[1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64) 
```

PyTorch创建一个与NumPy数组形状相同且包含相同数据的张量，甚至保留NumPy的默认64位浮点数据类型。

转换也可以同样轻松地进行另一种方式：

```py
[pytorch_rand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 3)
print([pytorch_rand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

numpy_rand = [pytorch_rand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").numpy()
print(numpy_rand) 
```

```py
tensor([[0.8716, 0.2459, 0.3499],
        [0.2853, 0.9091, 0.5695]])
[[0.87163675 0.2458961  0.34993553]
 [0.2853077  0.90905803 0.5695162 ]] 
```

重要的是要知道，这些转换后的对象使用*相同的底层内存*作为它们的源对象，这意味着对一个对象的更改会反映在另一个对象中：

```py
numpy_array[1, 1] = 23
print([pytorch_tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[pytorch_rand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[1, 1] = 17
print(numpy_rand) 
```

```py
tensor([[ 1.,  1.,  1.],
        [ 1., 23.,  1.]], dtype=torch.float64)
[[ 0.87163675  0.2458961   0.34993553]
 [ 0.2853077  17\.          0.5695162 ]] 
```

**脚本的总运行时间：**（0分钟0.294秒）

[`下载Python源代码：tensors_deeper_tutorial.py`](../../_downloads/be017e7b39198fdf668c138fd8d57abe/tensors_deeper_tutorial.py)

[`下载Jupyter笔记本：tensors_deeper_tutorial.ipynb`](../../_downloads/63a0f0fc7b3ffb15d3a5ac8db3d521ee/tensors_deeper_tutorial.ipynb)

[由Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)
