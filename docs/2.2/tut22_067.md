# 树莓派 4 上的实时推理（30 fps！）

> 原文：[`pytorch.org/tutorials/intermediate/realtime_rpi.html`](https://pytorch.org/tutorials/intermediate/realtime_rpi.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

**作者**：[Tristan Rice](https://github.com/d4l3k)

PyTorch 对树莓派 4 有开箱即用的支持。本教程将指导您如何为运行 PyTorch 的树莓派 4 设置树莓派 4，并在 CPU 上实时运行 MobileNet v2 分类模型（30 fps+）。

这一切都是在树莓派 4 型 B 4GB 上测试的，但也应该适用于 2GB 变体以及性能降低的 3B。

![`user-images.githubusercontent.com/909104/153093710-bc736b6f-69d9-4a50-a3e8-9f2b2c9e04fd.gif`](img/e1b6e9e801c40dcecd46ba020ff59fce.png)

## 先决条件

要按照本教程进行操作，您需要一个树莓派 4，一个相机以及所有其他标准配件。

+   [树莓派 4 型 B 2GB+](https://www.raspberrypi.com/products/raspberry-pi-4-model-b/)

+   [树莓派摄像头模块](https://www.raspberrypi.com/products/camera-module-v2/)

+   散热片和风扇（可选但建议）

+   5V 3A USB-C 电源适配器

+   SD 卡（至少 8GB）

+   SD 卡读/写器

## 树莓派 4 设置

PyTorch 仅为 Arm 64 位（aarch64）提供 pip 软件包，因此您需要在树莓派上安装 64 位版本的操作系统

您可以从[`downloads.raspberrypi.org/raspios_arm64/images/`](https://downloads.raspberrypi.org/raspios_arm64/images/)下载最新的 arm64 树莓派 OS，并通过 rpi-imager 安装它。

**32 位树莓派 OS 将无法工作。**

![`user-images.githubusercontent.com/909104/152866212-36ce29b1-aba6-4924-8ae6-0a283f1fca14.gif`](img/a74749f46e1b7b1c4cca5b95d030994f.png)

安装将至少需要几分钟，具体取决于您的互联网速度和 sd 卡速度。完成后，应如下所示：

![`user-images.githubusercontent.com/909104/152867425-c005cff0-5f3f-47f1-922d-e0bbb541cd25.png`](img/16e60b917befb99c2f0717800d2d5fbd.png)

现在是时候将您的 sd 卡放入树莓派中，连接摄像头并启动它。

![`user-images.githubusercontent.com/909104/152869862-c239c980-b089-4bd5-84eb-0a1e5cf22df2.png`](img/a7acb9a95909dde5e3117930780632d9.png)

一旦启动并完成初始设置，您需要编辑`/boot/config.txt`文件以启用摄像头。

```py
# This enables the extended features such as the camera.
start_x=1

# This needs to be at least 128M for the camera processing, if it's bigger you can just leave it as is.
gpu_mem=128

# You need to commment/remove the existing camera_auto_detect line since this causes issues with OpenCV/V4L2 capture.
#camera_auto_detect=1 
```

然后重新启动。重新启动后，video4linux2 设备`/dev/video0`应该存在。

## 安装 PyTorch 和 OpenCV

PyTorch 和我们需要的所有其他库都有 ARM 64 位/aarch64 变体，因此您可以通过 pip 安装它们，并使其像任何其他 Linux 系统一样工作。

```py
$  pip  install  torch  torchvision  torchaudio
$  pip  install  opencv-python
$  pip  install  numpy  --upgrade 
```

![`user-images.githubusercontent.com/909104/152874260-95a7a8bd-0f9b-438a-9c0b-5b67729e233f.png`](img/6905c748f74b28be8422c72e188095a7.png)

我们现在可以检查所有安装是否正确：

```py
$  python  -c  "import torch; print(torch.__version__)" 
```

![`user-images.githubusercontent.com/909104/152874271-d7057c2d-80fd-4761-aed4-df6c8b7aa99f.png`](img/5d0e59134b5e88fc00e83deb3a3ccab2.png)

## 视频捕获

对于视频捕获，我们将使用 OpenCV 来流式传输视频帧，而不是更常见的`picamera`。 picamera 在 64 位树莓派 OS 上不可用，而且比 OpenCV 慢得多。 OpenCV 直接访问`/dev/video0`设备以抓取帧。

我们正在使用的模型（MobileNetV2）接受`224x224`的图像尺寸，因此我们可以直接从 OpenCV 请求 36fps。我们的目标是模型的 30fps，但我们请求的帧率略高于此，以确保始终有足够的帧。

```py
import cv2
from PIL import Image

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)
cap.set(cv2.CAP_PROP_FPS, 36) 
```

OpenCV 以 BGR 返回一个`numpy`数组，因此我们需要读取并进行一些调整，以使其符合预期的 RGB 格式。

```py
ret, image = cap.read()
# convert opencv output from BGR to RGB
image = image[:, :, [2, 1, 0]] 
```

这个数据读取和处理大约需要`3.5 毫秒`。

## 图像预处理

我们需要获取帧并将其转换为模型期望的格式。这与您在任何具有标准 torchvision 转换的机器上执行的处理相同。

```py
from torchvision import transforms

preprocess = transforms.Compose([
    # convert the frame to a CHW torch tensor for training
    transforms.ToTensor(),
    # normalize the colors to the range that mobilenet_v2/3 expect
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
input_tensor = preprocess(image)
# The model can handle multiple images simultaneously so we need to add an
# empty dimension for the batch.
# [3, 224, 224] -> [1, 3, 224, 224]
input_batch = input_tensor.unsqueeze(0) 
```

## 模型选择

您可以选择多种模型，具有不同的性能特征。并非所有模型都提供`qnnpack`预训练变体，因此为了测试目的，您应该选择一个提供此功能的模型，但如果您训练和量化自己的模型，可以使用其中任何一个。

我们在本教程中使用`mobilenet_v2`，因为它具有良好的性能和准确性。

树莓派 4 基准测试结果：

| 模型 | FPS | 总时间（毫秒/帧） | 模型时间（毫秒/帧） | qnnpack 预训练 |
| --- | --- | --- | --- | --- |
| mobilenet_v2 | 33.7 | 29.7 | 26.4 | True |
| mobilenet_v3_large | 29.3 | 34.1 | 30.7 | True |
| resnet18 | 9.2 | 109.0 | 100.3 | False |
| resnet50 | 4.3 | 233.9 | 225.2 | False |
| resnext101_32x8d | 1.1 | 892.5 | 885.3 | False |
| inception_v3 | 4.9 | 204.1 | 195.5 | False |
| googlenet | 7.4 | 135.3 | 132.0 | False |
| shufflenet_v2_x0_5 | 46.7 | 21.4 | 18.2 | False |
| shufflenet_v2_x1_0 | 24.4 | 41.0 | 37.7 | False |
| shufflenet_v2_x1_5 | 16.8 | 59.6 | 56.3 | False |
| shufflenet_v2_x2_0 | 11.6 | 86.3 | 82.7 | False |

## MobileNetV2：量化和 JIT

为了获得最佳性能，我们希望使用量化和融合的模型。量化意味着使用 int8 进行计算，这比标准的 float32 数学更高效。融合意味着连续的操作已经被合并成更高效的版本，可能会合并像激活函数（`ReLU`）这样的操作到推断期间的前一层（`Conv2d`）中。

pytorch 的 aarch64 版本需要使用`qnnpack`引擎。

```py
import torch
torch.backends.quantized.engine = 'qnnpack' 
```

在这个示例中，我们将使用 torchvision 提供的预量化和融合版本的 MobileNetV2。

```py
from torchvision import models
net = models.quantization.mobilenet_v2(pretrained=True, quantize=True) 
```

然后，我们希望对模型进行 jit 以减少 Python 开销并融合任何操作。jit 使我们的帧率达到了约 30fps，而没有 jit 时只有约 20fps。

```py
net = torch.jit.script(net) 
```

## 将其放在一起

现在我们可以将所有部分组合在一起并运行它：

```py
import time

import torch
import numpy as np
from torchvision import models, transforms

import cv2
from PIL import Image

torch.backends.quantized.engine = 'qnnpack'

cap = cv2.VideoCapture(0, cv2.CAP_V4L2)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)
cap.set(cv2.CAP_PROP_FPS, 36)

preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

net = models.quantization.mobilenet_v2(pretrained=True, quantize=True)
# jit model to take it from ~20fps to ~30fps
net = torch.jit.script(net)

started = time.time()
last_logged = time.time()
frame_count = 0

with torch.no_grad():
    while True:
        # read frame
        ret, image = cap.read()
        if not ret:
            raise RuntimeError("failed to read frame")

        # convert opencv output from BGR to RGB
        image = image[:, :, [2, 1, 0]]
        permuted = image

        # preprocess
        input_tensor = preprocess(image)

        # create a mini-batch as expected by the model
        input_batch = input_tensor.unsqueeze(0)

        # run model
        output = net(input_batch)
        # do something with output ...

        # log model performance
        frame_count += 1
        now = time.time()
        if now - last_logged > 1:
            print(f"{frame_count  /  (now-last_logged)} fps")
            last_logged = now
            frame_count = 0 
```

运行后，我们发现帧率约为 30fps。

![`user-images.githubusercontent.com/909104/152892609-7d115705-3ec9-4f8d-beed-a51711503a32.png`](img/85471d8bad6acb9e759049d828861c14.png)

这是在 Raspberry Pi OS 中的所有默认设置下。如果您禁用了默认启用的 UI 和所有其他后台服务，性能和稳定性会更好。

如果我们检查`htop`，我们会看到几乎 100%的利用率。

![`user-images.githubusercontent.com/909104/152892630-f094b84b-19ba-48f6-8632-1b954abc59c7.png`](img/a869ca455dfc3672a29fa30bda2b03a0.png)

为了验证它是否正常工作，我们可以计算类别的概率并[使用 ImageNet 类标签](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)来打印检测结果。

```py
top = list(enumerate(output[0].softmax(dim=0)))
top.sort(key=lambda x: x[1], reverse=True)
for idx, val in top[:10]:
    print(f"{val.item()*100:.2f}% {classes[idx]}") 
```

`mobilenet_v3_large`实时运行：

![`user-images.githubusercontent.com/909104/153093710-bc736b6f-69d9-4a50-a3e8-9f2b2c9e04fd.gif`](img/e1b6e9e801c40dcecd46ba020ff59fce.png)

检测一个橙色物体：

![`user-images.githubusercontent.com/909104/153092153-d9c08dfe-105b-408a-8e1e-295da8a78c19.jpg`](img/bc46e0b298d88972360b661b4bbe5b49.png)

检测一个杯子：

![`user-images.githubusercontent.com/909104/153092155-4b90002f-a0f3-4267-8d70-e713e7b4d5a0.jpg`](img/9d6ef3cc6c8976013a2cc76e7328778a.png)

## 故障排除：性能

PyTorch 默认会使用所有可用的核心。如果您的树莓派上有任何后台运行的东西，可能会导致模型推断时出现延迟峰值。为了缓解这个问题，您可以减少线程数，这将减少峰值延迟，但会有一点性能损失。

```py
torch.set_num_threads(2) 
```

对于`shufflenet_v2_x1_5`，使用`2 个线程`而不是`4 个线程`会将最佳情况下的延迟增加到`72 毫秒`，而不是`60 毫秒`，但会消除`128 毫秒`的延迟峰值。

## 下一步

您可以创建自己的模型或微调现有模型。如果您在[torchvision.models.quantized](https://pytorch.org/vision/stable/models.html#quantized-models)中的一个模型上进行微调，大部分融合和量化的工作已经为您完成，因此您可以直接在树莓派上部署并获得良好的性能。

查看更多：

+   [量化](https://pytorch.org/docs/stable/quantization.html)获取有关如何量化和融合您的模型的更多信息。

+   [迁移学习教程](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)介绍如何使用迁移学习来微调预先存在的模型以适应您的数据集。
