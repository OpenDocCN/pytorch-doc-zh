# 分析您的 PyTorch 模块

> 原文：[`pytorch.org/tutorials/beginner/profiler.html`](https://pytorch.org/tutorials/beginner/profiler.html)

注意

点击这里下载完整示例代码

**作者：**[Suraj Subramanian](https://github.com/suraj813)

PyTorch 包含一个分析器 API，可用于识别代码中各种 PyTorch 操作的时间和内存成本。分析器可以轻松集成到您的代码中，并且结果可以打印为表格或返回为 JSON 跟踪文件。

注意

分析器支持多线程模型。分析器在与操作相同的线程中运行，但也会分析可能在另一个线程中运行的子操作符。同时运行的分析器将被限定在自己的线程中，以防止结果混合。

注意

PyTorch 1.8 引入了新的 API，将在未来版本中取代旧的分析器 API。请查看新 API 页面：[此处](https://pytorch.org/docs/master/profiler.html)。

前往[此处的教程](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)快速了解分析器 API 的使用。

* * *

```py
import torch
import numpy as np
from torch import nn
import torch.autograd.profiler as profiler 
```

## 使用分析器进行性能调试

分析器可用于识别模型中的性能瓶颈。在此示例中，我们构建了一个执行两个子任务的自定义模块：

+   对输入进行线性变换，并

+   使用转换结果在掩码张量上获取索引。

我们使用`profiler.record_function("label")`将每个子任务的代码包装在单独的带标签的上下文管理器中。在分析器输出中，子任务中所有操作的聚合性能指标将显示在相应的标签下。

请注意，使用分析器会产生一些开销，最好仅用于调查代码。如果您正在进行运行时间基准测试，请记得将其删除。

```py
class MyModule(nn.Module):
    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        super(MyModule, self).__init__()
        self.linear = nn.Linear(in_features, out_features, bias)

    def forward(self, input, mask):
        with profiler.record_function("LINEAR PASS"):
            out = self.linear(input)

        with profiler.record_function("MASK INDICES"):
            threshold = out.sum(axis=1).mean().item()
            hi_idx = np.argwhere(mask.cpu().numpy() > threshold)
            hi_idx = torch.from_numpy(hi_idx).cuda()

        return out, hi_idx 
```

## 分析前向传递

我们初始化随机输入和掩码张量，以及模型。

在运行分析器之前，我们先热身 CUDA 以确保准确的性能基准测试。我们将模块的前向传递包装在`profiler.profile`上下文管理器中。`with_stack=True`参数会在跟踪中附加操作的文件和行号。

警告

`with_stack=True`会产生额外的开销，更适合用于调查代码。如果您正在进行性能基准测试，请记得将其删除。

```py
model = MyModule(500, 10).cuda()
input = torch.rand(128, 500).cuda()
mask = torch.rand((500, 500, 500), dtype=torch.double).cuda()

# warm-up
model(input, mask)

with profiler.profile(with_stack=True, profile_memory=True) as prof:
    out, idx = model(input, mask) 
```

## 打印分析器结果

最后，我们打印分析器结果。`profiler.key_averages`按运算符名称聚合结果，并可选择按输入形状和/或堆栈跟踪事件进行分组。按输入形状分组有助于识别模型使用的张量形状。

在这里，我们使用`group_by_stack_n=5`，它按操作及其回溯（截断为最近的 5 个事件）对运行时间进行聚合，并按其注册顺序显示事件。表格也可以通过传递`sort_by`参数进行排序（请参考[文档](https://pytorch.org/docs/stable/autograd.html#profiler)以获取有效的排序键）。

注意

在笔记本中运行分析器时，您可能会看到类似`<ipython-input-18-193a910735e8>(13): forward`的条目，而不是堆栈跟踪中的文件名。这些对应于`<notebook-cell>(行号): 调用函数`。

```py
print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))

"""
(Some columns are omitted)

-------------  ------------  ------------  ------------  ---------------------------------
 Name    Self CPU %      Self CPU  Self CPU Mem   Source Location
-------------  ------------  ------------  ------------  ---------------------------------
 MASK INDICES        87.88%        5.212s    -953.67 Mb  /mnt/xarfuse/.../torch/au
 <ipython-input-...>(10): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(9): <module>
 /mnt/xarfuse/.../IPython/

 aten::copy_        12.07%     715.848ms           0 b  <ipython-input-...>(12): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(9): <module>
 /mnt/xarfuse/.../IPython/
 /mnt/xarfuse/.../IPython/

 LINEAR PASS         0.01%     350.151us         -20 b  /mnt/xarfuse/.../torch/au
 <ipython-input-...>(7): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(9): <module>
 /mnt/xarfuse/.../IPython/

 aten::addmm         0.00%     293.342us           0 b  /mnt/xarfuse/.../torch/nn
 /mnt/xarfuse/.../torch/nn
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(8): forward
 /mnt/xarfuse/.../torch/nn

 aten::mean         0.00%     235.095us           0 b  <ipython-input-...>(11): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(9): <module>
 /mnt/xarfuse/.../IPython/
 /mnt/xarfuse/.../IPython/

-----------------------------  ------------  ---------- ----------------------------------
Self CPU time total: 5.931s

""" 
```

## 提高内存性能

请注意，从内存和时间方面来看，最昂贵的操作是`forward (10)`，代表 MASK INDICES 内的操作。让我们先尝试解决内存消耗问题。我们可以看到第 12 行的`.to()`操作消耗了 953.67 Mb。此操作将`mask`复制到 CPU。`mask`是用`torch.double`数据类型初始化的。我们是否可以通过将其转换为`torch.float`来减少内存占用？

```py
model = MyModule(500, 10).cuda()
input = torch.rand(128, 500).cuda()
mask = torch.rand((500, 500, 500), dtype=torch.float).cuda()

# warm-up
model(input, mask)

with profiler.profile(with_stack=True, profile_memory=True) as prof:
    out, idx = model(input, mask)

print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))

"""
(Some columns are omitted)

-----------------  ------------  ------------  ------------  --------------------------------
 Name    Self CPU %      Self CPU  Self CPU Mem   Source Location
-----------------  ------------  ------------  ------------  --------------------------------
 MASK INDICES        93.61%        5.006s    -476.84 Mb  /mnt/xarfuse/.../torch/au
 <ipython-input-...>(10): forward
 /mnt/xarfuse/  /torch/nn
 <ipython-input-...>(9): <module>
 /mnt/xarfuse/.../IPython/

 aten::copy_         6.34%     338.759ms           0 b  <ipython-input-...>(12): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(9): <module>
 /mnt/xarfuse/.../IPython/
 /mnt/xarfuse/.../IPython/

 aten::as_strided         0.01%     281.808us           0 b  <ipython-input-...>(11): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(9): <module>
 /mnt/xarfuse/.../IPython/
 /mnt/xarfuse/.../IPython/

 aten::addmm         0.01%     275.721us           0 b  /mnt/xarfuse/.../torch/nn
 /mnt/xarfuse/.../torch/nn
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(8): forward
 /mnt/xarfuse/.../torch/nn

 aten::_local        0.01%     268.650us           0 b  <ipython-input-...>(11): forward
 _scalar_dense                                          /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(9): <module>
 /mnt/xarfuse/.../IPython/
 /mnt/xarfuse/.../IPython/

-----------------  ------------  ------------  ------------  --------------------------------
Self CPU time total: 5.347s

""" 
```

此操作的 CPU 内存占用减半。

## 提高时间性能

虽然消耗的时间也有所减少，但仍然太高。原来从 CUDA 到 CPU 复制矩阵是非常昂贵的！`forward (12)`中的`aten::copy_`操作符将`mask`复制到 CPU，以便可以使用 NumPy 的`argwhere`函数。`forward(13)`中的`aten::copy_`将数组复制回 CUDA 作为张量。如果我们在这里使用`torch`函数`nonzero()`，就可以消除这两个操作。

```py
class MyModule(nn.Module):
    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        super(MyModule, self).__init__()
        self.linear = nn.Linear(in_features, out_features, bias)

    def forward(self, input, mask):
        with profiler.record_function("LINEAR PASS"):
            out = self.linear(input)

        with profiler.record_function("MASK INDICES"):
            threshold = out.sum(axis=1).mean()
            hi_idx = (mask > threshold).nonzero(as_tuple=True)

        return out, hi_idx

model = MyModule(500, 10).cuda()
input = torch.rand(128, 500).cuda()
mask = torch.rand((500, 500, 500), dtype=torch.float).cuda()

# warm-up
model(input, mask)

with profiler.profile(with_stack=True, profile_memory=True) as prof:
    out, idx = model(input, mask)

print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))

"""
(Some columns are omitted)

--------------  ------------  ------------  ------------  ---------------------------------
 Name    Self CPU %      Self CPU  Self CPU Mem   Source Location
--------------  ------------  ------------  ------------  ---------------------------------
 aten::gt        57.17%     129.089ms           0 b  <ipython-input-...>(12): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(25): <module>
 /mnt/xarfuse/.../IPython/
 /mnt/xarfuse/.../IPython/

 aten::nonzero        37.38%      84.402ms           0 b  <ipython-input-...>(12): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(25): <module>
 /mnt/xarfuse/.../IPython/
 /mnt/xarfuse/.../IPython/

 INDEX SCORE         3.32%       7.491ms    -119.21 Mb  /mnt/xarfuse/.../torch/au
 <ipython-input-...>(10): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(25): <module>
 /mnt/xarfuse/.../IPython/

aten::as_strided         0.20%    441.587us          0 b  <ipython-input-...>(12): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(25): <module>
 /mnt/xarfuse/.../IPython/
 /mnt/xarfuse/.../IPython/

 aten::nonzero
 _numpy             0.18%     395.602us           0 b  <ipython-input-...>(12): forward
 /mnt/xarfuse/.../torch/nn
 <ipython-input-...>(25): <module>
 /mnt/xarfuse/.../IPython/
 /mnt/xarfuse/.../IPython/
--------------  ------------  ------------  ------------  ---------------------------------
Self CPU time total: 225.801ms

""" 
```

## 进一步阅读

我们已经看到了如何使用分析器来调查 PyTorch 模型中的时间和内存瓶颈。在这里阅读更多关于分析器的信息：

+   [分析器使用方法](https://pytorch.org/tutorials/recipes/recipes/profiler.html)

+   [基于 RPC 的工作负载分析](https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html)

+   [分析器 API 文档](https://pytorch.org/docs/stable/autograd.html?highlight=profiler#profiler)

**脚本的总运行时间:** ( 0 分钟 0.000 秒)

`下载 Python 源代码: profiler.py`

`下载 Jupyter 笔记本: profiler.ipynb`

[Sphinx-Gallery 生成的图库](https://sphinx-gallery.github.io)
