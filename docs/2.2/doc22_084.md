# torch.utils.bottleneck

> 原文：[`pytorch.org/docs/stable/bottleneck.html`](https://pytorch.org/docs/stable/bottleneck.html)> 
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)


torch.utils.bottleneck 是一个工具，可用作调试程序中瓶颈的初始步骤。它使用 Python 分析器和 PyTorch 的 autograd 分析器对脚本运行进行总结。

在命令行上运行它

```py
python -m torch.utils.bottleneck /path/to/source/script.py [args] 
```

[args]是传递给 script.py 的任意数量的参数，或者运行`python -m torch.utils.bottleneck -h`以获取更多用法说明。

警告

由于您的脚本将被分析，请确保它在有限的时间内退出。

警告

由于 CUDA 内核的异步性质，当针对 CUDA 代码运行时，cProfile 输出和 CPU 模式的 autograd 分析器可能不会显示正确的时间：报告的 CPU 时间报告了用于启动内核的时间，但不包括内核在 GPU 上执行的时间，除非操作进行同步。在常规 CPU 模式分析器下，执行同步的操作似乎非常昂贵。在这些时间不正确的情况下，CUDA 模式的 autograd 分析器可能有所帮助。

注意

要决定查看哪种（仅 CPU 模式还是 CUDA 模式）autograd 分析器输出，请首先检查您的脚本是否受 CPU 限制（“CPU 总时间远远大于 CUDA 总时间”）。如果是 CPU 受限的，查看 CPU 模式 autograd 分析器的结果将有所帮助。另一方面，如果您的脚本大部分时间在 GPU 上执行，则有意义的是开始查找 CUDA 模式 autograd 分析器输出中负责的 CUDA 运算符。

当然，现实情况要复杂得多，根据您评估的模型部分，您的脚本可能不属于这两个极端之一。如果分析器的输出没有帮助，您可以尝试查看`torch.autograd.profiler.emit_nvtx()`的结果，使用`nvprof`。但请注意，NVTX 的开销非常高，通常会导致时间线严重倾斜。同样，`Intel® VTune™ Profiler`可以通过`torch.autograd.profiler.emit_itt()`进一步分析在 Intel 平台上的性能。

警告

如果您正在分析 CUDA 代码，`bottleneck`运行的第一个分析器（cProfile）将在其时间报告中包括 CUDA 启动时间（CUDA 缓冲区分配成本）。如果您的瓶颈导致代码比 CUDA 启动时间慢得多，则这并不重要。

对于分析器的更复杂用法（如在多 GPU 情况下），请参阅[`docs.python.org/3/library/profile.html`](https://docs.python.org/3/library/profile.html)或`torch.autograd.profiler.profile()`获取更多信息。
