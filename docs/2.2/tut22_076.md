# （beta）PyTorch 中的通道最后内存格式

> 原文：[`pytorch.org/tutorials/intermediate/memory_format_tutorial.html`](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)

注意

点击这里下载完整示例代码

**作者**：[Vitaly Fedyunin](https://github.com/VitalyFedyunin)

## 什么是通道最后

通道最后的内存格式是在保留维度顺序的同时对 NCHW 张量进行排序的另一种方式。通道最后的张量以通道成为最密集的维度（即按像素存储图像）的方式进行排序。

例如，NCHW 张量的经典（连续）存储（在我们的情况下，是两个具有 3 个颜色通道的 4x4 图像）如下所示：

![classic_memory_format](img/77e0660b596f377125122a2409288181.png)

通道最后内存格式以不同的方式对数据进行排序：

![channels_last_memory_format](img/462373919a0dfe17cd816fa0d8af140c.png)

Pytorch 通过利用现有的步幅结构来支持内存格式（并提供与现有模型（包括 eager、JIT 和 TorchScript）的向后兼容性）。例如，通道最后格式中的 10x3x16x16 批次将具有等于（768，1，48，3）的步幅。

通道最后内存格式仅适用于 4D NCHW 张量。

## 内存格式 API

以下是如何在连续和通道最后的内存格式之间转换张量的方法。

经典的 PyTorch 连续张量

```py
import torch

N, C, H, W = 10, 3, 32, 32
x = torch.empty(N, C, H, W)
print(x.stride())  # Outputs: (3072, 1024, 32, 1) 
```

```py
(3072, 1024, 32, 1) 
```

转换运算符

```py
x = x.to(memory_format=torch.channels_last)
print(x.shape)  # Outputs: (10, 3, 32, 32) as dimensions order preserved
print(x.stride())  # Outputs: (3072, 1, 96, 3) 
```

```py
torch.Size([10, 3, 32, 32])
(3072, 1, 96, 3) 
```

回到连续

```py
x = x.to(memory_format=torch.contiguous_format)
print(x.stride())  # Outputs: (3072, 1024, 32, 1) 
```

```py
(3072, 1024, 32, 1) 
```

备选选项

```py
x = x.contiguous(memory_format=torch.channels_last)
print(x.stride())  # Outputs: (3072, 1, 96, 3) 
```

```py
(3072, 1, 96, 3) 
```

格式检查

```py
print(x.is_contiguous(memory_format=torch.channels_last))  # Outputs: True 
```

```py
True 
```

`to`和`contiguous`这两个 API 之间存在一些细微差别。我们建议在明确转换张量的内存格式时坚持使用`to`。

对于一般情况，这两个 API 的行为是相同的。然而，在特殊情况下，对于大小为`NCHW`的 4D 张量，当`C==1`或`H==1 && W==1`时，只有`to`会生成适当的步幅以表示通道最后的内存格式。

这是因为在上述两种情况中，张量的内存格式是模糊的，即大小为`N1HW`的连续张量在内存存储中既是`contiguous`又是通道最后的。因此，它们已被视为给定内存格式的`is_contiguous`，因此`contiguous`调用变为无操作，并且不会更新步幅。相反，`to`会在尺寸为 1 的维度上重新调整张量的步幅，以正确表示预期的内存格式。

```py
special_x = torch.empty(4, 1, 4, 4)
print(special_x.is_contiguous(memory_format=torch.channels_last))  # Outputs: True
print(special_x.is_contiguous(memory_format=torch.contiguous_format))  # Outputs: True 
```

```py
True
True 
```

相同的情况也适用于显式置换 API `permute`。在可能发生模糊的特殊情况下，`permute`不能保证生成适当携带预期内存格式的步幅。我们建议使用`to`并明确指定内存格式，以避免意外行为。

另外需要注意的是，在极端情况下，当三个非批量维度都等于`1`时（`C==1 && H==1 && W==1`），当前的实现无法将张量标记为通道最后的内存格式。

创建为通道最后

```py
x = torch.empty(N, C, H, W, memory_format=torch.channels_last)
print(x.stride())  # Outputs: (3072, 1, 96, 3) 
```

```py
(3072, 1, 96, 3) 
```

`clone` 保留内存格式

```py
y = x.clone()
print(y.stride())  # Outputs: (3072, 1, 96, 3) 
```

```py
(3072, 1, 96, 3) 
```

`to`，`cuda`，`float` … 保留内存格式

```py
if torch.cuda.is_available():
    y = x.cuda()
    print(y.stride())  # Outputs: (3072, 1, 96, 3) 
```

```py
(3072, 1, 96, 3) 
```

`empty_like`，`*_like`运算符保留内存格式

```py
y = torch.empty_like(x)
print(y.stride())  # Outputs: (3072, 1, 96, 3) 
```

```py
(3072, 1, 96, 3) 
```

逐点运算符保留内存格式

```py
z = x + y
print(z.stride())  # Outputs: (3072, 1, 96, 3) 
```

```py
(3072, 1, 96, 3) 
```

使用`cudnn`后端的`Conv`，`Batchnorm`模块支持通道最后（仅适用于 cuDNN >= 7.6）。卷积模块，与二进制逐点运算符不同，通道最后是主导的内存格式。如果所有输入都在连续的内存格式中，操作符将以连续的内存格式生成输出。否则，输出将以通道最后的内存格式生成。

```py
if torch.backends.cudnn.is_available() and torch.backends.cudnn.version() >= 7603:
    model = torch.nn.Conv2d(8, 4, 3).cuda().half()
    model = model.to(memory_format=torch.channels_last)  # Module parameters need to be channels last

    input = torch.randint(1, 10, (2, 8, 4, 4), dtype=torch.float32, requires_grad=True)
    input = input.to(device="cuda", memory_format=torch.channels_last, dtype=torch.float16)

    out = model(input)
    print(out.is_contiguous(memory_format=torch.channels_last))  # Outputs: True 
```

```py
True 
```

当输入张量到达不支持通道最后的操作符时，内核应自动应用置换以恢复输入张量上的连续性。这会引入开销并停止通道最后的内存格式传播。尽管如此，它保证了正确的输出。

## 性能收益

Channels last 内存格式优化在 GPU 和 CPU 上都可用。在 GPU 上，观察到 NVIDIA 硬件上具有 Tensor Cores 支持的运行在降低精度（`torch.float16`）时，性能增益最显著。我们能够在使用‘AMP（自动混合精度）’训练脚本时，通过 Channels last 实现超过 22%的性能增益，同时利用了由 NVIDIA 提供的 AMP [`github.com/NVIDIA/apex`](https://github.com/NVIDIA/apex)。

`python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2  ./data`

```py
# opt_level = O2
# keep_batchnorm_fp32 = None <class 'NoneType'>
# loss_scale = None <class 'NoneType'>
# CUDNN VERSION: 7603
# => creating model 'resnet50'
# Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.
# Defaults for this optimization level are:
# enabled                : True
# opt_level              : O2
# cast_model_type        : torch.float16
# patch_torch_functions  : False
# keep_batchnorm_fp32    : True
# master_weights         : True
# loss_scale             : dynamic
# Processing user overrides (additional kwargs that are not None)...
# After processing overrides, optimization options are:
# enabled                : True
# opt_level              : O2
# cast_model_type        : torch.float16
# patch_torch_functions  : False
# keep_batchnorm_fp32    : True
# master_weights         : True
# loss_scale             : dynamic
# Epoch: [0][10/125] Time 0.866 (0.866) Speed 230.949 (230.949) Loss 0.6735125184 (0.6735) Prec@1 61.000 (61.000) Prec@5 100.000 (100.000)
# Epoch: [0][20/125] Time 0.259 (0.562) Speed 773.481 (355.693) Loss 0.6968704462 (0.6852) Prec@1 55.000 (58.000) Prec@5 100.000 (100.000)
# Epoch: [0][30/125] Time 0.258 (0.461) Speed 775.089 (433.965) Loss 0.7877287269 (0.7194) Prec@1 51.500 (55.833) Prec@5 100.000 (100.000)
# Epoch: [0][40/125] Time 0.259 (0.410) Speed 771.710 (487.281) Loss 0.8285319805 (0.7467) Prec@1 48.500 (54.000) Prec@5 100.000 (100.000)
# Epoch: [0][50/125] Time 0.260 (0.380) Speed 770.090 (525.908) Loss 0.7370464802 (0.7447) Prec@1 56.500 (54.500) Prec@5 100.000 (100.000)
# Epoch: [0][60/125] Time 0.258 (0.360) Speed 775.623 (555.728) Loss 0.7592862844 (0.7472) Prec@1 51.000 (53.917) Prec@5 100.000 (100.000)
# Epoch: [0][70/125] Time 0.258 (0.345) Speed 774.746 (579.115) Loss 1.9698858261 (0.9218) Prec@1 49.500 (53.286) Prec@5 100.000 (100.000)
# Epoch: [0][80/125] Time 0.260 (0.335) Speed 770.324 (597.659) Loss 2.2505953312 (1.0879) Prec@1 50.500 (52.938) Prec@5 100.000 (100.000) 
```

通过传递`--channels-last true`允许在 Channels last 格式中运行模型，观察到 22%的性能增益。

`python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 --channels-last true ./data`

```py
# opt_level = O2
# keep_batchnorm_fp32 = None <class 'NoneType'>
# loss_scale = None <class 'NoneType'>
#
# CUDNN VERSION: 7603
#
# => creating model 'resnet50'
# Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.
#
# Defaults for this optimization level are:
# enabled                : True
# opt_level              : O2
# cast_model_type        : torch.float16
# patch_torch_functions  : False
# keep_batchnorm_fp32    : True
# master_weights         : True
# loss_scale             : dynamic
# Processing user overrides (additional kwargs that are not None)...
# After processing overrides, optimization options are:
# enabled                : True
# opt_level              : O2
# cast_model_type        : torch.float16
# patch_torch_functions  : False
# keep_batchnorm_fp32    : True
# master_weights         : True
# loss_scale             : dynamic
#
# Epoch: [0][10/125] Time 0.767 (0.767) Speed 260.785 (260.785) Loss 0.7579724789 (0.7580) Prec@1 53.500 (53.500) Prec@5 100.000 (100.000)
# Epoch: [0][20/125] Time 0.198 (0.482) Speed 1012.135 (414.716) Loss 0.7007197738 (0.7293) Prec@1 49.000 (51.250) Prec@5 100.000 (100.000)
# Epoch: [0][30/125] Time 0.198 (0.387) Speed 1010.977 (516.198) Loss 0.7113101482 (0.7233) Prec@1 55.500 (52.667) Prec@5 100.000 (100.000)
# Epoch: [0][40/125] Time 0.197 (0.340) Speed 1013.023 (588.333) Loss 0.8943189979 (0.7661) Prec@1 54.000 (53.000) Prec@5 100.000 (100.000)
# Epoch: [0][50/125] Time 0.198 (0.312) Speed 1010.541 (641.977) Loss 1.7113249302 (0.9551) Prec@1 51.000 (52.600) Prec@5 100.000 (100.000)
# Epoch: [0][60/125] Time 0.198 (0.293) Speed 1011.163 (683.574) Loss 5.8537774086 (1.7716) Prec@1 50.500 (52.250) Prec@5 100.000 (100.000)
# Epoch: [0][70/125] Time 0.198 (0.279) Speed 1011.453 (716.767) Loss 5.7595844269 (2.3413) Prec@1 46.500 (51.429) Prec@5 100.000 (100.000)
# Epoch: [0][80/125] Time 0.198 (0.269) Speed 1011.827 (743.883) Loss 2.8196096420 (2.4011) Prec@1 47.500 (50.938) Prec@5 100.000 (100.000) 
```

以下模型列表完全支持 Channels last，并在 Volta 设备上显示 8%-35%的性能增益：`alexnet`，`mnasnet0_5`，`mnasnet0_75`，`mnasnet1_0`，`mnasnet1_3`，`mobilenet_v2`，`resnet101`，`resnet152`，`resnet18`，`resnet34`，`resnet50`，`resnext50_32x4d`，`shufflenet_v2_x0_5`，`shufflenet_v2_x1_0`，`shufflenet_v2_x1_5`，`shufflenet_v2_x2_0`，`squeezenet1_0`，`squeezenet1_1`，`vgg11`，`vgg11_bn`，`vgg13`，`vgg13_bn`，`vgg16`，`vgg16_bn`，`vgg19`，`vgg19_bn`，`wide_resnet101_2`，`wide_resnet50_2`

以下模型列表完全支持 Channels last，并在 Intel(R) Xeon(R) Ice Lake（或更新）CPU 上显示 26%-76%的性能增益：`alexnet`，`densenet121`，`densenet161`，`densenet169`，`googlenet`，`inception_v3`，`mnasnet0_5`，`mnasnet1_0`，`resnet101`，`resnet152`，`resnet18`，`resnet34`，`resnet50`，`resnext101_32x8d`，`resnext50_32x4d`，`shufflenet_v2_x0_5`，`shufflenet_v2_x1_0`，`squeezenet1_0`，`squeezenet1_1`，`vgg11`，`vgg11_bn`，`vgg13`，`vgg13_bn`，`vgg16`，`vgg16_bn`，`vgg19`，`vgg19_bn`，`wide_resnet101_2`，`wide_resnet50_2`

## 转换现有模型

Channels last 支持不仅限于现有模型，因为任何模型都可以转换为 Channels last 并在输入（或某些权重）正确格式化后通过图形传播格式。

```py
# Need to be done once, after model initialization (or load)
model = model.to(memory_format=torch.channels_last)  # Replace with your model

# Need to be done for every input
input = input.to(memory_format=torch.channels_last)  # Replace with your input
output = model(input) 
```

然而，并非所有运算符都完全转换为支持 Channels last（通常返回连续的输出）。在上面发布的示例中，不支持 Channels last 的层将停止内存格式传播。尽管如此，由于我们已将模型转换为 Channels last 格式，这意味着每个卷积层，其 4 维权重在 Channels last 内存格式中，将恢复 Channels last 内存格式并从更快的内核中受益。

但是，不支持 Channels last 的运算符会通过置换引入开销。可选地，您可以调查并识别模型中不支持 Channels last 的运算符，如果要改进转换模型的性能。

这意味着您需要根据支持的运算符列表[`github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support`](https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support)验证所使用的运算符列表，或者在急切执行模式中引入内存格式检查并运行您的模型。

在运行以下代码后，如果运算符的输出与输入的内存格式不匹配，运算符将引发异常。

```py
def contains_cl(args):
    for t in args:
        if isinstance(t, torch.Tensor):
            if t.is_contiguous(memory_format=torch.channels_last) and not t.is_contiguous():
                return True
        elif isinstance(t, list) or isinstance(t, tuple):
            if contains_cl(list(t)):
                return True
    return False

def print_inputs(args, indent=""):
    for t in args:
        if isinstance(t, torch.Tensor):
            print(indent, t.stride(), t.shape, t.device, t.dtype)
        elif isinstance(t, list) or isinstance(t, tuple):
            print(indent, type(t))
            print_inputs(list(t), indent=indent + "    ")
        else:
            print(indent, t)

def check_wrapper(fn):
    name = fn.__name__

    def check_cl(*args, **kwargs):
        was_cl = contains_cl(args)
        try:
            result = fn(*args, **kwargs)
        except Exception as e:
            print("`{}` inputs are:".format(name))
            print_inputs(args)
            print("-------------------")
            raise e
        failed = False
        if was_cl:
            if isinstance(result, torch.Tensor):
                if result.dim() == 4 and not result.is_contiguous(memory_format=torch.channels_last):
                    print(
                        "`{}` got channels_last input, but output is not channels_last:".format(name),
                        result.shape,
                        result.stride(),
                        result.device,
                        result.dtype,
                    )
                    failed = True
        if failed and True:
            print("`{}` inputs are:".format(name))
            print_inputs(args)
            raise Exception("Operator `{}` lost channels_last property".format(name))
        return result

    return check_cl

old_attrs = dict()

def attribute(m):
    old_attrs[m] = dict()
    for i in dir(m):
        e = getattr(m, i)
        exclude_functions = ["is_cuda", "has_names", "numel", "stride", "Tensor", "is_contiguous", "__class__"]
        if i not in exclude_functions and not i.startswith("_") and "__call__" in dir(e):
            try:
                old_attrs[m][i] = e
                setattr(m, i, check_wrapper(e))
            except Exception as e:
                print(i)
                print(e)

attribute(torch.Tensor)
attribute(torch.nn.functional)
attribute(torch) 
```

如果发现一个不支持 Channels last 张量的运算符，并且您想要贡献，可以随时使用以下开发者指南[`github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators`](https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators)。

以下代码是为了恢复 torch 的属性。

```py
for (m, attrs) in old_attrs.items():
    for (k, v) in attrs.items():
        setattr(m, k, v) 
```

## 需要做的工作

还有许多事情要做，例如：

+   解决`N1HW`和`NC11`张量的歧义；

+   测试分布式训练支持；

+   提高运算符覆盖率。

如果您有反馈和/或改进建议，请通过创建[一个问题](https://github.com/pytorch/pytorch/issues)让我们知道。

**脚本的总运行时间：**（0 分钟 0.038 秒）

`下载 Python 源代码：memory_format_tutorial.py`

`下载 Jupyter 笔记本：memory_format_tutorial.ipynb`

[Sphinx-Gallery 生成的图库](https://sphinx-gallery.github.io)
