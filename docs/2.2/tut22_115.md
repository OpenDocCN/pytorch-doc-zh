# 开始使用分布式数据并行

> [https://pytorch.org/tutorials/intermediate/ddp_tutorial.html](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)

**作者**：[Shen Li](https://mrshenli.github.io/)

**编辑者**：[Joe Zhu](https://github.com/gunandrose4u)

注意

查看并编辑此教程在[github](https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst)。

先决条件:

+   [PyTorch 分布式概述](../beginner/dist_overview.html)

+   [DistributedDataParallel API 文档](https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html)

+   [DistributedDataParallel 笔记](https://pytorch.org/docs/master/notes/ddp.html)

[DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#module-torch.nn.parallel)（DDP）在模块级别实现了数据并行，可以在多台机器上运行。使用DDP的应用程序应该生成多个进程，并为每个进程创建一个单独的DDP实例。DDP使用[torch.distributed](https://pytorch.org/tutorials/intermediate/dist_tuto.html)包中的集体通信来同步梯度和缓冲区。更具体地说，DDP为`model.parameters()`给定的每个参数注册一个自动求导钩子，当在反向传播中计算相应的梯度时，该钩子将触发。然后DDP使用该信号来触发跨进程的梯度同步。更多详细信息请参考[DDP设计说明](https://pytorch.org/docs/master/notes/ddp.html)。

使用DDP的推荐方式是为每个模型副本生成一个进程，其中一个模型副本可以跨多个设备。DDP进程可以放置在同一台机器上或跨多台机器，但GPU设备不能在进程之间共享。本教程从基本的DDP用例开始，然后演示更高级的用例，包括模型检查点和将DDP与模型并行结合使用。

注意

本教程中的代码在一个8-GPU服务器上运行，但可以很容易地推广到其他环境。

## `DataParallel`和`DistributedDataParallel`之间的比较

在我们深入讨论之前，让我们澄清一下为什么尽管增加了复杂性，你会考虑使用`DistributedDataParallel`而不是`DataParallel`：

+   首先，`DataParallel` 是单进程、多线程的，仅适用于单台机器，而 `DistributedDataParallel` 是多进程的，适用于单机和多机训练。由于线程之间的 GIL 冲突、每次迭代复制模型以及输入散布和输出聚集引入的额外开销，即使在单台机器上，`DataParallel` 通常比 `DistributedDataParallel` 慢。

+   回想一下从[之前的教程](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)中得知，如果你的模型太大无法放入单个GPU中，你必须使用**模型并行**将其分割到多个GPU上。`DistributedDataParallel`与**模型并行**一起工作；`DataParallel`目前不支持。当DDP与模型并行结合时，每个DDP进程都会使用模型并行，所有进程共同使用数据并行。

+   如果您的模型需要跨多台机器，或者您的用例不适合数据并行主义范式，请参阅[RPC API](https://pytorch.org/docs/stable/rpc.html)以获取更通用的分布式训练支持。

## 基本用例

要创建一个DDP模块，你必须首先正确设置进程组。更多细节可以在[使用PyTorch编写分布式应用程序](https://pytorch.org/tutorials/intermediate/dist_tuto.html)中找到。

```py
import os
import sys
import tempfile
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp

from torch.nn.parallel import DistributedDataParallel as DDP

# On Windows platform, the torch.distributed package only
# supports Gloo backend, FileStore and TcpStore.
# For FileStore, set init_method parameter in init_process_group
# to a local file. Example as follow:
# init_method="file:///f:/libtmp/some_file"
# dist.init_process_group(
#    "gloo",
#    rank=rank,
#    init_method=init_method,
#    world_size=world_size)
# For TcpStore, same way as on Linux.

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # initialize the process group
    dist.init_process_group("gloo", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group() 
```

现在，让我们创建一个玩具模块，用DDP包装它，并提供一些虚拟输入数据。请注意，由于DDP在构造函数中从rank 0进程向所有其他进程广播模型状态，您不需要担心不同的DDP进程从不同的初始模型参数值开始。

```py
class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(10, 5)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))

def demo_basic(rank, world_size):
    print(f"Running basic DDP example on rank {rank}.")
    setup(rank, world_size)

    # create model and move it to GPU with id rank
    model = ToyModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(rank)
    loss_fn(outputs, labels).backward()
    optimizer.step()

    cleanup()

def run_demo(demo_fn, world_size):
    mp.spawn(demo_fn,
             args=(world_size,),
             nprocs=world_size,
             join=True) 
```

正如您所看到的，DDP封装了较低级别的分布式通信细节，并提供了一个干净的API，就像它是一个本地模型一样。梯度同步通信发生在反向传播过程中，并与反向计算重叠。当`backward()`返回时，`param.grad`已经包含了同步的梯度张量。对于基本用例，DDP只需要几行额外的代码来设置进程组。当将DDP应用于更高级的用例时，一些注意事项需要谨慎处理。

## 处理速度不均衡

在DDP中，构造函数、前向传递和后向传递是分布式同步点。预期不同的进程将启动相同数量的同步，并按相同顺序到达这些同步点，并在大致相同的时间进入每个同步点。否则，快速进程可能会提前到达并在等待滞后者时超时。因此，用户负责在进程之间平衡工作负载分布。有时，由于网络延迟、资源竞争或不可预测的工作负载波动等原因，不可避免地会出现处理速度不均衡的情况。为了避免在这些情况下超时，请确保在调用[init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)时传递一个足够大的`timeout`值。

## 保存和加载检查点

在训练过程中，通常使用`torch.save`和`torch.load`来对模块进行检查点，并从检查点中恢复。有关更多详细信息，请参阅[SAVING AND LOADING MODELS](https://pytorch.org/tutorials/beginner/saving_loading_models.html)。在使用DDP时，一种优化是在一个进程中保存模型，然后加载到所有进程中，减少写入开销。这是正确的，因为所有进程都从相同的参数开始，并且在反向传递中梯度是同步的，因此优化器应该保持将参数设置为相同的值。如果使用此优化，请确保在保存完成之前没有进程开始加载。此外，在加载模块时，您需要提供一个适当的`map_location`参数，以防止一个进程进入其他设备。如果缺少`map_location`，`torch.load`将首先将模块加载到CPU，然后将每个参数复制到保存的位置，这将导致同一台机器上的所有进程使用相同的设备集。有关更高级的故障恢复和弹性支持，请参阅[TorchElastic](https://pytorch.org/elastic)。

```py
def demo_checkpoint(rank, world_size):
    print(f"Running DDP checkpoint example on rank {rank}.")
    setup(rank, world_size)

    model = ToyModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    CHECKPOINT_PATH = tempfile.gettempdir() + "/model.checkpoint"
    if rank == 0:
        # All processes should see same parameters as they all start from same
        # random parameters and gradients are synchronized in backward passes.
        # Therefore, saving it in one process is sufficient.
        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)

    # Use a barrier() to make sure that process 1 loads the model after process
    # 0 saves it.
    dist.barrier()
    # configure map_location properly
    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
    ddp_model.load_state_dict(
        torch.load(CHECKPOINT_PATH, map_location=map_location))

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(rank)

    loss_fn(outputs, labels).backward()
    optimizer.step()

    # Not necessary to use a dist.barrier() to guard the file deletion below
    # as the AllReduce ops in the backward pass of DDP already served as
    # a synchronization.

    if rank == 0:
        os.remove(CHECKPOINT_PATH)

    cleanup() 
```

## 将DDP与模型并行结合起来

DDP也适用于多GPU模型。在训练大型模型和大量数据时，DDP包装多GPU模型尤其有帮助。

```py
class ToyMpModel(nn.Module):
    def __init__(self, dev0, dev1):
        super(ToyMpModel, self).__init__()
        self.dev0 = dev0
        self.dev1 = dev1
        self.net1 = torch.nn.Linear(10, 10).to(dev0)
        self.relu = torch.nn.ReLU()
        self.net2 = torch.nn.Linear(10, 5).to(dev1)

    def forward(self, x):
        x = x.to(self.dev0)
        x = self.relu(self.net1(x))
        x = x.to(self.dev1)
        return self.net2(x) 
```

当将多GPU模型传递给DDP时，`device_ids`和`output_device`必须不设置。输入和输出数据将由应用程序或模型的`forward()`方法放置在适当的设备上。

```py
def demo_model_parallel(rank, world_size):
    print(f"Running DDP with model parallel example on rank {rank}.")
    setup(rank, world_size)

    # setup mp_model and devices for this process
    dev0 = rank * 2
    dev1 = rank * 2 + 1
    mp_model = ToyMpModel(dev0, dev1)
    ddp_mp_model = DDP(mp_model)

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    # outputs will be on dev1
    outputs = ddp_mp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(dev1)
    loss_fn(outputs, labels).backward()
    optimizer.step()

    cleanup()

if __name__ == "__main__":
    n_gpus = torch.cuda.device_count()
    assert n_gpus >= 2, f"Requires at least 2 GPUs to run, but got {n_gpus}"
    world_size = n_gpus
    run_demo(demo_basic, world_size)
    run_demo(demo_checkpoint, world_size)
    world_size = n_gpus//2
    run_demo(demo_model_parallel, world_size) 
```

## 使用torch.distributed.run/torchrun初始化DDP

我们可以利用PyTorch Elastic来简化DDP代码并更轻松地初始化作业。让我们仍然使用Toymodel示例并创建一个名为`elastic_ddp.py`的文件。

```py
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim

from torch.nn.parallel import DistributedDataParallel as DDP

class ToyModel(nn.Module):
    def __init__(self):
        super(ToyModel, self).__init__()
        self.net1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(10, 5)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))

def demo_basic():
    dist.init_process_group("nccl")
    rank = dist.get_rank()
    print(f"Start running basic DDP example on rank {rank}.")

    # create model and move it to GPU with id rank
    device_id = rank % torch.cuda.device_count()
    model = ToyModel().to(device_id)
    ddp_model = DDP(model, device_ids=[device_id])

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(device_id)
    loss_fn(outputs, labels).backward()
    optimizer.step()
    dist.destroy_process_group()

if __name__ == "__main__":
    demo_basic() 
```

然后可以在所有节点上运行 [torch elastic/torchrun](https://pytorch.org/docs/stable/elastic/quickstart.html) 命令来初始化上面创建的DDP作业：

```py
torchrun  --nnodes=2  --nproc_per_node=8  --rdzv_id=100  --rdzv_backend=c10d  --rdzv_endpoint=$MASTER_ADDR:29400  elastic_ddp.py 
```

我们在两台主机上运行DDP脚本，每台主机运行8个进程，也就是说我们在16个GPU上运行它。请注意，`$MASTER_ADDR`在所有节点上必须相同。

torchrun将启动8个进程，并在启动它的节点上的每个进程上调用`elastic_ddp.py`，但用户还需要应用类似slurm的集群管理工具来实际在2个节点上运行此命令。

例如，在启用了SLURM的集群上，我们可以编写一个脚本来运行上面的命令，并将`MASTER_ADDR`设置为：

```py
export  MASTER_ADDR=$(scontrol  show  hostname  ${SLURM_NODELIST}  |  head  -n  1) 
```

然后我们可以使用SLURM命令运行此脚本：`srun --nodes=2 ./torchrun_script.sh`。当然，这只是一个例子；您可以选择自己的集群调度工具来启动torchrun作业。

关于Elastic run 的更多信息，可以查看这个[快速入门文档](https://pytorch.org/docs/stable/elastic/quickstart.html)以了解更多。
