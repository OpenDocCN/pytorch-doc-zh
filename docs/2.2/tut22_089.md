# 使用自定义C++类扩展TorchScript

> 原文：[https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html](https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html)

本教程是[自定义运算符](torch_script_custom_ops.html)教程的后续，介绍了我们为将C++类绑定到TorchScript和Python中构建的API。该API与[pybind11](https://github.com/pybind/pybind11)非常相似，如果您熟悉该系统，大部分概念都会转移到这里。

## 在C++中实现和绑定类

在本教程中，我们将定义一个简单的C++类，该类在成员变量中维护持久状态。

```py
// This header is all you need to do the C++ portions of this
// tutorial
#include  <torch/script.h>
// This header is what defines the custom class registration
// behavior specifically. script.h already includes this, but
// we include it here so you know it exists in case you want
// to look at the API or implementation.
#include  <torch/custom_class.h>

#include  <string>
#include  <vector>

template  <class  T>
struct  MyStackClass  :  torch::CustomClassHolder  {
  std::vector<T>  stack_;
  MyStackClass(std::vector<T>  init)  :  stack_(init.begin(),  init.end())  {}

  void  push(T  x)  {
  stack_.push_back(x);
  }
  T  pop()  {
  auto  val  =  stack_.back();
  stack_.pop_back();
  return  val;
  }

  c10::intrusive_ptr<MyStackClass>  clone()  const  {
  return  c10::make_intrusive<MyStackClass>(stack_);
  }

  void  merge(const  c10::intrusive_ptr<MyStackClass>&  c)  {
  for  (auto&  elem  :  c->stack_)  {
  push(elem);
  }
  }
}; 
```

有几点需要注意：

+   `torch/custom_class.h`是您需要包含的头文件，以便使用自定义类扩展TorchScript。

+   请注意，每当我们使用自定义类的实例时，我们都是通过`c10::intrusive_ptr<>`的实例来进行的。将`intrusive_ptr`视为类似于`std::shared_ptr`的智能指针，但引用计数直接存储在对象中，而不是存储在单独的元数据块中（就像在`std::shared_ptr`中所做的那样）。`torch::Tensor`内部使用相同的指针类型；自定义类也必须使用这种指针类型，以便我们可以一致地管理不同的对象类型。

+   第二件要注意的事情是，用户定义的类必须继承自`torch::CustomClassHolder`。这确保了自定义类有空间来存储引用计数。

现在让我们看看如何使这个类对TorchScript可见，这个过程称为*绑定*类：

```py
// Notice a few things:
// - We pass the class to be registered as a template parameter to
//   `torch::class_`. In this instance, we've passed the
//   specialization of the MyStackClass class ``MyStackClass<std::string>``.
//   In general, you cannot register a non-specialized template
//   class. For non-templated classes, you can just pass the
//   class name directly as the template parameter.
// - The arguments passed to the constructor make up the "qualified name"
//   of the class. In this case, the registered class will appear in
//   Python and C++ as `torch.classes.my_classes.MyStackClass`. We call
//   the first argument the "namespace" and the second argument the
//   actual class name.
TORCH_LIBRARY(my_classes,  m)  {
  m.class_<MyStackClass<std::string>>("MyStackClass")
  // The following line registers the contructor of our MyStackClass
  // class that takes a single `std::vector<std::string>` argument,
  // i.e. it exposes the C++ method `MyStackClass(std::vector<T> init)`.
  // Currently, we do not support registering overloaded
  // constructors, so for now you can only `def()` one instance of
  // `torch::init`.
  .def(torch::init<std::vector<std::string>>())
  // The next line registers a stateless (i.e. no captures) C++ lambda
  // function as a method. Note that a lambda function must take a
  // `c10::intrusive_ptr<YourClass>` (or some const/ref version of that)
  // as the first argument. Other arguments can be whatever you want.
  .def("top",  [](const  c10::intrusive_ptr<MyStackClass<std::string>>&  self)  {
  return  self->stack_.back();
  })
  // The following four lines expose methods of the MyStackClass<std::string>
  // class as-is. `torch::class_` will automatically examine the
  // argument and return types of the passed-in method pointers and
  // expose these to Python and TorchScript accordingly. Finally, notice
  // that we must take the *address* of the fully-qualified method name,
  // i.e. use the unary `&` operator, due to C++ typing rules.
  .def("push",  &MyStackClass<std::string>::push)
  .def("pop",  &MyStackClass<std::string>::pop)
  .def("clone",  &MyStackClass<std::string>::clone)
  .def("merge",  &MyStackClass<std::string>::merge)
  ;
} 
```

## 使用CMake将示例构建为C++项目

现在，我们将使用[CMake](https://cmake.org)构建系统构建上述C++代码。首先，将我们迄今为止涵盖的所有C++代码放入一个名为`class.cpp`的文件中。然后，编写一个简单的`CMakeLists.txt`文件并将其放在同一目录中。`CMakeLists.txt`应该如下所示：

```py
cmake_minimum_required(VERSION  3.1  FATAL_ERROR)
project(custom_class)

find_package(Torch  REQUIRED)

# Define our library target
add_library(custom_class  SHARED  class.cpp)
set(CMAKE_CXX_STANDARD  14)
# Link against LibTorch
target_link_libraries(custom_class  "${TORCH_LIBRARIES}") 
```

同时，创建一个`build`目录。您的文件树应该如下所示：

```py
custom_class_project/
  class.cpp
  CMakeLists.txt
  build/ 
```

我们假设您已经按照[上一个教程](torch_script_custom_ops.html)中描述的方式设置了环境。继续调用cmake，然后进行构建：

```py
$  cd  build
$  cmake  -DCMAKE_PREFIX_PATH="$(python  -c  'import torch.utils; print(torch.utils.cmake_prefix_path)')"  ..
  --  The  C  compiler  identification  is  GNU  7.3.1
  --  The  CXX  compiler  identification  is  GNU  7.3.1
  --  Check  for  working  C  compiler:  /opt/rh/devtoolset-7/root/usr/bin/cc
  --  Check  for  working  C  compiler:  /opt/rh/devtoolset-7/root/usr/bin/cc  --  works
  --  Detecting  C  compiler  ABI  info
  --  Detecting  C  compiler  ABI  info  -  done
  --  Detecting  C  compile  features
  --  Detecting  C  compile  features  -  done
  --  Check  for  working  CXX  compiler:  /opt/rh/devtoolset-7/root/usr/bin/c++
  --  Check  for  working  CXX  compiler:  /opt/rh/devtoolset-7/root/usr/bin/c++  --  works
  --  Detecting  CXX  compiler  ABI  info
  --  Detecting  CXX  compiler  ABI  info  -  done
  --  Detecting  CXX  compile  features
  --  Detecting  CXX  compile  features  -  done
  --  Looking  for  pthread.h
  --  Looking  for  pthread.h  -  found
  --  Looking  for  pthread_create
  --  Looking  for  pthread_create  -  not  found
  --  Looking  for  pthread_create  in  pthreads
  --  Looking  for  pthread_create  in  pthreads  -  not  found
  --  Looking  for  pthread_create  in  pthread
  --  Looking  for  pthread_create  in  pthread  -  found
  --  Found  Threads:  TRUE
  --  Found  torch:  /torchbind_tutorial/libtorch/lib/libtorch.so
  --  Configuring  done
  --  Generating  done
  --  Build  files  have  been  written  to:  /torchbind_tutorial/build
$  make  -j
  Scanning  dependencies  of  target  custom_class
  [  50%]  Building  CXX  object  CMakeFiles/custom_class.dir/class.cpp.o
  [100%]  Linking  CXX  shared  library  libcustom_class.so
  [100%]  Built  target  custom_class 
```

您会发现现在（除其他内容外）在构建目录中存在一个动态库文件。在Linux上，这个文件可能被命名为`libcustom_class.so`。因此，文件树应该如下所示：

```py
custom_class_project/
  class.cpp
  CMakeLists.txt
  build/
    libcustom_class.so 
```

## 从Python和TorchScript中使用C++类

现在，我们已经将我们的类及其注册编译到一个`.so`文件中，我们可以将该.so加载到Python中并尝试它。以下是演示这一点的脚本：

```py
import torch

# `torch.classes.load_library()` allows you to pass the path to your .so file
# to load it in and make the custom C++ classes available to both Python and
# TorchScript
torch.classes.load_library("build/libcustom_class.so")
# You can query the loaded libraries like this:
print(torch.classes.loaded_libraries)
# prints {'/custom_class_project/build/libcustom_class.so'}

# We can find and instantiate our custom C++ class in python by using the
# `torch.classes` namespace:
#
# This instantiation will invoke the MyStackClass(std::vector<T> init)
# constructor we registered earlier
s = torch.classes.my_classes.MyStackClass(["foo", "bar"])

# We can call methods in Python
s.push("pushed")
assert s.pop() == "pushed"

# Test custom operator
s.push("pushed")
torch.ops.my_classes.manipulate_instance(s)  # acting as s.pop()
assert s.top() == "bar" 

# Returning and passing instances of custom classes works as you'd expect
s2 = s.clone()
s.merge(s2)
for expected in ["bar", "foo", "bar", "foo"]:
    assert s.pop() == expected

# We can also use the class in TorchScript
# For now, we need to assign the class's type to a local in order to
# annotate the type on the TorchScript function. This may change
# in the future.
MyStackClass = torch.classes.my_classes.MyStackClass

@torch.jit.script
def do_stacks(s: MyStackClass):  # We can pass a custom class instance
    # We can instantiate the class
    s2 = torch.classes.my_classes.MyStackClass(["hi", "mom"])
    s2.merge(s)  # We can call a method on the class
    # We can also return instances of the class
    # from TorchScript function/methods
    return s2.clone(), s2.top()

stack, top = do_stacks(torch.classes.my_classes.MyStackClass(["wow"]))
assert top == "wow"
for expected in ["wow", "mom", "hi"]:
    assert stack.pop() == expected 
```

## 使用自定义类保存、加载和运行TorchScript代码

我们还可以在C++进程中使用自定义注册的C++类使用libtorch。例如，让我们定义一个简单的`nn.Module`，该模块实例化并调用我们的MyStackClass类的方法：

```py
import torch

torch.classes.load_library('build/libcustom_class.so')

class Foo(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, s: str) -> str:
        stack = torch.classes.my_classes.MyStackClass(["hi", "mom"])
        return stack.pop() + s

scripted_foo = torch.jit.script(Foo())
print(scripted_foo.graph)

scripted_foo.save('foo.pt') 
```

我们的文件系统中的`foo.pt`现在包含了我们刚刚定义的序列化的TorchScript程序。

现在，我们将定义一个新的CMake项目，以展示如何加载这个模型及其所需的.so文件。有关如何执行此操作的完整说明，请查看[在C++中加载TorchScript模型的教程](https://pytorch.org/tutorials/advanced/cpp_export.html)。

与之前类似，让我们创建一个包含以下内容的文件结构：

```py
cpp_inference_example/
  infer.cpp
  CMakeLists.txt
  foo.pt
  build/
  custom_class_project/
    class.cpp
    CMakeLists.txt
    build/ 
```

请注意，我们已经复制了序列化的`foo.pt`文件，以及上面的`custom_class_project`的源代码树。我们将把`custom_class_project`作为这个C++项目的依赖项，以便我们可以将自定义类构建到二进制文件中。

让我们用以下内容填充`infer.cpp`：

```py
#include  <torch/script.h>

#include  <iostream>
#include  <memory>

int  main(int  argc,  const  char*  argv[])  {
  torch::jit::Module  module;
  try  {
  // Deserialize the ScriptModule from a file using torch::jit::load().
  module  =  torch::jit::load("foo.pt");
  }
  catch  (const  c10::Error&  e)  {
  std::cerr  <<  "error loading the model\n";
  return  -1;
  }

  std::vector<c10::IValue>  inputs  =  {"foobarbaz"};
  auto  output  =  module.forward(inputs).toString();
  std::cout  <<  output->string()  <<  std::endl;
} 
```

类似地，让我们定义我们的CMakeLists.txt文件：

```py
cmake_minimum_required(VERSION  3.1  FATAL_ERROR)
project(infer)

find_package(Torch  REQUIRED)

add_subdirectory(custom_class_project)

# Define our library target
add_executable(infer  infer.cpp)
set(CMAKE_CXX_STANDARD  14)
# Link against LibTorch
target_link_libraries(infer  "${TORCH_LIBRARIES}")
# This is where we link in our libcustom_class code, making our
# custom class available in our binary.
target_link_libraries(infer  -Wl,--no-as-needed  custom_class) 
```

您知道该怎么做：`cd build`，`cmake`，然后`make`：

```py
$  cd  build
$  cmake  -DCMAKE_PREFIX_PATH="$(python  -c  'import torch.utils; print(torch.utils.cmake_prefix_path)')"  ..
  --  The  C  compiler  identification  is  GNU  7.3.1
  --  The  CXX  compiler  identification  is  GNU  7.3.1
  --  Check  for  working  C  compiler:  /opt/rh/devtoolset-7/root/usr/bin/cc
  --  Check  for  working  C  compiler:  /opt/rh/devtoolset-7/root/usr/bin/cc  --  works
  --  Detecting  C  compiler  ABI  info
  --  Detecting  C  compiler  ABI  info  -  done
  --  Detecting  C  compile  features
  --  Detecting  C  compile  features  -  done
  --  Check  for  working  CXX  compiler:  /opt/rh/devtoolset-7/root/usr/bin/c++
  --  Check  for  working  CXX  compiler:  /opt/rh/devtoolset-7/root/usr/bin/c++  --  works
  --  Detecting  CXX  compiler  ABI  info
  --  Detecting  CXX  compiler  ABI  info  -  done
  --  Detecting  CXX  compile  features
  --  Detecting  CXX  compile  features  -  done
  --  Looking  for  pthread.h
  --  Looking  for  pthread.h  -  found
  --  Looking  for  pthread_create
  --  Looking  for  pthread_create  -  not  found
  --  Looking  for  pthread_create  in  pthreads
  --  Looking  for  pthread_create  in  pthreads  -  not  found
  --  Looking  for  pthread_create  in  pthread
  --  Looking  for  pthread_create  in  pthread  -  found
  --  Found  Threads:  TRUE
  --  Found  torch:  /local/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch.so
  --  Configuring  done
  --  Generating  done
  --  Build  files  have  been  written  to:  /cpp_inference_example/build
$  make  -j
  Scanning  dependencies  of  target  custom_class
  [  25%]  Building  CXX  object  custom_class_project/CMakeFiles/custom_class.dir/class.cpp.o
  [  50%]  Linking  CXX  shared  library  libcustom_class.so
  [  50%]  Built  target  custom_class
  Scanning  dependencies  of  target  infer
  [  75%]  Building  CXX  object  CMakeFiles/infer.dir/infer.cpp.o
  [100%]  Linking  CXX  executable  infer
  [100%]  Built  target  infer 
```

现在我们可以运行我们令人兴奋的C++二进制文件了：

```py
$  ./infer
  momfoobarbaz 
```

令人难以置信！

## 将自定义类移动到/从IValues

还可能需要将自定义类移入或移出`IValue`，例如当您从TorchScript方法中获取或返回`IValue`时，或者您想在C++中实例化自定义类属性时。要从自定义C++类实例创建`IValue`：

+   `torch::make_custom_class<T>()`提供了类似于c10::intrusive_ptr<T>的API，它将接受您提供的一组参数，调用与该参数集匹配的T的构造函数，并将该实例包装起来并返回。但是，与仅返回自定义类对象的指针不同，它返回包装对象的`IValue`。然后，您可以直接将此`IValue`传递给TorchScript。

+   如果您已经有一个指向您的类的`intrusive_ptr`，则可以直接使用构造函数`IValue(intrusive_ptr<T>)`从中构造一个IValue。

将`IValue`转换回自定义类：

+   `IValue::toCustomClass<T>()`将返回指向`IValue`包含的自定义类的`intrusive_ptr<T>`。在内部，此函数正在检查`T`是否已注册为自定义类，并且`IValue`确实包含自定义类。您可以通过调用`isCustomClass()`手动检查`IValue`是否包含自定义类。

## 为自定义C++类定义序列化/反序列化方法

如果尝试将具有自定义绑定的C++类作为属性保存为`ScriptModule`，将会收到以下错误：

```py
# export_attr.py
import torch

torch.classes.load_library('build/libcustom_class.so')

class Foo(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.stack = torch.classes.my_classes.MyStackClass(["just", "testing"])

    def forward(self, s: str) -> str:
        return self.stack.pop() + s

scripted_foo = torch.jit.script(Foo())

scripted_foo.save('foo.pt')
loaded = torch.jit.load('foo.pt')

print(loaded.stack.pop()) 
```

```py
$  python  export_attr.py
RuntimeError:  Cannot  serialize  custom  bound  C++  class  __torch__.torch.classes.my_classes.MyStackClass.  Please  define  serialization  methods  via  def_pickle  for  this  class.  (pushIValueImpl  at  ../torch/csrc/jit/pickler.cpp:128) 
```

这是因为TorchScript无法自动确定从您的C++类中保存哪些信息。您必须手动指定。方法是在类上使用`class_`的特殊`def_pickle`方法定义`__getstate__`和`__setstate__`方法。

注意

TorchScript中`__getstate__`和`__setstate__`的语义与Python pickle模块的相同。您可以[阅读更多](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md#getstate-and-setstate)关于我们如何使用这些方法。

这里是我们可以添加到`MyStackClass`注册中的`def_pickle`调用的示例，以包含序列化方法：

```py
 // class_<>::def_pickle allows you to define the serialization
  // and deserialization methods for your C++ class.
  // Currently, we only support passing stateless lambda functions
  // as arguments to def_pickle
  .def_pickle(
  // __getstate__
  // This function defines what data structure should be produced
  // when we serialize an instance of this class. The function
  // must take a single `self` argument, which is an intrusive_ptr
  // to the instance of the object. The function can return
  // any type that is supported as a return value of the TorchScript
  // custom operator API. In this instance, we've chosen to return
  // a std::vector<std::string> as the salient data to preserve
  // from the class.
  [](const  c10::intrusive_ptr<MyStackClass<std::string>>&  self)
  ->  std::vector<std::string>  {
  return  self->stack_;
  },
  // __setstate__
  // This function defines how to create a new instance of the C++
  // class when we are deserializing. The function must take a
  // single argument of the same type as the return value of
  // `__getstate__`. The function must return an intrusive_ptr
  // to a new instance of the C++ class, initialized however
  // you would like given the serialized state.
  [](std::vector<std::string>  state)
  ->  c10::intrusive_ptr<MyStackClass<std::string>>  {
  // A convenient way to instantiate an object and get an
  // intrusive_ptr to it is via `make_intrusive`. We use
  // that here to allocate an instance of MyStackClass<std::string>
  // and call the single-argument std::vector<std::string>
  // constructor with the serialized state.
  return  c10::make_intrusive<MyStackClass<std::string>>(std::move(state));
  }); 
```

注意

我们在pickle API中采用了与pybind11不同的方法。而pybind11有一个特殊函数`pybind11::pickle()`，您可以将其传递给`class_::def()`，我们为此目的有一个单独的方法`def_pickle`。这是因为名称`torch::jit::pickle`已经被使用，我们不想引起混淆。

一旦以这种方式定义了（反）序列化行为，我们的脚本现在可以成功运行：

```py
$  python  ../export_attr.py
testing 
```

## 定义接受或返回绑定的C++类的自定义运算符

一旦定义了自定义C++类，您还可以将该类用作自定义运算符（即自由函数）的参数或返回值。假设您有以下自由函数：

```py
c10::intrusive_ptr<MyStackClass<std::string>>  manipulate_instance(const  c10::intrusive_ptr<MyStackClass<std::string>>&  instance)  {
  instance->pop();
  return  instance;
} 
```

您可以在`TORCH_LIBRARY`块内运行以下代码来注册它：

```py
 m.def(
  "manipulate_instance(__torch__.torch.classes.my_classes.MyStackClass x) -> __torch__.torch.classes.my_classes.MyStackClass Y",
  manipulate_instance
  ); 
```

有关注册API的更多详细信息，请参考[自定义操作教程](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html)。

完成后，您可以像以下示例一样使用该运算符：

```py
class TryCustomOp(torch.nn.Module):
    def __init__(self):
        super(TryCustomOp, self).__init__()
        self.f = torch.classes.my_classes.MyStackClass(["foo", "bar"])

    def forward(self):
        return torch.ops.my_classes.manipulate_instance(self.f) 
```

注意

接受C++类作为参数的运算符的注册要求自定义类已经注册。您可以通过确保自定义类注册和您的自由函数定义位于同一个`TORCH_LIBRARY`块中，并且自定义类注册位于首位来强制执行此要求。在未来，我们可能会放宽此要求，以便可以以任何顺序注册这些内容。

## 结论

本教程向您展示了如何将一个C++类暴露给TorchScript（以及Python），如何注册其方法，如何从Python和TorchScript中使用该类，以及如何使用该类保存和加载代码，并在独立的C++进程中运行该代码。现在，您可以准备使用与第三方C++库进行交互的C++类来扩展您的TorchScript模型，或者实现任何其他需要在Python、TorchScript和C++之间平滑过渡的用例。

如常，如果您遇到任何问题或有疑问，您可以使用我们的[论坛](https://discuss.pytorch.org/)或[GitHub问题页面](https://github.com/pytorch/pytorch/issues)联系我们。此外，我们的[常见问题（FAQ）页面](https://pytorch.org/cppdocs/notes/faq.html)可能包含有用的信息。
