# CPU 线程和 TorchScript 推理

> 原文：[`pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html`](https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

PyTorch 允许在 TorchScript 模型推理期间使用多个 CPU 线程。以下图显示了在典型应用程序中可能找到的不同级别的并行性：

![../_images/cpu_threading_torchscript_inference.svg](img/cpu_threading_torchscript_inference.svg)

一个或多个推理线程在给定输入上执行模型的前向传递。每个推理线程调用 JIT 解释器，逐个执行模型的操作。模型可以利用 `fork` TorchScript 原语启动一个异步任务。一次分叉多个操作会导致并行执行的任务。`fork` 操作符返回一个 `Future` 对象，可以稍后用于同步，例如：

```py
@torch.jit.script
def compute_z(x):
    return torch.mm(x, self.w_z)

@torch.jit.script
def forward(x):
    # launch compute_z asynchronously:
    fut = torch.jit._fork(compute_z, x)
    # execute the next operation in parallel to compute_z:
    y = torch.mm(x, self.w_y)
    # wait for the result of compute_z:
    z = torch.jit._wait(fut)
    return y + z 
```

PyTorch 使用一个线程池来进行操作间的并行处理，这个线程池被应用程序进程中的所有分叉推理任务共享。

除了操作间的并行性，PyTorch 还可以利用操作内的多个线程（操作内的并行性）。这在许多情况下都很有用，包括大张量的逐元素操作、卷积、GEMM、嵌入查找等。

## 构建选项

PyTorch 使用内部的 ATen 库来实现操作。除此之外，PyTorch 还可以构建支持外部库，如 [MKL](https://software.intel.com/en-us/mkl) 和 [MKL-DNN](https://github.com/intel/mkl-dnn)，以加速 CPU 上的计算。

ATen、MKL 和 MKL-DNN 支持操作内的并行性，并依赖以下并行化库来实现：

+   [OpenMP](https://www.openmp.org/) - 一个标准（通常随编译器一起提供的库），在外部库中被广泛使用；

+   [TBB](https://github.com/intel/tbb) - 一个针对任务并行性和并发环境进行了优化的较新的并行化库。

OpenMP 历史上被许多库使用。它以相对易用和支持基于循环的并行性和其他原语而闻名。

TBB 在外部库中使用较少，但同时也针对并发环境进行了优化。PyTorch 的 TBB 后端保证应用程序中所有运行的操作都使用一个单独的、每个进程的手术过程线程池。

根据使用情况，一个人可能会发现在他们的应用程序中选择一个或另一个并行化库更好。

PyTorch 允许在构建时选择 ATen 和其他库使用的并行化后端，具体的构建选项如下：

| 库 | 构建选项 | 值 | 备注 |
| --- | --- | --- | --- |
| ATen | `ATEN_THREADING` | `OMP`（默认），`TBB` |  |
| MKL | `MKL_THREADING` | （相同） | 要启用 MKL，请使用 `BLAS=MKL` |
| MKL-DNN | `MKLDNN_CPU_RUNTIME` | （相同） | 要启用 MKL-DNN，请使用 `USE_MKLDNN=1` |

建议不要在一个构建中混合使用 OpenMP 和 TBB。

上述任何 `TBB` 值都需要 `USE_TBB=1` 构建设置（默认为 OFF）。OpenMP 并行性需要单独设置 `USE_OPENMP=1`（默认为 ON）。

## 运行时 API

以下 API 用于控制线程设置：

| 并行性类型 | 设置 | 备注 |
| --- | --- | --- |
| 操作间的并行性 | `at::set_num_interop_threads`，`at::get_num_interop_threads`（C++）`set_num_interop_threads`，`get_num_interop_threads`（Python，`torch` 模块） | 默认线程数：CPU 核心数。 |
| 手术过程中的并行性 | `at::set_num_threads`，`at::get_num_threads`（C++）`set_num_threads`，`get_num_threads`（Python，`torch` 模块）环境变量：`OMP_NUM_THREADS` 和 `MKL_NUM_THREADS` |

对于内部操作并行设置，`at::set_num_threads`，`torch.set_num_threads`始终优先于环境变量，`MKL_NUM_THREADS`变量优先于`OMP_NUM_THREADS`。

## 调整线程数量[]（＃tuning-the-number-of-threads“跳转到此标题”）

以下简单脚本显示了矩阵乘法的运行时如何随线程数量变化而变化：

```py
import timeit
runtimes = []
threads = [1] + [t for t in range(2, 49, 2)]
for t in threads:
    torch.set_num_threads(t)
    r = timeit.timeit(setup = "import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)", stmt="torch.mm(x, y)", number=100)
    runtimes.append(r)
# ... plotting (threads, runtimes) ... 
```

在具有 24 个物理 CPU 核心的系统（基于 Xeon E5-2680、MKL 和 OpenMP 构建）上运行脚本会产生以下运行时间：

![../_images/cpu_threading_runtimes.svg](img/cpu_threading_runtimes.svg)

调整内部和外部操作线程数量时应考虑以下因素：

+   在选择线程数量时，需要避免过度订阅（使用太多线程会导致性能下降）。例如，在使用大型应用程序线程池或严重依赖于内部操作并行性的应用程序中，可以考虑禁用内部操作并行性（即通过调用`set_num_threads(1)`）；

+   在典型应用程序中，可能会在延迟（用于处理推理请求的时间）和吞吐量（单位时间内完成的工作量）之间进行权衡。调整线程数量可以是调整这种权衡的有用工具。例如，在对延迟敏感的应用程序中，可能希望增加内部操作线程的数量，以尽可能快地处理每个请求。同时，操作的并行实现可能会增加额外的开销，增加单个请求的工作量，从而降低整体吞吐量。

警告

OpenMP 不能保证应用程序将使用单个进程内部操作线程池。相反，两个不同的应用程序或内部操作线程可能会使用不同的 OpenMP 线程池进行内部操作工作。这可能导致应用程序使用大量线程。在 OpenMP 情况下，需要特别注意调整线程数量，以避免多线程应用程序中的过度订阅。

注意

预编译的 PyTorch 版本已编译为支持 OpenMP。

注意

`parallel_info`实用程序打印有关线程设置的信息，可用于调试。在 Python 中也可以通过`torch.__config__.parallel_info()`调用获得类似的输出。
