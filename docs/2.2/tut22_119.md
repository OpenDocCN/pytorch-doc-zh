# 使用 Cpp 扩展自定义流程组后端

> [`pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html`](https://pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html)

**作者**：Howard Huang <https://github.com/H-Huang>，[Feng Tian](https://github.com/ftian1)，[Shen Li](https://mrshenli.github.io/)，[Min Si](https://minsii.github.io/)

注意

![edit](img/a8aa37bcc5edbf2ba5fcf18dba1e55f9.png) 在 [github](https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst) 上查看并编辑本教程。

先决条件:

+   PyTorch 分布式概述

+   [PyTorch 集体通信包](https://pytorch.org/docs/stable/distributed.html)

+   [PyTorch Cpp 扩展](https://pytorch.org/docs/stable/cpp_extension.html)

+   [使用 PyTorch 编写分布式应用程序](https://pytorch.org/tutorials/intermediate/dist_tuto.html)

本教程演示了如何实现一个自定义的`Backend`并将其插入[PyTorch 分布式包](https://pytorch.org/docs/stable/distributed.html)，使用[cpp 扩展](https://pytorch.org/docs/stable/cpp_extension.html)。当您需要为硬件定制专门的软件堆栈，或者想要尝试新的集体通信算法时，这将非常有帮助。

## 基础知识

PyTorch 集体通信支持多种广泛采用的分布式训练功能，包括[DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)，[ZeroRedundancyOptimizer](https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer)，[FullyShardedDataParallel](https://github.com/pytorch/pytorch/blob/master/torch/distributed/_fsdp/fully_sharded_data_parallel.py)。为了使相同的集体通信 API 能够与不同的通信后端一起工作，分布式包将集体通信操作抽象为[Backend](https://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/Backend.hpp)类。不同的后端可以作为`Backend`的子类使用首选的第三方库来实现。PyTorch 分布式带有三个默认后端，`ProcessGroupNCCL`，`ProcessGroupGloo`和`ProcessGroupMPI`。然而，除了这三个后端之外，还有其他通信库（例如[UCC](https://github.com/openucx/ucc)，[OneCCL](https://github.com/oneapi-src/oneCCL)），不同类型的硬件（例如[TPU](https://cloud.google.com/tpu)，[Trainum](https://aws.amazon.com/machine-learning/trainium/)）和新兴的通信算法（例如[Herring](https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud)，[Reduction Server](https://cloud.google.com/blog/topics/developers-practitioners/optimize-training-performance-reduction-server-vertex-ai)）。因此，分布式包提供了扩展 API 来允许定制集体通信后端。

以下 4 个步骤展示了如何在 Python 应用程序代码中实现一个虚拟的`Backend`后端并使用它。请注意，本教程侧重于演示扩展 API，而不是开发一个功能完善的通信后端。因此，`dummy`后端只涵盖了 API 的一个子集（`all_reduce`和`all_gather`），并且只是将张量的值设置为 0。

## 步骤 1：实现`Backend`的子类

第一步是实现一个`Backend`子类，覆盖目标集体通信 API，并运行自定义通信算法。扩展还需要实现一个`Work`子类，作为通信结果的 future，并允许在应用代码中异步执行。如果扩展使用第三方库，可以在`BackendDummy`子类中包含头文件并调用库 API。下面的两个代码片段展示了`dummy.h`和`dummy.cpp`的实现。请查看[dummy collectives](https://github.com/H-Huang/torch_collective_extension)存储库以获取完整的实现。

```py
// file name: dummy.hpp
#include  <torch/python.h>

#include  <torch/csrc/distributed/c10d/Backend.hpp>
#include  <torch/csrc/distributed/c10d/Work.hpp>
#include  <torch/csrc/distributed/c10d/Store.hpp>
#include  <torch/csrc/distributed/c10d/Types.hpp>
#include  <torch/csrc/distributed/c10d/Utils.hpp>

#include  <pybind11/chrono.h>

namespace  c10d  {

class  BackendDummy  :  public  Backend  {
  public:
  BackendDummy(int  rank,  int  size);

  c10::intrusive_ptr<Work>  allgather(
  std::vector<std::vector<at::Tensor>>&  outputTensors,
  std::vector<at::Tensor>&  inputTensors,
  const  AllgatherOptions&  opts  =  AllgatherOptions())  override;

  c10::intrusive_ptr<Work>  allreduce(
  std::vector<at::Tensor>&  tensors,
  const  AllreduceOptions&  opts  =  AllreduceOptions())  override;

  // The collective communication APIs without a custom implementation
  // will error out if invoked by application code.
};

class  WorkDummy  :  public  Work  {
  public:
  WorkDummy(
  OpType  opType,
  c10::intrusive_ptr<c10::ivalue::Future>  future)  // future of the output
  :  Work(
  -1,  // rank, only used by recvAnySource, irrelevant in this demo
  opType),
  future_(std::move(future))  {}
  bool  isCompleted()  override;
  bool  isSuccess()  const  override;
  bool  wait(std::chrono::milliseconds  timeout  =  kUnsetTimeout)  override;
  virtual  c10::intrusive_ptr<c10::ivalue::Future>  getFuture()  override;

  private:
  c10::intrusive_ptr<c10::ivalue::Future>  future_;
};
}  // namespace c10d 
```

```py
// file name: dummy.cpp
#include  "dummy.hpp"

namespace  c10d  {

// This is a dummy allgather that sets all output tensors to zero
// Modify the implementation to conduct real communication asynchronously
c10::intrusive_ptr<Work>  BackendDummy::allgather(
  std::vector<std::vector<at::Tensor>>&  outputTensors,
  std::vector<at::Tensor>&  inputTensors,
  const  AllgatherOptions&  /* unused */)  {
  for  (auto&  outputTensorVec  :  outputTensors)  {
  for  (auto&  outputTensor  :  outputTensorVec)  {
  outputTensor.zero_();
  }
  }

  auto  future  =  c10::make_intrusive<c10::ivalue::Future>(
  c10::ListType::create(c10::ListType::create(c10::TensorType::get())));
  future->markCompleted(c10::IValue(outputTensors));
  return  c10::make_intrusive<WorkDummy>(OpType::ALLGATHER,  std::move(future));
}

// This is a dummy allreduce that sets all output tensors to zero
// Modify the implementation to conduct real communication asynchronously
c10::intrusive_ptr<Work>  BackendDummy::allreduce(
  std::vector<at::Tensor>&  tensors,
  const  AllreduceOptions&  opts)  {
  for  (auto&  tensor  :  tensors)  {
  tensor.zero_();
  }

  auto  future  =  c10::make_intrusive<c10::ivalue::Future>(
  c10::ListType::create(c10::TensorType::get()));
  future->markCompleted(c10::IValue(tensors));
  return  c10::make_intrusive<WorkDummy>(OpType::ALLGATHER,  std::move(future));
}
}  // namespace c10d 
```

## 步骤 2：暴露扩展 Python API

后端构造函数是从 Python 端调用的，因此扩展还需要向 Python 公开构造函数 API。这可以通过添加以下方法来实现。在这个例子中，`store`和`timeout`被`BackendDummy`实例化方法忽略，因为在这个虚拟实现中没有使用它们。然而，真实世界的扩展应该考虑使用`store`来执行会合并支持`timeout`参数。

```py
// file name: dummy.hpp
class  BackendDummy  :  public  Backend  {
  ...
  <Step  1  code>
  ...

  static  c10::intrusive_ptr<Backend>  createBackendDummy(
  const  c10::intrusive_ptr<::c10d::Store>&  store,
  int  rank,
  int  size,
  const  std::chrono::duration<float>&  timeout);

  static  void  BackendDummyConstructor()  __attribute__((constructor))  {
  py::object  module  =  py::module::import("torch.distributed");
  py::object  register_backend  =
  module.attr("Backend").attr("register_backend");
  // torch.distributed.Backend.register_backend will add `dummy` as a
  // new valid backend.
  register_backend("dummy",  py::cpp_function(createBackendDummy));
  }
} 
```

```py
// file name: dummy.cpp
c10::intrusive_ptr<Backend>  BackendDummy::createBackendDummy(
  const  c10::intrusive_ptr<::c10d::Store>&  /* unused */,
  int  rank,
  int  size,
  const  std::chrono::duration<float>&  /* unused */)  {
  return  c10::make_intrusive<BackendDummy>(rank,  size);
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME,  m)  {
  m.def("createBackendDummy",  &BackendDummy::createBackendDummy);
} 
```

## 步骤 3：构建自定义扩展

现在，扩展源代码文件已经准备好。我们可以使用[cpp extensions](https://pytorch.org/docs/stable/cpp_extension.html)来构建它。为此，创建一个`setup.py`文件，准备路径和命令。然后调用`python setup.py develop`来安装扩展。

如果扩展依赖于第三方库，您还可以在 cpp 扩展 API 中指定`libraries_dirs`和`libraries`。请参考[torch ucc](https://github.com/openucx/torch-ucc)项目作为一个真实的例子。

```py
# file name: setup.py
import os
import sys
import torch
from setuptools import setup
from torch.utils import cpp_extension

sources = ["src/dummy.cpp"]
include_dirs = [f"{os.path.dirname(os.path.abspath(__file__))}/include/"]

if torch.cuda.is_available():
    module = cpp_extension.CUDAExtension(
        name = "dummy_collectives",
        sources = sources,
        include_dirs = include_dirs,
    )
else:
    module = cpp_extension.CppExtension(
        name = "dummy_collectives",
        sources = sources,
        include_dirs = include_dirs,
    )

setup(
    name = "Dummy-Collectives",
    version = "0.0.1",
    ext_modules = [module],
    cmdclass={'build_ext': cpp_extension.BuildExtension}
) 
```

## 步骤 4：在应用程序中使用扩展。

安装完成后，您可以在调用[init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)时方便地使用`dummy`后端，就像它是一个内置后端一样。

我们可以根据后端来指定调度，方法是改变`init_process_group`的`backend`参数。我们可以通过将后端参数指定为`cpu:gloo,cuda:dummy`，将 CPU 张量的集体分发到`gloo`后端，将 CUDA 张量的集体分发到`dummy`后端。

要将所有张量发送到`dummy`后端，我们可以简单地将`dummy`指定为后端参数。

```py
import os

import torch
# importing dummy_collectives makes torch.distributed recognize `dummy`
# as a valid backend.
import dummy_collectives

import torch.distributed as dist

os.environ['MASTER_ADDR'] = 'localhost'
os.environ['MASTER_PORT'] = '29500'

# Alternatively:
# dist.init_process_group("dummy", rank=0, world_size=1)
dist.init_process_group("cpu:gloo,cuda:dummy", rank=0, world_size=1)

# this goes through gloo
x = torch.ones(6)
dist.all_reduce(x)
print(f"cpu allreduce: {x}")

# this goes through dummy
if torch.cuda.is_available():
    y = x.cuda()
    dist.all_reduce(y)
    print(f"cuda allreduce: {y}")

    try:
        dist.broadcast(y, 0)
    except RuntimeError:
        print("got RuntimeError when calling broadcast") 
```
