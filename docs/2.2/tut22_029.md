# TorchVision目标检测微调教程

> 原文：[https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)

注意

点击[这里](#sphx-glr-download-intermediate-torchvision-tutorial-py)下载完整示例代码

在本教程中，我们将对[宾夕法尼亚大学行人检测和分割数据库](https://www.cis.upenn.edu/~jshi/ped_html/)上的预训练[Mask R-CNN](https://arxiv.org/abs/1703.06870)模型进行微调。它包含170张图像，有345个行人实例，我们将用它来演示如何使用torchvision中的新功能来训练自定义数据集上的目标检测和实例分割模型。

注意

本教程仅适用于torchvision版本>=0.16或夜间版本。如果您使用的是torchvision<=0.15，请按照[此教程](https://github.com/pytorch/tutorials/blob/d686b662932a380a58b7683425faa00c06bcf502/intermediate_source/torchvision_tutorial.rst)操作。

## 定义数据集

用于训练目标检测、实例分割和人体关键点检测的参考脚本允许轻松支持添加新的自定义数据集。数据集应该继承自标准[`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset "(在PyTorch v2.2中)")类，并实现`__len__`和`__getitem__`。

我们唯一要求的特定性是数据集`__getitem__`应返回一个元组：

+   image：形状为`[3, H, W]`的[`torchvision.tv_tensors.Image`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Image.html#torchvision.tv_tensors.Image "(在Torchvision v0.17中")，一个纯张量，或大小为`(H, W)`的PIL图像

+   目标：包含以下字段的字典

    +   `boxes`，形状为`[N, 4]`的[`torchvision.tv_tensors.BoundingBoxes`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.BoundingBoxes.html#torchvision.tv_tensors.BoundingBoxes "(在Torchvision v0.17中)")：`N`个边界框的坐标，格式为`[x0, y0, x1, y1]`，范围从`0`到`W`和`0`到`H`

    +   `labels`，形状为`[N]`的整数[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "(在PyTorch v2.2中)")：每个边界框的标签。`0`始终表示背景类。

    +   `image_id`，整数：图像标识符。它应该在数据集中的所有图像之间是唯一的，并在评估过程中使用

    +   `area`，形状为`[N]`的浮点数[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "(在PyTorch v2.2中)")：边界框的面积。在使用COCO指标进行评估时使用，以区分小、中和大框之间的指标分数。

    +   `iscrowd`，形状为`[N]`的uint8 [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "(在PyTorch v2.2中)")：具有`iscrowd=True`的实例在评估过程中将被忽略。

    +   （可选）`masks`，形状为`[N, H, W]`的[`torchvision.tv_tensors.Mask`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Mask.html#torchvision.tv_tensors.Mask "(在Torchvision v0.17中)")：每个对象的分割掩码

如果您的数据集符合上述要求，则可以在参考脚本中的训练和评估代码中使用。评估代码将使用`pycocotools`中的脚本，可以通过`pip install pycocotools`安装。

注意

对于Windows，请使用以下命令从[gautamchitnis](https://github.com/gautamchitnis/cocoapi)安装`pycocotools`

`pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI`

关于`labels`的一点说明。模型将类`0`视为背景。如果您的数据集不包含背景类，则在`labels`中不应该有`0`。例如，假设您只有两类，*猫*和*狗*，您可以定义`1`（而不是`0`）表示*猫*，`2`表示*狗*。因此，例如，如果一张图像同时包含两类，则您的`labels`张量应该如下所示`[1, 2]`。

此外，如果您想在训练期间使用纵横比分组（以便每个批次只包含具有相似纵横比的图像），则建议还实现一个`get_height_and_width`方法，该方法返回图像的高度和宽度。如果未提供此方法，我们将通过`__getitem__`查询数据集的所有元素，这会将图像加载到内存中，比提供自定义方法慢。

### 为PennFudan编写自定义数据集

让我们为PennFudan数据集编写一个数据集。首先，让我们下载数据集并提取[zip文件](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip)：

```py
wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data
cd data && unzip PennFudanPed.zip 
```

我们有以下文件夹结构：

```py
PennFudanPed/
  PedMasks/
    FudanPed00001_mask.png
    FudanPed00002_mask.png
    FudanPed00003_mask.png
    FudanPed00004_mask.png
    ...
  PNGImages/
    FudanPed00001.png
    FudanPed00002.png
    FudanPed00003.png
    FudanPed00004.png 
```

这是一对图像和分割蒙版的示例

```py
import matplotlib.pyplot as plt
from torchvision.io import [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image "torchvision.io.read_image")

[image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image "torchvision.io.read_image")("data/PennFudanPed/PNGImages/FudanPed00046.png")
[mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image "torchvision.io.read_image")("data/PennFudanPed/PedMasks/FudanPed00046_mask.png")

plt.figure(figsize=(16, 8))
plt.subplot(121)
plt.title("Image")
plt.imshow([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").permute(1, 2, 0))
plt.subplot(122)
plt.title("Mask")
plt.imshow([mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").permute(1, 2, 0)) 
```

![Image, Mask](../Images/af4053119e5bd4687c7021a5d001f282.png)

```py
<matplotlib.image.AxesImage object at 0x7f489920ffd0> 
```

因此，每个图像都有一个相应的分割蒙版，其中每种颜色对应不同的实例。让我们为这个数据集编写一个[`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset "(在PyTorch v2.2)")类。在下面的代码中，我们将图像、边界框和蒙版封装到[`torchvision.tv_tensors.TVTensor`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.TVTensor.html#torchvision.tv_tensors.TVTensor "(在Torchvision v0.17)")类中，以便我们能够应用torchvision内置的转换（[新的转换API](https://pytorch.org/vision/stable/transforms.html)）来完成给定的目标检测和分割任务。换句话说，图像张量将被[`torchvision.tv_tensors.Image`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Image.html#torchvision.tv_tensors.Image "(在Torchvision v0.17)")封装，边界框将被封装为[`torchvision.tv_tensors.BoundingBoxes`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.BoundingBoxes.html#torchvision.tv_tensors.BoundingBoxes "(在Torchvision v0.17)")，蒙版将被封装为[`torchvision.tv_tensors.Mask`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Mask.html#torchvision.tv_tensors.Mask "(在Torchvision v0.17)")。由于[`torchvision.tv_tensors.TVTensor`](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.TVTensor.html#torchvision.tv_tensors.TVTensor "(在Torchvision v0.17)")是[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "(在PyTorch v2.2)")的子类，封装的对象也是张量，并继承了普通的[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "(在PyTorch v2.2)") API。有关torchvision `tv_tensors`的更多信息，请参阅[此文档](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-are-tvtensors)。

```py
import os
import torch

from torchvision.io import [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image "torchvision.io.read_image")
from torchvision.ops.boxes import [masks_to_boxes](https://pytorch.org/vision/stable/generated/torchvision.ops.masks_to_boxes.html#torchvision.ops.masks_to_boxes "torchvision.ops.masks_to_boxes")
from torchvision import tv_tensors
from torchvision.transforms.v2 import functional as F

class PennFudanDataset([torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset "torch.utils.data.Dataset")):
    def __init__(self, root, transforms):
        self.root = root
        self.transforms = transforms
        # load all image files, sorting them to
        # ensure that they are aligned
        self.imgs = list(sorted(os.listdir(os.path.join(root, "PNGImages"))))
        self.[masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = list(sorted(os.listdir(os.path.join(root, "PedMasks"))))

    def __getitem__(self, idx):
        # load images and masks
        img_path = os.path.join(self.root, "PNGImages", self.imgs[idx])
        mask_path = os.path.join(self.root, "PedMasks", self.[masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[idx])
        img = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image "torchvision.io.read_image")(img_path)
        [mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image "torchvision.io.read_image")(mask_path)
        # instances are encoded as different colors
        obj_ids = [torch.unique](https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique "torch.unique")([mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
        # first id is the background, so remove it
        obj_ids = obj_ids[1:]
        num_objs = len(obj_ids)

        # split the color-encoded mask into a set
        # of binary masks
        [masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ([mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") == obj_ids[:, None, None]).to(dtype=[torch.uint8](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype"))

        # get bounding box coordinates for each mask
        boxes = [masks_to_boxes](https://pytorch.org/vision/stable/generated/torchvision.ops.masks_to_boxes.html#torchvision.ops.masks_to_boxes "torchvision.ops.masks_to_boxes")([masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

        # there is only one class
        labels = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")((num_objs,), dtype=[torch.int64](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype"))

        image_id = idx
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        # suppose all instances are not crowd
        iscrowd = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros "torch.zeros")((num_objs,), dtype=[torch.int64](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype"))

        # Wrap sample and targets into torchvision tv_tensors:
        img = [tv_tensors.Image](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Image.html#torchvision.tv_tensors.Image "torchvision.tv_tensors.Image")(img)

        target = {}
        target["boxes"] = [tv_tensors.BoundingBoxes](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.BoundingBoxes.html#torchvision.tv_tensors.BoundingBoxes "torchvision.tv_tensors.BoundingBoxes")(boxes, format="XYXY", canvas_size=F.get_size(img))
        target["masks"] = [tv_tensors.Mask](https://pytorch.org/vision/stable/generated/torchvision.tv_tensors.Mask.html#torchvision.tv_tensors.Mask "torchvision.tv_tensors.Mask")([masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transforms is not None:
            img, target = self.transforms(img, target)

        return img, target

    def __len__(self):
        return len(self.imgs) 
```

这就是数据集的全部内容。现在让我们定义一个可以在此数据集上执行预测的模型。

## 定义您的模型

在本教程中，我们将使用基于[Faster R-CNN](https://arxiv.org/abs/1506.01497)的[Mask R-CNN](https://arxiv.org/abs/1703.06870)。Faster R-CNN是一个模型，用于预测图像中潜在对象的边界框和类别分数。

![../_static/img/tv_tutorial/tv_image03.png](../Images/611c2725bdfb89e258da9a99fca53433.png)

Mask R-CNN在Faster R-CNN中添加了一个额外的分支，还为每个实例预测分割蒙版。

![../_static/img/tv_tutorial/tv_image04.png](../Images/afd408b97567c661cc8cb8a80c7c777c.png)

有两种常见情况可能需要修改 TorchVision Model Zoo 中的可用模型之一。第一种情况是当我们想要从预训练模型开始，只微调最后一层时。另一种情况是当我们想要用不同的主干替换模型的主干时（例如为了更快的预测）。

让我们看看在以下部分中我们将如何执行其中一个或另一个。

### 1 - 从预训练模型微调

假设您想从在 COCO 上预训练的模型开始，并希望对其进行微调以适应您的特定类别。以下是可能的操作方式：

```py
import torchvision
from torchvision.models.detection.faster_rcnn import [FastRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")

# load a model pre-trained on COCO
model = [torchvision.models.detection.fasterrcnn_resnet50_fpn](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn "torchvision.models.detection.fasterrcnn_resnet50_fpn")(weights="DEFAULT")

# replace the classifier with a new one, that has
# num_classes which is user-defined
num_classes = 2  # 1 class (person) + background
# get number of input features for the classifier
in_features = model.roi_heads.box_predictor.cls_score.in_features
# replace the pre-trained head with a new one
model.roi_heads.box_predictor = [FastRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")(in_features, num_classes) 
```

```py
Downloading: "https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth

  0%|          | 0.00/160M [00:00<?, ?B/s]
  8%|7         | 12.1M/160M [00:00<00:01, 127MB/s]
 16%|#6        | 25.8M/160M [00:00<00:01, 137MB/s]
 25%|##4       | 39.6M/160M [00:00<00:00, 140MB/s]
 34%|###3      | 53.6M/160M [00:00<00:00, 143MB/s]
 42%|####2     | 67.5M/160M [00:00<00:00, 144MB/s]
 51%|#####     | 81.4M/160M [00:00<00:00, 145MB/s]
 60%|#####9    | 95.4M/160M [00:00<00:00, 145MB/s]
 68%|######8   | 109M/160M [00:00<00:00, 145MB/s]
 77%|#######7  | 123M/160M [00:00<00:00, 146MB/s]
 86%|########5 | 137M/160M [00:01<00:00, 146MB/s]
 95%|#########4| 151M/160M [00:01<00:00, 146MB/s]
100%|##########| 160M/160M [00:01<00:00, 144MB/s] 
```

### 2 - 修改模型以添加不同的主干

```py
import torchvision
from torchvision.models.detection import [FasterRCNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")
from torchvision.models.detection.rpn import [AnchorGenerator](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")

# load a pre-trained model for classification and return
# only the features
[backbone](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential") = [torchvision.models.mobilenet_v2](https://pytorch.org/vision/stable/models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.mobilenet_v2 "torchvision.models.mobilenet_v2")(weights="DEFAULT").features
# ``FasterRCNN`` needs to know the number of
# output channels in a backbone. For mobilenet_v2, it's 1280
# so we need to add it here
[backbone](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential").out_channels = 1280

# let's make the RPN generate 5 x 3 anchors per spatial
# location, with 5 different sizes and 3 different aspect
# ratios. We have a Tuple[Tuple[int]] because each feature
# map could potentially have different sizes and
# aspect ratios
anchor_generator = [AnchorGenerator](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")(
    sizes=((32, 64, 128, 256, 512),),
    aspect_ratios=((0.5, 1.0, 2.0),)
)

# let's define what are the feature maps that we will
# use to perform the region of interest cropping, as well as
# the size of the crop after rescaling.
# if your backbone returns a Tensor, featmap_names is expected to
# be [0]. More generally, the backbone should return an
# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which
# feature maps to use.
[roi_pooler](https://pytorch.org/vision/stable/generated/torchvision.ops.MultiScaleRoIAlign.html#torchvision.ops.MultiScaleRoIAlign "torchvision.ops.MultiScaleRoIAlign") = [torchvision.ops.MultiScaleRoIAlign](https://pytorch.org/vision/stable/generated/torchvision.ops.MultiScaleRoIAlign.html#torchvision.ops.MultiScaleRoIAlign "torchvision.ops.MultiScaleRoIAlign")(
    featmap_names=['0'],
    output_size=7,
    sampling_ratio=2
)

# put the pieces together inside a Faster-RCNN model
model = [FasterRCNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")(
    [backbone](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential"),
    num_classes=2,
    rpn_anchor_generator=anchor_generator,
    box_roi_pool=[roi_pooler](https://pytorch.org/vision/stable/generated/torchvision.ops.MultiScaleRoIAlign.html#torchvision.ops.MultiScaleRoIAlign "torchvision.ops.MultiScaleRoIAlign")
) 
```

```py
Downloading: "https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth

  0%|          | 0.00/13.6M [00:00<?, ?B/s]
 92%|#########1| 12.5M/13.6M [00:00<00:00, 131MB/s]
100%|##########| 13.6M/13.6M [00:00<00:00, 131MB/s] 
```

### PennFudan 数据集的目标检测和实例分割模型

在我们的情况下，我们希望从预训练模型进行微调，鉴于我们的数据集非常小，因此我们将遵循第一种方法。

在这里，我们还想计算实例分割掩模，因此我们将使用 Mask R-CNN：

```py
import torchvision
from torchvision.models.detection.faster_rcnn import [FastRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")
from torchvision.models.detection.mask_rcnn import [MaskRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential")

def get_model_instance_segmentation(num_classes):
    # load an instance segmentation model pre-trained on COCO
    model = [torchvision.models.detection.maskrcnn_resnet50_fpn](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html#torchvision.models.detection.maskrcnn_resnet50_fpn "torchvision.models.detection.maskrcnn_resnet50_fpn")(weights="DEFAULT")

    # get number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = [FastRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")(in_features, num_classes)

    # now get the number of input features for the mask classifier
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    # and replace the mask predictor with a new one
    model.roi_heads.mask_predictor = [MaskRCNNPredictor](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential")(
        in_features_mask,
        hidden_layer,
        num_classes
    )

    return model 
```

这样，`model` 就准备好在您的自定义数据集上进行训练和评估了。

## 将所有内容放在一起

在 `references/detection/` 中，我们有许多辅助函数来简化训练和评估检测模型。在这里，我们将使用 `references/detection/engine.py` 和 `references/detection/utils.py`。只需将 `references/detection` 下的所有内容下载到您的文件夹中并在此处使用它们。在 Linux 上，如果您有 `wget`，您可以使用以下命令下载它们：

```py
os.system("wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py")
os.system("wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py")
os.system("wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py")
os.system("wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py")
os.system("wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py") 
```

```py
0 
```

自 v0.15.0 起，torchvision 提供了[新的 Transforms API](https://pytorch.org/vision/stable/transforms.html)，以便为目标检测和分割任务轻松编写数据增强流水线。

让我们编写一些辅助函数用于数据增强/转换：

```py
from torchvision.transforms import v2 as T

def get_transform(train):
    transforms = []
    if train:
        transforms.append([T.RandomHorizontalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomHorizontalFlip.html#torchvision.transforms.v2.RandomHorizontalFlip "torchvision.transforms.v2.RandomHorizontalFlip")(0.5))
    transforms.append([T.ToDtype](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToDtype.html#torchvision.transforms.v2.ToDtype "torchvision.transforms.v2.ToDtype")([torch.float](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype"), scale=True))
    transforms.append([T.ToPureTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToPureTensor.html#torchvision.transforms.v2.ToPureTensor "torchvision.transforms.v2.ToPureTensor")())
    return [T.Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose "torchvision.transforms.v2.Compose")(transforms) 
```

## 测试 `forward()` 方法（可选）

在迭代数据集之前，查看模型在训练和推断时对样本数据的期望是很好的。

```py
import utils

model = [torchvision.models.detection.fasterrcnn_resnet50_fpn](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn "torchvision.models.detection.fasterrcnn_resnet50_fpn")(weights="DEFAULT")
[dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset") = [PennFudanDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset "torch.utils.data.Dataset")('data/PennFudanPed', get_transform(train=True))
[data_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader") = [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader")(
    [dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset"),
    batch_size=2,
    shuffle=True,
    num_workers=4,
    collate_fn=utils.collate_fn
)

# For Training
images, targets = next(iter([data_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader")))
images = list([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") for [image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") in images)
targets = [{k: v for k, v in t.items()} for t in targets]
output = model(images, targets)  # Returns losses and detections
print(output)

# For inference
[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval "torch.nn.Module.eval")()
[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [[torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(3, 300, 400), [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(3, 500, 400)]
predictions = model([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))  # Returns predictions
print(predictions[0]) 
```

```py
{'loss_classifier': tensor(0.0689, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0268, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0055, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0036, grad_fn=<DivBackward0>)}
{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)} 
```

现在让我们编写执行训练和验证的主要函数：

```py
from engine import train_one_epoch, evaluate

# train on the GPU or on the CPU, if a GPU is not available
[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")('cuda') if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available "torch.cuda.is_available")() else [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")('cpu')

# our dataset has two classes only - background and person
num_classes = 2
# use our dataset and defined transformations
[dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset") = [PennFudanDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset "torch.utils.data.Dataset")('data/PennFudanPed', get_transform(train=True))
[dataset_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset") = [PennFudanDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset "torch.utils.data.Dataset")('data/PennFudanPed', get_transform(train=False))

# split the dataset in train and test set
indices = [torch.randperm](https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm "torch.randperm")(len([dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset"))).tolist()
[dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset") = [torch.utils.data.Subset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset")([dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset"), indices[:-50])
[dataset_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset") = [torch.utils.data.Subset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset")([dataset_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset"), indices[-50:])

# define training and validation data loaders
[data_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader") = [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader")(
    [dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset"),
    batch_size=2,
    shuffle=True,
    num_workers=4,
    collate_fn=utils.collate_fn
)

[data_loader_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader") = [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader")(
    [dataset_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset "torch.utils.data.Subset"),
    batch_size=1,
    shuffle=False,
    num_workers=4,
    collate_fn=utils.collate_fn
)

# get the model using our helper function
model = get_model_instance_segmentation(num_classes)

# move model to the right device
[model.to](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to "torch.nn.Module.to")([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"))

# construct an optimizer
params = [p for p in [model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters "torch.nn.Module.parameters")() if p.requires_grad]
[optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD") = [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD")(
    params,
    lr=0.005,
    momentum=0.9,
    weight_decay=0.0005
)

# and a learning rate scheduler
[lr_scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR "torch.optim.lr_scheduler.StepLR") = [torch.optim.lr_scheduler.StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR "torch.optim.lr_scheduler.StepLR")(
    [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD"),
    step_size=3,
    gamma=0.1
)

# let's train it just for 2 epochs
num_epochs = 2

for epoch in range(num_epochs):
    # train for one epoch, printing every 10 iterations
    train_one_epoch(model, [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD"), [data_loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"), epoch, print_freq=10)
    # update the learning rate
    [lr_scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR "torch.optim.lr_scheduler.StepLR").step()
    # evaluate on the test dataset
    evaluate(model, [data_loader_test](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader"), [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"))

print("That's it!") 
```

```py
Downloading: "https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth

  0%|          | 0.00/170M [00:00<?, ?B/s]
  8%|7         | 12.7M/170M [00:00<00:01, 134MB/s]
 16%|#5        | 26.8M/170M [00:00<00:01, 142MB/s]
 24%|##3       | 40.7M/170M [00:00<00:00, 144MB/s]
 32%|###2      | 54.8M/170M [00:00<00:00, 145MB/s]
 41%|####      | 68.9M/170M [00:00<00:00, 146MB/s]
 49%|####8     | 83.0M/170M [00:00<00:00, 147MB/s]
 57%|#####7    | 97.1M/170M [00:00<00:00, 147MB/s]
 65%|######5   | 111M/170M [00:00<00:00, 147MB/s]
 74%|#######3  | 125M/170M [00:00<00:00, 148MB/s]
 82%|########2 | 140M/170M [00:01<00:00, 148MB/s]
 90%|######### | 154M/170M [00:01<00:00, 148MB/s]
 99%|#########8| 168M/170M [00:01<00:00, 148MB/s]
100%|##########| 170M/170M [00:01<00:00, 147MB/s]
Epoch: [0]  [ 0/60]  eta: 0:02:32  lr: 0.000090  loss: 3.8792 (3.8792)  loss_classifier: 0.4863 (0.4863)  loss_box_reg: 0.2543 (0.2543)  loss_mask: 3.1288 (3.1288)  loss_objectness: 0.0043 (0.0043)  loss_rpn_box_reg: 0.0055 (0.0055)  time: 2.5479  data: 0.2985  max mem: 2783
Epoch: [0]  [10/60]  eta: 0:00:52  lr: 0.000936  loss: 1.7038 (2.3420)  loss_classifier: 0.3913 (0.3626)  loss_box_reg: 0.2683 (0.2687)  loss_mask: 1.1038 (1.6881)  loss_objectness: 0.0204 (0.0184)  loss_rpn_box_reg: 0.0049 (0.0043)  time: 1.0576  data: 0.0315  max mem: 3158
Epoch: [0]  [20/60]  eta: 0:00:39  lr: 0.001783  loss: 0.9972 (1.5790)  loss_classifier: 0.2425 (0.2735)  loss_box_reg: 0.2683 (0.2756)  loss_mask: 0.3489 (1.0043)  loss_objectness: 0.0127 (0.0184)  loss_rpn_box_reg: 0.0051 (0.0072)  time: 0.9143  data: 0.0057  max mem: 3158
Epoch: [0]  [30/60]  eta: 0:00:28  lr: 0.002629  loss: 0.5966 (1.2415)  loss_classifier: 0.0979 (0.2102)  loss_box_reg: 0.2580 (0.2584)  loss_mask: 0.2155 (0.7493)  loss_objectness: 0.0119 (0.0165)  loss_rpn_box_reg: 0.0057 (0.0071)  time: 0.9036  data: 0.0065  max mem: 3158
Epoch: [0]  [40/60]  eta: 0:00:18  lr: 0.003476  loss: 0.5234 (1.0541)  loss_classifier: 0.0737 (0.1749)  loss_box_reg: 0.2241 (0.2505)  loss_mask: 0.1796 (0.6080)  loss_objectness: 0.0055 (0.0135)  loss_rpn_box_reg: 0.0047 (0.0071)  time: 0.8759  data: 0.0064  max mem: 3158
Epoch: [0]  [50/60]  eta: 0:00:09  lr: 0.004323  loss: 0.3642 (0.9195)  loss_classifier: 0.0435 (0.1485)  loss_box_reg: 0.1648 (0.2312)  loss_mask: 0.1585 (0.5217)  loss_objectness: 0.0025 (0.0113)  loss_rpn_box_reg: 0.0047 (0.0069)  time: 0.8693  data: 0.0065  max mem: 3158
Epoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.3504 (0.8381)  loss_classifier: 0.0379 (0.1339)  loss_box_reg: 0.1343 (0.2178)  loss_mask: 0.1585 (0.4690)  loss_objectness: 0.0011 (0.0102)  loss_rpn_box_reg: 0.0048 (0.0071)  time: 0.8884  data: 0.0066  max mem: 3158
Epoch: [0] Total time: 0:00:55 (0.9230 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:23  model_time: 0.2550 (0.2550)  evaluator_time: 0.0066 (0.0066)  time: 0.4734  data: 0.2107  max mem: 3158
Test:  [49/50]  eta: 0:00:00  model_time: 0.1697 (0.1848)  evaluator_time: 0.0057 (0.0078)  time: 0.1933  data: 0.0034  max mem: 3158
Test: Total time: 0:00:10 (0.2022 s / it)
Averaged stats: model_time: 0.1697 (0.1848)  evaluator_time: 0.0057 (0.0078)
Accumulating evaluation results...
DONE (t=0.02s).
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.686
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.974
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.802
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.322
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.611
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.708
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.314
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.738
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.739
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.727
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.750
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.697
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.871
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.339
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.332
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.719
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.314
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.736
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.737
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.709
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.744
Epoch: [1]  [ 0/60]  eta: 0:01:12  lr: 0.005000  loss: 0.3167 (0.3167)  loss_classifier: 0.0377 (0.0377)  loss_box_reg: 0.1232 (0.1232)  loss_mask: 0.1439 (0.1439)  loss_objectness: 0.0022 (0.0022)  loss_rpn_box_reg: 0.0097 (0.0097)  time: 1.2113  data: 0.2601  max mem: 3158
Epoch: [1]  [10/60]  eta: 0:00:45  lr: 0.005000  loss: 0.3185 (0.3209)  loss_classifier: 0.0377 (0.0376)  loss_box_reg: 0.1053 (0.1058)  loss_mask: 0.1563 (0.1684)  loss_objectness: 0.0012 (0.0017)  loss_rpn_box_reg: 0.0064 (0.0073)  time: 0.9182  data: 0.0290  max mem: 3158
Epoch: [1]  [20/60]  eta: 0:00:36  lr: 0.005000  loss: 0.2989 (0.2902)  loss_classifier: 0.0338 (0.0358)  loss_box_reg: 0.0875 (0.0952)  loss_mask: 0.1456 (0.1517)  loss_objectness: 0.0009 (0.0017)  loss_rpn_box_reg: 0.0050 (0.0058)  time: 0.8946  data: 0.0062  max mem: 3158
Epoch: [1]  [30/60]  eta: 0:00:27  lr: 0.005000  loss: 0.2568 (0.2833)  loss_classifier: 0.0301 (0.0360)  loss_box_reg: 0.0836 (0.0912)  loss_mask: 0.1351 (0.1482)  loss_objectness: 0.0008 (0.0018)  loss_rpn_box_reg: 0.0031 (0.0061)  time: 0.8904  data: 0.0065  max mem: 3158
Epoch: [1]  [40/60]  eta: 0:00:17  lr: 0.005000  loss: 0.2630 (0.2794)  loss_classifier: 0.0335 (0.0363)  loss_box_reg: 0.0804 (0.0855)  loss_mask: 0.1381 (0.1497)  loss_objectness: 0.0020 (0.0022)  loss_rpn_box_reg: 0.0030 (0.0056)  time: 0.8667  data: 0.0065  max mem: 3158
Epoch: [1]  [50/60]  eta: 0:00:08  lr: 0.005000  loss: 0.2729 (0.2829)  loss_classifier: 0.0365 (0.0375)  loss_box_reg: 0.0685 (0.0860)  loss_mask: 0.1604 (0.1515)  loss_objectness: 0.0022 (0.0022)  loss_rpn_box_reg: 0.0031 (0.0056)  time: 0.8834  data: 0.0064  max mem: 3158
Epoch: [1]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2930 (0.2816)  loss_classifier: 0.0486 (0.0381)  loss_box_reg: 0.0809 (0.0847)  loss_mask: 0.1466 (0.1511)  loss_objectness: 0.0012 (0.0021)  loss_rpn_box_reg: 0.0042 (0.0056)  time: 0.8855  data: 0.0064  max mem: 3158
Epoch: [1] Total time: 0:00:53 (0.8890 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:23  model_time: 0.2422 (0.2422)  evaluator_time: 0.0061 (0.0061)  time: 0.4774  data: 0.2283  max mem: 3158
Test:  [49/50]  eta: 0:00:00  model_time: 0.1712 (0.1832)  evaluator_time: 0.0051 (0.0066)  time: 0.1911  data: 0.0036  max mem: 3158
Test: Total time: 0:00:10 (0.2001 s / it)
Averaged stats: model_time: 0.1712 (0.1832)  evaluator_time: 0.0051 (0.0066)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.791
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.981
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.961
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.673
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.809
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.361
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.826
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.826
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.500
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.838
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.745
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.984
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.902
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.334
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.504
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.769
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.782
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.782
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.500
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.709
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.797
That's it! 
```

因此，在训练一个周期后，我们获得了 COCO 风格的 mAP > 50，以及 65 的 mask mAP。

但是预测结果是什么样的呢？让我们看看数据集中的一张图片并验证

```py
import matplotlib.pyplot as plt

from torchvision.utils import [draw_bounding_boxes](https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html#torchvision.utils.draw_bounding_boxes "torchvision.utils.draw_bounding_boxes"), [draw_segmentation_masks](https://pytorch.org/vision/stable/generated/torchvision.utils.draw_segmentation_masks.html#torchvision.utils.draw_segmentation_masks "torchvision.utils.draw_segmentation_masks")

[image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [read_image](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image "torchvision.io.read_image")("data/PennFudanPed/PNGImages/FudanPed00046.png")
[eval_transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose "torchvision.transforms.v2.Compose") = get_transform(train=False)

[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval "torch.nn.Module.eval")()
with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad "torch.no_grad")():
    [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [eval_transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose "torchvision.transforms.v2.Compose")([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
    # convert RGBA -> RGB and move to device
    [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[:3, ...].to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"))
    predictions = model([[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), ])
    pred = predictions[0]

[image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = (255.0 * ([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") - [image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").min()) / ([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").max() - [image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").min())).to([torch.uint8](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype"))
[image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[:3, ...]
pred_labels = [f"pedestrian: {score:.3f}" for label, score in zip(pred["labels"], pred["scores"])]
[pred_boxes](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = pred["boxes"].long()
[output_image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [draw_bounding_boxes](https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html#torchvision.utils.draw_bounding_boxes "torchvision.utils.draw_bounding_boxes")([image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [pred_boxes](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), pred_labels, colors="red")

[masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = (pred["masks"] > 0.7).squeeze(1)
[output_image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [draw_segmentation_masks](https://pytorch.org/vision/stable/generated/torchvision.utils.draw_segmentation_masks.html#torchvision.utils.draw_segmentation_masks "torchvision.utils.draw_segmentation_masks")([output_image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [masks](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), alpha=0.5, colors="blue")

plt.figure(figsize=(12, 12))
plt.imshow([output_image](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").permute(1, 2, 0)) 
```

![torchvision 教程](../Images/ece431a9b916de8c06f03c6efa4b7cc4.png)

```py
<matplotlib.image.AxesImage object at 0x7f48881f2830> 
```

结果看起来不错！

## 总结

在本教程中，您已经学会了如何为自定义数据集创建自己的目标检测模型训练流程。为此，您编写了一个[`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)类，该类返回图像和真实边界框以及分割掩模。您还利用了一个在 COCO train2017 上预训练的 Mask R-CNN 模型，以便在这个新数据集上进行迁移学习。

要查看包括多机器/多GPU训练在内的更完整示例，请查看 `references/detection/train.py`，该文件位于 torchvision 仓库中。

**脚本的总运行时间：**（2 分钟 27.747 秒）

[`下载 Python 源代码：torchvision_tutorial.py`](../_downloads/7590258df9f28b5ae0994c3b5b035edf/torchvision_tutorial.py)

[`下载 Jupyter 笔记本：torchvision_tutorial.ipynb`](../_downloads/4a542c9f39bedbfe7de5061767181d36/torchvision_tutorial.ipynb)

[Sphinx-Gallery 生成的画廊](https://sphinx-gallery.github.io)
