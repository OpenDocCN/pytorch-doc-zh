# 分布式和并行训练教程

> 原文：[`pytorch.org/tutorials/distributed/home.html`](https://pytorch.org/tutorials/distributed/home.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

分布式训练是一种模型训练范式，涉及将训练工作负载分布到多个工作节点，从而显著提高训练速度和模型准确性。虽然分布式训练可用于任何类型的 ML 模型训练，但对于大型模型和计算密集型任务（如深度学习）使用它最为有益。

在 PyTorch 中有几种方法可以进行分布式训练，每种方法在特定用例中都有其优势：

+   DistributedDataParallel (DDP)

+   完全分片数据并行（FSDP）

+   设备网格

+   远程过程调用（RPC）分布式训练

+   自定义扩展

在分布式概述中了解更多关于这些选项的信息。

## 学习 DDP

DDP 简介视频教程

一系列逐步视频教程，介绍如何开始使用 DistributedDataParallel，并逐步深入更复杂的主题

代码视频

开始使用分布式数据并行处理

本教程为 PyTorch DistributedData Parallel 提供了简短而温和的介绍。

代码

使用 Join 上下文管理器进行不均匀输入的分布式训练

本教程描述了 Join 上下文管理器，并演示了如何与 DistributedData Parallel 一起使用。

代码  ## 学习 FSDP

开始使用 FSDP

本教程演示了如何在 MNIST 数据集上使用 FSDP 进行分布式训练。

代码

FSDP 高级

在本教程中，您将学习如何使用 FSDP 对 HuggingFace（HF）T5 模型进行微调，用于文本摘要。

代码  ## 学习 DeviceMesh

开始使用 DeviceMesh

在本教程中，您将了解 DeviceMesh 以及它如何帮助进行分布式训练。

代码  ## 学习 RPC

开始使用分布式 RPC 框架

本教程演示了如何开始使用基于 RPC 的分布式训练。

代码

使用分布式 RPC 框架实现参数服务器

本教程将带您完成一个简单的示例，使用 PyTorch 的分布式 RPC 框架实现参数服务器。

代码

使用异步执行实现批处理 RPC 处理

在本教程中，您将使用@rpc.functions.async_execution 装饰器构建批处理 RPC 应用程序。

代码

将分布式 DataParallel 与分布式 RPC 框架结合

在本教程中，您将学习如何将分布式数据并行性与分布式模型并行性结合起来。

代码  ## 自定义扩展

使用 Cpp 扩展自定义 Process Group 后端

在本教程中，您将学习如何实现自定义的 ProcessGroup 后端，并将其插入到 PyTorch 分布式包中使用 cpp 扩展。

代码
