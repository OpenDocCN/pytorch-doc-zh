# 分布式和并行训练教程

> 原文：[https://pytorch.org/tutorials/distributed/home.html](https://pytorch.org/tutorials/distributed/home.html)

分布式训练是一种模型训练范式，涉及将训练工作负载分布到多个工作节点，从而显著提高训练速度和模型准确性。虽然分布式训练可用于任何类型的ML模型训练，但对于大型模型和计算密集型任务（如深度学习）使用它最为有益。

在PyTorch中有几种方法可以进行分布式训练，每种方法在特定用例中都有其优势：

+   [DistributedDataParallel (DDP)](#learn-ddp)

+   [完全分片数据并行（FSDP）](#learn-fsdp)

+   [设备网格](#device-mesh)

+   [远程过程调用（RPC）分布式训练](#learn-rpc)

+   [自定义扩展](#custom-extensions)

在[分布式概述](../beginner/dist_overview.html)中了解更多关于这些选项的信息。

## 学习DDP

DDP简介视频教程

一系列逐步视频教程，介绍如何开始使用DistributedDataParallel，并逐步深入更复杂的主题

代码视频

开始使用分布式数据并行处理

本教程为PyTorch DistributedData Parallel提供了简短而温和的介绍。

代码

使用Join上下文管理器进行不均匀输入的分布式训练

本教程描述了Join上下文管理器，并演示了如何与DistributedData Parallel一起使用。

代码  ## 学习FSDP

开始使用FSDP

本教程演示了如何在MNIST数据集上使用FSDP进行分布式训练。

代码

FSDP 高级

在本教程中，您将学习如何使用FSDP对HuggingFace（HF）T5模型进行微调，用于文本摘要。

代码  ## 学习DeviceMesh

开始使用DeviceMesh

在本教程中，您将了解DeviceMesh以及它如何帮助进行分布式训练。

代码  ## 学习RPC

开始使用分布式RPC框架

本教程演示了如何开始使用基于RPC的分布式训练。

代码

使用分布式RPC框架实现参数服务器

本教程将带您完成一个简单的示例，使用PyTorch的分布式RPC框架实现参数服务器。

代码

使用异步执行实现批处理RPC处理

在本教程中，您将使用@rpc.functions.async_execution装饰器构建批处理RPC应用程序。

代码

将分布式DataParallel与分布式RPC框架结合

在本教程中，您将学习如何将分布式数据并行性与分布式模型并行性结合起来。

代码  ## 自定义扩展

使用Cpp扩展自定义Process Group后端

在本教程中，您将学习如何实现自定义的ProcessGroup后端，并将其插入到PyTorch分布式包中使用cpp扩展。

代码
