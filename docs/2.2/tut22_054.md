# 使用 Torchtext 预处理自定义文本数据集

> [`pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html`](https://pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html)

注意

点击这里下载完整示例代码

**作者**：[Anupam Sharma](https://anp-scp.github.io/)

本教程演示了 torchtext 在非内置数据集上的用法。在本教程中，我们将预处理一个数据集，可以进一步用于训练用于机器翻译的序列到序列模型（类似于本教程中的内容：[使用神经网络进行序列到序列学习](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)），但不使用 torchtext 的旧版本。

在本教程中，我们将学习如何：

+   读取数据集

+   标记化句子

+   对句子应用转换

+   执行桶批处理

假设我们需要准备一个数据集来训练一个能够进行英语到德语翻译的模型。我们将使用[Tatoeba Project](https://tatoeba.org/en)提供的制表符分隔的德语 - 英语句对，可以从[此链接](https://www.manythings.org/anki/deu-eng.zip)下载。

其他语言的句子对可以在[此链接](https://www.manythings.org/anki/)找到。

## 设置

首先，下载数据集，提取 zip 文件，并记下文件 deu.txt 的路径。

确保已安装以下软件包：

+   [Torchdata 0.6.0](https://pytorch.org/data/beta/index.html)（[安装说明](https://github.com/pytorch/data)）

+   [Torchtext 0.15.0](https://pytorch.org/text/stable/index.html)（[安装说明](https://github.com/pytorch/text)）

+   [Spacy](https://spacy.io/usage)

在这里，我们使用 Spacy 对文本进行标记化。简单来说，标记化意味着将句子转换为单词列表。Spacy 是一个用于各种自然语言处理（NLP）任务的 Python 包。

从 Spacy 下载英语和德语模型，如下所示：

```py
python  -m  spacy  download  en_core_web_sm
python  -m  spacy  download  de_core_news_sm 
```

让我们从导入所需模块开始：

```py
import torchdata.datapipes as dp
import torchtext.transforms as T
import spacy
from torchtext.vocab import [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator "torchtext.vocab.build_vocab_from_iterator")
eng = spacy.load("en_core_web_sm") # Load the English model to tokenize English text
de = spacy.load("de_core_news_sm") # Load the German model to tokenize German text 
```

现在我们将加载数据集

```py
FILE_PATH = 'data/deu.txt'
data_pipe = [dp.iter.IterableWrapper](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset "torch.utils.data.IterableDataset")([FILE_PATH])
data_pipe = [dp.iter.FileOpener](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset "torch.utils.data.IterableDataset")(data_pipe, mode='rb')
data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\t', as_tuple=True) 
```

在上述代码块中，我们正在做以下事情：

1.  在第 2 行，我们正在创建一个文件名的可迭代对象

1.  在第 3 行，我们将可迭代对象传递给 FileOpener，然后以读取模式打开文件

1.  在第 4 行，我们调用一个函数来解析文件，该函数再次返回一个元组的可迭代对象，表示制表符分隔文件的每一行

DataPipes 可以被视为类似数据集对象的东西，我们可以在其上执行各种操作。查看[此教程](https://pytorch.org/data/beta/dp_tutorial.html)以获取有关 DataPipes 的更多详细信息。

我们可以验证可迭代对象是否包含句子对，如下所示：

```py
for sample in data_pipe:
    print(sample)
    break 
```

```py
('Go.', 'Geh.', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)') 
```

请注意，我们还有句子对的归属细节。我们将编写一个小函数来删除归属细节：

```py
def removeAttribution(row):
  """
 Function to keep the first two elements in a tuple
 """
    return row[:2]
data_pipe = data_pipe.map(removeAttribution) 
```

上述代码块中第 6 行的 map 函数可用于在 data_pipe 的每个元素上应用某个函数。现在，我们可以验证 data_pipe 只包含句子对。

```py
for sample in data_pipe:
    print(sample)
    break 
```

```py
('Go.', 'Geh.') 
```

现在，让我们定义一些函数来执行标记化：

```py
def engTokenize(text):
  """
 Tokenize an English text and return a list of tokens
 """
    return [token.text for token in eng.tokenizer(text)]

def deTokenize(text):
  """
 Tokenize a German text and return a list of tokens
 """
    return [token.text for token in de.tokenizer(text)] 
```

上述函数接受文本并返回如下所示的单词列表：

```py
print(engTokenize("Have a good day!!!"))
print(deTokenize("Haben Sie einen guten Tag!!!")) 
```

```py
['Have', 'a', 'good', 'day', '!', '!', '!']
['Haben', 'Sie', 'einen', 'guten', 'Tag', '!', '!', '!'] 
```

## 构建词汇表

让我们将英语句子作为源，德语句子作为目标。

词汇可以被视为数据集中我们拥有的唯一单词集合。我们现在将为源和目标构建词汇表。

让我们定义一个函数，从迭代器中的元组元素获取标记。

```py
def getTokens(data_iter, place):
  """
 Function to yield tokens from an iterator. Since, our iterator contains
 tuple of sentences (source and target), `place` parameters defines for which
 index to return the tokens for. `place=0` for source and `place=1` for target
 """
    for english, german in data_iter:
        if place == 0:
            yield engTokenize(english)
        else:
            yield deTokenize(german) 
```

现在，我们将为源构建词汇表：

```py
[source_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab "torchtext.vocab.Vocab") = [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator "torchtext.vocab.build_vocab_from_iterator")(
    getTokens(data_pipe,0),
    min_freq=2,
    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],
    special_first=True
)
[source_vocab.set_default_index](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.set_default_index "torchtext.vocab.Vocab.set_default_index")([source_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab "torchtext.vocab.Vocab")['<unk>']) 
```

上面的代码从迭代器构建词汇表。在上述代码块中：

+   在第 2 行，我们调用 getTokens()函数，并将 place=0，因为我们需要源句子的词汇表。

+   在第 3 行，我们设置 min_freq=2。这意味着该函数将跳过出现少于 2 次的单词。

+   在第 4 行，我们指定了一些特殊标记：

    +   <sos>表示句子的开始

    +   <eos>表示句子结束

    +   <unk>表示未知单词。一个未知单词的示例是由于 min_freq=2 而被跳过的单词。

    +   <pad>是填充标记。在训练模型时，我们大多数情况下是以批量的形式训练。在一个批次中，可能会有不同长度的句子。因此，我们用<pad>标记填充较短的句子，使批次中所有序列的长度相等。

+   在第 5 行，我们设置 special_first=True。这意味着<pad>将在词汇表中得到索引 0，<sos>得到索引 1，<eos>得到索引 2，<unk>将在词汇表中得到索引 3。

+   在第 7 行，我们将默认索引设置为<unk>的索引。这意味着如果某个单词不在词汇表中，我们将使用<unk>代替该未知单词。

类似地，我们将为目标句子构建词汇表：

```py
[target_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab "torchtext.vocab.Vocab") = [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator "torchtext.vocab.build_vocab_from_iterator")(
    getTokens(data_pipe,1),
    min_freq=2,
    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],
    special_first=True
)
[target_vocab.set_default_index](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.set_default_index "torchtext.vocab.Vocab.set_default_index")([target_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab "torchtext.vocab.Vocab")['<unk>']) 
```

请注意，上面的示例显示了如何向我们的词汇表添加特殊标记。特殊标记可能会根据需求而变化。

现在，我们可以验证特殊标记是放在开头的，然后是其他单词。在下面的代码中，source_vocab.get_itos()返回一个基于词汇表的索引的标记列表。

```py
print([source_vocab.get_itos](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.get_itos "torchtext.vocab.Vocab.get_itos")()[:9]) 
```

```py
['<pad>', '<sos>', '<eos>', '<unk>', '.', 'I', 'Tom', 'to', 'you'] 
```

## 使用词汇表对句子进行数字化

构建词汇表后，我们需要将我们的句子转换为相应的索引。让我们为此定义一些函数：

```py
def getTransform(vocab):
  """
 Create transforms based on given vocabulary. The returned transform is applied to sequence
 of tokens.
 """
    text_tranform = [T.Sequential](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.Sequential "torchtext.transforms.Sequential")(
        ## converts the sentences to indices based on given vocabulary
        [T.VocabTransform](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.VocabTransform "torchtext.transforms.VocabTransform")(vocab=vocab),
        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is
        # 1 as seen in previous section
        [T.AddToken](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.AddToken "torchtext.transforms.AddToken")(1, begin=True),
        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is
        # 2 as seen in previous section
        [T.AddToken](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.AddToken "torchtext.transforms.AddToken")(2, begin=False)
    )
    return text_tranform 
```

现在，让我们看看如何使用上述函数。该函数返回一个 Transforms 对象，我们将在我们的句子上使用它。让我们取一个随机句子并检查转换的工作方式。

```py
temp_list = list(data_pipe)
some_sentence = temp_list[798][0]
print("Some sentence=", end="")
print(some_sentence)
transformed_sentence = getTransform([source_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab "torchtext.vocab.Vocab"))(engTokenize(some_sentence))
print("Transformed sentence=", end="")
print(transformed_sentence)
index_to_string = [source_vocab.get_itos](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.get_itos "torchtext.vocab.Vocab.get_itos")()
for index in transformed_sentence:
    print(index_to_string[index], end=" ") 
```

```py
Some sentence=I fainted.
Transformed sentence=[1, 5, 2897, 4, 2]
<sos> I fainted . <eos> 
```

在上面的代码中：

+   在第 2 行，我们从在第 1 行从 data_pipe 创建的列表中取一个源句子

+   在第 5 行，我们根据源词汇表获取一个转换，并将其应用于一个标记化的句子。请注意，转换接受单词列表而不是句子。

+   在第 8 行，我们获取索引到字符串的映射，然后使用它来获取转换后的句子

现在我们将使用 DataPipe 函数来对所有句子应用转换。让我们为此定义一些更多的函数。

```py
def applyTransform(sequence_pair):
  """
 Apply transforms to sequence of tokens in a sequence pair
 """

    return (
        getTransform([source_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab "torchtext.vocab.Vocab"))(engTokenize(sequence_pair[0])),
        getTransform([target_vocab](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab "torchtext.vocab.Vocab"))(deTokenize(sequence_pair[1]))
    )
data_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator
temp_list = list(data_pipe)
print(temp_list[0]) 
```

```py
([1, 616, 4, 2], [1, 739, 4, 2]) 
```

## 制作批次（使用 bucket batch）

通常，我们以批量的形式训练模型。在为序列到序列模型工作时，建议保持批次中序列的长度相似。为此，我们将使用 data_pipe 的 bucketbatch 函数。

让我们定义一些将被 bucketbatch 函数使用的函数。

```py
def sortBucket(bucket):
  """
 Function to sort a given bucket. Here, we want to sort based on the length of
 source and target sequence.
 """
    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1]))) 
```

现在，我们将应用 bucketbatch 函数：

```py
data_pipe = data_pipe.bucketbatch(
    batch_size = 4, batch_num=5,  bucket_num=1,
    use_in_batch_shuffle=False, sort_key=sortBucket
) 
```

在上面的代码块中：

> +   我们保持批量大小为 4。
> +   
> +   batch_num 是要在桶中保留的批次数
> +   
> +   bucket_num 是要在池中保留的桶数以进行洗牌。
> +   
> +   sort_key 指定一个函数，该函数接受一个桶并对其进行排序

现在，让我们将一批源句子表示为 X，将一批目标句子表示为 y。通常，在训练模型时，我们对一批 X 进行预测，并将结果与 y 进行比较。但是，在我们的 data_pipe 中，一个批次的形式是[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]：

```py
print(list(data_pipe)[0]) 
```

```py
[([1, 11105, 17, 4, 2], [1, 507, 29, 24, 2]), ([1, 11105, 17, 4, 2], [1, 7994, 1487, 24, 2]), ([1, 5335, 21, 4, 2], [1, 6956, 32, 24, 2]), ([1, 5335, 21, 4, 2], [1, 16003, 32, 24, 2])] 
```

因此，我们现在将把它们转换为这种形式：((X_1,X_2,X_3,X_4)，(y_1,y_2,y_3,y_4))。为此，我们将编写一个小函数：

```py
def separateSourceTarget(sequence_pairs):
  """
 input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`
 output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`
 """
    sources,targets = zip(*sequence_pairs)
    return sources,targets

## Apply the function to each element in the iterator
data_pipe = data_pipe.map(separateSourceTarget)
print(list(data_pipe)[0]) 
```

```py
(([1, 6860, 23, 10, 2], [1, 6860, 23, 10, 2], [1, 29, 466, 4, 2], [1, 29, 466, 4, 2]), ([1, 20825, 8, 2], [1, 11118, 8, 2], [1, 31, 1152, 4, 2], [1, 31, 1035, 4, 2])) 
```

现在，我们已经得到了所需的数据。

## 填充

如前所述，在构建词汇表时，我们需要填充批次中较短的句子，以使批次中所有序列的长度相等。我们可以按以下方式执行填充：

```py
def applyPadding(pair_of_sequences):
  """
 Convert sequences to tensors and apply padding
 """
    return ([T.ToTensor](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.ToTensor "torchtext.transforms.ToTensor")(0)(list(pair_of_sequences[0])), [T.ToTensor](https://pytorch.org/text/stable/transforms.html#torchtext.transforms.ToTensor "torchtext.transforms.ToTensor")(0)(list(pair_of_sequences[1])))
## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies
# padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the
# vocabulary.
data_pipe = data_pipe.map(applyPadding) 
```

现在，我们可以使用索引到字符串映射来查看序列如何以标记而不是索引的形式呈现：

```py
source_index_to_string = [source_vocab.get_itos](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.get_itos "torchtext.vocab.Vocab.get_itos")()
target_index_to_string = [target_vocab.get_itos](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.get_itos "torchtext.vocab.Vocab.get_itos")()

def showSomeTransformedSentences(data_pipe):
  """
 Function to show how the sentences look like after applying all transforms.
 Here we try to print actual words instead of corresponding index
 """
    for sources,targets in data_pipe:
        if sources[0][-1] != 0:
            continue # Just to visualize padding of shorter sentences
        for i in range(4):
            source = ""
            for token in sources[i]:
                source += " " + source_index_to_string[token]
            target = ""
            for token in targets[i]:
                target += " " + target_index_to_string[token]
            print(f"Source: {source}")
            print(f"Traget: {target}")
        break

showSomeTransformedSentences(data_pipe) 
```

```py
Source:  <sos> Freeze ! <eos> <pad>
Traget:  <sos> Stehenbleiben ! <eos> <pad>
Source:  <sos> <unk> ! <eos> <pad>
Traget:  <sos> Zum Wohl ! <eos>
Source:  <sos> Freeze ! <eos> <pad>
Traget:  <sos> Keine Bewegung ! <eos>
Source:  <sos> Got it ! <eos>
Traget:  <sos> Verstanden ! <eos> <pad> 
```

在上面的输出中，我们可以观察到较短的句子被填充为<pad>。现在，我们可以在编写训练函数时使用 data_pipe。

本教程的部分内容受到了[这篇文章](https://medium.com/@bitdribble/migrate-torchtext-to-the-new-0-9-0-api-1ff1472b5d71)的启发。

**脚本的总运行时间：**（4 分钟 41.756 秒）

`下载 Python 源代码：torchtext_custom_dataset_tutorial.py`

`下载 Jupyter 笔记本：torchtext_custom_dataset_tutorial.ipynb`

[由 Sphinx-Gallery 生成的图库](https://sphinx-gallery.github.io)
