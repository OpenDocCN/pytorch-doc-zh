# （beta）使用 FX 构建一个简单的 CPU 性能分析器

> 原文：[`pytorch.org/tutorials/intermediate/fx_profiling_tutorial.html`](https://pytorch.org/tutorials/intermediate/fx_profiling_tutorial.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

注意

点击这里下载完整的示例代码

**作者**：[James Reed](https://github.com/jamesr66a)

在本教程中，我们将使用 FX 来执行以下操作：

1.  以一种我们可以检查和收集关于代码结构和执行的统计信息的方式捕获 PyTorch Python 代码

1.  构建一个小类，作为一个简单的性能“分析器”，收集关于模型各部分的运行时统计信息。

在本教程中，我们将使用 torchvision ResNet18 模型进行演示。

```py
import torch
import torch.fx
import torchvision.models as models

rn18 = models.resnet18()
rn18.eval() 
```

```py
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
) 
```

现在我们有了我们的模型，我们想要更深入地检查其性能。也就是说，在以下调用中，模型的哪些部分花费时间最长？

```py
input = torch.randn(5, 3, 224, 224)
output = rn18(input) 
```

回答这个问题的常见方法是浏览程序源代码，在程序的各个点添加收集时间戳的代码，并比较这些时间戳之间的差异，以查看这些时间戳之间的区域需要多长时间。

这种技术当然适用于 PyTorch 代码，但如果我们不必复制模型代码并进行编辑，尤其是我们没有编写的代码（比如这个 torchvision 模型），那将更好。相反，我们将使用 FX 自动化这个“仪器化”过程，而无需修改任何源代码。

首先，让我们解决一些导入问题（我们稍后将在代码中使用所有这些）。

```py
import statistics, tabulate, time
from typing import Any, Dict, List
from torch.fx import Interpreter 
```

注意

`tabulate`是一个外部库，不是 PyTorch 的依赖项。我们将使用它来更轻松地可视化性能数据。请确保您已从您喜欢的 Python 软件包源安装了它。

## 使用符号跟踪捕获模型

接下来，我们将使用 FX 的符号跟踪机制来捕获我们模型的定义，以便我们可以操作和检查它。

```py
traced_rn18 = torch.fx.symbolic_trace(rn18)
print(traced_rn18.graph) 
```

```py
graph():
    %x : torch.Tensor [num_users=1] = placeholder[target=x]
    %conv1 : [num_users=1] = call_moduletarget=conv1, kwargs = {})
    %bn1 : [num_users=1] = call_moduletarget=bn1, kwargs = {})
    %relu : [num_users=1] = call_moduletarget=relu, kwargs = {})
    %maxpool : [num_users=2] = call_moduletarget=maxpool, kwargs = {})
    %layer1_0_conv1 : [num_users=1] = call_moduletarget=layer1.0.conv1, kwargs = {})
    %layer1_0_bn1 : [num_users=1] = call_moduletarget=layer1.0.bn1, kwargs = {})
    %layer1_0_relu : [num_users=1] = call_moduletarget=layer1.0.relu, kwargs = {})
    %layer1_0_conv2 : [num_users=1] = call_moduletarget=layer1.0.conv2, kwargs = {})
    %layer1_0_bn2 : [num_users=1] = call_moduletarget=layer1.0.bn2, kwargs = {})
    %add : [num_users=1] = call_functiontarget=operator.add, kwargs = {})
    %layer1_0_relu_1 : [num_users=2] = call_moduletarget=layer1.0.relu, kwargs = {})
    %layer1_1_conv1 : [num_users=1] = call_moduletarget=layer1.1.conv1, kwargs = {})
    %layer1_1_bn1 : [num_users=1] = call_moduletarget=layer1.1.bn1, kwargs = {})
    %layer1_1_relu : [num_users=1] = call_moduletarget=layer1.1.relu, kwargs = {})
    %layer1_1_conv2 : [num_users=1] = call_moduletarget=layer1.1.conv2, kwargs = {})
    %layer1_1_bn2 : [num_users=1] = call_moduletarget=layer1.1.bn2, kwargs = {})
    %add_1 : [num_users=1] = call_functiontarget=operator.add, kwargs = {})
    %layer1_1_relu_1 : [num_users=2] = call_moduletarget=layer1.1.relu, kwargs = {})
    %layer2_0_conv1 : [num_users=1] = call_moduletarget=layer2.0.conv1, kwargs = {})
    %layer2_0_bn1 : [num_users=1] = call_moduletarget=layer2.0.bn1, kwargs = {})
    %layer2_0_relu : [num_users=1] = call_moduletarget=layer2.0.relu, kwargs = {})
    %layer2_0_conv2 : [num_users=1] = call_moduletarget=layer2.0.conv2, kwargs = {})
    %layer2_0_bn2 : [num_users=1] = call_moduletarget=layer2.0.bn2, kwargs = {})
    %layer2_0_downsample_0 : [num_users=1] = call_moduletarget=layer2.0.downsample.0, kwargs = {})
    %layer2_0_downsample_1 : [num_users=1] = call_moduletarget=layer2.0.downsample.1, kwargs = {})
    %add_2 : [num_users=1] = call_functiontarget=operator.add, kwargs = {})
    %layer2_0_relu_1 : [num_users=2] = call_moduletarget=layer2.0.relu, kwargs = {})
    %layer2_1_conv1 : [num_users=1] = call_moduletarget=layer2.1.conv1, kwargs = {})
    %layer2_1_bn1 : [num_users=1] = call_moduletarget=layer2.1.bn1, kwargs = {})
    %layer2_1_relu : [num_users=1] = call_moduletarget=layer2.1.relu, kwargs = {})
    %layer2_1_conv2 : [num_users=1] = call_moduletarget=layer2.1.conv2, kwargs = {})
    %layer2_1_bn2 : [num_users=1] = call_moduletarget=layer2.1.bn2, kwargs = {})
    %add_3 : [num_users=1] = call_functiontarget=operator.add, kwargs = {})
    %layer2_1_relu_1 : [num_users=2] = call_moduletarget=layer2.1.relu, kwargs = {})
    %layer3_0_conv1 : [num_users=1] = call_moduletarget=layer3.0.conv1, kwargs = {})
    %layer3_0_bn1 : [num_users=1] = call_moduletarget=layer3.0.bn1, kwargs = {})
    %layer3_0_relu : [num_users=1] = call_moduletarget=layer3.0.relu, kwargs = {})
    %layer3_0_conv2 : [num_users=1] = call_moduletarget=layer3.0.conv2, kwargs = {})
    %layer3_0_bn2 : [num_users=1] = call_moduletarget=layer3.0.bn2, kwargs = {})
    %layer3_0_downsample_0 : [num_users=1] = call_moduletarget=layer3.0.downsample.0, kwargs = {})
    %layer3_0_downsample_1 : [num_users=1] = call_moduletarget=layer3.0.downsample.1, kwargs = {})
    %add_4 : [num_users=1] = call_functiontarget=operator.add, kwargs = {})
    %layer3_0_relu_1 : [num_users=2] = call_moduletarget=layer3.0.relu, kwargs = {})
    %layer3_1_conv1 : [num_users=1] = call_moduletarget=layer3.1.conv1, kwargs = {})
    %layer3_1_bn1 : [num_users=1] = call_moduletarget=layer3.1.bn1, kwargs = {})
    %layer3_1_relu : [num_users=1] = call_moduletarget=layer3.1.relu, kwargs = {})
    %layer3_1_conv2 : [num_users=1] = call_moduletarget=layer3.1.conv2, kwargs = {})
    %layer3_1_bn2 : [num_users=1] = call_moduletarget=layer3.1.bn2, kwargs = {})
    %add_5 : [num_users=1] = call_functiontarget=operator.add, kwargs = {})
    %layer3_1_relu_1 : [num_users=2] = call_moduletarget=layer3.1.relu, kwargs = {})
    %layer4_0_conv1 : [num_users=1] = call_moduletarget=layer4.0.conv1, kwargs = {})
    %layer4_0_bn1 : [num_users=1] = call_moduletarget=layer4.0.bn1, kwargs = {})
    %layer4_0_relu : [num_users=1] = call_moduletarget=layer4.0.relu, kwargs = {})
    %layer4_0_conv2 : [num_users=1] = call_moduletarget=layer4.0.conv2, kwargs = {})
    %layer4_0_bn2 : [num_users=1] = call_moduletarget=layer4.0.bn2, kwargs = {})
    %layer4_0_downsample_0 : [num_users=1] = call_moduletarget=layer4.0.downsample.0, kwargs = {})
    %layer4_0_downsample_1 : [num_users=1] = call_moduletarget=layer4.0.downsample.1, kwargs = {})
    %add_6 : [num_users=1] = call_functiontarget=operator.add, kwargs = {})
    %layer4_0_relu_1 : [num_users=2] = call_moduletarget=layer4.0.relu, kwargs = {})
    %layer4_1_conv1 : [num_users=1] = call_moduletarget=layer4.1.conv1, kwargs = {})
    %layer4_1_bn1 : [num_users=1] = call_moduletarget=layer4.1.bn1, kwargs = {})
    %layer4_1_relu : [num_users=1] = call_moduletarget=layer4.1.relu, kwargs = {})
    %layer4_1_conv2 : [num_users=1] = call_moduletarget=layer4.1.conv2, kwargs = {})
    %layer4_1_bn2 : [num_users=1] = call_moduletarget=layer4.1.bn2, kwargs = {})
    %add_7 : [num_users=1] = call_functiontarget=operator.add, kwargs = {})
    %layer4_1_relu_1 : [num_users=1] = call_moduletarget=layer4.1.relu, kwargs = {})
    %avgpool : [num_users=1] = call_moduletarget=avgpool, kwargs = {})
    %flatten : [num_users=1] = call_functiontarget=torch.flatten, kwargs = {})
    %fc : [num_users=1] = call_moduletarget=fc, kwargs = {})
    return fc 
```

这为我们提供了 ResNet18 模型的图形表示。图形由一系列相互连接的节点组成。每个节点代表 Python 代码中的调用点（无论是函数、模块还是方法），边缘（在每个节点上表示为`args`和`kwargs`）代表这些调用点之间传递的值。有关图形表示和 FX 的其余 API 的更多信息，请参阅 FX 文档[`pytorch.org/docs/master/fx.html`](https://pytorch.org/docs/master/fx.html)。

## 创建一个性能分析解释器

接下来，我们将创建一个从`torch.fx.Interpreter`继承的类。虽然`symbolic_trace`生成的`GraphModule`编译了 Python 代码，当您调用`GraphModule`时运行，但运行`GraphModule`的另一种方法是逐个执行`Graph`中的每个`Node`。这就是`Interpreter`提供的功能：它逐个解释图节点。

通过继承`Interpreter`，我们可以重写各种功能，并安装我们想要的分析行为。目标是有一个对象，我们可以将一个模型传递给它，调用模型 1 次或多次，然后获取关于模型和模型各部分在这些运行中花费多长时间的统计信息。

让我们定义我们的`ProfilingInterpreter`类：

```py
class ProfilingInterpreter(Interpreter):
    def __init__(self, mod : torch.nn.Module):
        # Rather than have the user symbolically trace their model,
        # we're going to do it in the constructor. As a result, the
        # user can pass in any ``Module`` without having to worry about
        # symbolic tracing APIs
        gm = torch.fx.symbolic_trace(mod)
        super().__init__(gm)

        # We are going to store away two things here:
        #
        # 1\. A list of total runtimes for ``mod``. In other words, we are
        #    storing away the time ``mod(...)`` took each time this
        #    interpreter is called.
        self.total_runtime_sec : List[float] = []
        # 2\. A map from ``Node`` to a list of times (in seconds) that
        #    node took to run. This can be seen as similar to (1) but
        #    for specific sub-parts of the model.
        self.runtimes_sec : Dict[torch.fx.Node, List[float]] = {}

    ######################################################################
    # Next, let's override our first method: ``run()``. ``Interpreter``'s ``run``
    # method is the top-level entry point for execution of the model. We will
    # want to intercept this so that we can record the total runtime of the
    # model.

    def run(self, *args) -> Any:
        # Record the time we started running the model
        t_start = time.time()
        # Run the model by delegating back into Interpreter.run()
        return_val = super().run(*args)
        # Record the time we finished running the model
        t_end = time.time()
        # Store the total elapsed time this model execution took in the
        # ``ProfilingInterpreter``
        self.total_runtime_sec.append(t_end - t_start)
        return return_val

    ######################################################################
    # Now, let's override ``run_node``. ``Interpreter`` calls ``run_node`` each
    # time it executes a single node. We will intercept this so that we
    # can measure and record the time taken for each individual call in
    # the model.

    def run_node(self, n : torch.fx.Node) -> Any:
        # Record the time we started running the op
        t_start = time.time()
        # Run the op by delegating back into Interpreter.run_node()
        return_val = super().run_node(n)
        # Record the time we finished running the op
        t_end = time.time()
        # If we don't have an entry for this node in our runtimes_sec
        # data structure, add one with an empty list value.
        self.runtimes_sec.setdefault(n, [])
        # Record the total elapsed time for this single invocation
        # in the runtimes_sec data structure
        self.runtimes_sec[n].append(t_end - t_start)
        return return_val

    ######################################################################
    # Finally, we are going to define a method (one which doesn't override
    # any ``Interpreter`` method) that provides us a nice, organized view of
    # the data we have collected.

    def summary(self, should_sort : bool = False) -> str:
        # Build up a list of summary information for each node
        node_summaries : List[List[Any]] = []
        # Calculate the mean runtime for the whole network. Because the
        # network may have been called multiple times during profiling,
        # we need to summarize the runtimes. We choose to use the
        # arithmetic mean for this.
        mean_total_runtime = statistics.mean(self.total_runtime_sec)

        # For each node, record summary statistics
        for node, runtimes in self.runtimes_sec.items():
            # Similarly, compute the mean runtime for ``node``
            mean_runtime = statistics.mean(runtimes)
            # For easier understanding, we also compute the percentage
            # time each node took with respect to the whole network.
            pct_total = mean_runtime / mean_total_runtime * 100
            # Record the node's type, name of the node, mean runtime, and
            # percent runtime.
            node_summaries.append(
                [node.op, str(node), mean_runtime, pct_total])

        # One of the most important questions to answer when doing performance
        # profiling is "Which op(s) took the longest?". We can make this easy
        # to see by providing sorting functionality in our summary view
        if should_sort:
            node_summaries.sort(key=lambda s: s[2], reverse=True)

        # Use the ``tabulate`` library to create a well-formatted table
        # presenting our summary information
        headers : List[str] = [
            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'
        ]
        return tabulate.tabulate(node_summaries, headers=headers) 
```

注意

我们使用 Python 的`time.time`函数来获取墙钟时间戳并进行比较。这不是衡量性能的最准确方法，只会给我们一个一阶近似。我们仅仅出于演示目的使用这种简单的技术。

## 调查 ResNet18 的性能

我们现在可以使用`ProfilingInterpreter`来检查我们的 ResNet18 模型的性能特征；

```py
interp = ProfilingInterpreter(rn18)
interp.run(input)
print(interp.summary(True)) 
```

```py
Op type        Op                       Average runtime (s)    Pct total runtime
-------------  ---------------------  ---------------------  -------------------
call_module    maxpool                          0.0058043              9.43883
call_module    conv1                            0.00556087             9.04297
call_module    layer4_0_conv2                   0.00342155             5.56404
call_module    layer4_1_conv2                   0.00325394             5.29148
call_module    layer4_1_conv1                   0.00316119             5.14066
call_module    layer1_0_conv2                   0.00267935             4.3571
call_module    layer1_1_conv1                   0.00267816             4.35516
call_module    layer3_0_conv2                   0.00267792             4.35477
call_module    layer3_1_conv1                   0.00261283             4.24893
call_module    layer3_1_conv2                   0.00259137             4.21403
call_module    layer1_0_conv1                   0.00256515             4.17138
call_module    layer2_1_conv1                   0.00249219             4.05274
call_module    layer2_1_conv2                   0.0024581              3.9973
call_module    layer2_0_conv2                   0.00242114             3.93721
call_module    layer1_1_conv2                   0.00241613             3.92906
call_module    layer4_0_conv1                   0.00203657             3.31183
call_module    layer3_0_conv1                   0.00165725             2.69498
call_module    layer2_0_conv1                   0.00164604             2.67676
call_module    bn1                              0.00133991             2.17894
call_module    layer2_0_downsample_0            0.000616312            1.00223
call_module    layer3_0_downsample_0            0.000507832            0.825825
call_module    layer4_0_downsample_0            0.000471115            0.766117
call_function  add                              0.00034976             0.568772
call_module    relu                             0.000216722            0.352429
call_function  add_1                            0.000201702            0.328004
call_module    fc                               0.000183105            0.297762
call_module    layer1_0_bn1                     0.000178337            0.290008
call_module    layer1_0_bn2                     0.000164032            0.266745
call_module    layer1_1_bn1                     0.000163794            0.266358
call_module    layer1_1_bn2                     0.000160933            0.261705
call_module    avgpool                          0.000149012            0.242319
call_module    layer2_1_bn2                     0.000141621            0.2303
call_module    layer2_0_downsample_1            0.000141382            0.229913
call_module    layer4_0_bn2                     0.000140429            0.228362
call_module    layer2_0_bn1                     0.000137806            0.224097
call_module    layer4_1_bn2                     0.000136852            0.222546
call_module    layer2_1_bn1                     0.000136137            0.221383
call_module    layer2_0_bn2                     0.000132799            0.215955
call_module    layer1_1_relu                    0.000128984            0.209752
call_function  add_2                            0.000127316            0.207038
call_module    layer3_1_bn1                     0.000127316            0.207038
call_module    layer3_0_downsample_1            0.0001266              0.205875
call_module    layer3_0_bn1                     0.000126362            0.205487
call_module    layer3_0_bn2                     0.000125647            0.204324
call_function  add_3                            0.000124454            0.202385
call_module    layer3_1_bn2                     0.000123978            0.20161
call_module    layer4_1_bn1                     0.000119686            0.194631
call_module    layer4_0_downsample_1            0.000118017            0.191917
call_module    layer4_0_bn1                     0.000117779            0.191529
call_module    layer1_0_relu                    0.000107288            0.17447
call_module    layer1_0_relu_1                  9.91821e-05            0.161288
call_module    layer1_1_relu_1                  9.63211e-05            0.156635
call_module    layer4_0_relu                    8.51154e-05            0.138413
call_function  add_5                            8.46386e-05            0.137637
call_module    layer4_1_relu                    8.44002e-05            0.13725
call_module    layer2_1_relu                    8.36849e-05            0.136087
call_function  add_4                            8.24928e-05            0.134148
call_module    layer2_0_relu                    8.10623e-05            0.131822
call_module    layer2_1_relu_1                  8.01086e-05            0.130271
call_module    layer2_0_relu_1                  7.96318e-05            0.129496
call_module    layer3_0_relu_1                  7.9155e-05             0.12872
call_module    layer4_0_relu_1                  7.7486e-05             0.126006
call_function  add_7                            7.7486e-05             0.126006
call_module    layer3_1_relu                    7.70092e-05            0.125231
call_function  add_6                            7.67708e-05            0.124843
call_module    layer4_1_relu_1                  7.67708e-05            0.124843
call_module    layer3_0_relu                    7.65324e-05            0.124455
call_module    layer3_1_relu_1                  7.10487e-05            0.115538
call_function  flatten                          4.3869e-05             0.0713388
placeholder    x                                2.59876e-05            0.0422605
output         output                           1.95503e-05            0.0317923 
```

这里有两件事情我们应该注意：

+   `MaxPool2d`占用了最多的时间。这是一个已知问题：[`github.com/pytorch/pytorch/issues/51393`](https://github.com/pytorch/pytorch/issues/51393)

+   BatchNorm2d 也占用了相当多的时间。我们可以继续这种思路，并在 Conv-BN Fusion with FX [教程](https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html)中对其进行优化。

## 结论

正如我们所看到的，使用 FX，我们可以轻松地捕获 PyTorch 程序（甚至是我们没有源代码的程序！）以机器可解释的格式进行分析，比如我们在这里所做的性能分析。FX 为使用 PyTorch 程序开辟了一个充满可能性的世界。

最后，由于 FX 仍处于测试阶段，我们很乐意听取您对其使用的任何反馈意见。请随时使用 PyTorch 论坛（[`discuss.pytorch.org/`](https://discuss.pytorch.org/)）和问题跟踪器（[`github.com/pytorch/pytorch/issues`](https://github.com/pytorch/pytorch/issues)）提供您可能有的任何反馈意见。

脚本的总运行时间：（0 分钟 0.374 秒）

`下载 Python 源代码：fx_profiling_tutorial.py`

`下载 Jupyter 笔记本：fx_profiling_tutorial.ipynb`

[Sphinx-Gallery 生成的图库](https://sphinx-gallery.github.io)
