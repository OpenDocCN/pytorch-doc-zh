# 优化用于部署的 Vision Transformer 模型

> 原文：[`pytorch.org/tutorials/beginner/vt_tutorial.html`](https://pytorch.org/tutorials/beginner/vt_tutorial.html)

注意

点击此处下载完整示例代码

[Jeff Tang](https://github.com/jeffxtang), [Geeta Chauhan](https://github.com/gchauhan/)

Vision Transformer 模型应用了引入自自然语言处理的最先进的基于注意力的 Transformer 模型，以实现各种最先进（SOTA）结果，用于计算机视觉任务。Facebook Data-efficient Image Transformers [DeiT](https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification)是在 ImageNet 上进行图像分类训练的 Vision Transformer 模型。

在本教程中，我们将首先介绍 DeiT 是什么以及如何使用它，然后逐步介绍脚本化、量化、优化和在 iOS 和 Android 应用程序中使用模型的完整步骤。我们还将比较量化、优化和非量化、非优化模型的性能，并展示在各个步骤中应用量化和优化对模型的好处。

## 什么是 DeiT

自 2012 年深度学习兴起以来，卷积神经网络（CNNs）一直是图像分类的主要模型，但 CNNs 通常需要数亿张图像进行训练才能实现 SOTA 结果。DeiT 是一个视觉 Transformer 模型，需要更少的数据和计算资源进行训练，以与领先的 CNNs 竞争执行图像分类，这是由 DeiT 的两个关键组件实现的：

+   数据增强模拟在更大数据集上进行训练；

+   原生蒸馏允许 Transformer 网络从 CNN 的输出中学习。

DeiT 表明 Transformer 可以成功应用于计算机视觉任务，且对数据和资源的访问有限。有关 DeiT 的更多详细信息，请参见[存储库](https://github.com/facebookresearch/deit)和[论文](https://arxiv.org/abs/2012.12877)。

## 使用 DeiT 对图像进行分类

请按照 DeiT 存储库中的`README.md`中的详细信息来对图像进行分类，或者进行快速测试，首先安装所需的软件包：

```py
pip install torch torchvision timm pandas requests 
```

要在 Google Colab 中运行，请通过运行以下命令安装依赖项：

```py
!pip install timm pandas requests 
```

然后运行下面的脚本：

```py
from PIL import Image
import torch
import timm
import requests
import torchvision.transforms as transforms
from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD

print(torch.__version__)
# should be 1.8.0

model = [torch.hub.load](https://pytorch.org/docs/stable/hub.html#torch.hub.load "torch.hub.load")('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)
[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval "torch.nn.Module.eval")()

[transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose "torchvision.transforms.Compose") = [transforms.Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose "torchvision.transforms.Compose")([
    [transforms.Resize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html#torchvision.transforms.Resize "torchvision.transforms.Resize")(256, interpolation=3),
    [transforms.CenterCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.CenterCrop.html#torchvision.transforms.CenterCrop "torchvision.transforms.CenterCrop")(224),
    [transforms.ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor "torchvision.transforms.ToTensor")(),
    [transforms.Normalize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize "torchvision.transforms.Normalize")(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),
])

[img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = Image.open(requests.get("https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png", stream=True).raw)
[img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [transform](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose "torchvision.transforms.Compose")([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))[None,]
[out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax "torch.argmax")([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").item()) 
```

```py
2.2.0+cu121
Downloading: "https://github.com/facebookresearch/deit/zipball/main" to /var/lib/jenkins/.cache/torch/hub/main.zip
/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:63: UserWarning:

Overwriting deit_tiny_patch16_224 in registry with models.deit_tiny_patch16_224\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.

/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:78: UserWarning:

Overwriting deit_small_patch16_224 in registry with models.deit_small_patch16_224\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.

/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:93: UserWarning:

Overwriting deit_base_patch16_224 in registry with models.deit_base_patch16_224\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.

/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:108: UserWarning:

Overwriting deit_tiny_distilled_patch16_224 in registry with models.deit_tiny_distilled_patch16_224\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.

/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:123: UserWarning:

Overwriting deit_small_distilled_patch16_224 in registry with models.deit_small_distilled_patch16_224\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.

/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:138: UserWarning:

Overwriting deit_base_distilled_patch16_224 in registry with models.deit_base_distilled_patch16_224\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.

/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:153: UserWarning:

Overwriting deit_base_patch16_384 in registry with models.deit_base_patch16_384\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.

/var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main/models.py:168: UserWarning:

Overwriting deit_base_distilled_patch16_384 in registry with models.deit_base_distilled_patch16_384\. This is because the name being registered conflicts with an existing name. Please check if this is not expected.

Downloading: "https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth" to /var/lib/jenkins/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth

  0%|          | 0.00/330M [00:00<?, ?B/s]
  4%|3         | 12.4M/330M [00:00<00:02, 130MB/s]
  7%|7         | 24.7M/330M [00:00<00:02, 110MB/s]
 11%|#1        | 36.8M/330M [00:00<00:02, 117MB/s]
 15%|#4        | 49.2M/330M [00:00<00:02, 121MB/s]
 19%|#8        | 62.2M/330M [00:00<00:02, 127MB/s]
 23%|##3       | 76.7M/330M [00:00<00:01, 135MB/s]
 27%|##7       | 90.6M/330M [00:00<00:01, 139MB/s]
 32%|###1      | 106M/330M [00:00<00:01, 144MB/s]
 36%|###6      | 119M/330M [00:00<00:01, 125MB/s]
 40%|###9      | 132M/330M [00:01<00:01, 122MB/s]
 45%|####4     | 147M/330M [00:01<00:01, 132MB/s]
 49%|####8     | 162M/330M [00:01<00:01, 138MB/s]
 53%|#####3    | 176M/330M [00:01<00:01, 142MB/s]
 58%|#####7    | 190M/330M [00:01<00:01, 144MB/s]
 62%|######2   | 205M/330M [00:01<00:00, 147MB/s]
 67%|######6   | 220M/330M [00:01<00:00, 149MB/s]
 71%|#######   | 234M/330M [00:01<00:00, 148MB/s]
 76%|#######5  | 250M/330M [00:01<00:00, 155MB/s]
 81%|########1 | 268M/330M [00:01<00:00, 162MB/s]
 86%|########6 | 285M/330M [00:02<00:00, 168MB/s]
 91%|#########1| 302M/330M [00:02<00:00, 172MB/s]
 97%|#########6| 319M/330M [00:02<00:00, 175MB/s]
100%|##########| 330M/330M [00:02<00:00, 147MB/s]
269 
```

输出应该是 269，根据 ImageNet 类索引到[标签文件](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)，对应`timber wolf, grey wolf, gray wolf, Canis lupus`。

现在我们已经验证了可以使用 DeiT 模型对图像进行分类，让我们看看如何修改模型以便在 iOS 和 Android 应用程序上运行。

## 脚本化 DeiT

要在移动设备上使用模型，我们首先需要对模型进行脚本化。查看[脚本化和优化配方](https://pytorch.org/tutorials/recipes/script_optimized.html)以获取快速概述。运行下面的代码将 DeiT 模型转换为 TorchScript 格式，以便在移动设备上运行。

```py
model = [torch.hub.load](https://pytorch.org/docs/stable/hub.html#torch.hub.load "torch.hub.load")('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)
[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval "torch.nn.Module.eval")()
scripted_model = [torch.jit.script](https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script "torch.jit.script")(model)
[scripted_model.save](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.save "torch.jit.ScriptModule.save")("fbdeit_scripted.pt") 
```

```py
Using cache found in /var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main 
```

生成的脚本模型文件`fbdeit_scripted.pt`大小约为 346MB。

## 量化 DeiT

为了显著减小训练模型的大小，同时保持推理准确性大致相同，可以对模型应用量化。由于 DeiT 中使用的 Transformer 模型，我们可以轻松地将动态量化应用于模型，因为动态量化最适用于 LSTM 和 Transformer 模型（有关更多详细信息，请参见[此处](https://pytorch.org/docs/stable/quantization.html?highlight=quantization#dynamic-quantization)）。

现在运行下面的代码：

```py
# Use 'x86' for server inference (the old 'fbgemm' is still available but 'x86' is the recommended default) and ``qnnpack`` for mobile inference.
backend = "x86" # replaced with ``qnnpack`` causing much worse inference speed for quantized model on this notebook
[model.qconfig](https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig "torch.ao.quantization.qconfig.QConfig") = torch.quantization.get_default_qconfig(backend)
torch.backends.quantized.engine = backend

quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={[torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")}, dtype=[torch.qint8](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype"))
scripted_quantized_model = [torch.jit.script](https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script "torch.jit.script")(quantized_model)
[scripted_quantized_model.save](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.save "torch.jit.ScriptModule.save")("fbdeit_scripted_quantized.pt") 
```

```py
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/ao/quantization/observer.py:220: UserWarning:

Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch. 
```

这将生成脚本化和量化版本的模型`fbdeit_quantized_scripted.pt`，大小约为 89MB，比 346MB 的非量化模型大小减少了 74％！

您可以使用`scripted_quantized_model`生成相同的推理结果：

```py
[out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = scripted_quantized_model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax "torch.argmax")([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").item())
# The same output 269 should be printed 
```

```py
269 
```

## 优化 DeiT

在将量化和脚本化模型应用于移动设备之前的最后一步是对其进行优化：

```py
from torch.utils.mobile_optimizer import [optimize_for_mobile](https://pytorch.org/docs/stable/mobile_optimizer.html#torch.utils.mobile_optimizer.optimize_for_mobile "torch.utils.mobile_optimizer.optimize_for_mobile")
optimized_scripted_quantized_model = [optimize_for_mobile](https://pytorch.org/docs/stable/mobile_optimizer.html#torch.utils.mobile_optimizer.optimize_for_mobile "torch.utils.mobile_optimizer.optimize_for_mobile")(scripted_quantized_model)
[optimized_scripted_quantized_model.save](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.save "torch.jit.ScriptModule.save")("fbdeit_optimized_scripted_quantized.pt") 
```

生成的`fbdeit_optimized_scripted_quantized.pt`文件的大小与量化、脚本化但非优化模型的大小大致相同。推理结果保持不变。

```py
[out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = optimized_scripted_quantized_model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax "torch.argmax")([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([clsidx](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").item())
# Again, the same output 269 should be printed 
```

```py
269 
```

## 使用 Lite 解释器

要查看 Lite 解释器可以导致多少模型大小减小和推理速度提升，请创建模型的精简版本。

```py
optimized_scripted_quantized_model._save_for_lite_interpreter("fbdeit_optimized_scripted_quantized_lite.ptl")
ptl = [torch.jit.load](https://pytorch.org/docs/stable/generated/torch.jit.load.html#torch.jit.load "torch.jit.load")("fbdeit_optimized_scripted_quantized_lite.ptl") 
```

尽管精简模型的大小与非精简版本相当，但在移动设备上运行精简版本时，预计会加快推理速度。

## 比较推理速度

要查看四个模型的推理速度差异 - 原始模型、脚本模型、量化和脚本模型、优化的量化和脚本模型 - 运行下面的代码：

```py
with [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile")(use_cuda=False) as [prof1](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile"):
    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
with [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile")(use_cuda=False) as [prof2](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile"):
    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = scripted_model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
with [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile")(use_cuda=False) as [prof3](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile"):
    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = scripted_quantized_model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
with [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile")(use_cuda=False) as [prof4](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile"):
    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = optimized_scripted_quantized_model([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
with [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile")(use_cuda=False) as [prof5](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile"):
    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ptl([img](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

print("original model: {:.2f}ms".format([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")/1000))
print("scripted model: {:.2f}ms".format([prof2.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")/1000))
print("scripted & quantized model: {:.2f}ms".format([prof3.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")/1000))
print("scripted & quantized & optimized model: {:.2f}ms".format([prof4.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")/1000))
print("lite model: {:.2f}ms".format([prof5.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")/1000)) 
```

```py
original model: 123.27ms
scripted model: 111.89ms
scripted & quantized model: 129.99ms
scripted & quantized & optimized model: 129.94ms
lite model: 120.00ms 
```

在 Google Colab 上运行的结果是：

```py
original  model:  1236.69ms
scripted  model:  1226.72ms
scripted  &  quantized  model:  593.19ms
scripted  &  quantized  &  optimized  model:  598.01ms
lite  model:  600.72ms 
```

以下结果总结了每个模型的推理时间以及相对于原始模型的每个模型的百分比减少。

```py
import pandas as pd
import numpy as np

df = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})
df = pd.concat([df, pd.DataFrame([
    ["{:.2f}ms".format([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")/1000), "0%"],
    ["{:.2f}ms".format([prof2.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")/1000),
     "{:.2f}%".format(([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")-[prof2.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total"))/[prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")*100)],
    ["{:.2f}ms".format([prof3.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")/1000),
     "{:.2f}%".format(([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")-[prof3.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total"))/[prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")*100)],
    ["{:.2f}ms".format([prof4.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")/1000),
     "{:.2f}%".format(([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")-[prof4.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total"))/[prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")*100)],
    ["{:.2f}ms".format([prof5.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")/1000),
     "{:.2f}%".format(([prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")-[prof5.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total"))/[prof1.self_cpu_time_total](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total "torch.autograd.profiler.profile.self_cpu_time_total")*100)]],
    columns=['Inference Time', 'Reduction'])], axis=1)

print(df)

"""
 Model                             Inference Time    Reduction
0   original model                             1236.69ms           0%
1   scripted model                             1226.72ms        0.81%
2   scripted & quantized model                  593.19ms       52.03%
3   scripted & quantized & optimized model      598.01ms       51.64%
4   lite model                                  600.72ms       51.43%
""" 
```

```py
 Model  ... Reduction
0                          original model  ...        0%
1                          scripted model  ...     9.23%
2              scripted & quantized model  ...    -5.45%
3  scripted & quantized & optimized model  ...    -5.41%
4                              lite model  ...     2.65%

[5 rows x 3 columns]

'\n        Model                             Inference Time    Reduction\n0\toriginal model                             1236.69ms           0%\n1\tscripted model                             1226.72ms        0.81%\n2\tscripted & quantized model                  593.19ms       52.03%\n3\tscripted & quantized & optimized model      598.01ms       51.64%\n4\tlite model                                  600.72ms       51.43%\n' 
```

### 了解更多

+   [Facebook 数据高效图像变换器](https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification)

+   [使用 ImageNet 和 MNIST 在 iOS 上的 Vision Transformer](https://github.com/pytorch/ios-demo-app/tree/master/ViT4MNIST)

+   [使用 ImageNet 和 MNIST 在 Android 上的 Vision Transformer](https://github.com/pytorch/android-demo-app/tree/master/ViT4MNIST)

**脚本的总运行时间：**（0 分钟 20.779 秒）

`下载 Python 源代码：vt_tutorial.py`

`下载 Jupyter 笔记本：vt_tutorial.ipynb`

[Sphinx-Gallery 生成的画廊](https://sphinx-gallery.github.io)
