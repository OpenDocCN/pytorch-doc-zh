# 快速入门

> 阅读原文：[https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)

注意

点击[这里](#sphx-glr-download-beginner-basics-quickstart-tutorial-py)下载完整示例代码

[学习基础知识](intro.html) || **快速入门** || [张量](tensorqs_tutorial.html) || [数据集和数据加载器](data_tutorial.html) || [转换](transforms_tutorial.html) || [构建模型](buildmodel_tutorial.html) || [自动求导](autogradqs_tutorial.html) || [优化](optimization_tutorial.html) || [保存和加载模型](saveloadrun_tutorial.html)

本节介绍了机器学习中常见任务的API。请参考每个部分中的链接以深入了解。

## 处理数据

PyTorch有两个[用于处理数据的基本方法](https://pytorch.org/docs/stable/data.html)：`torch.utils.data.DataLoader`和`torch.utils.data.Dataset`。`Dataset`存储样本及其对应的标签，而`DataLoader`将一个可迭代对象包装在`Dataset`周围。

```py
import torch
from torch import nn
from torch.utils.data import [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader")
from torchvision import datasets
from torchvision.transforms import [ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor "torchvision.transforms.ToTensor") 
```

PyTorch提供了领域特定的库，如[TorchText](https://pytorch.org/text/stable/index.html)、[TorchVision](https://pytorch.org/vision/stable/index.html)和[TorchAudio](https://pytorch.org/audio/stable/index.html)，其中包括数据集。在本教程中，我们将使用一个TorchVision数据集。

`torchvision.datasets`模块包含许多现实世界视觉数据的`Dataset`对象，如CIFAR、COCO（[完整列表在此](https://pytorch.org/vision/stable/datasets.html)）。在本教程中，我们使用FashionMNIST数据集。每个TorchVision `Dataset`都包括两个参数：`transform`和`target_transform`，分别用于修改样本和标签。

```py
# Download training data from open datasets.
[training_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST "torchvision.datasets.FashionMNIST") = [datasets.FashionMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST "torchvision.datasets.FashionMNIST")(
    root="data",
    train=True,
    download=True,
    transform=[ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor "torchvision.transforms.ToTensor")(),
)

# Download test data from open datasets.
[test_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST "torchvision.datasets.FashionMNIST") = [datasets.FashionMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST "torchvision.datasets.FashionMNIST")(
    root="data",
    train=False,
    download=True,
    transform=[ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor "torchvision.transforms.ToTensor")(),
) 
```

```py
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz

  0%|          | 0/26421880 [00:00<?, ?it/s]
  0%|          | 65536/26421880 [00:00<01:12, 362268.71it/s]
  1%|          | 229376/26421880 [00:00<00:38, 680481.08it/s]
  3%|3         | 819200/26421880 [00:00<00:13, 1853717.26it/s]
 11%|#1        | 3014656/26421880 [00:00<00:03, 7167253.78it/s]
 24%|##3       | 6258688/26421880 [00:00<00:01, 11757636.19it/s]
 42%|####1     | 11075584/26421880 [00:00<00:00, 20718315.26it/s]
 55%|#####4    | 14483456/26421880 [00:01<00:00, 20324854.10it/s]
 74%|#######4  | 19562496/26421880 [00:01<00:00, 27572084.42it/s]
 87%|########7 | 23068672/26421880 [00:01<00:00, 27527140.28it/s]
100%|#########9| 26312704/26421880 [00:01<00:00, 26297445.36it/s]
100%|##########| 26421880/26421880 [00:01<00:00, 18147607.68it/s]
Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz

  0%|          | 0/29515 [00:00<?, ?it/s]
100%|##########| 29515/29515 [00:00<00:00, 327172.52it/s]
Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz

  0%|          | 0/4422102 [00:00<?, ?it/s]
  1%|1         | 65536/4422102 [00:00<00:11, 363567.22it/s]
  5%|5         | 229376/4422102 [00:00<00:06, 694276.06it/s]
 19%|#9        | 851968/4422102 [00:00<00:01, 1962897.43it/s]
 64%|######3   | 2818048/4422102 [00:00<00:00, 5508389.41it/s]
100%|##########| 4422102/4422102 [00:00<00:00, 6087122.93it/s]
Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz

  0%|          | 0/5148 [00:00<?, ?it/s]
100%|##########| 5148/5148 [00:00<00:00, 36228652.67it/s]
Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw 
```

我们将`Dataset`作为参数传递给`DataLoader`。这会将一个可迭代对象包装在我们的数据集周围，并支持自动批处理、采样、洗牌和多进程数据加载。在这里，我们定义了一个批量大小为64，即数据加载器可迭代对象中的每个元素将返回一个包含64个特征和标签的批次。

```py
batch_size = 64

# Create data loaders.
[train_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader") = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader")([training_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST "torchvision.datasets.FashionMNIST"), batch_size=batch_size)
[test_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader") = [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader")([test_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST "torchvision.datasets.FashionMNIST"), batch_size=batch_size)

for [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), y in [test_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader"):
    print(f"Shape of X [N, C, H, W]: {[X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").shape}")
    print(f"Shape of y: {y.shape}  {y.dtype}")
    break 
```

```py
Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])
Shape of y: torch.Size([64]) torch.int64 
```

阅读更多关于[在PyTorch中加载数据](data_tutorial.html)。

* * *

## 创建模型

要在PyTorch中定义神经网络，我们创建一个从[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)继承的类。我们在`__init__`函数中定义网络的层，并在`forward`函数中指定数据如何通过网络传递。为了加速神经网络中的操作，我们将其移动到GPU或MPS（如果可用）。

```py
# Get cpu, gpu or mps device for training.
device = (
    "cuda"
    if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available "torch.cuda.is_available")()
    else "mps"
    if [torch.backends.mps.is_available](https://pytorch.org/docs/stable/backends.html#torch.backends.mps.is_available "torch.backends.mps.is_available")()
    else "cpu"
)
print(f"Using {device} device")

# Define model
class NeuralNetwork([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")):
    def __init__(self):
        super().__init__()
        self.flatten = [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten "torch.nn.Flatten")()
        self.linear_relu_stack = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential")(
            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(28*28, 512),
            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU "torch.nn.ReLU")(),
            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(512, 512),
            [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU "torch.nn.ReLU")(),
            [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(512, 10)
        )

    def forward(self, [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
        [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = self.flatten([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
        logits = self.linear_relu_stack([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
        return logits

model = [NeuralNetwork](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")().to(device)
print(model) 
```

```py
Using cuda device
NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
) 
```

阅读更多关于[在PyTorch中构建神经网络](buildmodel_tutorial.html)。

* * *

## 优化模型参数

要训练一个模型，我们需要一个[损失函数](https://pytorch.org/docs/stable/nn.html#loss-functions)和一个[优化器](https://pytorch.org/docs/stable/optim.html)。

```py
[loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss "torch.nn.CrossEntropyLoss") = [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss "torch.nn.CrossEntropyLoss")()
[optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD") = [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters "torch.nn.Module.parameters")(), lr=1e-3) 
```

在单个训练循环中，模型对训练数据集进行预测（以批量方式提供），并将预测错误反向传播以调整模型的参数。

```py
def train(dataloader, model, [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss "torch.nn.CrossEntropyLoss"), [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD")):
    size = len(dataloader.dataset)
    [model.train](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train "torch.nn.Module.train")()
    for batch, ([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), y) in enumerate(dataloader):
        [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), y = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").to(device), y.to(device)

        # Compute prediction error
        [pred](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = model([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
        loss = [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss "torch.nn.CrossEntropyLoss")([pred](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), y)

        # Backpropagation
        loss.backward()
        [optimizer.step](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.step "torch.optim.SGD.step")()
        [optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.zero_grad "torch.optim.SGD.zero_grad")()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
            print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]") 
```

我们还会检查模型在测试数据集上的表现，以确保它正在学习。

```py
def test(dataloader, model, [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss "torch.nn.CrossEntropyLoss")):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    [model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval "torch.nn.Module.eval")()
    test_loss, correct = 0, 0
    with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad "torch.no_grad")():
        for [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), y in dataloader:
            [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), y = [X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").to(device), y.to(device)
            [pred](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = model([X](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
            test_loss += [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss "torch.nn.CrossEntropyLoss")([pred](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), y).item()
            correct += ([pred](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").argmax(1) == y).type([torch.float](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype")).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}  \n") 
```

训练过程是在几个迭代（*epochs*）中进行的。在每个迭代中，模型学习参数以做出更好的预测。我们在每个迭代中打印模型的准确性和损失；我们希望看到准确性随着每个迭代的增加而增加，损失随着每个迭代的减少而减少。

```py
epochs = 5
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train([train_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader"), model, [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss "torch.nn.CrossEntropyLoss"), [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD"))
    test([test_dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader "torch.utils.data.DataLoader"), model, [loss_fn](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss "torch.nn.CrossEntropyLoss"))
print("Done!") 
```

```py
Epoch 1
-------------------------------
loss: 2.303494  [   64/60000]
loss: 2.294637  [ 6464/60000]
loss: 2.277102  [12864/60000]
loss: 2.269977  [19264/60000]
loss: 2.254235  [25664/60000]
loss: 2.237146  [32064/60000]
loss: 2.231055  [38464/60000]
loss: 2.205037  [44864/60000]
loss: 2.203240  [51264/60000]
loss: 2.170889  [57664/60000]
Test Error:
 Accuracy: 53.9%, Avg loss: 2.168588

Epoch 2
-------------------------------
loss: 2.177787  [   64/60000]
loss: 2.168083  [ 6464/60000]
loss: 2.114910  [12864/60000]
loss: 2.130412  [19264/60000]
loss: 2.087473  [25664/60000]
loss: 2.039670  [32064/60000]
loss: 2.054274  [38464/60000]
loss: 1.985457  [44864/60000]
loss: 1.996023  [51264/60000]
loss: 1.917241  [57664/60000]
Test Error:
 Accuracy: 60.2%, Avg loss: 1.920374

Epoch 3
-------------------------------
loss: 1.951705  [   64/60000]
loss: 1.919516  [ 6464/60000]
loss: 1.808730  [12864/60000]
loss: 1.846550  [19264/60000]
loss: 1.740618  [25664/60000]
loss: 1.698733  [32064/60000]
loss: 1.708889  [38464/60000]
loss: 1.614436  [44864/60000]
loss: 1.646475  [51264/60000]
loss: 1.524308  [57664/60000]
Test Error:
 Accuracy: 61.4%, Avg loss: 1.547092

Epoch 4
-------------------------------
loss: 1.612695  [   64/60000]
loss: 1.570870  [ 6464/60000]
loss: 1.424730  [12864/60000]
loss: 1.489542  [19264/60000]
loss: 1.367256  [25664/60000]
loss: 1.373464  [32064/60000]
loss: 1.376744  [38464/60000]
loss: 1.304962  [44864/60000]
loss: 1.347154  [51264/60000]
loss: 1.230661  [57664/60000]
Test Error:
 Accuracy: 62.7%, Avg loss: 1.260891

Epoch 5
-------------------------------
loss: 1.337803  [   64/60000]
loss: 1.313278  [ 6464/60000]
loss: 1.151837  [12864/60000]
loss: 1.252142  [19264/60000]
loss: 1.123048  [25664/60000]
loss: 1.159531  [32064/60000]
loss: 1.175011  [38464/60000]
loss: 1.115554  [44864/60000]
loss: 1.160974  [51264/60000]
loss: 1.062730  [57664/60000]
Test Error:
 Accuracy: 64.6%, Avg loss: 1.087374

Done! 
```

阅读更多关于[训练模型](optimization_tutorial.html)。

* * *

## 保存模型

保存模型的常见方法是序列化内部状态字典（包含模型参数）。

```py
[torch.save](https://pytorch.org/docs/stable/generated/torch.save.html#torch.save "torch.save")([model.state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict "torch.nn.Module.state_dict")(), "model.pth")
print("Saved PyTorch Model State to model.pth") 
```

```py
Saved PyTorch Model State to model.pth 
```

## 加载模型

加载模型的过程包括重新创建模型结构并将状态字典加载到其中。

```py
model = [NeuralNetwork](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")().to(device)
[model.load_state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict "torch.nn.Module.load_state_dict")([torch.load](https://pytorch.org/docs/stable/generated/torch.load.html#torch.load "torch.load")("model.pth")) 
```

```py
<All keys matched successfully> 
```

现在可以使用这个模型进行预测了。

```py
classes = [
    "T-shirt/top",
    "Trouser",
    "Pullover",
    "Dress",
    "Coat",
    "Sandal",
    "Shirt",
    "Sneaker",
    "Bag",
    "Ankle boot",
]

[model.eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval "torch.nn.Module.eval")()
[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), y = [test_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST "torchvision.datasets.FashionMNIST")[0][0], [test_data](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST "torchvision.datasets.FashionMNIST")[0][1]
with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad "torch.no_grad")():
    [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").to(device)
    [pred](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = model([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
    predicted, actual = classes[[pred](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[0].argmax(0)], classes[y]
    print(f'Predicted: "{predicted}", Actual: "{actual}"') 
```

```py
Predicted: "Ankle boot", Actual: "Ankle boot" 
```

阅读更多关于[保存和加载模型](saveloadrun_tutorial.html)。

**脚本的总运行时间：**（0分钟58.630秒）

[`下载Python源代码：quickstart_tutorial.py`](../../_downloads/51f1e1167acc0fda8f9d8fd8597ee626/quickstart_tutorial.py)

[`下载Jupyter笔记本：quickstart_tutorial.ipynb`](../../_downloads/af0caf6d7af0dda755f4c9d7af9ccc2c/quickstart_tutorial.ipynb)

[Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)
