# PyTorch中的分布式数据并行 - 视频教程

> 原文：[https://pytorch.org/tutorials/beginner/ddp_series_intro.html](https://pytorch.org/tutorials/beginner/ddp_series_intro.html)

**介绍** || [什么是DDP](ddp_series_theory.html) || [单节点多GPU训练](ddp_series_multigpu.html) || [容错性](ddp_series_fault_tolerance.html) || [多节点训练](../intermediate/ddp_series_multinode.html) || [minGPT训练](../intermediate/ddp_series_minGPT.html)

作者：[Suraj Subramanian](https://github.com/suraj813)

跟随下面的视频或在[youtube](https://www.youtube.com/watch/-K3bZYHYHEA)上观看。

[https://www.youtube.com/embed/-K3bZYHYHEA](https://www.youtube.com/embed/-K3bZYHYHEA)

这一系列视频教程将带您了解通过DDP在PyTorch中进行分布式训练。

该系列从简单的非分布式训练作业开始，最终部署到集群中的多台机器上进行训练。在此过程中，您还将了解到关于[torchrun](https://pytorch.org/docs/stable/elastic/run.html)用于容错分布式训练。

本教程假定您对PyTorch中的模型训练有基本的了解。

## 运行代码

您需要多个CUDA GPU来运行教程代码。通常可以在具有多个GPU的云实例上完成此操作（教程使用具有4个GPU的Amazon EC2 P3实例）。

教程代码托管在这个[github仓库](https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-series)。克隆该仓库并跟随教程！

## 教程部分

1.  介绍（本页）

1.  [DDP是什么？](ddp_series_theory.html) 温和地介绍了DDP在幕后的工作

1.  [单节点多GPU训练](ddp_series_multigpu.html) 在单台机器上使用多个GPU训练模型

1.  [容错分布式训练](ddp_series_fault_tolerance.html) 使用torchrun使您的分布式训练工作更加稳健

1.  [多节点训练](../intermediate/ddp_series_multinode.html) 使用多台机器上的多个GPU进行模型训练

1.  [使用DDP训练GPT模型](../intermediate/ddp_series_minGPT.html) 使用DDP训练[minGPT](https://github.com/karpathy/minGPT)模型的“真实世界”示例
