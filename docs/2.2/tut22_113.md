# PyTorch 中的分布式数据并行 - 视频教程

> 原文：[`pytorch.org/tutorials/beginner/ddp_series_intro.html`](https://pytorch.org/tutorials/beginner/ddp_series_intro.html)

**介绍** || 什么是 DDP || 单节点多 GPU 训练 || 容错性 || 多节点训练 || minGPT 训练

作者：[Suraj Subramanian](https://github.com/suraj813)

跟随下面的视频或在[youtube](https://www.youtube.com/watch/-K3bZYHYHEA)上观看。

[`www.youtube.com/embed/-K3bZYHYHEA`](https://www.youtube.com/embed/-K3bZYHYHEA)

这一系列视频教程将带您了解通过 DDP 在 PyTorch 中进行分布式训练。

该系列从简单的非分布式训练作业开始，最终部署到集群中的多台机器上进行训练。在此过程中，您还将了解到关于[torchrun](https://pytorch.org/docs/stable/elastic/run.html)用于容错分布式训练。

本教程假定您对 PyTorch 中的模型训练有基本的了解。

## 运行代码

您需要多个 CUDA GPU 来运行教程代码。通常可以在具有多个 GPU 的云实例上完成此操作（教程使用具有 4 个 GPU 的 Amazon EC2 P3 实例）。

教程代码托管在这个[github 仓库](https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-series)。克隆该仓库并跟随教程！

## 教程部分

1.  介绍（本页）

1.  DDP 是什么？ 温和地介绍了 DDP 在幕后的工作

1.  单节点多 GPU 训练 在单台机器上使用多个 GPU 训练模型

1.  容错分布式训练 使用 torchrun 使您的分布式训练工作更加稳健

1.  多节点训练 使用多台机器上的多个 GPU 进行模型训练

1.  使用 DDP 训练 GPT 模型 使用 DDP 训练[minGPT](https://github.com/karpathy/minGPT)模型的“真实世界”示例
