# TorchRec简介

> 原文：[https://pytorch.org/tutorials/intermediate/torchrec_tutorial.html](https://pytorch.org/tutorials/intermediate/torchrec_tutorial.html)

提示

为了充分利用本教程，我们建议使用这个[Colab版本](https://colab.research.google.com/github/pytorch/torchrec/blob/main/Torchrec_Introduction.ipynb)。这将使您能够尝试下面提供的信息。

请跟随下面的视频或在[youtube](https://www.youtube.com/watch?v=cjgj41dvSeQ)上观看。

[https://www.youtube.com/embed/cjgj41dvSeQ](https://www.youtube.com/embed/cjgj41dvSeQ)

在构建推荐系统时，我们经常希望用嵌入来表示产品或页面等实体。例如，参见Meta AI的[深度学习推荐模型](https://arxiv.org/abs/1906.00091)，或DLRM。随着实体数量的增长，嵌入表的大小可能超过单个GPU的内存。一种常见做法是将嵌入表分片到不同设备上，这是一种模型并行的类型。为此，TorchRec引入了其主要API称为[`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel)，或DMP。与PyTorch的DistributedDataParallel类似，DMP包装了一个模型以实现分布式训练。

## 安装

要求：python >= 3.7

在使用TorchRec时，我们强烈建议使用CUDA（如果使用CUDA：cuda >= 11.0）。

```py
# install pytorch with cudatoolkit 11.3
conda  install  pytorch  cudatoolkit=11.3  -c  pytorch-nightly  -y
# install TorchRec
pip3  install  torchrec-nightly 
```

## 概述

本教程将涵盖TorchRec的三个部分：`nn.module` [`EmbeddingBagCollection`](https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection)，[`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel) API和数据结构[`KeyedJaggedTensor`](https://pytorch.org/torchrec/torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor)。

### 分布式设置

我们使用torch.distributed设置我们的环境。有关分布式的更多信息，请参见此[tutorial](https://pytorch.org/tutorials/beginner/dist_overview.html)。

在这里，我们使用一个rank（colab进程）对应于我们的1个colab GPU。

```py
import os
import torch
import torchrec
import torch.distributed as dist

os.environ["RANK"] = "0"
os.environ["WORLD_SIZE"] = "1"
os.environ["MASTER_ADDR"] = "localhost"
os.environ["MASTER_PORT"] = "29500"

# Note - you will need a V100 or A100 to run tutorial as as!
# If using an older GPU (such as colab free K80),
# you will need to compile fbgemm with the appripriate CUDA architecture
# or run with "gloo" on CPUs
dist.init_process_group(backend="nccl") 
```

### 从EmbeddingBag到EmbeddingBagCollection

PyTorch通过[`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)和[`torch.nn.EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html)来表示嵌入。EmbeddingBag是Embedding的池化版本。

TorchRec通过创建嵌入的集合来扩展这些模块。我们将使用[`EmbeddingBagCollection`](https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection)来表示一组EmbeddingBags。

在这里，我们创建了一个包含两个嵌入包的EmbeddingBagCollection（EBC）。每个表，`product_table`和`user_table`，由大小为4096的64维嵌入表示。请注意，我们最初将EBC分配到设备“meta”。这将告诉EBC暂时不要分配内存。

```py
ebc = torchrec.EmbeddingBagCollection(
    device="meta",
    tables=[
        torchrec.EmbeddingBagConfig(
            name="product_table",
            embedding_dim=64,
            num_embeddings=4096,
            feature_names=["product"],
            pooling=torchrec.PoolingType.SUM,
        ),
        torchrec.EmbeddingBagConfig(
            name="user_table",
            embedding_dim=64,
            num_embeddings=4096,
            feature_names=["user"],
            pooling=torchrec.PoolingType.SUM,
        )
    ]
) 
```

### DistributedModelParallel

现在，我们准备用[`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel) (DMP)包装我们的模型。实例化DMP将：

1.  决定如何分片模型。DMP将收集可用的“分片器”并提出一种最佳方式来分片嵌入表（即EmbeddingBagCollection）的“计划”。

1.  实际分片模型。这包括为每个嵌入表在适当设备上分配内存。

在这个示例中，由于我们有两个EmbeddingTables和一个GPU，TorchRec将两者都放在单个GPU上。

```py
model = torchrec.distributed.DistributedModelParallel(ebc, device=torch.device("cuda"))
print(model)
print(model.plan) 
```

### 使用输入和偏移查询普通的nn.EmbeddingBag

我们使用`input`和`offsets`查询[`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)和[`nn.EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html)。Input是包含查找值的1-D张量。Offsets是一个1-D张量，其中序列是每个示例要汇总的值的累积和。

让我们看一个例子，重新创建上面的产品EmbeddingBag：

```py
|------------|
| product ID |
|------------|
| [101, 202] |
| []         |
| [303]      |
|------------| 
```

```py
product_eb = torch.nn.EmbeddingBag(4096, 64)
product_eb(input=torch.tensor([101, 202, 303]), offsets=torch.tensor([0, 2, 2])) 
```

### 使用KeyedJaggedTensor表示小批量

我们需要一个有效的表示，每个示例的每个特征中有任意数量的实体ID的多个示例。为了实现这种“不规则”表示，我们使用TorchRec数据结构[`KeyedJaggedTensor`](https://pytorch.org/torchrec/torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor)（KJT）。

让我们看看如何查找两个嵌入包“product”和“user”的集合。假设小批量由三个用户的三个示例组成。第一个示例有两个产品ID，第二个没有，第三个有一个产品ID。

```py
|------------|------------|
| product ID | user ID    |
|------------|------------|
| [101, 202] | [404]      |
| []         | [505]      |
| [303]      | [606]      |
|------------|------------| 
```

查询应该是：

```py
mb = torchrec.KeyedJaggedTensor(
    keys = ["product", "user"],
    values = torch.tensor([101, 202, 303, 404, 505, 606]).cuda(),
    lengths = torch.tensor([2, 0, 1, 1, 1, 1], dtype=torch.int64).cuda(),
)

print(mb.to(torch.device("cpu"))) 
```

请注意，KJT批量大小为`batch_size = len(lengths)//len(keys)`。在上面的例子中，batch_size为3。

### 将所有内容整合在一起，使用KJT小批量查询我们的分布式模型

最后，我们可以使用我们的产品和用户的小批量查询我们的模型。

结果查找将包含一个KeyedTensor，其中每个键（或特征）包含一个大小为3x64（batch_size x embedding_dim）的2D张量。

```py
pooled_embeddings = model(mb)
print(pooled_embeddings) 
```

## 更多资源

有关更多信息，请参阅我们的[dlrm](https://github.com/pytorch/torchrec/tree/main/examples/dlrm)示例，其中包括在criteo terabyte数据集上进行多节点训练，使用Meta的[DLRM](https://arxiv.org/abs/1906.00091)。
