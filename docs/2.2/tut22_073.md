# （beta）在FX中构建一个卷积/批量归一化融合器

> 原文：[https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html](https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html)

注意

点击[这里](#sphx-glr-download-intermediate-fx-conv-bn-fuser-py)下载完整示例代码

**作者**：[Horace He](https://github.com/chillee)

在本教程中，我们将使用FX，一个用于PyTorch可组合函数转换的工具包，执行以下操作：

1.  在数据依赖关系中查找卷积/批量归一化的模式。

1.  对于在1)中找到的模式，将批量归一化统计数据折叠到卷积权重中。

请注意，此优化仅适用于处于推理模式的模型（即mode.eval()）

我们将构建存在于此处的融合器：[https://github.com/pytorch/pytorch/blob/orig/release/1.8/torch/fx/experimental/fuser.py](https://github.com/pytorch/pytorch/blob/orig/release/1.8/torch/fx/experimental/fuser.py)

首先，让我们导入一些模块（我们稍后将在代码中使用所有这些）。

```py
from typing import Type, Dict, Any, Tuple, Iterable
import copy
import torch.fx as fx
import torch
import torch.nn as nn 
```

对于本教程，我们将创建一个由卷积和批量归一化组成的模型。请注意，这个模型有一些棘手的组件 - 一些卷积/批量归一化模式隐藏在Sequential中，一个`BatchNorms`被包装在另一个模块中。

```py
class WrappedBatchNorm([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")):
    def __init__(self):
        super().__init__()
        self.mod = [nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d "torch.nn.BatchNorm2d")(1)
    def forward(self, x):
        return self.mod(x)

class M([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")):
    def __init__(self):
        super().__init__()
        self.conv1 = [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d "torch.nn.Conv2d")(1, 1, 1)
        self.bn1 = [nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d "torch.nn.BatchNorm2d")(1)
        self.conv2 = [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d "torch.nn.Conv2d")(1, 1, 1)
        self.nested = [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential "torch.nn.Sequential")(
            [nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d "torch.nn.BatchNorm2d")(1),
            [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d "torch.nn.Conv2d")(1, 1, 1),
        )
        self.wrapped = WrappedBatchNorm()

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.conv2(x)
        x = self.nested(x)
        x = self.wrapped(x)
        return x

model = M()

model.eval() 
```

## 融合卷积与批量归一化

尝试在PyTorch中自动融合卷积和批量归一化的主要挑战之一是PyTorch没有提供一种轻松访问计算图的方法。FX通过符号跟踪实际调用的操作来解决这个问题，这样我们就可以通过前向调用、嵌套在Sequential模块中或包装在用户定义模块中来跟踪计算。

```py
traced_model = [torch.fx.symbolic_trace](https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace "torch.fx.symbolic_trace")(model)
print(traced_model.graph) 
```

这给我们提供了模型的图形表示。请注意，顺序内部的模块以及包装的模块都已内联到图中。这是默认的抽象级别，但可以由通道编写者配置。更多信息请参阅FX概述[https://pytorch.org/docs/master/fx.html#module-torch.fx](https://pytorch.org/docs/master/fx.html#module-torch.fx)

## 融合卷积与批量归一化

与其他一些融合不同，卷积与批量归一化的融合不需要任何新的运算符。相反，在推理期间，批量归一化由逐点加法和乘法组成，这些操作可以“烘烤”到前面卷积的权重中。这使我们能够完全从我们的模型中删除批量归一化！阅读[https://nenadmarkus.com/p/fusing-batchnorm-and-conv/](https://nenadmarkus.com/p/fusing-batchnorm-and-conv/)获取更多详细信息。这里的代码是从[https://github.com/pytorch/pytorch/blob/orig/release/1.8/torch/nn/utils/fusion.py](https://github.com/pytorch/pytorch/blob/orig/release/1.8/torch/nn/utils/fusion.py)复制的，以便更清晰。

```py
def fuse_conv_bn_eval(conv, bn):
  """
 Given a conv Module `A` and an batch_norm module `B`, returns a conv
 module `C` such that C(x) == B(A(x)) in inference mode.
 """
    assert(not (conv.training or bn.training)), "Fusion only for eval!"
    fused_conv = copy.deepcopy(conv)

    fused_conv.weight, fused_conv.bias = \
        fuse_conv_bn_weights(fused_conv.weight, fused_conv.bias,
                             bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)

    return fused_conv

def fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):
    if conv_b is None:
        conv_b = [torch.zeros_like](https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like "torch.zeros_like")(bn_rm)
    if bn_w is None:
        bn_w = [torch.ones_like](https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like "torch.ones_like")(bn_rm)
    if bn_b is None:
        bn_b = [torch.zeros_like](https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like "torch.zeros_like")(bn_rm)
    bn_var_rsqrt = [torch.rsqrt](https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt "torch.rsqrt")(bn_rv + bn_eps)

    conv_w = conv_w * (bn_w * bn_var_rsqrt).reshape([-1] + [1] * (len(conv_w.shape) - 1))
    conv_b = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b

    return torch.nn.Parameter(conv_w), torch.nn.Parameter(conv_b) 
```

## FX Fusion Pass

现在我们有了我们的计算图以及融合卷积和批量归一化的方法，剩下的就是迭代FX图并应用所需的融合。

```py
def _parent_name(target : str) -> Tuple[str, str]:
  """
 Splits a ``qualname`` into parent path and last atom.
 For example, `foo.bar.baz` -> (`foo.bar`, `baz`)
 """
    *parent, name = target.rsplit('.', 1)
    return parent[0] if parent else '', name

def replace_node_module(node: [fx.Node](https://pytorch.org/docs/stable/fx.html#torch.fx.Node "torch.fx.Node"), modules: Dict[str, Any], new_module: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")):
    assert(isinstance(node.target, str))
    parent_name, name = _parent_name(node.target)
    setattr(modules[parent_name], name, new_module)

def fuse(model: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")) -> [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module"):
    model = copy.deepcopy(model)
    # The first step of most FX passes is to symbolically trace our model to
    # obtain a `GraphModule`. This is a representation of our original model
    # that is functionally identical to our original model, except that we now
    # also have a graph representation of our forward pass.
    fx_model: [fx.GraphModule](https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule "torch.fx.GraphModule") = [fx.symbolic_trace](https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace "torch.fx.symbolic_trace")(model)
    modules = dict(fx_model.named_modules())

    # The primary representation for working with FX are the `Graph` and the
    # `Node`. Each `GraphModule` has a `Graph` associated with it - this
    # `Graph` is also what generates `GraphModule.code`.
    # The `Graph` itself is represented as a list of `Node` objects. Thus, to
    # iterate through all of the operations in our graph, we iterate over each
    # `Node` in our `Graph`.
    for node in fx_model.graph.nodes:
        # The FX IR contains several types of nodes, which generally represent
        # call sites to modules, functions, or methods. The type of node is
        # determined by `Node.op`.
        if node.op != 'call_module': # If our current node isn't calling a Module then we can ignore it.
            continue
        # For call sites, `Node.target` represents the module/function/method
        # that's being called. Here, we check `Node.target` to see if it's a
        # batch norm module, and then check `Node.args[0].target` to see if the
        # input `Node` is a convolution.
        if type(modules[node.target]) is [nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d "torch.nn.BatchNorm2d") and type(modules[node.args[0].target]) is [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d "torch.nn.Conv2d"):
            if len(node.args[0].users) > 1:  # Output of conv is used by other nodes
                continue
            conv = modules[node.args[0].target]
            bn = modules[node.target]
            fused_conv = fuse_conv_bn_eval(conv, bn)
            replace_node_module(node.args[0], modules, fused_conv)
            # As we've folded the batch nor into the conv, we need to replace all uses
            # of the batch norm with the conv.
            node.replace_all_uses_with(node.args[0])
            # Now that all uses of the batch norm have been replaced, we can
            # safely remove the batch norm.
            fx_model.graph.erase_node(node)
    fx_model.graph.lint()
    # After we've modified our graph, we need to recompile our graph in order
    # to keep the generated code in sync.
    fx_model.recompile()
    return fx_model 
```

注意

为了演示目的，我们在这里进行了一些简化，比如只匹配2D卷积。查看[https://github.com/pytorch/pytorch/blob/master/torch/fx/experimental/fuser.py](https://github.com/pytorch/pytorch/blob/master/torch/fx/experimental/fuser.py)以获取更可用的通道。

## 测试我们的融合通道

现在我们可以在初始的玩具模型上运行这个融合通道，并验证我们的结果是相同的。此外，我们可以打印出我们融合模型的代码，并验证是否还有批量归一化。

```py
fused_model = fuse(model)
print(fused_model.code)
inp = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(5, 1, 1, 1)
[torch.testing.assert_allclose](https://pytorch.org/docs/stable/testing.html#torch.testing.assert_allclose "torch.testing.assert_allclose")(fused_model(inp), model(inp)) 
```

## 在ResNet18上对我们的融合进行基准测试

我们可以在像ResNet18这样的较大模型上测试我们的融合通道，看看这个通道如何提高推理性能。

```py
import torchvision.models as models
import time

rn18 = [models.resnet18](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18 "torchvision.models.resnet18")()
rn18.eval()

inp = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 3, 224, 224)
output = rn18(inp)

def benchmark(model, iters=20):
    for _ in range(10):
        model(inp)
    begin = time.time()
    for _ in range(iters):
        model(inp)
    return str(time.time()-begin)

fused_rn18 = fuse(rn18)
print("Unfused time: ", benchmark(rn18))
print("Fused time: ", benchmark(fused_rn18)) 
```

正如我们之前看到的，我们的FX转换的输出是（“torchscriptable”）PyTorch代码，我们可以轻松地`jit.script`输出，尝试进一步提高性能。通过这种方式，我们的FX模型转换与TorchScript组合在一起，没有任何问题。

```py
jit_rn18 = [torch.jit.script](https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script "torch.jit.script")(fused_rn18)
print("jit time: ", benchmark(jit_rn18))

############
# Conclusion
# ----------
# As we can see, using FX we can easily write static graph transformations on
# PyTorch code.
#
# Since FX is still in beta, we would be happy to hear any
# feedback you have about using it. Please feel free to use the
# PyTorch Forums (https://discuss.pytorch.org/) and the issue tracker
# (https://github.com/pytorch/pytorch/issues) to provide any feedback
# you might have. 
```

脚本的总运行时间：（0分钟0.000秒）

[`下载Python源代码：fx_conv_bn_fuser.py`](../_downloads/a8a58591f09624693bd748be066141fd/fx_conv_bn_fuser.py)

[`下载Jupyter笔记本：fx_conv_bn_fuser.ipynb`](../_downloads/a22e5922e71fe39ad848a64968a5570e/fx_conv_bn_fuser.ipynb)

[Sphinx-Gallery生成的画廊](https://sphinx-gallery.github.io)
