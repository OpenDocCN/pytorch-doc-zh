# 模型集成

> [`pytorch.org/tutorials/intermediate/ensembling.html`](https://pytorch.org/tutorials/intermediate/ensembling.html)

注意

点击这里下载完整的示例代码

这个教程演示了如何使用`torch.vmap`来对模型集合进行向量化。

## 什么是模型集成？

模型集成将多个模型的预测组合在一起。传统上，这是通过分别在一些输入上运行每个模型，然后组合预测来完成的。然而，如果您正在运行具有相同架构的模型，则可能可以使用`torch.vmap`将它们组合在一起。`vmap`是一个函数变换，它将函数映射到输入张量的维度。它的一个用例是通过向量化消除 for 循环并加速它们。

让我们演示如何使用简单 MLP 的集成来做到这一点。

注意

这个教程需要 PyTorch 2.0.0 或更高版本。

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
torch.manual_seed(0)

# Here's a simple MLP
class SimpleMLP(nn.Module):
    def __init__(self):
        super(SimpleMLP, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.flatten(1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        return x 
```

让我们生成一批虚拟数据，并假装我们正在处理一个 MNIST 数据集。因此，虚拟图像是 28x28，我们有一个大小为 64 的小批量。此外，假设我们想要将来自 10 个不同模型的预测组合起来。

```py
device = 'cuda'
num_models = 10

data = torch.randn(100, 64, 1, 28, 28, device=device)
targets = torch.randint(10, (6400,), device=device)

models = [SimpleMLP().to(device) for _ in range(num_models)] 
```

我们有几种选项来生成预测。也许我们想给每个模型一个不同的随机小批量数据。或者，也许我们想通过每个模型运行相同的小批量数据（例如，如果我们正在测试不同模型初始化的效果）。

选项 1：为每个模型使用不同的小批量

```py
minibatches = data[:num_models]
predictions_diff_minibatch_loop = [model(minibatch) for model, minibatch in zip(models, minibatches)] 
```

选项 2：相同的小批量

```py
minibatch = data[0]
predictions2 = [model(minibatch) for model in models] 
```

## 使用`vmap`来对集合进行向量化

让我们使用`vmap`来加速 for 循环。我们必须首先准备好模型以便与`vmap`一起使用。

首先，让我们通过堆叠每个参数来将模型的状态组合在一起。例如，`model[i].fc1.weight`的形状是`[784, 128]`；我们将堆叠这 10 个模型的`.fc1.weight`以产生形状为`[10, 784, 128]`的大权重。

PyTorch 提供了`torch.func.stack_module_state`便利函数来执行此操作。

```py
from torch.func import stack_module_state

params, buffers = stack_module_state(models) 
```

接下来，我们需要定义一个要在上面`vmap`的函数。给定参数和缓冲区以及输入，该函数应该使用这些参数、缓冲区和输入来运行模型。我们将使用`torch.func.functional_call`来帮助：

```py
from torch.func import functional_call
import copy

# Construct a "stateless" version of one of the models. It is "stateless" in
# the sense that the parameters are meta Tensors and do not have storage.
base_model = copy.deepcopy(models[0])
base_model = base_model.to('meta')

def fmodel(params, buffers, x):
    return functional_call(base_model, (params, buffers), (x,)) 
```

选项 1：为每个模型使用不同的小批量获取预测。

默认情况下，`vmap`将一个函数映射到传入函数的所有输入的第一个维度。在使用`stack_module_state`之后，每个`params`和缓冲区在前面都有一个大小为“num_models”的额外维度，小批量有一个大小为“num_models”的维度。

```py
print([p.size(0) for p in params.values()]) # show the leading 'num_models' dimension

assert minibatches.shape == (num_models, 64, 1, 28, 28) # verify minibatch has leading dimension of size 'num_models'

from torch import vmap

predictions1_vmap = vmap(fmodel)(params, buffers, minibatches)

# verify the ``vmap`` predictions match the
assert torch.allclose(predictions1_vmap, torch.stack(predictions_diff_minibatch_loop), atol=1e-3, rtol=1e-5) 
```

```py
[10, 10, 10, 10, 10, 10] 
```

选项 2：使用相同的小批量数据获取预测。

`vmap`有一个`in_dims`参数，指定要映射的维度。通过使用`None`，我们告诉`vmap`我们希望相同的小批量适用于所有 10 个模型。

```py
predictions2_vmap = vmap(fmodel, in_dims=(0, 0, None))(params, buffers, minibatch)

assert torch.allclose(predictions2_vmap, torch.stack(predictions2), atol=1e-3, rtol=1e-5) 
```

一个快速说明：关于哪些类型的函数可以被`vmap`转换存在一些限制。最适合转换的函数是纯函数：输出仅由没有副作用（例如突变）的输入决定的函数。`vmap`无法处理任意 Python 数据结构的突变，但它可以处理许多原地 PyTorch 操作。

## 性能

对性能数字感到好奇吗？这里是数字的表现。

```py
from torch.utils.benchmark import Timer
without_vmap = Timer(
    stmt="[model(minibatch) for model, minibatch in zip(models, minibatches)]",
    globals=globals())
with_vmap = Timer(
    stmt="vmap(fmodel)(params, buffers, minibatches)",
    globals=globals())
print(f'Predictions without vmap {without_vmap.timeit(100)}')
print(f'Predictions with vmap {with_vmap.timeit(100)}') 
```

```py
Predictions without vmap <torch.utils.benchmark.utils.common.Measurement object at 0x7f48efb85b40>
[model(minibatch) for model, minibatch in zip(models, minibatches)]
  2.26 ms
  1 measurement, 100 runs , 1 thread
Predictions with vmap <torch.utils.benchmark.utils.common.Measurement object at 0x7f48efb85ea0>
vmap(fmodel)(params, buffers, minibatches)
  791.58 us
  1 measurement, 100 runs , 1 thread 
```

使用`vmap`有很大的加速！

一般来说，使用`vmap`进行向量化应该比在 for 循环中运行函数更快，并且与手动批处理竞争。不过也有一些例外，比如如果我们没有为特定操作实现`vmap`规则，或者底层内核没有针对旧硬件（GPU）进行优化。如果您看到这些情况，请通过在 GitHub 上开启一个问题来告诉我们。

**脚本的总运行时间：**（0 分钟 0.798 秒）

`下载 Python 源代码：ensembling.py`

`下载 Jupyter 笔记本: ensembling.ipynb`

[Sphinx-Gallery 生成的画廊](https://sphinx-gallery.github.io)
