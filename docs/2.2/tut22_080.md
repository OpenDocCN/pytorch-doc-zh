# 每个样本的梯度

> [`pytorch.org/tutorials/intermediate/per_sample_grads.html`](https://pytorch.org/tutorials/intermediate/per_sample_grads.html)

注意

点击这里下载完整示例代码

## 它是什么？

每个样本梯度计算是计算批量数据中每个样本的梯度。在差分隐私、元学习和优化研究中，这是一个有用的量。

注意

本教程需要 PyTorch 2.0.0 或更高版本。

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
[torch.manual_seed](https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed "torch.manual_seed")(0)

# Here's a simple CNN and loss function:

class SimpleCNN([nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")):
    def __init__(self):
        super([SimpleCNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module"), self).__init__()
        self.conv1 = [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d "torch.nn.Conv2d")(1, 32, 3, 1)
        self.conv2 = [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d "torch.nn.Conv2d")(32, 64, 3, 1)
        self.fc1 = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(9216, 128)
        self.fc2 = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = [F.relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu "torch.nn.functional.relu")(x)
        x = self.conv2(x)
        x = [F.relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu "torch.nn.functional.relu")(x)
        x = [F.max_pool2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d "torch.nn.functional.max_pool2d")(x, 2)
        x = [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten "torch.flatten")(x, 1)
        x = self.fc1(x)
        x = [F.relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu "torch.nn.functional.relu")(x)
        x = self.fc2(x)
        output = [F.log_softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax "torch.nn.functional.log_softmax")(x, dim=1)
        output = x
        return output

def loss_fn([predictions](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [targets](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
    return [F.nll_loss](https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss "torch.nn.functional.nll_loss")([predictions](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [targets](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

让我们生成一批虚拟数据，并假装我们正在处理一个 MNIST 数据集。虚拟图像是 28x28，我们使用大小为 64 的小批量。

```py
device = 'cuda'

num_models = 10
batch_size = 64
[data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(batch_size, 1, 28, 28, device=device)

[targets](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randint](https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint "torch.randint")(10, (64,), device=device) 
```

在常规模型训练中，人们会将小批量数据通过模型前向传播，然后调用 .backward() 来计算梯度。这将生成整个小批量的‘平均’梯度：

```py
model = [SimpleCNN](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")().to(device=device)
[predictions](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = model([data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))  # move the entire mini-batch through the model

[loss](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = loss_fn([predictions](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [targets](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[loss.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward "torch.Tensor.backward")()  # back propagate the 'average' gradient of this mini-batch 
```

与上述方法相反，每个样本梯度计算等同于：

+   对于数据的每个单独样本，执行前向和后向传递以获得单个（每个样本）梯度。

```py
def compute_grad(sample, target):
    sample = sample.unsqueeze(0)  # prepend batch dimension for processing
    target = target.unsqueeze(0)

    prediction = model(sample)
    [loss](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = loss_fn(prediction, target)

    return [torch.autograd.grad](https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad "torch.autograd.grad")([loss](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), list([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters "torch.nn.Module.parameters")()))

def compute_sample_grads([data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [targets](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
  """ manually process each sample with per sample gradient """
    sample_grads = [compute_grad([data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[i], [targets](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[i]) for i in range(batch_size)]
    sample_grads = zip(*sample_grads)
    sample_grads = [[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html#torch.stack "torch.stack")(shards) for shards in sample_grads]
    return sample_grads

per_sample_grads = compute_sample_grads([data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [targets](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

`sample_grads[0]` 是模型 `conv1.weight` 的每个样本梯度。`model.conv1.weight.shape` 是 `[32, 1, 3, 3]`；注意每个样本在批处理中有一个梯度，总共有 64 个。

```py
print(per_sample_grads[0].shape) 
```

```py
torch.Size([64, 32, 1, 3, 3]) 
```

## 每个样本梯度，*高效的方式*，使用函数转换

我们可以通过使用函数转换来高效地计算每个样本的梯度。

`torch.func` 函数转换 API 对函数进行转换。我们的策略是定义一个计算损失的函数，然后应用转换来构建一个计算每个样本梯度的函数。

我们将使用 `torch.func.functional_call` 函数来将 `nn.Module` 视为一个函数。

首先，让我们从 `model` 中提取状态到两个字典中，parameters 和 buffers。我们将对它们进行分离，因为我们不会使用常规的 PyTorch autograd（例如 Tensor.backward()，torch.autograd.grad）。

```py
from torch.func import [functional_call](https://pytorch.org/docs/stable/generated/torch.func.functional_call.html#torch.func.functional_call "torch.func.functional_call"), [vmap](https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap "torch.vmap"), [grad](https://pytorch.org/docs/stable/generated/torch.func.grad.html#torch.func.grad "torch.func.grad")

params = {k: v.detach() for k, v in [model.named_parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters "torch.nn.Module.named_parameters")()}
buffers = {k: v.detach() for k, v in [model.named_buffers](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_buffers "torch.nn.Module.named_buffers")()} 
```

接下来，让我们定义一个函数来计算模型给定单个输入而不是一批输入的损失。这个函数接受参数、输入和目标是很重要的，因为我们将对它们进行转换。

注意 - 因为模型最初是为处理批量而编写的，我们将使用 `torch.unsqueeze` 来添加一个批处理维度。

```py
def compute_loss(params, buffers, sample, target):
    batch = sample.unsqueeze(0)
    [targets](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = target.unsqueeze(0)

    [predictions](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [functional_call](https://pytorch.org/docs/stable/generated/torch.func.functional_call.html#torch.func.functional_call "torch.func.functional_call")(model, (params, buffers), (batch,))
    [loss](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = loss_fn([predictions](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [targets](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
    return [loss](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") 
```

现在，让我们使用 `grad` 转换来创建一个新函数，该函数计算相对于 `compute_loss` 的第一个参数（即 `params`）的梯度。

```py
ft_compute_grad = [grad](https://pytorch.org/docs/stable/generated/torch.func.grad.html#torch.func.grad "torch.func.grad")(compute_loss) 
```

`ft_compute_grad` 函数计算单个（样本，目标）对的梯度。我们可以使用 `vmap` 来让它计算整个批量样本和目标的梯度。注意 `in_dims=(None, None, 0, 0)`，因为我们希望将 `ft_compute_grad` 映射到数据和目标的第 0 维，并对每个使用相同的 `params` 和 buffers。

```py
ft_compute_sample_grad = [vmap](https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap "torch.vmap")(ft_compute_grad, in_dims=(None, None, 0, 0)) 
```

最后，让我们使用我们转换后的函数来计算每个样本的梯度：

```py
ft_per_sample_grads = ft_compute_sample_grad(params, buffers, [data](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [targets](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

我们可以通过使用 `grad` 和 `vmap` 来双重检查结果，以确保与手动处理每个结果一致：

```py
for [per_sample_grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [ft_per_sample_grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") in zip(per_sample_grads, ft_per_sample_grads.values()):
    assert [torch.allclose](https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose "torch.allclose")([per_sample_grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [ft_per_sample_grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), atol=3e-3, rtol=1e-5) 
```

一个快速说明：关于哪些类型的函数可以被 `vmap` 转换存在一些限制。最适合转换的函数是纯函数：输出仅由输入决定，并且没有副作用（例如突变）。`vmap` 无法处理任意 Python 数据结构的突变，但它可以处理许多原地 PyTorch 操作。

## 性能比较

想知道 `vmap` 的性能如何？

目前最佳结果是在新型 GPU（如 A100（Ampere））上获得的，在这个示例中我们看到了高达 25 倍的加速，但是这里是我们构建机器上的一些结果：

```py
def get_perf(first, first_descriptor, second, second_descriptor):
  """takes torch.benchmark objects and compares delta of second vs first."""
    second_res = second.times[0]
    first_res = first.times[0]

    gain = (first_res-second_res)/first_res
    if gain < 0: gain *=-1
    final_gain = gain*100

    print(f"Performance delta: {final_gain:.4f} percent improvement with {first_descriptor} ")

from torch.utils.benchmark import [Timer](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer "torch.utils.benchmark.utils.timer.Timer")

[without_vmap](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer "torch.utils.benchmark.utils.timer.Timer") = [Timer](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer "torch.utils.benchmark.utils.timer.Timer")(stmt="compute_sample_grads(data, targets)", globals=globals())
[with_vmap](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer "torch.utils.benchmark.utils.timer.Timer") = [Timer](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer "torch.utils.benchmark.utils.timer.Timer")(stmt="ft_compute_sample_grad(params, buffers, data, targets)",globals=globals())
[no_vmap_timing](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Measurement "torch.utils.benchmark.utils.common.Measurement") = [without_vmap.timeit](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer.timeit "torch.utils.benchmark.Timer.timeit")(100)
[with_vmap_timing](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Measurement "torch.utils.benchmark.utils.common.Measurement") = [with_vmap.timeit](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer.timeit "torch.utils.benchmark.Timer.timeit")(100)

print(f'Per-sample-grads without vmap {[no_vmap_timing](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Measurement "torch.utils.benchmark.utils.common.Measurement")}')
print(f'Per-sample-grads with vmap {[with_vmap_timing](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Measurement "torch.utils.benchmark.utils.common.Measurement")}')

get_perf([with_vmap_timing](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Measurement "torch.utils.benchmark.utils.common.Measurement"), "vmap", [no_vmap_timing](https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Measurement "torch.utils.benchmark.utils.common.Measurement"), "no vmap") 
```

```py
Per-sample-grads without vmap <torch.utils.benchmark.utils.common.Measurement object at 0x7f883d01eaa0>
compute_sample_grads(data, targets)
  92.24 ms
  1 measurement, 100 runs , 1 thread
Per-sample-grads with vmap <torch.utils.benchmark.utils.common.Measurement object at 0x7f883cf3bf40>
ft_compute_sample_grad(params, buffers, data, targets)
  8.65 ms
  1 measurement, 100 runs , 1 thread
Performance delta: 966.7210 percent improvement with vmap 
```

在 PyTorch 中有其他优化的解决方案（例如 [`github.com/pytorch/opacus`](https://github.com/pytorch/opacus)）来计算每个样本的梯度，这些解决方案的性能也比朴素方法更好。但是将 `vmap` 和 `grad` 组合起来给我们带来了一个很好的加速。

一般来说，使用 `vmap` 进行向量化应该比在 for 循环中运行函数更快，并且与手动分批处理相竞争。但也有一些例外情况，比如如果我们没有为特定操作实现 `vmap` 规则，或者如果底层内核没有针对旧硬件（GPU）进行优化。如果您遇到这些情况，请通过在 GitHub 上开启一个问题来告诉我们。

**脚本的总运行时间:** ( 0 分钟 10.810 秒)

`下载 Python 源代码: per_sample_grads.py`

`下载 Jupyter 笔记本: per_sample_grads.ipynb`

[Sphinx-Gallery 生成的画廊](https://sphinx-gallery.github.io)
