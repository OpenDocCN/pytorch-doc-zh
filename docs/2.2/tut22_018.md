# 自动微分的基础知识

> 原文：[https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html](https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html)

注意

点击[这里](#sphx-glr-download-beginner-introyt-autogradyt-tutorial-py)下载完整的示例代码

[介绍](introyt1_tutorial.html) || [张量](tensors_deeper_tutorial.html) || **自动微分** || [构建模型](modelsyt_tutorial.html) || [TensorBoard支持](tensorboardyt_tutorial.html) || [训练模型](trainingyt.html) || [模型理解](captumyt.html)

请跟随下面的视频或在[youtube](https://www.youtube.com/watch?v=M0fX15_-xrY)观看。

[https://www.youtube.com/embed/M0fX15_-xrY](https://www.youtube.com/embed/M0fX15_-xrY)

PyTorch的*Autograd*功能是PyTorch灵活和快速构建机器学习项目的一部分。它允许快速且轻松地计算复杂计算中的多个偏导数（也称为*梯度*）。这个操作对基于反向传播的神经网络学习至关重要。

自动微分的强大之处在于它在运行时动态跟踪您的计算，这意味着如果您的模型具有决策分支，或者循环的长度直到运行时才知道，计算仍将被正确跟踪，并且您将获得正确的梯度来推动学习。这与您的模型是在Python中构建的事实结合在一起，比依赖于更严格结构化模型的静态分析来计算梯度的框架提供了更灵活的选择。

## 我们为什么需要自动微分？

机器学习模型是一个*函数*，具有输入和输出。在这里讨论中，我们将输入视为一个*i*维向量\(\vec{x}\)，其中元素为\(x_{i}\)。然后我们可以将模型*M*表示为输入的矢量值函数：\(\vec{y} = \vec{M}(\vec{x})\)。（我们将M的输出值视为矢量，因为一般来说，模型可能具有任意数量的输出。）

由于我们将主要讨论自动微分在训练的上下文中，我们感兴趣的输出将是模型的损失。*损失函数* L(\(\vec{y}\)) = L(\(\vec{M}\)(\(\vec{x}\)))是模型输出的单值标量函数。这个函数表达了我们的模型预测与特定输入的*理想*输出相差多远。*注意：在此之后，我们经常会省略向量符号，只要在上下文中清楚即可 - 例如，* \(y\) 而不是 \(\vec y\)。

在训练模型时，我们希望最小化损失。在理想情况下，对于一个完美的模型，这意味着调整其学习权重 - 即函数的可调参数 - 使得所有输入的损失为零。在现实世界中，这意味着一个迭代的过程，微调学习权重，直到我们看到我们对各种输入获得了可接受的损失。

我们如何决定在多远和哪个方向微调权重？我们希望*最小化*损失，这意味着使其对输入的一阶导数等于0：\(\frac{\partial L}{\partial x} = 0\)。

然而，请记住，损失并不是*直接*从输入导出的，而是模型输出的函数（这是输入的函数），\(\frac{\partial L}{\partial x}\) = \(\frac{\partial {L({\vec y})}}{\partial x}\)。根据微分计算的链式法则，我们有\(\frac{\partial {L({\vec y})}}{\partial x}\) = \(\frac{\partial L}{\partial y}\frac{\partial y}{\partial x}\) = \(\frac{\partial L}{\partial y}\frac{\partial M(x)}{\partial x}\)。

\(\frac{\partial M(x)}{\partial x}\) 是复杂的地方。如果我们再次使用链式法则展开表达式，模型输出相对于输入的偏导数将涉及每个乘以学习权重、每个激活函数和模型中的每个其他数学变换的许多局部偏导数。每个这样的局部偏导数的完整表达式是通过计算图中以我们试图测量梯度的变量结尾的*每条可能路径*的局部梯度的乘积之和。

特别是，我们对学习权重上的梯度感兴趣 - 它们告诉我们*改变每个权重的方向*以使损失函数更接近零。

由于这种局部导数的数量（每个对应于模型计算图中的一个单独路径）往往会随着神经网络的深度呈指数增长，因此计算它们的复杂性也会增加。这就是自动微分的作用：它跟踪每次计算的历史。您PyTorch模型中的每个计算张量都携带其输入张量的历史记录以及用于创建它的函数。结合PyTorch函数旨在作用于张量的事实，每个函数都有一个用于计算自己导数的内置实现，这极大地加速了用于学习的局部导数的计算。

## 一个简单的例子

这是很多理论 - 但在实践中使用自动微分是什么样子呢？

让我们从一个简单的例子开始。首先，我们将进行一些导入，以便让我们绘制我们的结果：

```py
# %matplotlib inline

import torch

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import math 
```

接下来，我们将创建一个输入张量，其中包含区间\([0, 2{\pi}]\)上均匀间隔的值，并指定`requires_grad=True`。（像大多数创建张量的函数一样，`torch.linspace()`接受一个可选的`requires_grad`选项。）设置此标志意味着在接下来的每次计算中，autograd将在该计算的输出张量中累积计算的历史。

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.linspace](https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace "torch.linspace")(0., 2. * math.pi, steps=25, requires_grad=True)
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([0.0000, 0.2618, 0.5236, 0.7854, 1.0472, 1.3090, 1.5708, 1.8326, 2.0944,
        2.3562, 2.6180, 2.8798, 3.1416, 3.4034, 3.6652, 3.9270, 4.1888, 4.4506,
        4.7124, 4.9742, 5.2360, 5.4978, 5.7596, 6.0214, 6.2832],
       requires_grad=True) 
```

接下来，我们将进行计算，并以输入为单位绘制其输出：

```py
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.sin](https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin "torch.sin")([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
plt.plot([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").detach(), [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").detach()) 
```

![autogradyt tutorial](../Images/c0fd39cae39bc44746dc67d7e9a22ff1.png)

```py
[<matplotlib.lines.Line2D object at 0x7f7f4ccf6f50>] 
```

让我们更仔细地看看张量`b`。当我们打印它时，我们看到一个指示它正在跟踪其计算历史的指示器：

```py
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([ 0.0000e+00,  2.5882e-01,  5.0000e-01,  7.0711e-01,  8.6603e-01,
         9.6593e-01,  1.0000e+00,  9.6593e-01,  8.6603e-01,  7.0711e-01,
         5.0000e-01,  2.5882e-01, -8.7423e-08, -2.5882e-01, -5.0000e-01,
        -7.0711e-01, -8.6603e-01, -9.6593e-01, -1.0000e+00, -9.6593e-01,
        -8.6603e-01, -7.0711e-01, -5.0000e-01, -2.5882e-01,  1.7485e-07],
       grad_fn=<SinBackward0>) 
```

这个`grad_fn`给了我们一个提示，即当我们执行反向传播步骤并计算梯度时，我们需要计算所有这个张量的输入的\(\sin(x)\)的导数。

让我们进行更多的计算：

```py
[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = 2 * [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") + 1
print([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([ 0.0000e+00,  5.1764e-01,  1.0000e+00,  1.4142e+00,  1.7321e+00,
         1.9319e+00,  2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,
         1.0000e+00,  5.1764e-01, -1.7485e-07, -5.1764e-01, -1.0000e+00,
        -1.4142e+00, -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00,
        -1.7321e+00, -1.4142e+00, -1.0000e+00, -5.1764e-01,  3.4969e-07],
       grad_fn=<MulBackward0>)
tensor([ 1.0000e+00,  1.5176e+00,  2.0000e+00,  2.4142e+00,  2.7321e+00,
         2.9319e+00,  3.0000e+00,  2.9319e+00,  2.7321e+00,  2.4142e+00,
         2.0000e+00,  1.5176e+00,  1.0000e+00,  4.8236e-01, -3.5763e-07,
        -4.1421e-01, -7.3205e-01, -9.3185e-01, -1.0000e+00, -9.3185e-01,
        -7.3205e-01, -4.1421e-01,  4.7684e-07,  4.8236e-01,  1.0000e+00],
       grad_fn=<AddBackward0>) 
```

最后，让我们计算一个单元素输出。当您在没有参数的张量上调用`.backward()`时，它期望调用张量仅包含一个元素，就像在计算损失函数时一样。

```py
[out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").sum()
print([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor(25., grad_fn=<SumBackward0>) 
```

我们的张量中存储的每个`grad_fn`都允许您通过其`next_functions`属性一直回溯到其输入。我们可以看到，深入研究`d`的这个属性会显示出所有先前张量的梯度函数。请注意，`a.grad_fn`报告为`None`，表示这是一个没有自己历史记录的函数的输入。

```py
print('d:')
print([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").grad_fn)
print([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").grad_fn.next_functions)
print([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").grad_fn.next_functions[0][0].next_functions)
print([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").grad_fn.next_functions[0][0].next_functions[0][0].next_functions)
print([d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)
print('\nc:')
print([c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").grad_fn)
print('\nb:')
print([b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").grad_fn)
print('\na:')
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").grad_fn) 
```

```py
d:
<AddBackward0 object at 0x7f7f4ccf6ef0>
((<MulBackward0 object at 0x7f7f2c9c6650>, 0), (None, 0))
((<SinBackward0 object at 0x7f7f4ccf6ef0>, 0), (None, 0))
((<AccumulateGrad object at 0x7f7f2c9c6650>, 0),)
()

c:
<MulBackward0 object at 0x7f7f4ccf6ef0>

b:
<SinBackward0 object at 0x7f7f4ccf6ef0>

a:
None 
```

有了所有这些机制，我们如何得到导数？您在输出上调用`backward()`方法，并检查输入的`grad`属性以检查梯度：

```py
[out.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward "torch.Tensor.backward")()
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").grad)
plt.plot([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").detach(), [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").grad.detach()) 
```

![autogradyt tutorial](../Images/240722184f25ec9362a34b6c16336c3a.png)

```py
tensor([ 2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,  1.0000e+00,
         5.1764e-01, -8.7423e-08, -5.1764e-01, -1.0000e+00, -1.4142e+00,
        -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00, -1.7321e+00,
        -1.4142e+00, -1.0000e+00, -5.1764e-01,  2.3850e-08,  5.1764e-01,
         1.0000e+00,  1.4142e+00,  1.7321e+00,  1.9319e+00,  2.0000e+00])

[<matplotlib.lines.Line2D object at 0x7f7f4cd6aa40>] 
```

回顾我们走过的计算步骤：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.linspace](https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace "torch.linspace")(0., 2. * math.pi, steps=25, requires_grad=True)
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.sin](https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin "torch.sin")([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = 2 * [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")
[d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [c](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") + 1
[out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [d](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").sum() 
```

添加一个常数，就像我们计算`d`时所做的那样，不会改变导数。这留下了\(c = 2 * b = 2 * \sin(a)\)，其导数应该是\(2 * \cos(a)\)。从上面的图中可以看到，这正是我们看到的。

请注意，只有计算的*叶节点*的梯度被计算。例如，如果您尝试`print(c.grad)`，您会得到`None`。在这个简单的例子中，只有输入是叶节点，因此只有它的梯度被计算。

## 训练中的自动微分

我们已经简要了解了自动求导的工作原理，但是当它用于其预期目的时会是什么样子呢？让我们定义一个小模型，并检查在单个训练批次后它是如何变化的。首先，定义一些常量，我们的模型，以及一些输入和输出的替代品：

```py
BATCH_SIZE = 16
DIM_IN = 1000
HIDDEN_SIZE = 100
DIM_OUT = 10

class TinyModel([torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")):

    def __init__(self):
        super([TinyModel](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module"), self).__init__()

        self.layer1 = [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(1000, 100)
        self.relu = [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU "torch.nn.ReLU")()
        self.layer2 = [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(100, 10)

    def forward(self, [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
        [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = self.layer1([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
        [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = self.relu([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
        [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = self.layer2([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
        return [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

[some_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(BATCH_SIZE, DIM_IN, requires_grad=False)
[ideal_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(BATCH_SIZE, DIM_OUT, requires_grad=False)

model = [TinyModel](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")() 
```

您可能会注意到，我们从未为模型的层指定`requires_grad=True`。在`torch.nn.Module`的子类中，我们假设我们希望跟踪层的权重以进行学习。

如果我们查看模型的层，我们可以检查权重的值，并验证尚未计算梯度：

```py
print([model.layer2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter")[0][0:10]) # just a small slice
print([model.layer2.weight.grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([ 0.0920,  0.0916,  0.0121,  0.0083, -0.0055,  0.0367,  0.0221, -0.0276,
        -0.0086,  0.0157], grad_fn=<SliceBackward0>)
None 
```

让我们看看当我们运行一个训练批次时会发生什么变化。对于损失函数，我们将使用`prediction`和`ideal_output`之间的欧几里德距离的平方，我们将使用基本的随机梯度下降优化器。

```py
[optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD") = [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD")([model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters "torch.nn.Module.parameters")(), lr=0.001)

[prediction](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = model([some_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[loss](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ([ideal_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") - [prediction](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")).pow(2).sum()
print([loss](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor(211.2634, grad_fn=<SumBackward0>) 
```

现在，让我们调用`loss.backward()`并看看会发生什么：

```py
[loss.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward "torch.Tensor.backward")()
print([model.layer2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter")[0][0:10])
print([model.layer2.weight.grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[0][0:10]) 
```

```py
tensor([ 0.0920,  0.0916,  0.0121,  0.0083, -0.0055,  0.0367,  0.0221, -0.0276,
        -0.0086,  0.0157], grad_fn=<SliceBackward0>)
tensor([12.8997,  2.9572,  2.3021,  1.8887,  5.0710,  7.3192,  3.5169,  2.4319,
         0.1732, -5.3835]) 
```

我们可以看到为每个学习权重计算了梯度，但是权重保持不变，因为我们还没有运行优化器。优化器负责根据计算出的梯度更新模型权重。

```py
[optimizer.step](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.step "torch.optim.SGD.step")()
print([model.layer2.weight](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter")[0][0:10])
print([model.layer2.weight.grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[0][0:10]) 
```

```py
tensor([ 0.0791,  0.0886,  0.0098,  0.0064, -0.0106,  0.0293,  0.0186, -0.0300,
        -0.0088,  0.0211], grad_fn=<SliceBackward0>)
tensor([12.8997,  2.9572,  2.3021,  1.8887,  5.0710,  7.3192,  3.5169,  2.4319,
         0.1732, -5.3835]) 
```

您应该看到`layer2`的权重已经改变了。

关于这个过程的一个重要事项：在调用`optimizer.step()`之后，您需要调用`optimizer.zero_grad()`，否则每次运行`loss.backward()`时，学习权重上的梯度将会累积：

```py
print([model.layer2.weight.grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[0][0:10])

for i in range(0, 5):
    [prediction](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = model([some_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
    [loss](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ([ideal_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") - [prediction](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")).pow(2).sum()
    [loss.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward "torch.Tensor.backward")()

print([model.layer2.weight.grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[0][0:10])

[optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.zero_grad "torch.optim.SGD.zero_grad")(set_to_none=False)

print([model.layer2.weight.grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[0][0:10]) 
```

```py
tensor([12.8997,  2.9572,  2.3021,  1.8887,  5.0710,  7.3192,  3.5169,  2.4319,
         0.1732, -5.3835])
tensor([ 19.2095, -15.9459,   8.3306,  11.5096,   9.5471,   0.5391,  -0.3370,
          8.6386,  -2.5141, -30.1419])
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 
```

在运行上面的单元格后，您应该看到在多次运行`loss.backward()`后，大多数梯度的幅度会更大。在运行下一个训练批次之前未将梯度归零会导致梯度以这种方式增加，从而导致不正确和不可预测的学习结果。

## 关闭和打开自动求导

有些情况下，您需要对是否启用自动求导进行细粒度控制。根据情况，有多种方法可以实现这一点。

最简单的方法是直接在张量上更改`requires_grad`标志：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 3, requires_grad=True)
print([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[b1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = 2 * [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")
print([b1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").requires_grad = False
[b2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = 2 * [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")
print([b2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[1., 1., 1.],
        [1., 1., 1.]], requires_grad=True)
tensor([[2., 2., 2.],
        [2., 2., 2.]], grad_fn=<MulBackward0>)
tensor([[2., 2., 2.],
        [2., 2., 2.]]) 
```

在上面的单元格中，我们看到`b1`有一个`grad_fn`（即跟踪的计算历史），这是我们所期望的，因为它是从打开自动求导的张量`a`派生出来的。当我们使用`a.requires_grad = False`显式关闭自动求导时，计算历史不再被跟踪，这是我们在计算`b2`时看到的。

如果您只需要暂时关闭自动求导，更好的方法是使用`torch.no_grad()`：

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 3, requires_grad=True) * 2
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 3, requires_grad=True) * 3

[c1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") + [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")
print([c1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

with [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad "torch.no_grad")():
    [c2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") + [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

print([c2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[c3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")
print([c3](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[5., 5., 5.],
        [5., 5., 5.]], grad_fn=<AddBackward0>)
tensor([[5., 5., 5.],
        [5., 5., 5.]])
tensor([[6., 6., 6.],
        [6., 6., 6.]], grad_fn=<MulBackward0>) 
```

`torch.no_grad()`也可以作为函数或方法装饰器使用：

```py
def add_tensors1([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
    return [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") + [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

@torch.no_grad()
def add_tensors2([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
    return [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") + [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 3, requires_grad=True) * 2
[b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 3, requires_grad=True) * 3

[c1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = add_tensors1([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([c1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

[c2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = add_tensors2([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [b](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([c2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([[5., 5., 5.],
        [5., 5., 5.]], grad_fn=<AddBackward0>)
tensor([[5., 5., 5.],
        [5., 5., 5.]]) 
```

有一个相应的上下文管理器`torch.enable_grad()`，用于在自动求导尚未启用时打开自动求导。它也可以用作装饰器。

最后，您可能有一个需要跟踪梯度的张量，但您想要一个不需要的副本。为此，我们有`Tensor`对象的`detach()`方法-它创建一个与计算历史*分离*的张量的副本：

```py
[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(5, requires_grad=True)
[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").detach()

print([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
print([y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([0.0670, 0.3890, 0.7264, 0.3559, 0.6584], requires_grad=True)
tensor([0.0670, 0.3890, 0.7264, 0.3559, 0.6584]) 
```

当我们想要绘制一些张量时，我们在上面做了这个操作。这是因为`matplotlib`期望输入为NumPy数组，并且对于`requires_grad=True`的张量，PyTorch不会启用从PyTorch张量到NumPy数组的隐式转换。制作一个分离的副本让我们可以继续前进。

### 自动求导和原地操作

到目前为止，在本笔记本中的每个示例中，我们都使用变量来捕获计算的中间值。自动求导需要这些中间值来执行梯度计算。*因此，在使用自动求导时，您必须小心使用原地操作。*这样做可能会破坏您在`backward()`调用中需要计算导数的信息。如果您尝试对需要自动求导的叶变量进行原地操作，PyTorch甚至会阻止您，如下所示。

注意

> 以下代码单元格会抛出运行时错误。这是预期的。

```py
[a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.linspace](https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace "torch.linspace")(0., 2. * math.pi, steps=25, requires_grad=True)
torch.sin_([a](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

## 自动求导分析器

Autograd详细跟踪计算的每一步。这样的计算历史，结合时间信息，将成为一个方便的分析器 - autograd已经内置了这个功能。这里是一个快速示例用法：

```py
[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")('cpu')
run_on_gpu = False
if [torch.cuda.is_available](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available "torch.cuda.is_available")():
    [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device") = [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")('cuda')
    run_on_gpu = True

[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(2, 3, requires_grad=True)
[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(2, 3, requires_grad=True)
[z](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(2, 3, requires_grad=True)

with [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile")(use_cuda=run_on_gpu) as [prf](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile"):
    for _ in range(1000):
        [z](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ([z](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") / [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) * [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

print([prf.key_averages](https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.key_averages.html#torch.autograd.profiler.profile.key_averages "torch.autograd.profiler.profile.key_averages")().table(sort_by='self_cpu_time_total')) 
```

```py
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
          cudaEventRecord        43.53%       8.673ms        43.53%       8.673ms       2.168us       0.000us         0.00%       0.000us       0.000us          4000
                aten::div        28.70%       5.719ms        28.70%       5.719ms       5.719us      16.108ms        50.04%      16.108ms      16.108us          1000
                aten::mul        27.69%       5.518ms        27.69%       5.518ms       5.518us      16.083ms        49.96%      16.083ms      16.083us          1000
    cudaDeviceSynchronize         0.08%      15.000us         0.08%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 19.925ms
Self CUDA time total: 32.191ms 
```

分析器还可以标记代码的各个子块，按输入张量形状拆分数据，并将数据导出为Chrome跟踪工具文件。有关API的完整详细信息，请参阅[文档](https://pytorch.org/docs/stable/autograd.html#profiler)。

## 高级主题：更多Autograd细节和高级API

如果你有一个具有n维输入和m维输出的函数\(\vec{y}=f(\vec{x})\)，完整的梯度是一个矩阵，表示每个输出对每个输入的导数，称为*雅可比矩阵*：

\[J = \left(\begin{array}{ccc} \frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}}\\ \vdots & \ddots & \vdots\\ \frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}} \end{array}\right)\]

如果你有一个第二个函数，\(l=g\left(\vec{y}\right)\)，它接受m维输入（即与上面输出相同维度），并返回一个标量输出，你可以将其相对于\(\vec{y}\)的梯度表示为一个列向量，\(v=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} & \cdots & \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}\) - 这实际上只是一个一列雅可比矩阵。

更具体地，想象第一个函数是你的PyTorch模型（可能有许多输入和许多输出），第二个函数是一个损失函数（以模型的输出为输入，损失值为标量输出）。

如果我们将第一个函数的雅可比矩阵乘以第二个函数的梯度，并应用链式法则，我们得到：

\[J^{T}\cdot v=\left(\begin{array}{ccc} \frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{1}}\\ \vdots & \ddots & \vdots\\ \frac{\partial y_{1}}{\partial x_{n}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}} \end{array}\right)\left(\begin{array}{c} \frac{\partial l}{\partial y_{1}}\\ \vdots\\ \frac{\partial l}{\partial y_{m}} \end{array}\right)=\left(\begin{array}{c} \frac{\partial l}{\partial x_{1}}\\ \vdots\\ \frac{\partial l}{\partial x_{n}} \end{array}\right)\]

注意：你也可以使用等效的操作\(v^{T}\cdot J\)，并得到一个行向量。

得到的列向量是第二个函数相对于第一个函数的输入的*梯度* - 或者在我们的模型和损失函数的情况下，是损失相对于模型输入的梯度。

**``torch.autograd``是用于计算这些乘积的引擎。**这是我们在反向传播过程中累积梯度的方式。

因此，`backward()`调用也可以*同时*接受一个可选的向量输入。这个向量表示张量上的一组梯度，这些梯度将乘以其前面的autograd跟踪张量的雅可比矩阵。让我们尝试一个具体的例子，使用一个小向量：

```py
[x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(3, requires_grad=True)

[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * 2
while [y.data.norm](https://pytorch.org/docs/stable/generated/torch.Tensor.norm.html#torch.Tensor.norm "torch.Tensor.norm")() < 1000:
    [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * 2

print([y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([  299.4868,   425.4009, -1082.9885], grad_fn=<MulBackward0>) 
```

如果我们现在尝试调用`y.backward()`，我们会得到一个运行时错误和一个梯度只能*隐式*计算为标量输出的消息。对于多维输出，autograd希望我们提供这三个输出的梯度，以便将其乘入雅可比矩阵：

```py
[v](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([0.1, 1.0, 0.0001], dtype=[torch.float](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype")) # stand-in for gradients
[y.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward "torch.Tensor.backward")([v](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

print([x.grad](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
tensor([1.0240e+02, 1.0240e+03, 1.0240e-01]) 
```

（请注意，输出梯度都与2的幂相关 - 这是我们从重复加倍操作中期望的。）

### 高级API

在autograd上有一个API，它可以直接访问重要的微分矩阵和向量运算。特别是，它允许你计算特定输入的特定函数的雅可比矩阵和*Hessian*矩阵。（Hessian类似于雅可比矩阵，但表达了所有偏导数的*二阶*导数。）它还提供了用这些矩阵进行向量乘积的方法。

让我们计算一个简单函数的雅可比矩阵，对于2个单元素输入进行评估：

```py
def exp_adder([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
    return 2 * [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").exp() + 3 * [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

[inputs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ([torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(1), [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(1)) # arguments for the function
print([inputs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[torch.autograd.functional.jacobian](https://pytorch.org/docs/stable/generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian "torch.autograd.functional.jacobian")(exp_adder, [inputs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
(tensor([0.7212]), tensor([0.2079]))

(tensor([[4.1137]]), tensor([[3.]])) 
```

如果仔细观察，第一个输出应该等于\(2e^x\)（因为\(e^x\)的导数是\(e^x\)），第二个值应该是3。

当然，您也可以使用高阶张量来做到这一点：

```py
[inputs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ([torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(3), [torch.rand](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand "torch.rand")(3)) # arguments for the function
print([inputs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[torch.autograd.functional.jacobian](https://pytorch.org/docs/stable/generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian "torch.autograd.functional.jacobian")(exp_adder, [inputs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
(tensor([0.2080, 0.2604, 0.4415]), tensor([0.5220, 0.9867, 0.4288]))

(tensor([[2.4623, 0.0000, 0.0000],
        [0.0000, 2.5950, 0.0000],
        [0.0000, 0.0000, 3.1102]]), tensor([[3., 0., 0.],
        [0., 3., 0.],
        [0., 0., 3.]])) 
```

`torch.autograd.functional.hessian()`方法的工作方式相同（假设您的函数是两次可微的），但返回所有二阶导数的矩阵。

还有一个函数可以直接计算向量-Jacobian乘积，如果您提供向量的话：

```py
def do_some_doubling([x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
    [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [x](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * 2
    while [y.data.norm](https://pytorch.org/docs/stable/generated/torch.Tensor.norm.html#torch.Tensor.norm "torch.Tensor.norm")() < 1000:
        [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") * 2
    return [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

[inputs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(3)
[my_gradients](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor "torch.tensor")([0.1, 1.0, 0.0001])
[torch.autograd.functional.vjp](https://pytorch.org/docs/stable/generated/torch.autograd.functional.vjp.html#torch.autograd.functional.vjp "torch.autograd.functional.vjp")(do_some_doubling, [inputs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [v](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")=[my_gradients](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

```py
(tensor([-665.7186, -866.7054,  -58.4194]), tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])) 
```

`torch.autograd.functional.jvp()`方法执行与`vjp()`相同的矩阵乘法，但操作数的顺序相反。`vhp()`和`hvp()`方法对向量-海森乘积执行相同的操作。

有关更多信息，包括有关功能API的性能说明，请参阅[功能API文档](https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api)

**脚本的总运行时间：**（0分钟0.706秒）

[`下载Python源代码：autogradyt_tutorial.py`](../../_downloads/1a94e27be9b0e79da5acafc1f68a7143/autogradyt_tutorial.py)

[`下载Jupyter笔记本：autogradyt_tutorial.ipynb`](../../_downloads/ed9d4f94afb79f7dada6742a06c486a5/autogradyt_tutorial.ipynb)

[Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)
