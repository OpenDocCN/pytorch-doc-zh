# 使用 Better Transformer 进行快速 Transformer 推理

> 原文：[`pytorch.org/tutorials/beginner/bettertransformer_tutorial.html`](https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

**作者**：[Michael Gschwind](https://github.com/mikekgfb)

本教程将 Better Transformer（BT）作为 PyTorch 1.12 版本的一部分进行介绍。在本教程中，我们展示了如何使用 Better Transformer 进行 torchtext 的生产推理。Better Transformer 是一个生产就绪的快速路径，可加速在 CPU 和 GPU 上部署具有高性能的 Transformer 模型。快速路径功能对基于 PyTorch 核心`nn.module`或 torchtext 的模型透明地工作。

可以通过 Better Transformer 快速路径执行加速的模型是使用以下 PyTorch 核心`torch.nn.module`类`TransformerEncoder`、`TransformerEncoderLayer`和`MultiHeadAttention`的模型。此外，torchtext 已更新为使用核心库模块以从快速路径加速中受益。 （未来可能会启用其他模块以进行快速路径执行。）

Better Transformer 提供了两种加速类型：

+   为 CPU 和 GPU 实现的原生多头注意力（MHA）以提高整体执行效率。

+   利用 NLP 推理中的稀疏性。由于输入长度可变，输入标记可能包含大量填充标记，处理时可以跳过，从而实现显著加速。

快速路径执行受一些标准的限制。最重要的是，模型必须在推理模式下执行，并且在不收集梯度磁带信息的输入张量上运行（例如，使用 torch.no_grad 运行）。

要在 Google Colab 中查看此示例，请[点击这里](https://colab.research.google.com/drive/1KZnMJYhYkOMYtNIX5S3AGIYnjyG0AojN?usp=sharing)。

## 本教程中的 Better Transformer 功能

+   加载预训练模型（在 PyTorch 版本 1.12 之前创建，没有 Better Transformer）

+   在 CPU 上运行和基准推理，使用 BT 快速路径（仅原生 MHA）

+   在（可配置的）设备上运行和基准推理，使用 BT 快速路径（仅原生 MHA）

+   启用稀疏性支持

+   在（可配置的）设备上运行和基准推理，使用 BT 快速路径（原生 MHA + 稀疏性）

## 附加信息

有关 Better Transformer 的更多信息可以在 PyTorch.Org 博客[A Better Transformer for Fast Transformer Inference](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference//)中找到。

1.  设置

1.1 加载预训练模型

我们通过按照[torchtext.models](https://pytorch.org/text/main/models.html)中的说明从预定义的 torchtext 模型中下载 XLM-R 模型。我们还将设备设置为在加速器测试上执行。（根据需要启用 GPU 执行环境。）

```py
import torch
import torch.nn as nn

print(f"torch version: {torch.__version__}")

DEVICE = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

print(f"torch cuda available: {torch.cuda.is_available()}")

import torch, torchtext
from torchtext.models import RobertaClassificationHead
from torchtext.functional import to_tensor
xlmr_large = torchtext.models.XLMR_LARGE_ENCODER
classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)
model = xlmr_large.get_model(head=classifier_head)
transform = xlmr_large.transform() 
```

1.2 数据集设置

我们设置了两种类型的输入：一个小输入批次和一个带有稀疏性的大输入批次。

```py
small_input_batch = [
               "Hello world",
               "How are you!"
]
big_input_batch = [
               "Hello world",
               "How are you!",
  """`Well, Prince, so Genoa and Lucca are now just family estates of the
Buonapartes. But I warn you, if you don't tell me that this means war,
if you still try to defend the infamies and horrors perpetrated by
that Antichrist- I really believe he is Antichrist- I will have
nothing more to do with you and you are no longer my friend, no longer
my 'faithful slave,' as you call yourself! But how do you do? I see
I have frightened you- sit down and tell me all the news.`

It was in July, 1805, and the speaker was the well-known Anna
Pavlovna Scherer, maid of honor and favorite of the Empress Marya
Fedorovna. With these words she greeted Prince Vasili Kuragin, a man
of high rank and importance, who was the first to arrive at her
reception. Anna Pavlovna had had a cough for some days. She was, as
she said, suffering from la grippe; grippe being then a new word in
St. Petersburg, used only by the elite."""
] 
```

接下来，我们选择小批量或大批量输入，预处理输入并测试模型。

```py
input_batch=big_input_batch

model_input = to_tensor(transform(input_batch), padding_value=1)
output = model(model_input)
output.shape 
```

最后，我们设置基准迭代次数：

```py
ITERATIONS=10 
```

1.  执行

在 CPU 上运行和基准推理，使用 BT 快速路径（仅原生 MHA）

我们在 CPU 上运行模型，并收集性能信息：

+   第一次运行使用传统（“慢速路径”）执行。

+   第二次运行通过将模型置于推理模式并使用 model.eval()启用 BT 快速路径执行，并使用 torch.no_grad()禁用梯度收集。

当模型在 CPU 上执行时，您会看到改进（其幅度取决于 CPU 模型）。请注意，快速路径概要显示大部分执行时间在本地 TransformerEncoderLayer 实现 aten::_transformer_encoder_layer_fwd 中。

```py
print("slow path:")
print("==========")
with torch.autograd.profiler.profile(use_cuda=False) as prof:
  for i in range(ITERATIONS):
    output = model(model_input)
print(prof)

model.eval()

print("fast path:")
print("==========")
with torch.autograd.profiler.profile(use_cuda=False) as prof:
  with torch.no_grad():
    for i in range(ITERATIONS):
      output = model(model_input)
print(prof) 
```

在（可配置的）设备上运行和基准推理，使用 BT 快速路径（仅原生 MHA）

我们检查 BT 的稀疏性设置：

```py
model.encoder.transformer.layers.enable_nested_tensor 
```

我们禁用了 BT 的稀疏性：

```py
model.encoder.transformer.layers.enable_nested_tensor=False 
```

我们在设备上运行模型，并收集用于设备上原生 MHA 执行的性能信息：

+   第一次运行使用传统的（“慢路径”）执行。

+   第二次运行通过将模型置于推理模式并使用 model.eval()禁用梯度收集来启用 BT 快速执行路径。

在 GPU 上执行时，您应该看到显着的加速，特别是对于小输入批处理设置：

```py
model.to(DEVICE)
model_input = model_input.to(DEVICE)

print("slow path:")
print("==========")
with torch.autograd.profiler.profile(use_cuda=True) as prof:
  for i in range(ITERATIONS):
    output = model(model_input)
print(prof)

model.eval()

print("fast path:")
print("==========")
with torch.autograd.profiler.profile(use_cuda=True) as prof:
  with torch.no_grad():
    for i in range(ITERATIONS):
      output = model(model_input)
print(prof) 
```

2.3 在（可配置的）DEVICE 上运行和对比推理，包括 BT 快速执行路径和不包括 BT 快速执行路径（原生 MHA + 稀疏性）

我们启用稀疏性支持：

```py
model.encoder.transformer.layers.enable_nested_tensor = True 
```

我们在 DEVICE 上运行模型，并收集原生 MHA 和稀疏性支持在 DEVICE 上的执行的概要信息：

+   第一次运行使用传统的（“慢路径”）执行。

+   第二次运行通过将模型置于推理模式并使用 model.eval()禁用梯度收集来启用 BT 快速执行路径。

在 GPU 上执行时，您应该看到显着的加速，特别是对于包含稀疏性的大输入批处理设置：

```py
model.to(DEVICE)
model_input = model_input.to(DEVICE)

print("slow path:")
print("==========")
with torch.autograd.profiler.profile(use_cuda=True) as prof:
  for i in range(ITERATIONS):
    output = model(model_input)
print(prof)

model.eval()

print("fast path:")
print("==========")
with torch.autograd.profiler.profile(use_cuda=True) as prof:
  with torch.no_grad():
    for i in range(ITERATIONS):
      output = model(model_input)
print(prof) 
```

## 总结

在本教程中，我们介绍了在 torchtext 中使用 PyTorch 核心 Better Transformer 支持 Transformer 编码器模型的快速变压器推理。我们演示了在 BT 快速执行路径可用之前训练的模型中使用 Better Transformer 的方法。我们演示并对比了 BT 快速执行路径模式、原生 MHA 执行和 BT 稀疏性加速的使用。
