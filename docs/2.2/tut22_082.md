# TorchScript 中的动态并行性

> 原文：[`pytorch.org/tutorials/advanced/torch-script-parallelism.html`](https://pytorch.org/tutorials/advanced/torch-script-parallelism.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

在本教程中，我们介绍了在 TorchScript 中进行*动态跨操作并行性*的语法。这种并行性具有以下特性：

+   动态 - 并行任务的数量和它们的工作量可以取决于程序的控制流。

+   跨操作 - 并行性涉及并行运行 TorchScript 程序片段。这与*内部操作并行性*不同，内部操作并行性涉及将单个运算符拆分并并行运行运算符工作的子集。

## 基本语法

动态并行性的两个重要 API 是：

+   `torch.jit.fork(fn: Callable[..., T], *args, **kwargs) -> torch.jit.Future[T]`

+   `torch.jit.wait(fut: torch.jit.Future[T]) -> T`

通过一个例子演示这些工作的好方法是：

```py
import torch

def foo(x):
    return torch.neg(x)

@torch.jit.script
def example(x):
    # Call `foo` using parallelism:
    # First, we "fork" off a task. This task will run `foo` with argument `x`
    future = torch.jit.fork(foo, x)

    # Call `foo` normally
    x_normal = foo(x)

    # Second, we "wait" on the task. Since the task may be running in
    # parallel, we have to "wait" for its result to become available.
    # Notice that by having lines of code between the "fork()" and "wait()"
    # call for a given Future, we can overlap computations so that they
    # run in parallel.
    x_parallel = torch.jit.wait(future)

    return x_normal, x_parallel

print(example(torch.ones(1))) # (-1., -1.) 
```

`fork()`接受可调用的`fn`以及该可调用的参数`args`和`kwargs`，并为`fn`的执行创建一个异步任务。`fn`可以是一个函数、方法或模块实例。`fork()`返回对此执行结果值的引用，称为`Future`。由于`fork`在创建异步任务后立即返回，所以在`fork()`调用后的代码行执行时，`fn`可能尚未被执行。因此，使用`wait()`来等待异步任务完成并返回值。

这些结构可以用来重叠函数内语句的执行（在示例部分中显示），或者与其他语言结构如循环组合：

```py
import torch
from typing import List

def foo(x):
    return torch.neg(x)

@torch.jit.script
def example(x):
    futures : List[torch.jit.Future[torch.Tensor]] = []
    for _ in range(100):
        futures.append(torch.jit.fork(foo, x))

    results = []
    for future in futures:
        results.append(torch.jit.wait(future))

    return torch.sum(torch.stack(results))

print(example(torch.ones([]))) 
```

注意

当我们初始化一个空的 Future 列表时，我们需要为`futures`添加显式类型注释。在 TorchScript 中，空容器默认假定它们包含 Tensor 值，因此我们将列表构造函数的注释标记为`List[torch.jit.Future[torch.Tensor]]`

这个例子使用`fork()`启动 100 个`foo`函数的实例，等待这 100 个任务完成，然后对结果求和，返回`-100.0`。

## 应用示例：双向 LSTM 集合

让我们尝试将并行性应用于一个更现实的例子，看看我们能从中获得什么样的性能。首先，让我们定义基线模型：双向 LSTM 层的集合。

```py
import torch, time

# In RNN parlance, the dimensions we care about are:
# # of time-steps (T)
# Batch size (B)
# Hidden size/number of "channels" (C)
T, B, C = 50, 50, 1024

# A module that defines a single "bidirectional LSTM". This is simply two
# LSTMs applied to the same sequence, but one in reverse
class BidirectionalRecurrentLSTM(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C)
        self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C)

    def forward(self, x : torch.Tensor) -> torch.Tensor:
        # Forward layer
        output_f, _ = self.cell_f(x)

        # Backward layer. Flip input in the time dimension (dim 0), apply the
        # layer, then flip the outputs in the time dimension
        x_rev = torch.flip(x, dims=[0])
        output_b, _ = self.cell_b(torch.flip(x, dims=[0]))
        output_b_rev = torch.flip(output_b, dims=[0])

        return torch.cat((output_f, output_b_rev), dim=2)

# An "ensemble" of `BidirectionalRecurrentLSTM` modules. The modules in the
# ensemble are run one-by-one on the same input then their results are
# stacked and summed together, returning the combined result.
class LSTMEnsemble(torch.nn.Module):
    def __init__(self, n_models):
        super().__init__()
        self.n_models = n_models
        self.models = torch.nn.ModuleList([
            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])

    def forward(self, x : torch.Tensor) -> torch.Tensor:
        results = []
        for model in self.models:
            results.append(model(x))
        return torch.stack(results).sum(dim=0)

# For a head-to-head comparison to what we're going to do with fork/wait, let's
# instantiate the model and compile it with TorchScript
ens = torch.jit.script(LSTMEnsemble(n_models=4))

# Normally you would pull this input out of an embedding table, but for the
# purpose of this demo let's just use random data.
x = torch.rand(T, B, C)

# Let's run the model once to warm up things like the memory allocator
ens(x)

x = torch.rand(T, B, C)

# Let's see how fast it runs!
s = time.time()
ens(x)
print('Inference took', time.time() - s, ' seconds') 
```

在我的机器上，这个网络运行需要`2.05`秒。我们可以做得更好！

## 并行化前向和后向层

我们可以做的一个非常简单的事情是并行化`BidirectionalRecurrentLSTM`中的前向和后向层。对于这个结构的计算是静态的，所以我们实际上甚至不需要任何循环。让我们像这样重写`BidirectionalRecurrentLSTM`的`forward`方法：

```py
def forward(self, x : torch.Tensor) -> torch.Tensor:
    # Forward layer - fork() so this can run in parallel to the backward
    # layer
    future_f = torch.jit.fork(self.cell_f, x)

    # Backward layer. Flip input in the time dimension (dim 0), apply the
    # layer, then flip the outputs in the time dimension
    x_rev = torch.flip(x, dims=[0])
    output_b, _ = self.cell_b(torch.flip(x, dims=[0]))
    output_b_rev = torch.flip(output_b, dims=[0])

    # Retrieve the output from the forward layer. Note this needs to happen
    # *after* the stuff we want to parallelize with
    output_f, _ = torch.jit.wait(future_f)

    return torch.cat((output_f, output_b_rev), dim=2) 
```

在这个例子中，`forward()`将`cell_f`的执行委托给另一个线程，同时继续执行`cell_b`。这导致两个单元的执行互相重叠。

使用这个简单修改再次运行脚本，运行时间为`1.71`秒，提高了`17%`！

## 附注：可视化并行性

我们还没有优化完我们的模型，但值得介绍一下我们用于可视化性能的工具。一个重要的工具是[PyTorch 分析器](https://pytorch.org/docs/stable/autograd.html#profiler)。

让我们使用分析器以及 Chrome 跟踪导出功能来可视化我们并行化模型的性能：

```py
with torch.autograd.profiler.profile() as prof:
    ens(x)
prof.export_chrome_trace('parallel.json') 
```

这段代码将写出一个名为`parallel.json`的文件。如果你将 Google Chrome 导航到`chrome://tracing`，点击`Load`按钮，然后加载该 JSON 文件，你应该会看到如下时间线：

![`i.imgur.com/rm5hdG9.png`](img/6b495cb0cd4336a2469d9f07696faa3e.png)

时间线的水平轴表示时间，垂直轴表示执行线程。正如我们所看到的，我们同时运行两个`lstm`实例。这是我们并行化双向层的努力的结果！

## 在集成模型中并行化模型

您可能已经注意到我们的代码中还有进一步的并行化机会：我们也可以让包含在`LSTMEnsemble`中的模型相互并行运行。要做到这一点很简单，我们应该改变`LSTMEnsemble`的`forward`方法：

```py
def forward(self, x : torch.Tensor) -> torch.Tensor:
    # Launch tasks for each model
    futures : List[torch.jit.Future[torch.Tensor]] = []
    for model in self.models:
        futures.append(torch.jit.fork(model, x))

    # Collect the results from the launched tasks
    results : List[torch.Tensor] = []
    for future in futures:
        results.append(torch.jit.wait(future))

    return torch.stack(results).sum(dim=0) 
```

或者，如果您更看重简洁性，我们可以使用列表推导：

```py
def forward(self, x : torch.Tensor) -> torch.Tensor:
    futures = [torch.jit.fork(model, x) for model in self.models]
    results = [torch.jit.wait(fut) for fut in futures]
    return torch.stack(results).sum(dim=0) 
```

就像在介绍中描述的那样，我们使用循环为集成模型中的每个模型启动任务。然后我们使用另一个循环等待所有任务完成。这提供了更多的计算重叠。

通过这个小更新，脚本运行时间缩短至`1.4`秒，总体加速达到`32%`！两行代码的效果相当不错。

我们还可以再次使用 Chrome 跟踪器来查看发生了什么：

![`i.imgur.com/kA0gyQm.png`](img/ac8752539498c11001a65c1ff470d696.png)

现在我们可以看到所有的`LSTM`实例都在完全并行运行。

## 结论

在本教程中，我们学习了`fork()`和`wait()`，这是 TorchScript 中进行动态、跨操作并行处理的基本 API。我们看到了一些使用这些函数来并行执行函数、方法或`Modules`的典型用法。最后，我们通过一个优化模型的示例来探讨了这种技术，并探索了 PyTorch 中可用的性能测量和可视化工具。
