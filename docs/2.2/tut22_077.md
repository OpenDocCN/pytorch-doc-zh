# 前向模式自动微分（Beta）

> 原文：[https://pytorch.org/tutorials/intermediate/forward_ad_usage.html](https://pytorch.org/tutorials/intermediate/forward_ad_usage.html)

注意

点击[这里](#sphx-glr-download-intermediate-forward-ad-usage-py)下载完整示例代码

本教程演示了如何使用前向模式自动微分来计算方向导数（或等效地，雅可比向量积）。

下面的教程仅使用版本 >= 1.11（或夜间构建）中才可用的一些API。

还要注意，前向模式自动微分目前处于 beta 阶段。API 可能会发生变化，操作符覆盖仍然不完整。

## 基本用法

与反向模式自动微分不同，前向模式自动微分在前向传递过程中急切地计算梯度。我们可以使用前向模式自动微分来计算方向导数，方法是在执行前向传递之前，将我们的输入与另一个表示方向导数方向（或等效地，雅可比向量积中的 `v`）的张量相关联。当一个称为“原始”的输入与一个称为“切向”的“方向”张量相关联时，所得到的新张量对象被称为“双张量”，因为它与双重数的连接[0]。

在执行前向传递时，如果任何输入张量是双张量，则会执行额外的计算以传播函数的“敏感性”。

```py
import torch
import torch.autograd.forward_ad as fwAD

[primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)
[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)

def fn(x, [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
    return x ** 2 + [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") ** 2

# All forward AD computation must be performed in the context of
# a ``dual_level`` context. All dual tensors created in such a context
# will have their tangents destroyed upon exit. This is to ensure that
# if the output or intermediate results of this computation are reused
# in a future forward AD computation, their tangents (which are associated
# with this computation) won't be confused with tangents from the later
# computation.
with [fwAD.dual_level](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level "torch.autograd.forward_ad.dual_level")():
    # To create a dual tensor we associate a tensor, which we call the
    # primal with another tensor of the same size, which we call the tangent.
    # If the layout of the tangent is different from that of the primal,
    # The values of the tangent are copied into a new tensor with the same
    # metadata as the primal. Otherwise, the tangent itself is used as-is.
    #
    # It is also important to note that the dual tensor created by
    # ``make_dual`` is a view of the primal.
    [dual_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [fwAD.make_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual "torch.autograd.forward_ad.make_dual")([primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
    assert [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual "torch.autograd.forward_ad.unpack_dual")([dual_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") is [tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

    # To demonstrate the case where the copy of the tangent happens,
    # we pass in a tangent with a layout different from that of the primal
    [dual_input_alt](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [fwAD.make_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual "torch.autograd.forward_ad.make_dual")([primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [tangent.T](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
    assert [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual "torch.autograd.forward_ad.unpack_dual")([dual_input_alt](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") is not [tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

    # Tensors that do not have an associated tangent are automatically
    # considered to have a zero-filled tangent of the same shape.
    [plain_tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)
    [dual_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = fn([dual_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [plain_tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))

    # Unpacking the dual returns a ``namedtuple`` with ``primal`` and ``tangent``
    # as attributes
    [jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual "torch.autograd.forward_ad.unpack_dual")([dual_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

assert [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual "torch.autograd.forward_ad.unpack_dual")([dual_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") is None 
```

## 使用模块

要使用前向自动微分与 `nn.Module`，在执行前向传递之前，将模型的参数替换为双张量。在撰写本文时，不可能创建双张量 [`](#id1)nn.Parameter`。作为解决方法，必须将双张量注册为模块的非参数属性。

```py
import torch.nn as nn

[model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear") = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(5, 5)
input = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(16, 5)

params = {name: [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter") for name, [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter") in [model.named_parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters "torch.nn.Module.named_parameters")()}
tangents = {name: [torch.rand_like](https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like "torch.rand_like")([p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter")) for name, [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter") in params.items()}

with [fwAD.dual_level](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level "torch.autograd.forward_ad.dual_level")():
    for name, [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter") in params.items():
        delattr([model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear"), name)
        setattr([model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear"), name, [fwAD.make_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual "torch.autograd.forward_ad.make_dual")([p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter"), tangents[name]))

    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(input)
    [jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual "torch.autograd.forward_ad.unpack_dual")([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") 
```

## 使用功能模块 API（beta）

使用前向自动微分的另一种方法是利用功能模块 API（也称为无状态模块 API）。

```py
from torch.func import [functional_call](https://pytorch.org/docs/stable/generated/torch.func.functional_call.html#torch.func.functional_call "torch.func.functional_call")

# We need a fresh module because the functional call requires the
# the model to have parameters registered.
[model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear") = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(5, 5)

dual_params = {}
with [fwAD.dual_level](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level "torch.autograd.forward_ad.dual_level")():
    for name, [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter") in params.items():
        # Using the same ``tangents`` from the above section
        dual_params[name] = [fwAD.make_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual "torch.autograd.forward_ad.make_dual")([p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter"), tangents[name])
    [out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [functional_call](https://pytorch.org/docs/stable/generated/torch.func.functional_call.html#torch.func.functional_call "torch.func.functional_call")([model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear"), dual_params, input)
    [jvp2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual "torch.autograd.forward_ad.unpack_dual")([out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

# Check our results
assert [torch.allclose](https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose "torch.allclose")([jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [jvp2](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")) 
```

## 自定义 autograd 函数

自定义函数还支持前向模式自动微分。要创建支持前向模式自动微分的自定义函数，请注册 `jvp()` 静态方法。自定义函数可以支持前向和反向自动微分，但这不是强制的。有关更多信息，请参阅[文档](https://pytorch.org/docs/master/notes/extending.html#forward-mode-ad)。

```py
class Fn([torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function "torch.autograd.Function")):
    @staticmethod
    def forward(ctx, foo):
        result = [torch.exp](https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp "torch.exp")(foo)
        # Tensors stored in ``ctx`` can be used in the subsequent forward grad
        # computation.
        ctx.result = result
        return result

    @staticmethod
    def jvp(ctx, gI):
        gO = gI * ctx.result
        # If the tensor stored in`` ctx`` will not also be used in the backward pass,
        # one can manually free it using ``del``
        del ctx.result
        return gO

fn = Fn.apply

[primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10, dtype=[torch.double](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype"), requires_grad=True)
[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)

with [fwAD.dual_level](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level "torch.autograd.forward_ad.dual_level")():
    [dual_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [fwAD.make_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual "torch.autograd.forward_ad.make_dual")([primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
    [dual_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = fn([dual_input](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
    [jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [fwAD.unpack_dual](https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual "torch.autograd.forward_ad.unpack_dual")([dual_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")).[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")

# It is important to use ``autograd.gradcheck`` to verify that your
# custom autograd Function computes the gradients correctly. By default,
# ``gradcheck`` only checks the backward-mode (reverse-mode) AD gradients. Specify
# ``check_forward_ad=True`` to also check forward grads. If you did not
# implement the backward formula for your function, you can also tell ``gradcheck``
# to skip the tests that require backward-mode AD by specifying
# ``check_backward_ad=False``, ``check_undefined_grad=False``, and
# ``check_batched_grad=False``.
[torch.autograd.gradcheck](https://pytorch.org/docs/stable/autograd.html#module-torch.autograd.gradcheck "torch.autograd.gradcheck")(Fn.apply, ([primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"),), check_forward_ad=True,
                         check_backward_ad=False, check_undefined_grad=False,
                         check_batched_grad=False) 
```

```py
True 
```

## 功能 API（beta）

我们还提供了 functorch 中用于计算雅可比向量积的更高级功能 API，根据您的用例，您可能会发现更简单使用。

功能 API 的好处是不需要理解或使用较低级别的双张量 API，并且可以将其与其他 [functorch 转换（如 vmap）](https://pytorch.org/functorch/stable/notebooks/jacobians_hessians.html)组合；缺点是它提供的控制较少。

请注意，本教程的其余部分将需要 functorch ([https://github.com/pytorch/functorch](https://github.com/pytorch/functorch)) 来运行。请在指定的链接找到安装说明。

```py
import functorch as ft

[primal0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)
[tangent0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)
[primal1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)
[tangent1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)

def fn(x, [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")):
    return x ** 2 + [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") ** 2

# Here is a basic example to compute the JVP of the above function.
# The ``jvp(func, primals, tangents)`` returns ``func(*primals)`` as well as the
# computed Jacobian-vector product (JVP). Each primal must be associated with a tangent of the same shape.
[primal_out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [tangent_out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ft.[jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")(fn, ([primal0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [primal1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")), ([tangent0](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [tangent1](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")))

# ``functorch.jvp`` requires every primal to be associated with a tangent.
# If we only want to associate certain inputs to `fn` with tangents,
# then we'll need to create a new function that captures inputs without tangents:
[primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)
[tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)
[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(10, 10)

import functools
new_fn = functools.partial(fn, [y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")=[y](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))
[primal_out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [tangent_out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ft.[jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")(new_fn, ([primal](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"),), ([tangent](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"),)) 
```

```py
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/deprecated.py:77: UserWarning:

We've integrated functorch into PyTorch. As the final step of the integration, functorch.jvp is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3\. Please use torch.func.jvp instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html 
```

## 使用功能 API 与模块

要使用 `functorch.jvp` 与 `nn.Module` 一起计算相对于模型参数的雅可比向量积，我们需要将 `nn.Module` 重新构建为一个接受模型参数和模块输入的函数。

```py
[model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear") = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(5, 5)
input = [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn "torch.randn")(16, 5)
tangents = tuple([[torch.rand_like](https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like "torch.rand_like")([p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter")) for [p](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter "torch.nn.parameter.Parameter") in [model.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters "torch.nn.Module.parameters")()])

# Given a ``torch.nn.Module``, ``ft.make_functional_with_buffers`` extracts the state
# (``params`` and buffers) and returns a functional version of the model that
# can be invoked like a function.
# That is, the returned ``func`` can be invoked like
# ``func(params, buffers, input)``.
# ``ft.make_functional_with_buffers`` is analogous to the ``nn.Modules`` stateless API
# that you saw previously and we're working on consolidating the two.
func, params, buffers = ft.make_functional_with_buffers([model](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear"))

# Because ``jvp`` requires every input to be associated with a tangent, we need to
# create a new function that, when given the parameters, produces the output
def func_params_only(params):
    return func(params, buffers, input)

[model_output](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [jvp_out](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = ft.[jvp](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")(func_params_only, (params,), (tangents,)) 
```

```py
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/deprecated.py:104: UserWarning:

We've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional_with_buffers is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3\. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html

/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_functorch/deprecated.py:77: UserWarning:

We've integrated functorch into PyTorch. As the final step of the integration, functorch.jvp is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3\. Please use torch.func.jvp instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html 
```

[0] [https://en.wikipedia.org/wiki/Dual_number](https://en.wikipedia.org/wiki/Dual_number)

**脚本的总运行时间：**（0 分钟 0.149 秒）

[`下载 Python 源代码：forward_ad_usage.py`](../_downloads/3a285734c191abde60d7db0362f294b1/forward_ad_usage.py)

[`下载 Jupyter 笔记本：forward_ad_usage.ipynb`](../_downloads/31e117c487018c27130cd7b1fd3e3cad/forward_ad_usage.ipynb)

[Sphinx-Gallery 生成的画廊](https://sphinx-gallery.github.io)
