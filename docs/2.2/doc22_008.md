# CUDA 自动混合精度示例

> 原文：[`pytorch.org/docs/stable/notes/amp_examples.html`](https://pytorch.org/docs/stable/notes/amp_examples.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)

通常，“自动混合精度训练”意味着同时使用`torch.autocast`和`torch.cuda.amp.GradScaler`进行训练。

`torch.autocast`的实例使得可以为选择的区域进行自动转换。自动转换会自动选择 GPU 操作的精度，以提高性能同时保持准确性。

`torch.cuda.amp.GradScaler`的实例有助于方便地执行梯度缩放的步骤。梯度缩放通过最小化梯度下溢来提高具有`float16`梯度的网络的收敛性，如此处所解释。

`torch.autocast`和`torch.cuda.amp.GradScaler`是模块化的。在下面的示例中，每个都按照其各自的文档建议使用。

（这里的示例仅供参考。请查看[自动混合精度教程](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)以获得可运行的步骤。）

+   典型的混合精度训练

+   使用未缩放梯度

    +   梯度裁剪

+   使用缩放梯度

    +   梯度累积

    +   梯度惩罚

+   使用多个模型、损失和优化器

+   使用多个 GPU

    +   单进程中的 DataParallel

    +   DistributedDataParallel，每个进程一个 GPU

    +   DistributedDataParallel，每个进程多个 GPU

+   自动转换和自定义自动梯度函数

    +   具有多个输入或可自动转换操作的函数

    +   需要特定`dtype`的函数

## 典型的混合精度训练[](#typical-mixed-precision-training "跳转到此标题")

```py
# Creates model and optimizer in default precision
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)

# Creates a GradScaler once at the beginning of training.
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()

        # Runs the forward pass with autocasting.
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)

        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.
        # Backward passes under autocast are not recommended.
        # Backward ops run in the same dtype autocast chose for corresponding forward ops.
        scaler.scale(loss).backward()

        # scaler.step() first unscales the gradients of the optimizer's assigned params.
        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,
        # otherwise, optimizer.step() is skipped.
        scaler.step(optimizer)

        # Updates the scale for next iteration.
        scaler.update() 
```

## 使用未缩放梯度[](#working-with-unscaled-gradients "跳转到此标题")

`scaler.scale(loss).backward()`产生的所有梯度都是经过缩放的。如果您希望在`backward()`和`scaler.step(optimizer)`之间修改或检查参数的`.grad`属性，您应该首先取消缩放它们。例如，梯度裁剪操作会操纵一组梯度，使它们的全局范数（参见`torch.nn.utils.clip_grad_norm_()`）或最大幅度（参见`torch.nn.utils.clip_grad_value_()`）小于某个用户设定的阈值。如果您尝试在不取消缩放的情况下裁剪梯度，梯度的范数/最大幅度也会被缩放，因此您请求的阈值（本来是用于*未缩放*梯度的阈值）将无效。

`scaler.unscale_(optimizer)`取消缩放`optimizer`的参数所持有的梯度。如果您的模型包含其他分配给另一个优化器（比如`optimizer2`）的参数，您可以单独调用`scaler.unscale_(optimizer2)`来取消缩放这些参数的梯度。

### 梯度裁剪

在裁剪之前调用`scaler.unscale_(optimizer)`使您可以像往常一样裁剪未缩放的梯度：

```py
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)
        scaler.scale(loss).backward()

        # Unscales the gradients of optimizer's assigned params in-place
        scaler.unscale_(optimizer)

        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)

        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,
        # although it still skips optimizer.step() if the gradients contain infs or NaNs.
        scaler.step(optimizer)

        # Updates the scale for next iteration.
        scaler.update() 
```

`scaler`记录了在此迭代中已经为此优化器调用了`scaler.unscale_(optimizer)`，因此`scaler.step(optimizer)`知道在（内部）调用`optimizer.step()`之前不要冗余地对梯度进行未缩放处理。

警告

`unscale_`应该每个优化器每次`step`调用只调用一次，并且只在为该优化器分配的参数的所有梯度都被累积之后才调用。在每个`step`之间为给定的优化器调用两次`unscale_`会触发一个运行时错误。

## 使用缩放梯度[](#working-with-scaled-gradients "跳转到此标题")

### 梯度累积[](#gradient-accumulation "跳转到此标题")

梯度累积会在一个有效批次的大小上添加梯度，大小为`batch_per_iter * iters_to_accumulate`（如果是分布式的话还要乘以`num_procs`）。尺度应该校准到有效批次，这意味着在有效批次粒度上进行 inf/NaN 检查，如果发现 inf/NaN 梯度，则跳过步骤，同时尺度更新应该在有效批次粒度上发生。此外，梯度应该保持缩放，尺度因子应该保持恒定，而给定有效批次的梯度被累积。如果在累积完成之前梯度未被缩放（或尺度因子发生变化），下一个反向传播将会在将缩放的梯度添加到未缩放的梯度（或使用不同因子缩放的梯度）之后，无法恢复已累积的未缩放梯度，必须调用`step`。

因此，如果要对梯度进行`unscale_`（例如，允许裁剪未缩放的梯度），请在所有（缩放的）梯度为即将到来的`step`累积完成之后，在`step`之前调用`unscale_`。此外，只在调用了`step`进行完整有效批次的迭代结束时才调用`update`：

```py
scaler = GradScaler()

for epoch in epochs:
    for i, (input, target) in enumerate(data):
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)
            loss = loss / iters_to_accumulate

        # Accumulates scaled gradients.
        scaler.scale(loss).backward()

        if (i + 1) % iters_to_accumulate == 0:
            # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad() 
```

### 梯度惩罚

梯度惩罚的实现通常使用`torch.autograd.grad()`创建梯度，将它们组合以创建惩罚值，并将惩罚值添加到损失中。

这是一个普通的 L2 惩罚的例子，没有梯度缩放或自动转换：

```py
for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)

        # Creates gradients
        grad_params = torch.autograd.grad(outputs=loss,
                                          inputs=model.parameters(),
                                          create_graph=True)

        # Computes the penalty term and adds it to the loss
        grad_norm = 0
        for grad in grad_params:
            grad_norm += grad.pow(2).sum()
        grad_norm = grad_norm.sqrt()
        loss = loss + grad_norm

        loss.backward()

        # clip gradients here, if desired

        optimizer.step() 
```

要实现带有梯度缩放的梯度惩罚，传递给`torch.autograd.grad()`的`outputs`张量应该被缩放。因此，得到的梯度将被缩放，应该在组合创建惩罚值之前取消缩放。

此外，惩罚项的计算是前向传播的一部分，因此应该在`autocast`上下文中。

对于相同的 L2 惩罚，看起来是这样的：

```py
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)

        # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params
        scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss),
                                                 inputs=model.parameters(),
                                                 create_graph=True)

        # Creates unscaled grad_params before computing the penalty. scaled_grad_params are
        # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_:
        inv_scale = 1./scaler.get_scale()
        grad_params = [p * inv_scale for p in scaled_grad_params]

        # Computes the penalty term and adds it to the loss
        with autocast(device_type='cuda', dtype=torch.float16):
            grad_norm = 0
            for grad in grad_params:
                grad_norm += grad.pow(2).sum()
            grad_norm = grad_norm.sqrt()
            loss = loss + grad_norm

        # Applies scaling to the backward call as usual.
        # Accumulates leaf gradients that are correctly scaled.
        scaler.scale(loss).backward()

        # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)

        # step() and update() proceed as usual.
        scaler.step(optimizer)
        scaler.update() 
```

## 使用多个模型、损失和优化器[](#working-with-multiple-models-losses-and-optimizers "跳转到此标题")

如果您的网络有多个损失，您必须分别对每个损失调用`scaler.scale`。如果您的网络有多个优化器，您可以分别对其中任何一个调用`scaler.unscale_`，并且您必须分别对每个调用`scaler.step`。

然而，`scaler.update`应该只被调用一次，在本次迭代中所有优化器都已经执行完步骤之后：

```py
scaler = torch.cuda.amp.GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer0.zero_grad()
        optimizer1.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            output0 = model0(input)
            output1 = model1(input)
            loss0 = loss_fn(2 * output0 + 3 * output1, target)
            loss1 = loss_fn(3 * output0 - 5 * output1, target)

        # (retain_graph here is unrelated to amp, it's present because in this
        # example, both backward() calls share some sections of graph.)
        scaler.scale(loss0).backward(retain_graph=True)
        scaler.scale(loss1).backward()

        # You can choose which optimizers receive explicit unscaling, if you
        # want to inspect or modify the gradients of the params they own.
        scaler.unscale_(optimizer0)

        scaler.step(optimizer0)
        scaler.step(optimizer1)

        scaler.update() 
```

每个优化器都会检查其梯度是否包含无穷大/NaN，并独立决定是否跳过该步骤。这可能导致一个优化器跳过该步骤，而另一个不跳过。由于很少发生跳过步骤（每几百次迭代一次），这不应该影响收敛。如果您在将梯度缩放添加到多优化器模型后观察到收敛不佳，请报告错误。

## 使用多个 GPU

这里描述的问题只影响`autocast`。`GradScaler`的使用方式没有改变。

### 单进程中的 DataParallel

即使`torch.nn.DataParallel`会生成线程来在每个设备上运行前向传播。自动转换状态在每个线程中传播，并且以下操作将起作用：

```py
model = MyModel()
dp_model = nn.DataParallel(model)

# Sets autocast in the main thread
with autocast(device_type='cuda', dtype=torch.float16):
    # dp_model's internal threads will autocast.
    output = dp_model(input)
    # loss_fn also autocast
    loss = loss_fn(output) 
```

### 分布式数据并行，每个进程一个 GPU

`torch.nn.parallel.DistributedDataParallel`的文档建议每个进程一个 GPU 以获得最佳性能。在这种情况下，`DistributedDataParallel`不会在内部生成线程，因此对`autocast`和`GradScaler`的使用不受影响。

### 分布式数据并行，每个进程多个 GPU

在这里，`torch.nn.parallel.DistributedDataParallel`可能会生成一个辅助线程来在每个设备上运行前向传播，就像`torch.nn.DataParallel`一样。修复方法相同：在模型的`forward`方法中应用 autocast，以确保在辅助线程中启用它。## 自动转换和自定义自动求导函数

如果您的网络使用自定义自动求导函数（`torch.autograd.Function`的子类），则需要对自动转换兼容性进行更改，如果任何函数

+   接受多个浮点张量输入，

+   包装任何可自动转换的操作（参见自动转换操作参考，或者

+   需要特定的`dtype`（例如，如果它包装了仅为`dtype`编译的[CUDA 扩展](https://pytorch.org/tutorials/advanced/cpp_extension.html)）。

在所有情况下，如果您正在导入该函数并且无法更改其定义，一个安全的备选方案是在出现错误的任何使用点禁用自动转换并强制执行为`float32`（或`dtype`）：

```py
with autocast(device_type='cuda', dtype=torch.float16):
    ...
    with autocast(device_type='cuda', dtype=torch.float16, enabled=False):
        output = imported_function(input1.float(), input2.float()) 
```

如果您是函数的作者（或可以更改其定义），更好的解决方案是在下面相关案例中所示使用`torch.cuda.amp.custom_fwd()`和`torch.cuda.amp.custom_bwd()`装饰器。

### 具有多个输入或可自动转换操作的函数

将`custom_fwd`和`custom_bwd`（不带参数）分别应用于`forward`和`backward`。这些确保`forward`以当前自动转换状态执行，`backward`以与`forward`相同的自动转换状态执行（可以防止类型不匹配错误）：

```py
class MyMM(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(ctx, a, b):
        ctx.save_for_backward(a, b)
        return a.mm(b)
    @staticmethod
    @custom_bwd
    def backward(ctx, grad):
        a, b = ctx.saved_tensors
        return grad.mm(b.t()), a.t().mm(grad) 
```

现在`MyMM`可以在任何地方调用，而无需禁用自动转换或手动转换输入：

```py
mymm = MyMM.apply

with autocast(device_type='cuda', dtype=torch.float16):
    output = mymm(input1, input2) 
```

### 需要特定`dtype`的函数

考虑一个需要`torch.float32`输入的自定义函数。将`custom_fwd(cast_inputs=torch.float32)`应用于`forward`，将`custom_bwd`（不带参数）应用于`backward`。如果`forward`在启用自动转换的区域运行，则装饰器将浮点 CUDA 张量输入转换为`float32`，并在`forward`和`backward`期间本地禁用自动转换：

```py
class MyFloat32Func(torch.autograd.Function):
    @staticmethod
    @custom_fwd(cast_inputs=torch.float32)
    def forward(ctx, input):
        ctx.save_for_backward(input)
        ...
        return fwd_output
    @staticmethod
    @custom_bwd
    def backward(ctx, grad):
        ... 
```

现在`MyFloat32Func`可以在任何地方调用，而无需手动禁用自动转换或转换输入：

```py
func = MyFloat32Func.apply

with autocast(device_type='cuda', dtype=torch.float16):
    # func will run in float32, regardless of the surrounding autocast state
    output = func(input) 
```
