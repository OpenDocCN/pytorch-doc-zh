# 模型和预训练权重

> 原文：[`pytorch.org/vision/stable/models.html`](https://pytorch.org/vision/stable/models.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)


`torchvision.models`子包含有用于解决不同任务的模型的定义，包括：图像分类、像素级语义分割、目标检测、实例分割、人体关键点检测、视频分类和光流。

## 有关预训练权重的一般信息

TorchVision 为每个提供的架构提供了预训练权重，使用 PyTorch [`torch.hub`](https://pytorch.org/docs/stable/hub.html#module-torch.hub "(在 PyTorch v2.2 中)")。实例化预训练模型将下载其权重到缓存目录。可以使用 TORCH_HOME 环境变量设置此目录。有关详细信息，请参阅[`torch.hub.load_state_dict_from_url()`](https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url "(在 PyTorch v2.2 中)")。

注意

此库中提供的预训练模型可能具有根据用于训练的数据集派生的自己的许可证或条款。您有责任确定是否有权限将这些模型用于您的用例。

注意

对于将序列化的`state_dict`加载到使用旧版本 PyTorch 创建的模型，向后兼容性是有保证的。相反，加载整个保存的模型或序列化的`ScriptModules`（使用旧版本 PyTorch 序列化）可能不会保留历史行为。请参考以下[文档](https://pytorch.org/docs/stable/notes/serialization.html#id6)

### 初始化预训练模型

从 v0.13 开始，TorchVision 提供了一个新的[多权重支持 API](https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/)，用于将不同权重加载到现有的模型构建器方法中：

```py
from torchvision.models import resnet50, ResNet50_Weights

# Old weights with accuracy 76.130%
resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)

# New weights with accuracy 80.858%
resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)

# Best available weights (currently alias for IMAGENET1K_V2)
# Note that these weights may change across versions
resnet50(weights=ResNet50_Weights.DEFAULT)

# Strings are also supported
resnet50(weights="IMAGENET1K_V2")

# No weights - random initialization
resnet50(weights=None) 
```

迁移到新 API 非常简单。在这两个 API 之间的以下方法调用是等效的：

```py
from torchvision.models import resnet50, ResNet50_Weights

# Using pretrained weights:
resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
resnet50(weights="IMAGENET1K_V1")
resnet50(pretrained=True)  # deprecated
resnet50(True)  # deprecated

# Using no weights:
resnet50(weights=None)
resnet50()
resnet50(pretrained=False)  # deprecated
resnet50(False)  # deprecated 
```

请注意，`pretrained`参数现在已弃用，使用它将发出警告，并将在 v0.15 中删除。

### 使用预训练模型

在使用预训练模型之前，必须对图像进行预处理（调整大小以获得正确的分辨率/插值，应用推理变换，重新缩放值等）。没有标准的方法可以做到这一点，因为它取决于给定模型的训练方式。它可能会因模型系列、变体甚至权重版本而有所不同。使用正确的预处理方法至关重要，否则可能导致准确性降低或输出不正确。

每个预训练模型的推理变换的所有必要信息都在其权重文档中提供。为了简化推理，TorchVision 将必要的预处理变换捆绑到每个模型权重中。这些可以通过`weight.transforms`属性访问：

```py
# Initialize the Weight Transforms
weights = ResNet50_Weights.DEFAULT
preprocess = weights.transforms()

# Apply it to the input image
img_transformed = preprocess(img) 
```

有些模型使用具有不同训练和评估行为的模块，例如批量归一化。要在这些模式之间切换，请适当使用`model.train()`或`model.eval()`。有关详细信息，请参阅[`train()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train "(在 PyTorch v2.2 中)")或[`eval()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval "(在 PyTorch v2.2 中)")。

```py
# Initialize model
weights = ResNet50_Weights.DEFAULT
model = resnet50(weights=weights)

# Set model to eval mode
model.eval() 
```

### 列出和检索可用模型

从 v0.14 开始，TorchVision 提供了一种新机制，允许按名称列出和检索模型和权重。以下是如何使用它们的几个示例：

```py
# List available models
all_models = list_models()
classification_models = list_models(module=torchvision.models)

# Initialize models
m1 = get_model("mobilenet_v3_large", weights=None)
m2 = get_model("quantized_mobilenet_v3_large", weights="DEFAULT")

# Fetch weights
weights = get_weight("MobileNet_V3_Large_QuantizedWeights.DEFAULT")
assert weights == MobileNet_V3_Large_QuantizedWeights.DEFAULT

weights_enum = get_model_weights("quantized_mobilenet_v3_large")
assert weights_enum == MobileNet_V3_Large_QuantizedWeights

weights_enum2 = get_model_weights(torchvision.models.quantization.mobilenet_v3_large)
assert weights_enum == weights_enum2 
```

以下是可用的公共函数，用于检索模型及其对应的权重：

| `get_model`(name, **config) | 获取模型名称和配置，并返回一个实例化的模型。 |
| --- | --- |
| `get_model_weights`(name) | 返回与给定模型关联的权重枚举类。 |
| `get_weight`(name) | 通过完整名称获取权重枚举值。 |
| `list_models`([module, include, exclude]) | 返回已注册模型名称的列表。 |

### 使用 Hub 中的模型

大多数预训练模型可以直接通过 PyTorch Hub 访问，无需安装 TorchVision：

```py
import torch

# Option 1: passing weights param as string
model = torch.hub.load("pytorch/vision", "resnet50", weights="IMAGENET1K_V2")

# Option 2: passing weights param as enum
weights = torch.hub.load("pytorch/vision", "get_weight", weights="ResNet50_Weights.IMAGENET1K_V2")
model = torch.hub.load("pytorch/vision", "resnet50", weights=weights) 
```

您还可以通过 PyTorch Hub 检索特定模型的所有可用权重：

```py
import torch

weight_enum = torch.hub.load("pytorch/vision", "get_model_weights", name="resnet50")
print([weight for weight in weight_enum]) 
```

上述唯一的例外是包含在 `torchvision.models.detection` 中的检测模型。这些模型需要安装 TorchVision，因为它们依赖于自定义的 C++ 运算符。

## 分类

以下分类模型可用，带有或不带有预训练权重：

+   AlexNet

+   ConvNeXt

+   DenseNet

+   EfficientNet

+   EfficientNetV2

+   GoogLeNet

+   Inception V3

+   MaxVit

+   MNASNet

+   MobileNet V2

+   MobileNet V3

+   RegNet

+   ResNet

+   ResNeXt

+   ShuffleNet V2

+   SqueezeNet

+   SwinTransformer

+   VGG

+   VisionTransformer

+   Wide ResNet

以下是如何使用预训练图像分类模型的示例：

```py
from torchvision.io import read_image
from torchvision.models import resnet50, ResNet50_Weights

img = read_image("test/assets/encode_jpeg/grace_hopper_517x606.jpg")

# Step 1: Initialize model with the best available weights
weights = ResNet50_Weights.DEFAULT
model = resnet50(weights=weights)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = preprocess(img).unsqueeze(0)

# Step 4: Use the model and print the predicted category
prediction = model(batch).squeeze(0).softmax(0)
class_id = prediction.argmax().item()
score = prediction[class_id].item()
category_name = weights.meta["categories"][class_id]
print(f"{category_name}: {100  *  score:.1f}%") 
```

预训练模型输出的类别可以在 `weights.meta["categories"]` 中找到。

### 所有可用分类权重的表格

在 ImageNet-1K 上使用单个裁剪报告准确性：

| **权重** | **Acc@1** | **Acc@5** | **参数** | **GFLOPS** | **Recipe** |
| --- | --- | --- | --- | --- | --- |
| `AlexNet_Weights.IMAGENET1K_V1` | 56.522 | 79.066 | 61.1M | 0.71 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| `ConvNeXt_Base_Weights.IMAGENET1K_V1` | 84.062 | 96.87 | 88.6M | 15.36 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#convnext) |
| `ConvNeXt_Large_Weights.IMAGENET1K_V1` | 84.414 | 96.976 | 197.8M | 34.36 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#convnext) |
| `ConvNeXt_Small_Weights.IMAGENET1K_V1` | 83.616 | 96.65 | 50.2M | 8.68 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#convnext) |
| `ConvNeXt_Tiny_Weights.IMAGENET1K_V1` | 82.52 | 96.146 | 28.6M | 4.46 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#convnext) |
| `DenseNet121_Weights.IMAGENET1K_V1` | 74.434 | 91.972 | 8.0M | 2.83 | [链接](https://github.com/pytorch/vision/pull/116) |
| `DenseNet161_Weights.IMAGENET1K_V1` | 77.138 | 93.56 | 28.7M | 7.73 | [链接](https://github.com/pytorch/vision/pull/116) |
| `DenseNet169_Weights.IMAGENET1K_V1` | 75.6 | 92.806 | 14.1M | 3.36 | [链接](https://github.com/pytorch/vision/pull/116) |
| `DenseNet201_Weights.IMAGENET1K_V1` | 76.896 | 93.37 | 20.0M | 4.29 | [链接](https://github.com/pytorch/vision/pull/116) |
| `EfficientNet_B0_Weights.IMAGENET1K_V1` | 77.692 | 93.532 | 5.3M | 0.39 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| `EfficientNet_B1_Weights.IMAGENET1K_V1` | 78.642 | 94.186 | 7.8M | 0.69 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| `EfficientNet_B1_Weights.IMAGENET1K_V2` | 79.838 | 94.934 | 7.8M | 0.69 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe-with-lr-wd-crop-tuning) |
| `EfficientNet_B2_Weights.IMAGENET1K_V1` | 80.608 | 95.31 | 9.1M | 1.09 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| `EfficientNet_B3_Weights.IMAGENET1K_V1` | 82.008 | 96.054 | 12.2M | 1.83 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| `EfficientNet_B4_Weights.IMAGENET1K_V1` | 83.384 | 96.594 | 19.3M | 4.39 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| `EfficientNet_B5_Weights.IMAGENET1K_V1` | 83.444 | 96.628 | 30.4M | 10.27 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| `EfficientNet_B6_Weights.IMAGENET1K_V1` | 84.008 | 96.916 | 43.0M | 19.07 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| `EfficientNet_B7_Weights.IMAGENET1K_V1` | 84.122 | 96.908 | 66.3M | 37.75 | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1) |
| `EfficientNet_V2_L_Weights.IMAGENET1K_V1` | 85.808 | 97.788 | 118.5M | 56.08 | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2) |
| `EfficientNet_V2_M_Weights.IMAGENET1K_V1` | 85.112 | 97.156 | 54.1M | 24.58 | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2) |
| `EfficientNet_V2_S_Weights.IMAGENET1K_V1` | 84.228 | 96.878 | 21.5M | 8.37 | [link](https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2) |
| `GoogLeNet_Weights.IMAGENET1K_V1` | 69.778 | 89.53 | 6.6M | 1.5 | [link](https://github.com/pytorch/vision/tree/main/references/classification#googlenet) |
| `Inception_V3_Weights.IMAGENET1K_V1` | 77.294 | 93.45 | 27.2M | 5.71 | [link](https://github.com/pytorch/vision/tree/main/references/classification#inception-v3) |
| `MNASNet0_5_Weights.IMAGENET1K_V1` | 67.734 | 87.49 | 2.2M | 0.1 | [link](https://github.com/1e100/mnasnet_trainer) |
| `MNASNet0_75_Weights.IMAGENET1K_V1` | 71.18 | 90.496 | 3.2M | 0.21 | [link](https://github.com/pytorch/vision/pull/6019) |
| `MNASNet1_0_Weights.IMAGENET1K_V1` | 73.456 | 91.51 | 4.4M | 0.31 | [link](https://github.com/1e100/mnasnet_trainer) |
| `MNASNet1_3_Weights.IMAGENET1K_V1` | 76.506 | 93.522 | 6.3M | 0.53 | [link](https://github.com/pytorch/vision/pull/6019) |
| `MaxVit_T_Weights.IMAGENET1K_V1` | 83.7 | 96.722 | 30.9M | 5.56 | [link](https://github.com/pytorch/vision/tree/main/references/classification#maxvit) |
| `MobileNet_V2_Weights.IMAGENET1K_V1` | 71.878 | 90.286 | 3.5M | 0.3 | [link](https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv2) |
| `MobileNet_V2_Weights.IMAGENET1K_V2` | 72.154 | 90.822 | 3.5M | 0.3 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning) |
| `MobileNet_V3_Large_Weights.IMAGENET1K_V1` | 74.042 | 91.34 | 5.5M | 0.22 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv3-large--small) |
| `MobileNet_V3_Large_Weights.IMAGENET1K_V2` | 75.274 | 92.566 | 5.5M | 0.22 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning) |
| `MobileNet_V3_Small_Weights.IMAGENET1K_V1` | 67.668 | 87.402 | 2.5M | 0.06 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv3-large--small) |
| `RegNet_X_16GF_Weights.IMAGENET1K_V1` | 80.058 | 94.944 | 54.3M | 15.94 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#medium-models) |
| `RegNet_X_16GF_Weights.IMAGENET1K_V2` | 82.716 | 96.196 | 54.3M | 15.94 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `RegNet_X_1_6GF_Weights.IMAGENET1K_V1` | 77.04 | 93.44 | 9.2M | 1.6 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| `RegNet_X_1_6GF_Weights.IMAGENET1K_V2` | 79.668 | 94.922 | 9.2M | 1.6 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |
| `RegNet_X_32GF_Weights.IMAGENET1K_V1` | 80.622 | 95.248 | 107.8M | 31.74 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#large-models) |
| `RegNet_X_32GF_Weights.IMAGENET1K_V2` | 83.014 | 96.288 | 107.8M | 31.74 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `RegNet_X_3_2GF_Weights.IMAGENET1K_V1` | 78.364 | 93.992 | 15.3M | 3.18 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#medium-models) |
| `RegNet_X_3_2GF_Weights.IMAGENET1K_V2` | 81.196 | 95.43 | 15.3M | 3.18 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `RegNet_X_400MF_Weights.IMAGENET1K_V1` | 72.834 | 90.95 | 5.5M | 0.41 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| `RegNet_X_400MF_Weights.IMAGENET1K_V2` | 74.864 | 92.322 | 5.5M | 0.41 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |
| `RegNet_X_800MF_Weights.IMAGENET1K_V1` | 75.212 | 92.348 | 7.3M | 0.8 | [link](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| `RegNet_X_800MF_Weights.IMAGENET1K_V2` | 77.522 | 93.826 | 7.3M | 0.8 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |
| `RegNet_X_8GF_Weights.IMAGENET1K_V1` | 79.344 | 94.686 | 39.6M | 8 | [link](https://github.com/pytorch/vision/tree/main/references/classification#medium-models) |
| `RegNet_X_8GF_Weights.IMAGENET1K_V2` | 81.682 | 95.678 | 39.6M | 8 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_E2E_V1` | 88.228 | 98.682 | 644.8M | 374.57 | [link](https://github.com/facebookresearch/SWAG) |
| `RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_LINEAR_V1` | 86.068 | 97.844 | 644.8M | 127.52 | [link](https://github.com/pytorch/vision/pull/5793) |
| `RegNet_Y_16GF_Weights.IMAGENET1K_V1` | 80.424 | 95.24 | 83.6M | 15.91 | [link](https://github.com/pytorch/vision/tree/main/references/classification#large-models) |
| `RegNet_Y_16GF_Weights.IMAGENET1K_V2` | 82.886 | 96.328 | 83.6M | 15.91 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1` | 86.012 | 98.054 | 83.6M | 46.73 | [link](https://github.com/facebookresearch/SWAG) |
| `RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_LINEAR_V1` | 83.976 | 97.244 | 83.6M | 15.91 | [link](https://github.com/pytorch/vision/pull/5793) |
| `RegNet_Y_1_6GF_Weights.IMAGENET1K_V1` | 77.95 | 93.966 | 11.2M | 1.61 | [link](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| `RegNet_Y_1_6GF_Weights.IMAGENET1K_V2` | 80.876 | 95.444 | 11.2M | 1.61 | [link](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `RegNet_Y_32GF_Weights.IMAGENET1K_V1` | 80.878 | 95.34 | 145.0M | 32.28 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#large-models) |
| `RegNet_Y_32GF_Weights.IMAGENET1K_V2` | 83.368 | 96.498 | 145.0M | 32.28 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_E2E_V1` | 86.838 | 98.362 | 145.0M | 94.83 | [链接](https://github.com/facebookresearch/SWAG) |
| `RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_LINEAR_V1` | 84.622 | 97.48 | 145.0M | 32.28 | [链接](https://github.com/pytorch/vision/pull/5793) |
| `RegNet_Y_3_2GF_Weights.IMAGENET1K_V1` | 78.948 | 94.576 | 19.4M | 3.18 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#medium-models) |
| `RegNet_Y_3_2GF_Weights.IMAGENET1K_V2` | 81.982 | 95.972 | 19.4M | 3.18 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `RegNet_Y_400MF_Weights.IMAGENET1K_V1` | 74.046 | 91.716 | 4.3M | 0.4 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| `RegNet_Y_400MF_Weights.IMAGENET1K_V2` | 75.804 | 92.742 | 4.3M | 0.4 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `RegNet_Y_800MF_Weights.IMAGENET1K_V1` | 76.42 | 93.136 | 6.4M | 0.83 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#small-models) |
| `RegNet_Y_800MF_Weights.IMAGENET1K_V2` | 78.828 | 94.502 | 6.4M | 0.83 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `RegNet_Y_8GF_Weights.IMAGENET1K_V1` | 80.032 | 95.048 | 39.4M | 8.47 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#medium-models) |
| `RegNet_Y_8GF_Weights.IMAGENET1K_V2` | 82.828 | 96.33 | 39.4M | 8.47 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `ResNeXt101_32X8D_Weights.IMAGENET1K_V1` | 79.312 | 94.526 | 88.8M | 16.41 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnext) |
| `ResNeXt101_32X8D_Weights.IMAGENET1K_V2` | 82.834 | 96.228 | 88.8M | 16.41 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |
| `ResNeXt101_64X4D_Weights.IMAGENET1K_V1` | 83.246 | 96.454 | 83.5M | 15.46 | [链接](https://github.com/pytorch/vision/pull/5935) |
| `ResNeXt50_32X4D_Weights.IMAGENET1K_V1` | 77.618 | 93.698 | 25.0M | 4.23 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnext) |
| `ResNeXt50_32X4D_Weights.IMAGENET1K_V2` | 81.198 | 95.34 | 25.0M | 4.23 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `ResNet101_Weights.IMAGENET1K_V1` | 77.374 | 93.546 | 44.5M | 7.8 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnet) |
| `ResNet101_Weights.IMAGENET1K_V2` | 81.886 | 95.78 | 44.5M | 7.8 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `ResNet152_Weights.IMAGENET1K_V1` | 78.312 | 94.046 | 60.2M | 11.51 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnet) |
| `ResNet152_Weights.IMAGENET1K_V2` | 82.284 | 96.002 | 60.2M | 11.51 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `ResNet18_Weights.IMAGENET1K_V1` | 69.758 | 89.078 | 11.7M | 1.81 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnet) |
| `ResNet34_Weights.IMAGENET1K_V1` | 73.314 | 91.42 | 21.8M | 3.66 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnet) |
| `ResNet50_Weights.IMAGENET1K_V1` | 76.13 | 92.862 | 25.6M | 4.09 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#resnet) |
| `ResNet50_Weights.IMAGENET1K_V2` | 80.858 | 95.434 | 25.6M | 4.09 | [链接](https://github.com/pytorch/vision/issues/3995#issuecomment-1013906621) |
| `ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1` | 60.552 | 81.746 | 1.4M | 0.04 | [链接](https://github.com/ericsun99/Shufflenet-v2-Pytorch) |
| `ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1` | 69.362 | 88.316 | 2.3M | 0.14 | [链接](https://github.com/ericsun99/Shufflenet-v2-Pytorch) |
| `ShuffleNet_V2_X1_5_Weights.IMAGENET1K_V1` | 72.996 | 91.086 | 3.5M | 0.3 | [链接](https://github.com/pytorch/vision/pull/5906) |
| `ShuffleNet_V2_X2_0_Weights.IMAGENET1K_V1` | 76.23 | 93.006 | 7.4M | 0.58 | [链接](https://github.com/pytorch/vision/pull/5906) |
| `SqueezeNet1_0_Weights.IMAGENET1K_V1` | 58.092 | 80.42 | 1.2M | 0.82 | [链接](https://github.com/pytorch/vision/pull/49#issuecomment-277560717) |
| `SqueezeNet1_1_Weights.IMAGENET1K_V1` | 58.178 | 80.624 | 1.2M | 0.35 | [链接](https://github.com/pytorch/vision/pull/49#issuecomment-277560717) |
| `Swin_B_Weights.IMAGENET1K_V1` | 83.582 | 96.64 | 87.8M | 15.43 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer) |
| `Swin_S_Weights.IMAGENET1K_V1` | 83.196 | 96.36 | 49.6M | 8.74 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer) |
| `Swin_T_Weights.IMAGENET1K_V1` | 81.474 | 95.776 | 28.3M | 4.49 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer) |
| `Swin_V2_B_Weights.IMAGENET1K_V1` | 84.112 | 96.864 | 87.9M | 20.32 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2) |
| `Swin_V2_S_Weights.IMAGENET1K_V1` | 83.712 | 96.816 | 49.7M | 11.55 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2) |
| `Swin_V2_T_Weights.IMAGENET1K_V1` | 82.072 | 96.132 | 28.4M | 5.94 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2) |
| `VGG11_BN_Weights.IMAGENET1K_V1` | 70.37 | 89.81 | 132.9M | 7.61 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| `VGG11_Weights.IMAGENET1K_V1` | 69.02 | 88.628 | 132.9M | 7.61 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| `VGG13_BN_Weights.IMAGENET1K_V1` | 71.586 | 90.374 | 133.1M | 11.31 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| `VGG13_Weights.IMAGENET1K_V1` | 69.928 | 89.246 | 133.0M | 11.31 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| `VGG16_BN_Weights.IMAGENET1K_V1` | 73.36 | 91.516 | 138.4M | 15.47 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| `VGG16_Weights.IMAGENET1K_V1` | 71.592 | 90.382 | 138.4M | 15.47 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| `VGG16_Weights.IMAGENET1K_FEATURES` | nan | nan | 138.4M | 15.47 | [链接](https://github.com/amdegroot/ssd.pytorch#training-ssd) |
| `VGG19_BN_Weights.IMAGENET1K_V1` | 74.218 | 91.842 | 143.7M | 19.63 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| `VGG19_Weights.IMAGENET1K_V1` | 72.376 | 90.876 | 143.7M | 19.63 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg) |
| `ViT_B_16_Weights.IMAGENET1K_V1` | 81.072 | 95.318 | 86.6M | 17.56 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#vit_b_16) |
| `ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1` | 85.304 | 97.65 | 86.9M | 55.48 | [链接](https://github.com/facebookresearch/SWAG) |
| `ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1` | 81.886 | 96.18 | 86.6M | 17.56 | [链接](https://github.com/pytorch/vision/pull/5793) |
| `ViT_B_32_Weights.IMAGENET1K_V1` | 75.912 | 92.466 | 88.2M | 4.41 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#vit_b_32) |
| `ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1` | 88.552 | 98.694 | 633.5M | 1016.72 | [链接](https://github.com/facebookresearch/SWAG) |
| `ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1` | 85.708 | 97.73 | 632.0M | 167.29 | [链接](https://github.com/pytorch/vision/pull/5793) |
| `ViT_L_16_Weights.IMAGENET1K_V1` | 79.662 | 94.638 | 304.3M | 61.55 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#vit_l_16) |
| `ViT_L_16_Weights.IMAGENET1K_SWAG_E2E_V1` | 88.064 | 98.512 | 305.2M | 361.99 | [链接](https://github.com/facebookresearch/SWAG) |
| `ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1` | 85.146 | 97.422 | 304.3M | 61.55 | [链接](https://github.com/pytorch/vision/pull/5793) |
| `ViT_L_32_Weights.IMAGENET1K_V1` | 76.972 | 93.07 | 306.5M | 15.38 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#vit_l_32) |
| `Wide_ResNet101_2_Weights.IMAGENET1K_V1` | 78.848 | 94.284 | 126.9M | 22.75 | [链接](https://github.com/pytorch/vision/pull/912#issue-445437439) |
| `Wide_ResNet101_2_Weights.IMAGENET1K_V2` | 82.51 | 96.02 | 126.9M | 22.75 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe) |
| `Wide_ResNet50_2_Weights.IMAGENET1K_V1` | 78.468 | 94.086 | 68.9M | 11.4 | [链接](https://github.com/pytorch/vision/pull/912#issue-445437439) |
| `Wide_ResNet50_2_Weights.IMAGENET1K_V2` | 81.602 | 95.758 | 68.9M | 11.4 | [链接](https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres) |

### 量化模型

以下架构支持带有或不带预训练权重的 INT8 量化模型：

+   量化的 GoogLeNet

+   量化的 InceptionV3

+   量化的 MobileNet V2

+   量化的 MobileNet V3

+   量化的 ResNet

+   量化的 ResNeXt

+   量化的 ShuffleNet V2

以下是如何使用预训练的量化图像分类模型的示例：

```py
from torchvision.io import read_image
from torchvision.models.quantization import resnet50, ResNet50_QuantizedWeights

img = read_image("test/assets/encode_jpeg/grace_hopper_517x606.jpg")

# Step 1: Initialize model with the best available weights
weights = ResNet50_QuantizedWeights.DEFAULT
model = resnet50(weights=weights, quantize=True)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = preprocess(img).unsqueeze(0)

# Step 4: Use the model and print the predicted category
prediction = model(batch).squeeze(0).softmax(0)
class_id = prediction.argmax().item()
score = prediction[class_id].item()
category_name = weights.meta["categories"][class_id]
print(f"{category_name}: {100  *  score}%") 
```

预训练模型输出的类别可以在`weights.meta["categories"]`中找到。

#### 所有可用的量化分类权重表

准确率是在 ImageNet-1K 上使用单个裁剪报告的：

| **权重** | **准确率@1** | **准确率@5** | **参数** | **GIPS** | **配方** |
| --- | --- | --- | --- | --- | --- |
| `GoogLeNet_QuantizedWeights.IMAGENET1K_FBGEMM_V1` | 69.826 | 89.404 | 6.6M | 1.5 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| `Inception_V3_QuantizedWeights.IMAGENET1K_FBGEMM_V1` | 77.176 | 93.354 | 27.2M | 5.71 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| `MobileNet_V2_QuantizedWeights.IMAGENET1K_QNNPACK_V1` | 71.658 | 90.15 | 3.5M | 0.3 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#qat-mobilenetv2) |
| `MobileNet_V3_Large_QuantizedWeights.IMAGENET1K_QNNPACK_V1` | 73.004 | 90.858 | 5.5M | 0.22 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#qat-mobilenetv3) |
| `ResNeXt101_32X8D_QuantizedWeights.IMAGENET1K_FBGEMM_V1` | 78.986 | 94.48 | 88.8M | 16.41 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| `ResNeXt101_32X8D_QuantizedWeights.IMAGENET1K_FBGEMM_V2` | 82.574 | 96.132 | 88.8M | 16.41 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| `ResNeXt101_64X4D_QuantizedWeights.IMAGENET1K_FBGEMM_V1` | 82.898 | 96.326 | 83.5M | 15.46 | [链接](https://github.com/pytorch/vision/pull/5935) |
| `ResNet18_QuantizedWeights.IMAGENET1K_FBGEMM_V1` | 69.494 | 88.882 | 11.7M | 1.81 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| `ResNet50_QuantizedWeights.IMAGENET1K_FBGEMM_V1` | 75.92 | 92.814 | 25.6M | 4.09 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| `ResNet50_QuantizedWeights.IMAGENET1K_FBGEMM_V2` | 80.282 | 94.976 | 25.6M | 4.09 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| `ShuffleNet_V2_X0_5_QuantizedWeights.IMAGENET1K_FBGEMM_V1` | 57.972 | 79.78 | 1.4M | 0.04 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| `ShuffleNet_V2_X1_0_QuantizedWeights.IMAGENET1K_FBGEMM_V1` | 68.36 | 87.582 | 2.3M | 0.14 | [链接](https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models) |
| `ShuffleNet_V2_X1_5_QuantizedWeights.IMAGENET1K_FBGEMM_V1` | 72.052 | 90.7 | 3.5M | 0.3 | [链接](https://github.com/pytorch/vision/pull/5906) |
| `ShuffleNet_V2_X2_0_QuantizedWeights.IMAGENET1K_FBGEMM_V1` | 75.354 | 92.488 | 7.4M | 0.58 | [链接](https://github.com/pytorch/vision/pull/5906) |

## 语义分割

警告

分割模块处于 Beta 阶段，不保证向后兼容性。

以下语义分割模型可用，带或不带预训练权重：

+   DeepLabV3

+   FCN

+   LRASPP

以下是如何使用预训练语义分割模型的示例：

```py
from torchvision.io.image import read_image
from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights
from torchvision.transforms.functional import to_pil_image

img = read_image("gallery/assets/dog1.jpg")

# Step 1: Initialize model with the best available weights
weights = FCN_ResNet50_Weights.DEFAULT
model = fcn_resnet50(weights=weights)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = preprocess(img).unsqueeze(0)

# Step 4: Use the model and visualize the prediction
prediction = model(batch)["out"]
normalized_masks = prediction.softmax(dim=1)
class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta["categories"])}
mask = normalized_masks[0, class_to_idx["dog"]]
to_pil_image(mask).show() 
```

预训练模型输出的类别可以在 `weights.meta["categories"]` 中找到。模型的输出格式在语义分割模型中有说明。

### 所有可用语义分割权重的表格

所有模型都是在 COCO val2017 的子集上评估的，涵盖了 Pascal VOC 数据集中存在的 20 个类别：

| **权重** | **平均 IoU** | **像素准确率** | **参数** | **GFLOPS** | **配置** |
| --- | --- | --- | --- | --- | --- |
| `DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1` | 60.3 | 91.2 | 11.0M | 10.45 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_mobilenet_v3_large) |
| `DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1` | 67.4 | 92.4 | 61.0M | 258.74 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#fcn_resnet101) |
| `DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1` | 66.4 | 92.4 | 42.0M | 178.72 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_resnet50) |
| `FCN_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1` | 63.7 | 91.9 | 54.3M | 232.74 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_resnet101) |
| `FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1` | 60.5 | 91.4 | 35.3M | 152.72 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#fcn_resnet50) |
| `LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1` | 57.9 | 91.2 | 3.2M | 2.09 | [链接](https://github.com/pytorch/vision/tree/main/references/segmentation#lraspp_mobilenet_v3_large) |

## 目标检测、实例分割和人体关键点检测

检测、实例分割和关键点检测的预训练模型是使用 torchvision 中的分类模型初始化的。这些模型期望一个`Tensor[C, H, W]`列表。查看模型的构造函数以获取更多信息。

警告

检测模块处于 Beta 阶段，不保证向后兼容性。

### 目标检测

以下目标检测模型可用，有或没有预训练权重：

+   Faster R-CNN

+   FCOS

+   RetinaNet

+   SSD

+   SSDlite

以下是如何使用预训练目标检测模型的示例：

```py
from torchvision.io.image import read_image
from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image

img = read_image("test/assets/encode_jpeg/grace_hopper_517x606.jpg")

# Step 1: Initialize model with the best available weights
weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = [preprocess(img)]

# Step 4: Use the model and visualize the prediction
prediction = model(batch)[0]
labels = [weights.meta["categories"][i] for i in prediction["labels"]]
box = draw_bounding_boxes(img, boxes=prediction["boxes"],
                          labels=labels,
                          colors="red",
                          width=4, font_size=30)
im = to_pil_image(box.detach())
im.show() 
```

预训练模型输出的类别可以在`weights.meta["categories"]`中找到。有关如何绘制模型边界框的详细信息，您可以参考实例分割模型。

#### 所有可用的目标检测权重表

在 COCO val2017 上报告了 Box MAPs：

| **权重** | **Box MAP** | **参数** | **GFLOPS** | **Recipe** |
| --- | --- | --- | --- | --- |
| `FCOS_ResNet50_FPN_Weights.COCO_V1` | 39.2 | 32.3M | 128.21 | [链接](https://github.com/pytorch/vision/tree/main/references/detection#fcos-resnet-50-fpn) |
| `FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.COCO_V1` | 22.8 | 19.4M | 0.72 | [链接](https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-mobilenetv3-large-320-fpn) |
| `FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1` | 32.8 | 19.4M | 4.49 | [链接](https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-mobilenetv3-large-fpn) |
| `FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1` | 46.7 | 43.7M | 280.37 | [链接](https://github.com/pytorch/vision/pull/5763) |
| `FasterRCNN_ResNet50_FPN_Weights.COCO_V1` | 37 | 41.8M | 134.38 | [链接](https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-resnet-50-fpn) |
| `RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1` | 41.5 | 38.2M | 152.24 | [link](https://github.com/pytorch/vision/pull/5756) |
| `RetinaNet_ResNet50_FPN_Weights.COCO_V1` | 36.4 | 34.0M | 151.54 | [link](https://github.com/pytorch/vision/tree/main/references/detection#retinanet) |
| `SSD300_VGG16_Weights.COCO_V1` | 25.1 | 35.6M | 34.86 | [link](https://github.com/pytorch/vision/tree/main/references/detection#ssd300-vgg16) |
| `SSDLite320_MobileNet_V3_Large_Weights.COCO_V1` | 21.3 | 3.4M | 0.58 | [link](https://github.com/pytorch/vision/tree/main/references/detection#ssdlite320-mobilenetv3-large) |

### 实例分割

以下是可用的实例分割模型，带或不带预训练权重：

+   Mask R-CNN

有关如何绘制模型的蒙版的详细信息，您可以参考实例分割模型。

#### 所有可用实例分割权重的表格

在 COCO val2017 上报告了框和蒙版 MAPs：

| **权重** | **框 MAP** | **蒙版 MAP** | **参数** | **GFLOPS** | **链接** |
| --- | --- | --- | --- | --- | --- |
| `MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1` | 47.4 | 41.8 | 46.4M | 333.58 | [link](https://github.com/pytorch/vision/pull/5773) |
| `MaskRCNN_ResNet50_FPN_Weights.COCO_V1` | 37.9 | 34.6 | 44.4M | 134.38 | [link](https://github.com/pytorch/vision/tree/main/references/detection#mask-r-cnn) |

### 关键点检测

以下是可用的人体关键点检测模型，带或不带预训练权重：

+   Keypoint R-CNN

预训练模型输出的类别可以在`weights.meta["keypoint_names"]`中找到。有关如何绘制模型的边界框的详细信息，您可以参考可视化关键点。

#### 所有可用关键点检测权重的表格

在 COCO val2017 上报告了框和关键点 MAPs：

| **权重** | **框 MAP** | **关键点 MAP** | **参数** | **GFLOPS** | **链接** |
| --- | --- | --- | --- | --- | --- |
| `KeypointRCNN_ResNet50_FPN_Weights.COCO_LEGACY` | 50.6 | 61.1 | 59.1M | 133.92 | [link](https://github.com/pytorch/vision/issues/1606) |
| `KeypointRCNN_ResNet50_FPN_Weights.COCO_V1` | 54.6 | 65 | 59.1M | 137.42 | [链接](https://github.com/pytorch/vision/tree/main/references/detection#keypoint-r-cnn) |

## 视频分类

警告

视频模块处于 Beta 阶段，不保证向后兼容性。

以下视频分类模型可用，带有或不带有预训练权重：

+   视频 MViT

+   视频 ResNet

+   视频 S3D

+   视频 SwinTransformer

以下是如何使用预训练视频分类模型的示例：

```py
from torchvision.io.video import read_video
from torchvision.models.video import r3d_18, R3D_18_Weights

vid, _, _ = read_video("test/assets/videos/v_SoccerJuggling_g23_c01.avi", output_format="TCHW")
vid = vid[:32]  # optionally shorten duration

# Step 1: Initialize model with the best available weights
weights = R3D_18_Weights.DEFAULT
model = r3d_18(weights=weights)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = preprocess(vid).unsqueeze(0)

# Step 4: Use the model and print the predicted category
prediction = model(batch).squeeze(0).softmax(0)
label = prediction.argmax().item()
score = prediction[label].item()
category_name = weights.meta["categories"][label]
print(f"{category_name}: {100  *  score}%") 
```

预训练模型输出的类别可以在`weights.meta["categories"]`中找到。

### 所有可用视频分类权重的表格

准确率是使用单个裁剪在剪辑长度为 16 的 Kinetics-400 上报告的：

| **权重** | **准确率@1** | **准确率@5** | **参数** | **GFLOPS** | **配置** |
| --- | --- | --- | --- | --- | --- |
| `MC3_18_Weights.KINETICS400_V1` | 63.96 | 84.13 | 11.7M | 43.34 | [链接](https://github.com/pytorch/vision/tree/main/references/video_classification) |
| `MViT_V1_B_Weights.KINETICS400_V1` | 78.477 | 93.582 | 36.6M | 70.6 | [链接](https://github.com/facebookresearch/pytorchvideo/blob/main/docs/source/model_zoo.md) |
| `MViT_V2_S_Weights.KINETICS400_V1` | 80.757 | 94.665 | 34.5M | 64.22 | [链接](https://github.com/facebookresearch/SlowFast/blob/main/MODEL_ZOO.md) |
| `R2Plus1D_18_Weights.KINETICS400_V1` | 67.463 | 86.175 | 31.5M | 40.52 | [链接](https://github.com/pytorch/vision/tree/main/references/video_classification) |
| `R3D_18_Weights.KINETICS400_V1` | 63.2 | 83.479 | 33.4M | 40.7 | [链接](https://github.com/pytorch/vision/tree/main/references/video_classification) |
| `S3D_Weights.KINETICS400_V1` | 68.368 | 88.05 | 8.3M | 17.98 | [链接](https://github.com/pytorch/vision/tree/main/references/video_classification#s3d) |
| `Swin3D_B_Weights.KINETICS400_V1` | 79.427 | 94.386 | 88.0M | 140.67 | [链接](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400) |
| `Swin3D_B_Weights.KINETICS400_IMAGENET22K_V1` | 81.643 | 95.574 | 88.0M | 140.67 | [链接](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400) |
| `Swin3D_S_Weights.KINETICS400_V1` | 79.521 | 94.158 | 49.8M | 82.84 | [链接](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400) |
| `Swin3D_T_Weights.KINETICS400_V1` | 77.715 | 93.519 | 28.2M | 43.88 | [链接](https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400) |

## 光流

以下光流模型可用，带有或不带有预训练

+   RAFT
