# torchtext.data.functional

> 原文：[`pytorch.org/text/stable/data_functional.html`](https://pytorch.org/text/stable/data_functional.html)
>
> 译者：[飞龙](https://github.com/wizardforcel)
>
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)


## generate_sp_model

```py
torchtext.data.functional.generate_sp_model(filename, vocab_size=20000, model_type='unigram', model_prefix='m_user')
```

训练一个 SentencePiece 标记器。

参数：

+   **filename** - 用于训练 SentencePiece 模型的数据文件。

+   **vocab_size** - 词汇表的大小（默认值：20,000）。

+   **model_type** - SentencePiece 模型的类型，包括 unigram、bpe、char、word。

+   **model_prefix** - 保存模型和词汇表文件的前缀。

输出：

模型和词汇表保存在两个单独的文件中

model_prefix。

示例

```py
>>> from torchtext.data.functional import generate_sp_model
>>> generate_sp_model('test.csv', vocab_size=23456, model_prefix='spm_user') 
```

## load_sp_model

```py
torchtext.data.functional.load_sp_model(spm)
```

加载一个用于文件的 sentencepiece 模型。

参数：

**spm** - 保存 sentencepiece 模型的文件路径或文件对象。

输出：

输出：一个 SentencePiece 模型。

示例

```py
>>> from torchtext.data.functional import load_sp_model
>>> sp_model = load_sp_model("m_user.model")
>>> sp_model = load_sp_model(open("m_user.model", 'rb')) 
```

## sentencepiece_numericalizer

```py
torchtext.data.functional.sentencepiece_numericalizer(sp_model)
```

一个 sentencepiece 模型，用于将文本句子数值化为

一个生成器，输出为 id。

参数：

**sp_model** - 一个 SentencePiece 模型。

输出：

输出：一个生成器，输入为文本句子，输出为

基于 SentencePiece 模型的相应 id。

示例

```py
>>> from torchtext.data.functional import sentencepiece_numericalizer
>>> sp_id_generator = sentencepiece_numericalizer(sp_model)
>>> list_a = ["sentencepiece encode as pieces", "examples to   try!"]
>>> list(sp_id_generator(list_a))
 [[9858, 9249, 1629, 1305, 1809, 53, 842],
 [2347, 13, 9, 150, 37]] 
```

## sentencepiece_tokenizer

```py
torchtext.data.functional.sentencepiece_tokenizer(sp_model)
```

一个 sentencepiece 模型，用于将文本句子标记为

一个令牌的生成器。

参数：

**sp_model** - 一个 SentencePiece 模型。

输出：

输出：一个生成器，输入为文本句子，输出为

基于 SentencePiece 模型的相应令牌。

示例

```py
>>> from torchtext.data.functional import sentencepiece_tokenizer
>>> sp_tokens_generator = sentencepiece_tokenizer(sp_model)
>>> list_a = ["sentencepiece encode as pieces", "examples to   try!"]
>>> list(sp_tokens_generator(list_a))
 [['_sentence', 'piece', '_en', 'co', 'de', '_as', '_pieces'],
 ['_example', 's', '_to', '_try', '!']] 
```

## custom_replace

```py
torchtext.data.functional.custom_replace(replace_pattern)
```

用于转换文本字符串的转换。

示例

```py
>>> from torchtext.data.functional import custom_replace
>>> custom_replace_transform = custom_replace([(r'S', 's'), (r'\s+', ' ')])
>>> list_a = ["Sentencepiece encode  aS  pieces", "exampleS to   try!"]
>>> list(custom_replace_transform(list_a))
 ['sentencepiece encode as pieces', 'examples to try!'] 
```

## simple_space_split

```py
torchtext.data.functional.simple_space_split(iterator)
```

用空格拆分文本字符串的转换。

示例

```py
>>> from torchtext.data.functional import simple_space_split
>>> list_a = ["Sentencepiece encode as pieces", "example to try!"]
>>> list(simple_space_split(list_a))
 [['Sentencepiece', 'encode', 'as', 'pieces'], ['example', 'to', 'try!']] 
```

## numericalize_tokens_from_iterator

```py
torchtext.data.functional.numericalize_tokens_from_iterator(vocab, iterator, removed_tokens=None)
```

从具有词汇表的令牌迭代器中产生一个 id 列表。

参数：

+   **vocab** - 将令牌转换为 id 的词汇表。

+   **iterator** - 产生一个令牌列表的迭代器。

+   **removed_tokens** - 从输出数据集中删除的令牌（默认值：无）

示例

```py
>>> from torchtext.data.functional import simple_space_split
>>> from torchtext.data.functional import numericalize_tokens_from_iterator
>>> vocab = {'Sentencepiece' : 0, 'encode' : 1, 'as' : 2, 'pieces' : 3}
>>> ids_iter = numericalize_tokens_from_iterator(vocab,
>>>                               simple_space_split(["Sentencepiece as pieces",
>>>                                                   "as pieces"]))
>>> for ids in ids_iter:
>>>     print([num for num in ids])
>>> [0, 2, 3]
>>> [2, 3] 
```

## filter_wikipedia_xml

```py
torchtext.data.functional.filter_wikipedia_xml(text_iterator)
```

根据[`github.com/facebookresearch/fastText/blob/master/wikifil.pl`](https://github.com/facebookresearch/fastText/blob/master/wikifil.pl)过滤维基百科 xml 行

参数：

**text_iterator** - 一个产生字符串的迭代器类型对象。示例包括字符串列表、文本 io、生成器等。

示例

```py
>>> from torchtext.data.functional import filter_wikipedia_xml
>>> from torchtext.datasets import EnWik9
>>> data_iter = EnWik9(split='train')
>>> filter_data_iter = filter_wikipedia_xml(data_iter)
>>> file_name = '.data/EnWik9/enwik9'
>>> filter_data_iter = filter_wikipedia_xml(open(file_name,'r')) 
```

## to_map_style_dataset

```py
torchtext.data.functional.to_map_style_dataset(iter_data)
```

将可迭代样式数据集转换为映射样式数据集。

参数：

**iter_data** - 一个迭代器类型对象。示例包括可迭代数据集、字符串列表、文本 io、生成器等。

示例

```py
>>> from torchtext.datasets import IMDB
>>> from torchtext.data import to_map_style_dataset
>>> train_iter = IMDB(split='train')
>>> train_dataset = to_map_style_dataset(train_iter)
>>> file_name = '.data/EnWik9/enwik9'
>>> data_iter = to_map_style_dataset(open(file_name,'r')) 
```
