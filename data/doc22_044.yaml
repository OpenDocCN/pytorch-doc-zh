- en: torch.backends
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torch.backends
- en: 原文：[https://pytorch.org/docs/stable/backends.html](https://pytorch.org/docs/stable/backends.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/backends.html](https://pytorch.org/docs/stable/backends.html)
- en: torch.backends controls the behavior of various backends that PyTorch supports.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: torch.backends控制PyTorch支持的各种后端的行为。
- en: 'These backends include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些后端包括：
- en: '`torch.backends.cpu`'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.backends.cpu`'
- en: '`torch.backends.cuda`'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.backends.cuda`'
- en: '`torch.backends.cudnn`'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.backends.cudnn`'
- en: '`torch.backends.mps`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.backends.mps`'
- en: '`torch.backends.mkl`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.backends.mkl`'
- en: '`torch.backends.mkldnn`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.backends.mkldnn`'
- en: '`torch.backends.openmp`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.backends.openmp`'
- en: '`torch.backends.opt_einsum`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.backends.opt_einsum`'
- en: '`torch.backends.xeon`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.backends.xeon`'
- en: '## torch.backends.cpu'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '## torch.backends.cpu'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Return cpu capability as a string value.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 返回CPU能力作为字符串值。
- en: 'Possible values: - “DEFAULT” - “VSX” - “Z VECTOR” - “NO AVX” - “AVX2” - “AVX512”'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的值：- “DEFAULT” - “VSX” - “Z VECTOR” - “NO AVX” - “AVX2” - “AVX512”
- en: Return type
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)")  ##
    torch.backends.cuda'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)")  ##
    torch.backends.cuda'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Return whether PyTorch is built with CUDA support.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 返回PyTorch是否构建有CUDA支持。
- en: Note that this doesn’t necessarily mean CUDA is available; just that if this
    PyTorch binary were run on a machine with working CUDA drivers and devices, we
    would be able to use it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这并不一定意味着CUDA可用；只是如果在具有工作CUDA驱动程序和设备的机器上运行此PyTorch二进制文件，我们将能够使用它。
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that controls whether TensorFloat-32 tensor cores may be used in matrix
    multiplications on Ampere or newer GPUs. See [TensorFloat-32 (TF32) on Ampere
    (and later) devices](notes/cuda.html#tf32-on-ampere).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个控制在安培或更新的GPU上是否可以使用TensorFloat-32张量核心进行矩阵乘法的布尔值。请参阅[Ampere（以及更高版本）设备上的TensorFloat-32（TF32）](notes/cuda.html#tf32-on-ampere)。
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that controls whether reduced precision reductions (e.g., with fp16 accumulation
    type) are allowed with fp16 GEMMs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个控制是否允许使用减少精度的规约（例如，使用fp16累积类型）与fp16 GEMM一起使用的布尔值。
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that controls whether reduced precision reductions are allowed with bf16
    GEMMs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个控制是否允许使用bf16 GEMM的减少精度规约的布尔值。
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`cufft_plan_cache` contains the cuFFT plan caches for each CUDA device. Query
    a specific device i’s cache via torch.backends.cuda.cufft_plan_cache[i].'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`cufft_plan_cache`包含每个CUDA设备的cuFFT计划缓存。通过torch.backends.cuda.cufft_plan_cache[i]查询特定设备i的缓存。'
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A readonly [`int`](https://docs.python.org/3/library/functions.html#int "(in
    Python v3.12)") that shows the number of plans currently in a cuFFT plan cache.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个只读[`int`](https://docs.python.org/3/library/functions.html#int "(in Python
    v3.12)")，显示cuFFT计划缓存中当前计划的数量。
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A [`int`](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)")
    that controls the capacity of a cuFFT plan cache.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[`int`](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)")，控制cuFFT计划缓存的容量。
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Clears a cuFFT plan cache.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 清除cuFFT计划缓存。
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Override the heuristic PyTorch uses to choose between cuSOLVER and MAGMA for
    CUDA linear algebra operations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖PyTorch用于在CUDA线性代数操作中选择cuSOLVER和MAGMA之间的启发式。
- en: Warning
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This flag is experimental and subject to change.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此标志是实验性的，可能会更改。
- en: When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER
    or MAGMA libraries, and if both are available it decides which to use with a heuristic.
    This flag (a [`str`](https://docs.python.org/3/library/stdtypes.html#str "(in
    Python v3.12)")) allows overriding those heuristics.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当PyTorch运行CUDA线性代数操作时，通常会使用cuSOLVER或MAGMA库，如果两者都可用，则会根据启发式决定使用哪个。此标志（一个[`str`](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")）允许覆盖这些启发式。
- en: If “cusolver” is set then cuSOLVER will be used wherever possible.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果设置为“cusolver”，则将尽可能使用cuSOLVER。
- en: If “magma” is set then MAGMA will be used wherever possible.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果设置为“magma”，则将尽可能使用MAGMA。
- en: If “default” (the default) is set then heuristics will be used to pick between
    cuSOLVER and MAGMA if both are available.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果设置为“default”（默认），则将使用启发式来在cuSOLVER和MAGMA之间进行选择（如果两者都可用）。
- en: When no input is given, this function returns the currently preferred library.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当没有输入时，此函数返回当前首选库。
- en: User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set
    the preferred library to cuSOLVER globally. This flag only sets the initial value
    of the preferred library and the preferred library may still be overridden by
    this function call later in your script.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户可以使用环境变量TORCH_LINALG_PREFER_CUSOLVER=1全局设置首选库为cuSOLVER。此标志仅设置首选库的初始值，首选库仍可能在脚本中的后续函数调用中被覆盖。
- en: 'Note: When a library is preferred other libraries may still be used if the
    preferred library doesn’t implement the operation(s) called. This flag may achieve
    better performance if PyTorch’s heuristic library selection is incorrect for your
    application’s inputs.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当首选库为其他库时，如果首选库未实现所调用的操作，则仍然可以使用其他库。如果PyTorch的启发式库选择对您应用程序的输入不正确，则此标志可能会实现更好的性能。
- en: 'Currently supported linalg operators:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当前支持的linalg运算符：
- en: '[`torch.linalg.inv()`](generated/torch.linalg.inv.html#torch.linalg.inv "torch.linalg.inv")'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.inv()`](generated/torch.linalg.inv.html#torch.linalg.inv "torch.linalg.inv")'
- en: '[`torch.linalg.inv_ex()`](generated/torch.linalg.inv_ex.html#torch.linalg.inv_ex
    "torch.linalg.inv_ex")'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.inv_ex()`](generated/torch.linalg.inv_ex.html#torch.linalg.inv_ex
    "torch.linalg.inv_ex")'
- en: '[`torch.linalg.cholesky()`](generated/torch.linalg.cholesky.html#torch.linalg.cholesky
    "torch.linalg.cholesky")'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.cholesky()`](generated/torch.linalg.cholesky.html#torch.linalg.cholesky
    "torch.linalg.cholesky")'
- en: '[`torch.linalg.cholesky_ex()`](generated/torch.linalg.cholesky_ex.html#torch.linalg.cholesky_ex
    "torch.linalg.cholesky_ex")'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.cholesky_ex()`](generated/torch.linalg.cholesky_ex.html#torch.linalg.cholesky_ex
    "torch.linalg.cholesky_ex")'
- en: '[`torch.cholesky_solve()`](generated/torch.cholesky_solve.html#torch.cholesky_solve
    "torch.cholesky_solve")'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.cholesky_solve()`](generated/torch.cholesky_solve.html#torch.cholesky_solve
    "torch.cholesky_solve")'
- en: '[`torch.cholesky_inverse()`](generated/torch.cholesky_inverse.html#torch.cholesky_inverse
    "torch.cholesky_inverse")'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.cholesky_inverse()`](generated/torch.cholesky_inverse.html#torch.cholesky_inverse
    "torch.cholesky_inverse")'
- en: '[`torch.linalg.lu_factor()`](generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor
    "torch.linalg.lu_factor")'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.lu_factor()`](generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor
    "torch.linalg.lu_factor")'
- en: '[`torch.linalg.lu()`](generated/torch.linalg.lu.html#torch.linalg.lu "torch.linalg.lu")'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.lu()`](generated/torch.linalg.lu.html#torch.linalg.lu "torch.linalg.lu")'
- en: '[`torch.linalg.lu_solve()`](generated/torch.linalg.lu_solve.html#torch.linalg.lu_solve
    "torch.linalg.lu_solve")'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.lu_solve()`](generated/torch.linalg.lu_solve.html#torch.linalg.lu_solve
    "torch.linalg.lu_solve")'
- en: '[`torch.linalg.qr()`](generated/torch.linalg.qr.html#torch.linalg.qr "torch.linalg.qr")'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.qr()`](generated/torch.linalg.qr.html#torch.linalg.qr "torch.linalg.qr")'
- en: '[`torch.linalg.eigh()`](generated/torch.linalg.eigh.html#torch.linalg.eigh
    "torch.linalg.eigh")'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.eigh()`](generated/torch.linalg.eigh.html#torch.linalg.eigh
    "torch.linalg.eigh")'
- en: '`torch.linalg.eighvals()`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.linalg.eighvals()`'
- en: '[`torch.linalg.svd()`](generated/torch.linalg.svd.html#torch.linalg.svd "torch.linalg.svd")'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.svd()`](generated/torch.linalg.svd.html#torch.linalg.svd "torch.linalg.svd")'
- en: '[`torch.linalg.svdvals()`](generated/torch.linalg.svdvals.html#torch.linalg.svdvals
    "torch.linalg.svdvals")'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`torch.linalg.svdvals()`](generated/torch.linalg.svdvals.html#torch.linalg.svdvals
    "torch.linalg.svdvals")'
- en: Return type
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '*_LinalgBackend*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*_LinalgBackend*'
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: alias of `_SDPBackend`
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 别名为 `_SDPBackend`
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: alias of `_SDPAParams`
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 别名为 `_SDPAParams`
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Warning
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This flag is beta and subject to change.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此标志为测试版，可能会更改。
- en: Returns whether flash scaled dot product attention is enabled or not.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 flash 缩放点积注意力是否已启用。
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Warning
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This flag is beta and subject to change.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此标志为测试版，可能会更改。
- en: Enables or disables memory efficient scaled dot product attention.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 启用或禁用内存高效的缩放点积注意力。
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Warning
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This flag is beta and subject to change.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此标志为测试版，可能会更改。
- en: Returns whether memory efficient scaled dot product attention is enabled or
    not.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 返回内存高效的缩放点积注意力是否已启用。
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Warning
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This flag is beta and subject to change.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此标志为测试版，可能会更改。
- en: Enables or disables flash scaled dot product attention.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 启用或禁用 flash 缩放点积注意力。
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Warning
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This flag is beta and subject to change.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此标志为测试版，可能会更改。
- en: Returns whether math scaled dot product attention is enabled or not.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 math 缩放点积注意力是否已启用。
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Warning
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This flag is beta and subject to change.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此标志为测试版，可能会更改。
- en: Enables or disables math scaled dot product attention.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 启用或禁用 math 缩放点积注意力。
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Check if FlashAttention can be utilized in scaled_dot_product_attention.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 检查是否可以在 scaled_dot_product_attention 中使用 FlashAttention。
- en: Parameters
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**params** (*_SDPAParams*) – An instance of SDPAParams containing the tensors
    for query, key, value, an optional attention mask, dropout rate, and a flag indicating
    if the attention is causal.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params** (*_SDPAParams*) – 包含查询、键、值张量、可选注意力掩码、丢弃率以及指示注意力是否因果的标志的 SDPAParams
    实例。'
- en: '**debug** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in
    Python v3.12)")) – Whether to logging.warn debug information as to why FlashAttention
    could not be run. Defaults to False.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**debug** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(在
    Python v3.12 中)")) – 是否记录警告调试信息，说明为什么无法运行 FlashAttention。默认为False。'
- en: Returns
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: True if FlashAttention can be used with the given parameters; otherwise, False.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可以使用给定参数，则为True；否则为False。
- en: Return type
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[bool](https://docs.python.org/3/library/functions.html#bool "(在 Python v3.12
    中)")'
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This function is dependent on a CUDA-enabled build of PyTorch. It will return
    False in non-CUDA environments.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数依赖于 PyTorch 的 CUDA 版本。在非CUDA环境中将返回False。
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Check if efficient_attention can be utilized in scaled_dot_product_attention.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 检查是否可以在 scaled_dot_product_attention 中使用 efficient_attention。
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**params** (*_SDPAParams*) – An instance of SDPAParams containing the tensors
    for query, key, value, an optional attention mask, dropout rate, and a flag indicating
    if the attention is causal.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params** (*_SDPAParams*) – 包含查询、键、值张量、可选注意力掩码、丢弃率以及指示注意力是否因果的标志的 SDPAParams
    实例。'
- en: '**debug** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in
    Python v3.12)")) – Whether to logging.warn with information as to why efficient_attention
    could not be run. Defaults to False.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**debug** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(在
    Python v3.12 中)")) – 是否记录警告信息，说明为什么无法运行 efficient_attention。默认为False。'
- en: Returns
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: True if efficient_attention can be used with the given parameters; otherwise,
    False.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可以使用给定参数，则为True；否则为False。
- en: Return type
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[bool](https://docs.python.org/3/library/functions.html#bool "(在 Python v3.12
    中)")'
- en: Note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This function is dependent on a CUDA-enabled build of PyTorch. It will return
    False in non-CUDA environments.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数依赖于 PyTorch 的 CUDA 版本。在非CUDA环境中将返回False。
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Warning
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This flag is beta and subject to change.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此标志为测试版，可能会更改。
- en: 'This context manager can be used to temporarily enable or disable any of the
    three backends for scaled dot product attention. Upon exiting the context manager,
    the previous state of the flags will be restored.  ## torch.backends.cudnn[](#module-torch.backends.cudnn
    "Permalink to this heading")'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此上下文管理器可用于临时启用或禁用缩放点积注意力的三个后端之一。退出上下文管理器时，将恢复标志的先前状态。## torch.backends.cudnn[](#module-torch.backends.cudnn
    "Permalink to this heading")
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Return the version of cuDNN.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 cuDNN 的版本。
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Return a bool indicating if CUDNN is currently available.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个布尔值，指示当前是否可用 CUDNN。
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that controls whether cuDNN is enabled.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个控制 cuDNN 是否启用的 [`bool`](https://docs.python.org/3/library/functions.html#bool
    "(在 Python v3.12 中)")。
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that controls where TensorFloat-32 tensor cores may be used in cuDNN
    convolutions on Ampere or newer GPUs. See [TensorFloat-32 (TF32) on Ampere (and
    later) devices](notes/cuda.html#tf32-on-ampere).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[`bool`](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")，控制在Ampere或更新的GPU上cuDNN卷积中是否可以使用TensorFloat-32张量核心。请参阅[Ampere（以及更高版本）设备上的TensorFloat-32（TF32）](notes/cuda.html#tf32-on-ampere)。
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that, if True, causes cuDNN to only use deterministic convolution algorithms.
    See also [`torch.are_deterministic_algorithms_enabled()`](generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled
    "torch.are_deterministic_algorithms_enabled") and [`torch.use_deterministic_algorithms()`](generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms
    "torch.use_deterministic_algorithms").
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[`bool`](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")，如果为True，则导致cuDNN仅使用确定性卷积算法。另请参阅[`torch.are_deterministic_algorithms_enabled()`](generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled
    "torch.are_deterministic_algorithms_enabled")和[`torch.use_deterministic_algorithms()`](generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms
    "torch.use_deterministic_algorithms")。
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: A [`bool`](https://docs.python.org/3/library/functions.html#bool "(in Python
    v3.12)") that, if True, causes cuDNN to benchmark multiple convolution algorithms
    and select the fastest.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[`bool`](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")，如果为True，则导致cuDNN对多个卷积算法进行基准测试并选择最快的。
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'A [`int`](https://docs.python.org/3/library/functions.html#int "(in Python
    v3.12)") that specifies the maximum number of cuDNN convolution algorithms to
    try when torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to
    try every available algorithm. Note that this setting only affects convolutions
    dispatched via the cuDNN v8 API.  ## torch.backends.mps[](#module-torch.backends.mps
    "Permalink to this heading")'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '一个[`int`](https://docs.python.org/3/library/functions.html#int "(在Python v3.12中)")，指定torch.backends.cudnn.benchmark为True时尝试的cuDNN卷积算法的最大数量。将benchmark_limit设置为零以尝试每个可用算法。请注意，此设置仅影响通过cuDNN
    v8 API分派的卷积。  ## torch.backends.mps[](#module-torch.backends.mps "跳转到此标题的永久链接")'
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Return a bool indicating if MPS is currently available.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个指示当前是否可用MPS的布尔值。
- en: Return type
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[bool](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")'
- en: '[PRE29]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Return whether PyTorch is built with MPS support.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 返回PyTorch是否构建有MPS支持。
- en: Note that this doesn’t necessarily mean MPS is available; just that if this
    PyTorch binary were run a machine with working MPS drivers and devices, we would
    be able to use it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这并不一定意味着MPS可用；只是如果在具有工作MPS驱动程序和设备的机器上运行此PyTorch二进制文件，我们将能够使用它。
- en: Return type
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")  ##
    torch.backends.mkl'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[bool](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")  ##
    torch.backends.mkl'
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Return whether PyTorch is built with MKL support.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 返回PyTorch是否构建有MKL支持。
- en: '[PRE31]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: On-demand oneMKL verbosing functionality.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 按需oneMKL详细功能。
- en: To make it easier to debug performance issues, oneMKL can dump verbose messages
    containing execution information like duration while executing the kernel. The
    verbosing functionality can be invoked via an environment variable named MKL_VERBOSE.
    However, this methodology dumps messages in all steps. Those are a large amount
    of verbose messages. Moreover, for investigating the performance issues, generally
    taking verbose messages for one single iteration is enough. This on-demand verbosing
    functionality makes it possible to control scope for verbose message dumping.
    In the following example, verbose messages will be dumped out for the second inference
    only.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易调试性能问题，oneMKL可以转储包含执行信息（如持续时间）的详细消息，同时执行内核。可以通过名为MKL_VERBOSE的环境变量调用详细功能。但是，这种方法在所有步骤中转储消息。这些是大量详细消息。此外，通常仅对单个迭代获取详细消息就足够用于调查性能问题。这种按需详细功能使得可以控制详细消息转储的范围。在以下示例中，仅为第二个推理转储详细消息。
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**level** – Verbose level - `VERBOSE_OFF`: Disable verbosing - `VERBOSE_ON`:
    Enable verbosing  ## torch.backends.mkldnn[](#module-torch.backends.mkldnn "Permalink
    to this heading")'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**level** – 详细级别 - `VERBOSE_OFF`：禁用详细 - `VERBOSE_ON`：启用详细  ## torch.backends.mkldnn[](#module-torch.backends.mkldnn
    "跳转到此标题的永久链接")'
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Return whether PyTorch is built with MKL-DNN support.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 返回PyTorch是否构建有MKL-DNN支持。
- en: '[PRE34]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: On-demand oneDNN (former MKL-DNN) verbosing functionality.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 按需oneDNN（前MKL-DNN）详细功能。
- en: To make it easier to debug performance issues, oneDNN can dump verbose messages
    containing information like kernel size, input data size and execution duration
    while executing the kernel. The verbosing functionality can be invoked via an
    environment variable named DNNL_VERBOSE. However, this methodology dumps messages
    in all steps. Those are a large amount of verbose messages. Moreover, for investigating
    the performance issues, generally taking verbose messages for one single iteration
    is enough. This on-demand verbosing functionality makes it possible to control
    scope for verbose message dumping. In the following example, verbose messages
    will be dumped out for the second inference only.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易调试性能问题，oneDNN可以转储包含内核大小、输入数据大小和执行持续时间等信息的详细消息，同时执行内核。可以通过名为DNNL_VERBOSE的环境变量调用详细功能。但是，这种方法在所有步骤中转储消息。这些是大量详细消息。此外，通常仅对单个迭代获取详细消息就足够用于调查性能问题。这种按需详细功能使得可以控制详细消息转储的范围。在以下示例中，仅为第二个推理转储详细消息。
- en: '[PRE35]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**level** – Verbose level - `VERBOSE_OFF`: Disable verbosing - `VERBOSE_ON`:
    Enable verbosing - `VERBOSE_ON_CREATION`: Enable verbosing, including oneDNN kernel
    creation  ## torch.backends.openmp[](#module-torch.backends.openmp "Permalink
    to this heading")'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**level** – 详细级别 - `VERBOSE_OFF`：禁用详细 - `VERBOSE_ON`：启用详细 - `VERBOSE_ON_CREATION`：启用详细，包括oneDNN内核创建  ##
    torch.backends.openmp[](#module-torch.backends.openmp "跳转到此标题的永久链接")'
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Return whether PyTorch is built with OpenMP support.  ## torch.backends.opt_einsum[](#module-torch.backends.opt_einsum
    "Permalink to this heading")'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '返回PyTorch是否构建有OpenMP支持。  ## torch.backends.opt_einsum[](#module-torch.backends.opt_einsum
    "跳转到此标题")'
- en: '[PRE37]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Return a bool indicating if opt_einsum is currently available.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个指示opt_einsum当前是否可用的bool值。
- en: Return type
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)")'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[bool](https://docs.python.org/3/library/functions.html#bool "(在Python v3.12中)")'
- en: '[PRE38]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Return the opt_einsum package if opt_einsum is currently available, else None.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果当前可用，则返回opt_einsum包，否则返回None。
- en: Return type
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型
- en: '[*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(in Python
    v3.12)")'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Any*](https://docs.python.org/3/library/typing.html#typing.Any "(在Python
    v3.12中)")'
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: A :class:`bool` that controls whether opt_einsum is enabled (`True` by default).
    If so, torch.einsum will use opt_einsum ([https://optimized-einsum.readthedocs.io/en/stable/path_finding.html](https://optimized-einsum.readthedocs.io/en/stable/path_finding.html))
    if available to calculate an optimal path of contraction for faster performance.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一个控制是否启用opt_einsum的`bool`（默认为`True`）。如果启用，torch.einsum将使用opt_einsum（[https://optimized-einsum.readthedocs.io/en/stable/path_finding.html](https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)）来计算更快性能的最佳收缩路径。
- en: If opt_einsum is not available, torch.einsum will fall back to the default contraction
    path of left to right.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果opt_einsum不可用，torch.einsum将退回到默认的从左到右的收缩路径。
- en: '[PRE40]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'A :class:`str` that specifies which strategies to try when `torch.backends.opt_einsum.enabled`
    is `True`. By default, torch.einsum will try the “auto” strategy, but the “greedy”
    and “optimal” strategies are also supported. Note that the “optimal” strategy
    is factorial on the number of inputs as it tries all possible paths. See more
    details in opt_einsum’s docs ([https://optimized-einsum.readthedocs.io/en/stable/path_finding.html](https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)).  ##
    torch.backends.xeon'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '一个指定当`torch.backends.opt_einsum.enabled`为`True`时要尝试哪些策略的`str`。默认情况下，torch.einsum将尝试“auto”策略，但也支持“greedy”和“optimal”策略。请注意，“optimal”策略在尝试所有可能路径时与输入数量的阶乘成正比。在opt_einsum的文档中查看更多细节（[https://optimized-einsum.readthedocs.io/en/stable/path_finding.html](https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)）。  ##
    torch.backends.xeon'
