- en: Pruning Tutorial
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修剪教程
- en: 原文：[https://pytorch.org/tutorials/intermediate/pruning_tutorial.html](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/intermediate/pruning_tutorial.html](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-intermediate-pruning-tutorial-py) to download
    the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-intermediate-pruning-tutorial-py)下载完整的示例代码
- en: '**Author**: [Michela Paganini](https://github.com/mickypaganini)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：[Michela Paganini](https://github.com/mickypaganini)'
- en: State-of-the-art deep learning techniques rely on over-parametrized models that
    are hard to deploy. On the contrary, biological neural networks are known to use
    efficient sparse connectivity. Identifying optimal techniques to compress models
    by reducing the number of parameters in them is important in order to reduce memory,
    battery, and hardware consumption without sacrificing accuracy. This in turn allows
    you to deploy lightweight models on device, and guarantee privacy with private
    on-device computation. On the research front, pruning is used to investigate the
    differences in learning dynamics between over-parametrized and under-parametrized
    networks, to study the role of lucky sparse subnetworks and initializations (“[lottery
    tickets](https://arxiv.org/abs/1803.03635)”) as a destructive neural architecture
    search technique, and more.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的深度学习技术依赖于难以部署的过度参数化模型。相反，生物神经网络已知使用高效的稀疏连接。通过识别优化的技术来通过减少模型中的参数数量来压缩模型是重要的，以便在减少内存、电池和硬件消耗的同时不牺牲准确性。这反过来使您能够在设备上部署轻量级模型，并通过设备上的私有计算来保证隐私。在研究方面，修剪被用来研究过度参数化和欠参数化网络之间学习动态的差异，研究幸运稀疏子网络和初始化（“[彩票票](https://arxiv.org/abs/1803.03635)”）的作用作为一种破坏性的神经架构搜索技术，等等。
- en: In this tutorial, you will learn how to use `torch.nn.utils.prune` to sparsify
    your neural networks, and how to extend it to implement your own custom pruning
    technique.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将学习如何使用`torch.nn.utils.prune`来稀疏化您的神经网络，以及如何扩展它以实现自己的自定义修剪技术。
- en: Requirements
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需求
- en: '`"torch>=1.4.0a0+8e8a5e0"`'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`"torch>=1.4.0a0+8e8a5e0"`'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Create a model
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个模型
- en: In this tutorial, we use the [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)
    architecture from LeCun et al., 1998.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们使用来自LeCun等人的1998年的[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)架构。
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Inspect a Module
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查一个模块
- en: Let’s inspect the (unpruned) `conv1` layer in our LeNet model. It will contain
    two parameters `weight` and `bias`, and no buffers, for now.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们LeNet模型中的（未修剪的）`conv1`层。它将包含两个参数`weight`和`bias`，目前没有缓冲区。
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Pruning a Module
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修剪一个模块
- en: To prune a module (in this example, the `conv1` layer of our LeNet architecture),
    first select a pruning technique among those available in `torch.nn.utils.prune`
    (or [implement](#extending-torch-nn-utils-pruning-with-custom-pruning-functions)
    your own by subclassing `BasePruningMethod`). Then, specify the module and the
    name of the parameter to prune within that module. Finally, using the adequate
    keyword arguments required by the selected pruning technique, specify the pruning
    parameters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要修剪一个模块（在这个例子中，是我们的LeNet架构中的`conv1`层），首先在`torch.nn.utils.prune`中选择一个修剪技术（或者[实现](#extending-torch-nn-utils-pruning-with-custom-pruning-functions)自己的方法，通过继承`BasePruningMethod`）。然后，指定要在该模块内修剪的模块和参数的名称。最后，使用所选修剪技术所需的适当关键字参数，指定修剪参数。
- en: In this example, we will prune at random 30% of the connections in the parameter
    named `weight` in the `conv1` layer. The module is passed as the first argument
    to the function; `name` identifies the parameter within that module using its
    string identifier; and `amount` indicates either the percentage of connections
    to prune (if it is a float between 0\. and 1.), or the absolute number of connections
    to prune (if it is a non-negative integer).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将在`conv1`层的名为`weight`的参数中随机修剪30%的连接。模块作为函数的第一个参数传递；`name`使用其字符串标识符在该模块内标识参数；`amount`指示要修剪的连接的百分比（如果是0和1之间的浮点数），或要修剪的连接的绝对数量（如果是非负整数）。
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Pruning acts by removing `weight` from the parameters and replacing it with
    a new parameter called `weight_orig` (i.e. appending `"_orig"` to the initial
    parameter `name`). `weight_orig` stores the unpruned version of the tensor. The
    `bias` was not pruned, so it will remain intact.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪通过从参数中删除`weight`并用一个名为`weight_orig`的新参数替换它（即将初始参数`name`附加`"_orig"`）。`weight_orig`存储张量的未修剪版本。`bias`没有被修剪，因此它将保持不变。
- en: '[PRE8]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The pruning mask generated by the pruning technique selected above is saved
    as a module buffer named `weight_mask` (i.e. appending `"_mask"` to the initial
    parameter `name`).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上面选择的修剪技术生成的修剪蒙版被保存为一个名为`weight_mask`的模块缓冲区（即将初始参数`name`附加`"_mask"`）。
- en: '[PRE10]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For the forward pass to work without modification, the `weight` attribute needs
    to exist. The pruning techniques implemented in `torch.nn.utils.prune` compute
    the pruned version of the weight (by combining the mask with the original parameter)
    and store them in the attribute `weight`. Note, this is no longer a parameter
    of the `module`, it is now simply an attribute.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使前向传播无需修改工作，`weight`属性需要存在。在`torch.nn.utils.prune`中实现的修剪技术计算权重的修剪版本（通过将蒙版与原始参数组合）并将它们存储在属性`weight`中。请注意，这不再是`module`的参数，现在只是一个属性。
- en: '[PRE12]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, pruning is applied prior to each forward pass using PyTorch’s `forward_pre_hooks`.
    Specifically, when the `module` is pruned, as we have done here, it will acquire
    a `forward_pre_hook` for each parameter associated with it that gets pruned. In
    this case, since we have so far only pruned the original parameter named `weight`,
    only one hook will be present.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在每次前向传播之前应用修剪，使用PyTorch的`forward_pre_hooks`。具体来说，当`module`被修剪时，就像我们在这里所做的那样，它将为与之关联的每个参数获取一个`forward_pre_hook`。在这种情况下，因为我们目前只修剪了名为`weight`的原始参数，所以只有一个钩子存在。
- en: '[PRE14]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For completeness, we can now prune the `bias` too, to see how the parameters,
    buffers, hooks, and attributes of the `module` change. Just for the sake of trying
    out another pruning technique, here we prune the 3 smallest entries in the bias
    by L1 norm, as implemented in the `l1_unstructured` pruning function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整起见，我们现在也可以修剪`bias`，以查看`module`的参数、缓冲区、hook和属性如何变化。为了尝试另一种修剪技术，这里我们通过L1范数在偏置中修剪最小的3个条目，如`l1_unstructured`修剪函数中实现的那样。
- en: '[PRE16]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We now expect the named parameters to include both `weight_orig` (from before)
    and `bias_orig`. The buffers will include `weight_mask` and `bias_mask`. The pruned
    versions of the two tensors will exist as module attributes, and the module will
    now have two `forward_pre_hooks`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在期望命名参数包括`weight_orig`（之前的）和`bias_orig`。缓冲区将包括`weight_mask`和`bias_mask`。这两个张量的修剪版本将存在作为模块属性，并且模块现在将有两个`forward_pre_hooks`。
- en: '[PRE18]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Iterative Pruning
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代修剪
- en: The same parameter in a module can be pruned multiple times, with the effect
    of the various pruning calls being equal to the combination of the various masks
    applied in series. The combination of a new mask with the old mask is handled
    by the `PruningContainer`’s `compute_mask` method.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模块中的同一参数可以多次修剪，各个修剪调用的效果等于串行应用的各个掩码的组合。新掩码与旧掩码的组合由`PruningContainer`的`compute_mask`方法处理。
- en: Say, for example, that we now want to further prune `module.weight`, this time
    using structured pruning along the 0th axis of the tensor (the 0th axis corresponds
    to the output channels of the convolutional layer and has dimensionality 6 for
    `conv1`), based on the channels’ L2 norm. This can be achieved using the `ln_structured`
    function, with `n=2` and `dim=0`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们现在想要进一步修剪`module.weight`，这次使用张量的第0轴（第0轴对应于卷积层的输出通道，并且对于`conv1`，维度为6）上的结构化修剪，基于通道的L2范数。这可以使用`ln_structured`函数实现，其中`n=2`和`dim=0`。
- en: '[PRE26]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The corresponding hook will now be of type `torch.nn.utils.prune.PruningContainer`,
    and will store the history of pruning applied to the `weight` parameter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的hook现在将是`torch.nn.utils.prune.PruningContainer`类型，并将存储应用于`weight`参数的修剪历史。
- en: '[PRE28]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Serializing a pruned model
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列化修剪模型
- en: All relevant tensors, including the mask buffers and the original parameters
    used to compute the pruned tensors are stored in the model’s `state_dict` and
    can therefore be easily serialized and saved, if needed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所有相关张量，包括掩码缓冲区和用于计算修剪张量的原始参数都存储在模型的`state_dict`中，因此如果需要，可以轻松序列化和保存。
- en: '[PRE30]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Remove pruning re-parametrization
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移除修剪重新参数化
- en: To make the pruning permanent, remove the re-parametrization in terms of `weight_orig`
    and `weight_mask`, and remove the `forward_pre_hook`, we can use the `remove`
    functionality from `torch.nn.utils.prune`. Note that this doesn’t undo the pruning,
    as if it never happened. It simply makes it permanent, instead, by reassigning
    the parameter `weight` to the model parameters, in its pruned version.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要使修剪永久化，删除关于`weight_orig`和`weight_mask`的重新参数化，并删除`forward_pre_hook`，我们可以使用`torch.nn.utils.prune`中的`remove`功能。请注意，这不会撤消修剪，就好像从未发生过一样。它只是使其永久化，通过将参数`weight`重新分配给模型参数，以其修剪版本。
- en: 'Prior to removing the re-parametrization:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在移除重新参数化之前：
- en: '[PRE32]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After removing the re-parametrization:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 移除重新参数化后：
- en: '[PRE38]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Pruning multiple parameters in a model
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在模型中修剪多个参数
- en: By specifying the desired pruning technique and parameters, we can easily prune
    multiple tensors in a network, perhaps according to their type, as we will see
    in this example.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定所需的修剪技术和参数，我们可以轻松地在网络中修剪多个张量，也许根据它们的类型，正如我们将在本例中看到的。
- en: '[PRE42]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Global pruning
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全局修剪
- en: So far, we only looked at what is usually referred to as “local” pruning, i.e.
    the practice of pruning tensors in a model one by one, by comparing the statistics
    (weight magnitude, activation, gradient, etc.) of each entry exclusively to the
    other entries in that tensor. However, a common and perhaps more powerful technique
    is to prune the model all at once, by removing (for example) the lowest 20% of
    connections across the whole model, instead of removing the lowest 20% of connections
    in each layer. This is likely to result in different pruning percentages per layer.
    Let’s see how to do that using `global_unstructured` from `torch.nn.utils.prune`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看了通常被称为“局部”修剪的内容，即通过将每个条目的统计数据（权重大小、激活、梯度等）与该张量中的其他条目进行比较，逐个修剪模型中的张量的做法。然而，一种常见且可能更强大的技术是一次性修剪整个模型，例如，通过删除整个模型中最低的20%连接，而不是在每一层中删除最低的20%连接。这可能会导致每层不同的修剪百分比。让我们看看如何使用`torch.nn.utils.prune`中的`global_unstructured`来实现这一点。
- en: '[PRE44]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now we can check the sparsity induced in every pruned parameter, which will
    not be equal to 20% in each layer. However, the global sparsity will be (approximately)
    20%.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查每个修剪参数中引入的稀疏性，这在每个层中不会等于20%。然而，全局稀疏性将是（大约）20%。
- en: '[PRE45]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Extending `torch.nn.utils.prune` with custom pruning functions
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自定义修剪函数扩展`torch.nn.utils.prune`
- en: 'To implement your own pruning function, you can extend the `nn.utils.prune`
    module by subclassing the `BasePruningMethod` base class, the same way all other
    pruning methods do. The base class implements the following methods for you: `__call__`,
    `apply_mask`, `apply`, `prune`, and `remove`. Beyond some special cases, you shouldn’t
    have to reimplement these methods for your new pruning technique. You will, however,
    have to implement `__init__` (the constructor), and `compute_mask` (the instructions
    on how to compute the mask for the given tensor according to the logic of your
    pruning technique). In addition, you will have to specify which type of pruning
    this technique implements (supported options are `global`, `structured`, and `unstructured`).
    This is needed to determine how to combine masks in the case in which pruning
    is applied iteratively. In other words, when pruning a prepruned parameter, the
    current pruning technique is expected to act on the unpruned portion of the parameter.
    Specifying the `PRUNING_TYPE` will enable the `PruningContainer` (which handles
    the iterative application of pruning masks) to correctly identify the slice of
    the parameter to prune.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现自己的修剪函数，您可以通过继承`BasePruningMethod`基类来扩展`nn.utils.prune`模块，就像所有其他修剪方法一样。基类为您实现了以下方法：`__call__`、`apply_mask`、`apply`、`prune`和`remove`。除了一些特殊情况，您不应该重新实现这些方法以适应您的新修剪技术。但是，您需要实现`__init__`（构造函数）和`compute_mask`（根据修剪技术的逻辑指示如何计算给定张量的掩码）。此外，您还需要指定此技术实现的修剪类型（支持的选项为`global`、`structured`和`unstructured`）。这是为了确定如何在迭代应用修剪时组合掩码。换句话说，当修剪预修剪参数时，当前修剪技术应该作用于参数的未修剪部分。指定`PRUNING_TYPE`将使`PruningContainer`（处理修剪掩码的迭代应用）能够正确识别要修剪的参数切片。
- en: Let’s assume, for example, that you want to implement a pruning technique that
    prunes every other entry in a tensor (or – if the tensor has previously been pruned
    – in the remaining unpruned portion of the tensor). This will be of `PRUNING_TYPE='unstructured'`
    because it acts on individual connections in a layer and not on entire units/channels
    (`'structured'`), or across different parameters (`'global'`).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您想要实现一种修剪技术，该技术修剪张量中的每个其他条目（或者 - 如果张量以前已被修剪 - 则修剪张量的剩余未修剪部分中的每个其他条目）。这将是`PRUNING_TYPE='unstructured'`，因为它作用于层中的单个连接，而不是整个单元/通道（`'structured'`）或跨不同参数（`'global'`）。
- en: '[PRE47]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now, to apply this to a parameter in an `nn.Module`, you should also provide
    a simple function that instantiates the method and applies it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要将此应用于`nn.Module`中的参数，您还应提供一个简单的函数，该函数实例化该方法并应用它。
- en: '[PRE48]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Let’s try it out!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试！
- en: '[PRE49]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '**Total running time of the script:** ( 0 minutes 0.373 seconds)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（0分钟0.373秒）'
- en: '[`Download Python source code: pruning_tutorial.py`](../_downloads/ef3541eb2ef78e22efa65b3d6f4ba737/pruning_tutorial.py)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：pruning_tutorial.py`](../_downloads/ef3541eb2ef78e22efa65b3d6f4ba737/pruning_tutorial.py)'
- en: '[`Download Jupyter notebook: pruning_tutorial.ipynb`](../_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：pruning_tutorial.ipynb`](../_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[由Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)'
