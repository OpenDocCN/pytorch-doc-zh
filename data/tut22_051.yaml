- en: 'NLP From Scratch: Translation with a Sequence to Sequence Network and Attention'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP从头开始：使用序列到序列网络和注意力进行翻译
- en: 原文：[https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-intermediate-seq2seq-translation-tutorial-py)
    to download the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-intermediate-seq2seq-translation-tutorial-py)下载完整的示例代码
- en: '**Author**: [Sean Robertson](https://github.com/spro)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：[Sean Robertson](https://github.com/spro)'
- en: This is the third and final tutorial on doing “NLP From Scratch”, where we write
    our own classes and functions to preprocess the data to do our NLP modeling tasks.
    We hope after you complete this tutorial that you’ll proceed to learn how torchtext
    can handle much of this preprocessing for you in the three tutorials immediately
    following this one.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于“从头开始进行NLP”的第三个也是最后一个教程，在这里我们编写自己的类和函数来预处理数据以执行NLP建模任务。我们希望在您完成本教程后，您将继续学习torchtext如何在接下来的三个教程中为您处理大部分预处理工作。
- en: In this project we will be teaching a neural network to translate from French
    to English.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将教授神经网络从法语翻译成英语。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: … to varying degrees of success.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: … 成功程度各不相同。
- en: This is made possible by the simple but powerful idea of the [sequence to sequence
    network](https://arxiv.org/abs/1409.3215), in which two recurrent neural networks
    work together to transform one sequence to another. An encoder network condenses
    an input sequence into a vector, and a decoder network unfolds that vector into
    a new sequence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这得益于[序列到序列网络](https://arxiv.org/abs/1409.3215)的简单而强大的思想，其中两个递归神经网络共同工作，将一个序列转换为另一个序列。编码器网络将输入序列压缩为向量，解码器网络将该向量展开为新序列。
- en: '![](../Images/b01274082109b1019682274a0d4ca4d8.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b01274082109b1019682274a0d4ca4d8.png)'
- en: To improve upon this model we’ll use an [attention mechanism](https://arxiv.org/abs/1409.0473),
    which lets the decoder learn to focus over a specific range of the input sequence.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进这个模型，我们将使用[注意机制](https://arxiv.org/abs/1409.0473)，让解码器学会专注于输入序列的特定范围。
- en: '**Recommended Reading:**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**推荐阅读：**'
- en: 'I assume you have at least installed PyTorch, know Python, and understand Tensors:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您至少已经安装了PyTorch，了解Python，并理解张量：
- en: '[https://pytorch.org/](https://pytorch.org/) For installation instructions'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/](https://pytorch.org/) 安装说明'
- en: '[Deep Learning with PyTorch: A 60 Minute Blitz](../beginner/deep_learning_60min_blitz.html)
    to get started with PyTorch in general'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyTorch进行深度学习：60分钟快速入门](../beginner/deep_learning_60min_blitz.html) 以开始使用PyTorch'
- en: '[Learning PyTorch with Examples](../beginner/pytorch_with_examples.html) for
    a wide and deep overview'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用示例学习PyTorch](../beginner/pytorch_with_examples.html) 以获取广泛而深入的概述'
- en: '[PyTorch for Former Torch Users](../beginner/former_torchies_tutorial.html)
    if you are former Lua Torch user'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch for Former Torch Users](../beginner/former_torchies_tutorial.html)
    如果您以前是Lua Torch用户'
- en: 'It would also be useful to know about Sequence to Sequence networks and how
    they work:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 了解序列到序列网络以及它们的工作原理也会很有用：
- en: '[Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation](https://arxiv.org/abs/1406.1078)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用RNN编码器-解码器学习短语表示进行统计机器翻译](https://arxiv.org/abs/1406.1078)'
- en: '[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用神经网络进行序列到序列学习](https://arxiv.org/abs/1409.3215)'
- en: '[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过联合学习对齐和翻译进行神经机器翻译](https://arxiv.org/abs/1409.0473)'
- en: '[A Neural Conversational Model](https://arxiv.org/abs/1506.05869)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经对话模型](https://arxiv.org/abs/1506.05869)'
- en: 'You will also find the previous tutorials on [NLP From Scratch: Classifying
    Names with a Character-Level RNN](char_rnn_classification_tutorial.html) and [NLP
    From Scratch: Generating Names with a Character-Level RNN](char_rnn_generation_tutorial.html)
    helpful as those concepts are very similar to the Encoder and Decoder models,
    respectively.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会发现之前的教程[NLP从头开始：使用字符级RNN对名字进行分类](char_rnn_classification_tutorial.html)和[NLP从头开始：使用字符级RNN生成名字](char_rnn_generation_tutorial.html)对理解编码器和解码器模型非常有帮助。
- en: '**Requirements**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**要求**'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Loading data files
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据文件
- en: The data for this project is a set of many thousands of English to French translation
    pairs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的数据是成千上万个英语到法语翻译对的集合。
- en: '[This question on Open Data Stack Exchange](https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)
    pointed me to the open translation site [https://tatoeba.org/](https://tatoeba.org/)
    which has downloads available at [https://tatoeba.org/eng/downloads](https://tatoeba.org/eng/downloads)
    - and better yet, someone did the extra work of splitting language pairs into
    individual text files here: [https://www.manythings.org/anki/](https://www.manythings.org/anki/)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[Open Data Stack Exchange上的这个问题](https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)指向了开放翻译网站[https://tatoeba.org/](https://tatoeba.org/)，可以在[https://tatoeba.org/eng/downloads](https://tatoeba.org/eng/downloads)下载数据
    - 更好的是，有人额外工作将语言对拆分为单独的文本文件，位于这里：[https://www.manythings.org/anki/](https://www.manythings.org/anki/)'
- en: 'The English to French pairs are too big to include in the repository, so download
    to `data/eng-fra.txt` before continuing. The file is a tab separated list of translation
    pairs:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 英语到法语的翻译对太大，无法包含在存储库中，请在继续之前下载到`data/eng-fra.txt`。该文件是一个制表符分隔的翻译对列表：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Download the data from [here](https://download.pytorch.org/tutorial/data.zip)
    and extract it to the current directory.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从[这里](https://download.pytorch.org/tutorial/data.zip)下载数据并将其解压到当前目录。
- en: Similar to the character encoding used in the character-level RNN tutorials,
    we will be representing each word in a language as a one-hot vector, or giant
    vector of zeros except for a single one (at the index of the word). Compared to
    the dozens of characters that might exist in a language, there are many many more
    words, so the encoding vector is much larger. We will however cheat a bit and
    trim the data to only use a few thousand words per language.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于字符级RNN教程中使用的字符编码，我们将每个语言中的每个单词表示为一个独热向量，或者除了一个单一的一之外全为零的巨大向量（在单词的索引处）。与语言中可能存在的几十个字符相比，单词要多得多，因此编码向量要大得多。但我们会稍微作弊，只使用每种语言中的几千个单词来修剪数据。
- en: '![](../Images/7fa129004e942671707f8f2d4fb80a20.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fa129004e942671707f8f2d4fb80a20.png)'
- en: We’ll need a unique index per word to use as the inputs and targets of the networks
    later. To keep track of all this we will use a helper class called `Lang` which
    has word → index (`word2index`) and index → word (`index2word`) dictionaries,
    as well as a count of each word `word2count` which will be used to replace rare
    words later.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要每个单词的唯一索引，以便稍后用作网络的输入和目标。为了跟踪所有这些，我们将使用一个名为`Lang`的辅助类，其中包含单词→索引（`word2index`）和索引→单词（`index2word`）字典，以及每个单词的计数`word2count`，稍后将用于替换稀有单词。
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The files are all in Unicode, to simplify we will turn Unicode characters to
    ASCII, make everything lowercase, and trim most punctuation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所有文件都是Unicode格式，为了简化，我们将Unicode字符转换为ASCII，将所有内容转换为小写，并修剪大部分标点符号。
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To read the data file we will split the file into lines, and then split lines
    into pairs. The files are all English → Other Language, so if we want to translate
    from Other Language → English I added the `reverse` flag to reverse the pairs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了读取数据文件，我们将文件拆分成行，然后将行拆分成对。所有文件都是英语→其他语言，因此如果我们想要从其他语言→英语翻译，我添加了`reverse`标志以反转对。
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Since there are a *lot* of example sentences and we want to train something
    quickly, we’ll trim the data set to only relatively short and simple sentences.
    Here the maximum length is 10 words (that includes ending punctuation) and we’re
    filtering to sentences that translate to the form “I am” or “He is” etc. (accounting
    for apostrophes replaced earlier).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有很多例句并且我们想要快速训练一些东西，我们将数据集修剪为相对较短和简单的句子。这里最大长度为10个单词（包括结束标点符号），我们正在过滤翻译为“I
    am”或“He is”等形式的句子（考虑之前替换的省略号）。
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The full process for preparing the data is:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数据的完整过程是：
- en: Read text file and split into lines, split lines into pairs
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取文本文件并拆分成行，将行拆分成对
- en: Normalize text, filter by length and content
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范化文本，按长度和内容过滤
- en: Make word lists from sentences in pairs
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从成对句子中制作单词列表
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The Seq2Seq Model
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seq2Seq模型
- en: A Recurrent Neural Network, or RNN, is a network that operates on a sequence
    and uses its own output as input for subsequent steps.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）是一个在序列上操作并将其自身输出用作后续步骤输入的网络。
- en: A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215), or seq2seq
    network, or [Encoder Decoder network](https://arxiv.org/pdf/1406.1078v3.pdf),
    is a model consisting of two RNNs called the encoder and decoder. The encoder
    reads an input sequence and outputs a single vector, and the decoder reads that
    vector to produce an output sequence.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sequence to Sequence network](https://arxiv.org/abs/1409.3215)，或seq2seq网络，或[编码器解码器网络](https://arxiv.org/pdf/1406.1078v3.pdf)，是由两个称为编码器和解码器的RNN组成的模型。编码器读取输入序列并输出一个单一向量，解码器读取该向量以产生一个输出序列。'
- en: '![](../Images/b01274082109b1019682274a0d4ca4d8.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b01274082109b1019682274a0d4ca4d8.png)'
- en: Unlike sequence prediction with a single RNN, where every input corresponds
    to an output, the seq2seq model frees us from sequence length and order, which
    makes it ideal for translation between two languages.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与单个RNN进行序列预测不同，其中每个输入对应一个输出，seq2seq模型使我们摆脱了序列长度和顺序的限制，这使其非常适合两种语言之间的翻译。
- en: Consider the sentence `Je ne suis pas le chat noir` → `I am not the black cat`.
    Most of the words in the input sentence have a direct translation in the output
    sentence, but are in slightly different orders, e.g. `chat noir` and `black cat`.
    Because of the `ne/pas` construction there is also one more word in the input
    sentence. It would be difficult to produce a correct translation directly from
    the sequence of input words.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑句子`Je ne suis pas le chat noir` → `I am not the black cat`。输入句子中的大多数单词在输出句子中有直接的翻译，但顺序略有不同，例如`chat
    noir`和`black cat`。由于`ne/pas`结构，在输入句子中还有一个单词。直接从输入单词序列中产生正确的翻译将会很困难。
- en: With a seq2seq model the encoder creates a single vector which, in the ideal
    case, encodes the “meaning” of the input sequence into a single vector — a single
    point in some N dimensional space of sentences.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用seq2seq模型，编码器创建一个单一向量，理想情况下，将输入序列的“含义”编码为一个单一向量——一个句子空间中的单一点。
- en: The Encoder
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: The encoder of a seq2seq network is a RNN that outputs some value for every
    word from the input sentence. For every input word the encoder outputs a vector
    and a hidden state, and uses the hidden state for the next input word.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq网络的编码器是一个RNN，它为输入句子中的每个单词输出某个值。对于每个输入单词，编码器输出一个向量和一个隐藏状态，并将隐藏状态用于下一个输入单词。
- en: '![](../Images/9b7e299515676cf41cd2c0fd6ab1295d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b7e299515676cf41cd2c0fd6ab1295d.png)'
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The Decoder
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器
- en: The decoder is another RNN that takes the encoder output vector(s) and outputs
    a sequence of words to create the translation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器是另一个RNN，它接收编码器输出的向量，并输出一系列单词以创建翻译。
- en: Simple Decoder
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简单解码器
- en: In the simplest seq2seq decoder we use only last output of the encoder. This
    last output is sometimes called the *context vector* as it encodes context from
    the entire sequence. This context vector is used as the initial hidden state of
    the decoder.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的seq2seq解码器中，我们仅使用编码器的最后输出。这个最后输出有时被称为*上下文向量*，因为它从整个序列中编码上下文。这个上下文向量被用作解码器的初始隐藏状态。
- en: At every step of decoding, the decoder is given an input token and hidden state.
    The initial input token is the start-of-string `<SOS>` token, and the first hidden
    state is the context vector (the encoder’s last hidden state).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码的每一步，解码器都会得到一个输入标记和隐藏状态。初始输入标记是起始字符串`<SOS>`标记，第一个隐藏状态是上下文向量（编码器的最后一个隐藏状态）。
- en: '![](../Images/34b376e0c7299810f7349ab99c2c5497.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34b376e0c7299810f7349ab99c2c5497.png)'
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: I encourage you to train and observe the results of this model, but to save
    space we’ll be going straight for the gold and introducing the Attention Mechanism.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励您训练并观察这个模型的结果，但为了节省空间，我们将直接引入注意力机制。
- en: Attention Decoder
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力解码器
- en: If only the context vector is passed between the encoder and decoder, that single
    vector carries the burden of encoding the entire sentence.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只传递上下文向量在编码器和解码器之间，那么这个单一向量将承担编码整个句子的负担。
- en: Attention allows the decoder network to “focus” on a different part of the encoder’s
    outputs for every step of the decoder’s own outputs. First we calculate a set
    of *attention weights*. These will be multiplied by the encoder output vectors
    to create a weighted combination. The result (called `attn_applied` in the code)
    should contain information about that specific part of the input sequence, and
    thus help the decoder choose the right output words.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力允许解码器网络在每一步解码器自身输出的不同部分上“聚焦”编码器的输出。首先我们计算一组*注意力权重*。这些将与编码器输出向量相乘，以创建加权组合。结果（代码中称为`attn_applied`）应该包含关于输入序列的特定部分的信息，从而帮助解码器选择正确的输出单词。
- en: '![](../Images/3313f4800c7d01049e2a2ef2079e5905.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3313f4800c7d01049e2a2ef2079e5905.png)'
- en: Calculating the attention weights is done with another feed-forward layer `attn`,
    using the decoder’s input and hidden state as inputs. Because there are sentences
    of all sizes in the training data, to actually create and train this layer we
    have to choose a maximum sentence length (input length, for encoder outputs) that
    it can apply to. Sentences of the maximum length will use all the attention weights,
    while shorter sentences will only use the first few.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 计算注意力权重是通过另一个前馈层`attn`完成的，使用解码器的输入和隐藏状态作为输入。由于训练数据中存在各种大小的句子，为了实际创建和训练这一层，我们必须选择一个最大句子长度（输入长度，用于编码器输出）来应用。最大长度的句子将使用所有的注意力权重，而较短的句子将只使用前几个。
- en: '![](../Images/32ec68a6e0d29efae32b0f50db877598.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32ec68a6e0d29efae32b0f50db877598.png)'
- en: Bahdanau attention, also known as additive attention, is a commonly used attention
    mechanism in sequence-to-sequence models, particularly in neural machine translation
    tasks. It was introduced by Bahdanau et al. in their paper titled [Neural Machine
    Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf).
    This attention mechanism employs a learned alignment model to compute attention
    scores between the encoder and decoder hidden states. It utilizes a feed-forward
    neural network to calculate alignment scores.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Bahdanau注意力，也被称为加性注意力，是序列到序列模型中常用的注意力机制，特别是在神经机器翻译任务中。它是由Bahdanau等人在他们的论文中引入的，标题为[Neural
    Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)。这种注意力机制利用了一个学习对齐模型来计算编码器和解码器隐藏状态之间的注意力分数。它利用一个前馈神经网络来计算对齐分数。
- en: However, there are alternative attention mechanisms available, such as Luong
    attention, which computes attention scores by taking the dot product between the
    decoder hidden state and the encoder hidden states. It does not involve the non-linear
    transformation used in Bahdanau attention.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有其他可用的注意力机制，比如Luong注意力，它通过解码器隐藏状态和编码器隐藏状态之间的点积计算注意力分数。它不涉及Bahdanau注意力中使用的非线性变换。
- en: In this tutorial, we will be using Bahdanau attention. However, it would be
    a valuable exercise to explore modifying the attention mechanism to use Luong
    attention.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用Bahdanau注意力。然而，探索修改注意力机制以使用Luong注意力将是一项有价值的练习。
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There are other forms of attention that work around the length limitation by
    using a relative position approach. Read about “local attention” in [Effective
    Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他形式的注意力机制，通过使用相对位置方法来解决长度限制的问题。阅读关于“局部注意力”的内容，详见[Effective Approaches to
    Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)。
- en: Training
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Preparing Training Data
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备训练数据
- en: To train, for each pair we will need an input tensor (indexes of the words in
    the input sentence) and target tensor (indexes of the words in the target sentence).
    While creating these vectors we will append the EOS token to both sequences.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练，对于每一对，我们将需要一个输入张量（输入句子中单词的索引）和目标张量（目标句子中单词的索引）。在创建这些向量时，我们将在两个序列中都附加EOS标记。
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Training the Model
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: To train we run the input sentence through the encoder, and keep track of every
    output and the latest hidden state. Then the decoder is given the `<SOS>` token
    as its first input, and the last hidden state of the encoder as its first hidden
    state.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练，我们将输入句子通过编码器，并跟踪每个输出和最新的隐藏状态。然后解码器将得到`<SOS>`标记作为其第一个输入，编码器的最后隐藏状态作为其第一个隐藏状态。
- en: “Teacher forcing” is the concept of using the real target outputs as each next
    input, instead of using the decoder’s guess as the next input. Using teacher forcing
    causes it to converge faster but [when the trained network is exploited, it may
    exhibit instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: “教师强制”是使用真实目标输出作为每个下一个输入的概念，而不是使用解码器的猜测作为下一个输入。使用教师强制会导致更快地收敛，但[当训练好的网络被利用时，可能会表现出不稳定性](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf)。
- en: You can observe outputs of teacher-forced networks that read with coherent grammar
    but wander far from the correct translation - intuitively it has learned to represent
    the output grammar and can “pick up” the meaning once the teacher tells it the
    first few words, but it has not properly learned how to create the sentence from
    the translation in the first place.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以观察到使用强制教师网络的输出，这些网络具有连贯的语法，但与正确的翻译相去甚远 - 直觉上它已经学会了表示输出语法，并且一旦老师告诉它前几个单词，它就可以“捡起”含义，但它并没有正确地学会如何从一开始的翻译中创建句子。
- en: Because of the freedom PyTorch’s autograd gives us, we can randomly choose to
    use teacher forcing or not with a simple if statement. Turn `teacher_forcing_ratio`
    up to use more of it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PyTorch的自动求导给了我们自由，我们可以随机选择是否使用强制教师，只需使用简单的if语句。将`teacher_forcing_ratio`调高以更多地使用它。
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is a helper function to print time elapsed and estimated time remaining
    given the current time and progress %.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个辅助函数，用于打印经过的时间和给定当前时间和进度百分比的估计剩余时间。
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The whole training process looks like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 整个训练过程如下：
- en: Start a timer
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动计时器
- en: Initialize optimizers and criterion
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化优化器和标准
- en: Create set of training pairs
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建训练对集合
- en: Start empty losses array for plotting
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为绘图开始空损失数组
- en: Then we call `train` many times and occasionally print the progress (% of examples,
    time so far, estimated time) and average loss.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们多次调用`train`，偶尔打印进度（示例的百分比，到目前为止的时间，估计时间）和平均损失。
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Plotting results
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绘制结果
- en: Plotting is done with matplotlib, using the array of loss values `plot_losses`
    saved while training.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图是用matplotlib完成的，使用在训练时保存的损失值数组`plot_losses`。
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Evaluation
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: Evaluation is mostly the same as training, but there are no targets so we simply
    feed the decoder’s predictions back to itself for each step. Every time it predicts
    a word we add it to the output string, and if it predicts the EOS token we stop
    there. We also store the decoder’s attention outputs for display later.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 评估主要与训练相同，但没有目标，因此我们只需将解码器的预测反馈给自身进行每一步。每次预测一个单词时，我们将其添加到输出字符串中，如果预测到EOS令牌，则停在那里。我们还存储解码器的注意力输出以供稍后显示。
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can evaluate random sentences from the training set and print out the input,
    target, and output to make some subjective quality judgements:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从训练集中评估随机句子，并打印出输入、目标和输出，以进行一些主观质量判断：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Training and Evaluating
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和评估
- en: With all these helper functions in place (it looks like extra work, but it makes
    it easier to run multiple experiments) we can actually initialize a network and
    start training.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有了所有这些辅助函数（看起来像是额外的工作，但这样做可以更容易地运行多个实验），我们实际上可以初始化一个网络并开始训练。
- en: Remember that the input sentences were heavily filtered. For this small dataset
    we can use relatively small networks of 256 hidden nodes and a single GRU layer.
    After about 40 minutes on a MacBook CPU we’ll get some reasonable results.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，输入句子经过了严格过滤。对于这个小数据集，我们可以使用相对较小的256个隐藏节点和一个单独的GRU层的网络。在MacBook CPU上大约40分钟后，我们将得到一些合理的结果。
- en: Note
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you run this notebook you can train, interrupt the kernel, evaluate, and
    continue training later. Comment out the lines where the encoder and decoder are
    initialized and run `trainIters` again.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行此笔记本，您可以训练，中断内核，评估，并稍后继续训练。注释掉初始化编码器和解码器的行，并再次运行`trainIters`。
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![seq2seq translation tutorial](../Images/5015200eb4481feb8a71a658b384ec39.png)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![seq2seq翻译教程](../Images/5015200eb4481feb8a71a658b384ec39.png)'
- en: '![seq2seq translation tutorial](../Images/89adff7333b116436cf785388029ba1a.png)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![seq2seq翻译教程](../Images/89adff7333b116436cf785388029ba1a.png)'
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Set dropout layers to `eval` mode
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将dropout层设置为`eval`模式
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Visualizing Attention
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化注意力
- en: A useful property of the attention mechanism is its highly interpretable outputs.
    Because it is used to weight specific encoder outputs of the input sequence, we
    can imagine looking where the network is focused most at each time step.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制的一个有用特性是其高度可解释的输出。因为它用于加权输入序列的特定编码器输出，我们可以想象在每个时间步骤网络关注的地方。
- en: 'You could simply run `plt.matshow(attentions)` to see attention output displayed
    as a matrix. For a better viewing experience we will do the extra work of adding
    axes and labels:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以简单地运行`plt.matshow(attentions)`来查看注意力输出显示为矩阵。为了获得更好的查看体验，我们将额外添加坐标轴和标签：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![seq2seq translation tutorial](../Images/5412faceb18bc6fa2823be3ae1bdfd8d.png)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![seq2seq翻译教程](../Images/5412faceb18bc6fa2823be3ae1bdfd8d.png)'
- en: '![seq2seq translation tutorial](../Images/6e09db671ada03c61bdd1f32b6a7624b.png)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![seq2seq翻译教程](../Images/6e09db671ada03c61bdd1f32b6a7624b.png)'
- en: '![seq2seq translation tutorial](../Images/08bcfed65e8ab03ac7f380e20d421434.png)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![seq2seq翻译教程](../Images/08bcfed65e8ab03ac7f380e20d421434.png)'
- en: '![seq2seq translation tutorial](../Images/b1ba956974f3e844b0b0cea490cc1148.png)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![seq2seq翻译教程](../Images/b1ba956974f3e844b0b0cea490cc1148.png)'
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Exercises
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Try with a different dataset
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的数据集
- en: Another language pair
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种语言对
- en: Human → Machine (e.g. IOT commands)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类 → 机器（例如IOT命令）
- en: Chat → Response
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天 → 回复
- en: Question → Answer
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题 → 答案
- en: Replace the embeddings with pretrained word embeddings such as `word2vec` or
    `GloVe`
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用预训练的词嵌入（如`word2vec`或`GloVe`）替换嵌入
- en: Try with more layers, more hidden units, and more sentences. Compare the training
    time and results.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用更多层，更多隐藏单元和更多句子。比较训练时间和结果。
- en: 'If you use a translation file where pairs have two of the same phrase (`I am
    test \t I am test`), you can use this as an autoencoder. Try this:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用的翻译文件中有两个相同短语的配对（`I am test \t I am test`），您可以将其用作自动编码器。尝试这样做：
- en: Train as an autoencoder
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为自动编码器进行训练
- en: Save only the Encoder network
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅保存编码器网络
- en: Train a new Decoder for translation from there
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为翻译训练一个新的解码器
- en: '**Total running time of the script:** ( 7 minutes 20.607 seconds)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（7分钟20.607秒）'
- en: '[`Download Python source code: seq2seq_translation_tutorial.py`](../_downloads/3baf9960a4be104931872ff3ffda07b7/seq2seq_translation_tutorial.py)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：seq2seq_translation_tutorial.py`](../_downloads/3baf9960a4be104931872ff3ffda07b7/seq2seq_translation_tutorial.py)'
- en: '[`Download Jupyter notebook: seq2seq_translation_tutorial.ipynb`](../_downloads/032d653a4f5a9c1ec32b9fc7c989ffe1/seq2seq_translation_tutorial.ipynb)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：seq2seq_translation_tutorial.ipynb`](../_downloads/032d653a4f5a9c1ec32b9fc7c989ffe1/seq2seq_translation_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)'
