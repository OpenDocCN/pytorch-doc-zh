- en: torchtext.nn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torchtext.nn
- en: 原文：[https://pytorch.org/text/stable/nn_modules.html](https://pytorch.org/text/stable/nn_modules.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/text/stable/nn_modules.html](https://pytorch.org/text/stable/nn_modules.html)
- en: '## MultiheadAttentionContainer[](#multiheadattentioncontainer "Permalink to
    this heading")'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '## MultiheadAttentionContainer[](#multiheadattentioncontainer "跳转到此标题")'
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A multi-head attention container
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个多头注意力容器
- en: 'Parameters:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**nhead** – the number of heads in the multiheadattention model'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nhead** - 多头注意力模型中的头数'
- en: '**in_proj_container** – A container of multi-head in-projection linear layers
    (a.k.a nn.Linear).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**in_proj_container** - 多头in-projection线性层的容器（又名nn.Linear）。'
- en: '**attention_layer** – The custom attention layer. The input sent from MHA container
    to the attention layer is in the shape of (…, L, N * H, E / H) for query and (…,
    S, N * H, E / H) for key/value while the output shape of the attention layer is
    expected to be (…, L, N * H, E / H). The attention_layer needs to support broadcast
    if users want the overall MultiheadAttentionContainer with broadcast.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**attention_layer** - 自定义注意力层。从MHA容器发送到注意力层的输入的形状为（...，L，N * H，E / H）用于查询和（...，S，N
    * H，E / H）用于键/值，而注意力层的输出形状预计为（...，L，N * H，E / H）。如果用户希望整体MultiheadAttentionContainer具有广播，则attention_layer需要支持广播。'
- en: '**out_proj** – The multi-head out-projection layer (a.k.a nn.Linear).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**out_proj** - 多头输出投影层（又名nn.Linear）。'
- en: '**batch_first** – If `True`, then the input and output tensors are provided
    as (…, N, L, E). Default: `False`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_first** - 如果为`True`，则输入和输出张量将提供为（...，N，L，E）。默认值：`False`'
- en: 'Examples::'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '示例::'
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Parameters:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**query** (*Tensor*) – The query of the attention function. See “Attention
    Is All You Need” for more details.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询**（*张量*） - 注意力函数的查询。有关更多详细信息，请参阅“注意力就是一切”。'
- en: '**key** (*Tensor*) – The keys of the attention function. See “Attention Is
    All You Need” for more details.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**key**（*张量*） - 注意力函数的键。有关更多详细信息，请参阅“注意力就是一切”。'
- en: '**value** (*Tensor*) – The values of the attention function. See “Attention
    Is All You Need” for more details.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**value**（*张量*） - 注意力函数的值。有关更多详细信息，请参阅“注意力就是一切”。'
- en: '**attn_mask** (*BoolTensor**,* *optional*) – 3D mask that prevents attention
    to certain positions.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**attn_mask**（*BoolTensor*，*可选*） - 防止注意力集中在某些位置的3D掩码。'
- en: '**bias_k** (*Tensor**,* *optional*) – one more key and value sequence to be
    added to keys at sequence dim (dim=-3). Those are used for incremental decoding.
    Users should provide `bias_v`.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bias_k**（*张量*，*可选*） - 要添加到键的一个以上的键和值序列，以在序列维度（dim=-3）上进行增量解码。用户应提供`bias_v`。'
- en: '**bias_v** (*Tensor**,* *optional*) – one more key and value sequence to be
    added to values at sequence dim (dim=-3). Those are used for incremental decoding.
    Users should also provide `bias_k`.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bias_v**（*张量*，*可选*） - 要添加到值的一个以上的键和值序列，以在序列维度（dim=-3）上进行增量解码。用户还应提供`bias_k`。'
- en: 'Shape:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 形状：
- en: 'Inputs:'
  id: totrans-23
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入：
- en: ''
  id: totrans-24
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-25
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-26
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'query: \((..., L, N, E)\)'
  id: totrans-27
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询：\((...，L，N，E)\)
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-29
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'key: \((..., S, N, E)\)'
  id: totrans-30
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键：\((...，S，N，E)\)
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'value: \((..., S, N, E)\)'
  id: totrans-33
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值：\((...，S，N，E)\)
- en: ''
  id: totrans-34
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'attn_mask, bias_k and bias_v: same with the shape of the corresponding args
    in attention layer.'
  id: totrans-36
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: attn_mask，bias_k和bias_v：与注意力层中相应参数的形状相同。
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-38
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Outputs:'
  id: totrans-39
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出：
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-42
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'attn_output: \((..., L, N, E)\)'
  id: totrans-43
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: attn_output：\((...，L，N，E)\)
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-45
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'attn_output_weights: \((N * H, L, S)\)'
  id: totrans-46
  prefs:
  - PREF_BQ
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: attn_output_weights：\((N * H，L，S)\)
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Note: It’s optional to have the query/key/value inputs with more than three
    dimensions (for broadcast purpose). The MultiheadAttentionContainer module will
    operate on the last three dimensions.'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：具有超过三个维度的查询/键/值输入是可选的（用于广播目的）。MultiheadAttentionContainer模块将在最后三个维度上操作。
- en: ''
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: where where L is the target length, S is the sequence length, H is the number
    of attention heads, N is the batch size, and E is the embedding dimension.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 其中L是目标长度，S是序列长度，H是注意力头数，N是批量大小，E是嵌入维度。
- en: InProjContainer[](#inprojcontainer "Permalink to this heading")
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InProjContainer[](#inprojcontainer "跳转到此标题")
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: A in-proj container to project query/key/value in MultiheadAttention. This module
    happens before reshaping the projected query/key/value into multiple heads. See
    the linear layers (bottom) of Multi-head Attention in Fig 2 of Attention Is All
    You Need paper. Also check the usage example in torchtext.nn.MultiheadAttentionContainer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在MultiheadAttention中投影查询/键/值的in-proj容器。此模块在将投影的查询/键/值重新整形为多个头之前发生。请参阅“注意力就是一切”论文中图2中的Multi-head
    Attention底部的线性层。还可以在torchtext.nn.MultiheadAttentionContainer中查看用法示例。
- en: 'Parameters:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**query_proj** – a proj layer for query. A typical projection layer is torch.nn.Linear.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**query_proj** - 用于查询的投影层。典型的投影层是torch.nn.Linear。'
- en: '**key_proj** – a proj layer for key. A typical projection layer is torch.nn.Linear.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**key_proj** - 用于键的投影层。典型的投影层是torch.nn.Linear。'
- en: '**value_proj** – a proj layer for value. A typical projection layer is torch.nn.Linear.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**value_proj** - 用于值的投影层。典型的投影层是torch.nn.Linear。'
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Projects the input sequences using in-proj layers. query/key/value are simply
    passed to the forward func of query/key/value_proj, respectively.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用in-proj层对输入序列进行投影。查询/键/值分别简单地传递给查询/键/值_proj的前向函数。
- en: 'Parameters:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**query** (*Tensor*) – The query to be projected.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询**（*张量*） - 要投影的查询。'
- en: '**key** (*Tensor*) – The keys to be projected.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**key**（*张量*） - 要投影的键。'
- en: '**value** (*Tensor*) – The values to be projected.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**value**（*张量*） - 要投影的值。'
- en: 'Examples::'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '示例::'
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ScaledDotProduct[](#scaleddotproduct "Permalink to this heading")
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ScaledDotProduct[](#scaleddotproduct "跳转到此标题")
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Processes a projected query and key-value pair to apply scaled dot product attention.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 处理投影的查询和键值对以应用缩放的点积注意力。
- en: 'Parameters:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**dropout** ([*float*](https://docs.python.org/3/library/functions.html#float
    "(in Python v3.12)")) – probability of dropping an attention weight.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**辍学**（[*float*](https://docs.python.org/3/library/functions.html#float "(在Python
    v3.12中)")) - 放弃注意力权重的概率。'
- en: '**batch_first** – If `True`, then the input and output tensors are provided
    as (batch, seq, feature). Default: `False`'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_first** - 如果为`True`，则输入和输出张量将提供为（批次，序列，特征）。默认值：`False`'
- en: 'Examples::'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '示例::'
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Uses a scaled dot product with the projected key-value pair to update the projected
    query.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用投影的键值对进行缩放的点积以更新投影的查询。
- en: 'Parameters:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**query** (*Tensor*) – Projected query'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**query**（*Tensor*）- 投影的查询'
- en: '**key** (*Tensor*) – Projected key'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**key**（*Tensor*）- 投影的键'
- en: '**value** (*Tensor*) – Projected value'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**value**（*Tensor*）- 投影的值'
- en: '**attn_mask** (*BoolTensor**,* *optional*) – 3D mask that prevents attention
    to certain positions.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**attn_mask**（*BoolTensor*，*可选*）- 3D掩码，防止注意力集中在某些位置。'
- en: '**attn_mask** – 3D mask that prevents attention to certain positions.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**attn_mask** - 3D掩码，防止注意力集中在某些位置。'
- en: '**bias_k** (*Tensor**,* *optional*) – one more key and value sequence to be
    added to keys at sequence dim (dim=-3). Those are used for incremental decoding.
    Users should provide `bias_v`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bias_k**（*Tensor*，*可选*）- 一个额外的键和值序列，将添加到键的序列维度（dim=-3）。这些用于增量解码。用户应提供`bias_v`。'
- en: '**bias_v** (*Tensor**,* *optional*) – one more key and value sequence to be
    added to values at sequence dim (dim=-3). Those are used for incremental decoding.
    Users should also provide `bias_k`.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bias_v**（*Tensor*，*可选*）- 一个额外的键和值序列，将添加到值的序列维度（dim=-3）。这些用于增量解码。用户还应提供`bias_k`。'
- en: 'Shape:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '形状:'
- en: 'query: \((..., L, N * H, E / H)\)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: query：\((..., L, N * H, E / H)\)
- en: 'key: \((..., S, N * H, E / H)\)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键：\((..., S, N * H, E / H)\)
- en: 'value: \((..., S, N * H, E / H)\)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: value：\((..., S, N * H, E / H)\)
- en: 'attn_mask: \((N * H, L, S)\), positions with `True` are not allowed to attend'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: attn_mask：\((N * H, L, S)\)，具有`True`的位置不允许参与
- en: while `False` values will be unchanged.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当`False`值将保持不变。
- en: 'bias_k and bias_v:bias: \((1, N * H, E / H)\)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bias_k和bias_v：bias：\((1, N * H, E / H)\)
- en: 'Output: \((..., L, N * H, E / H)\), \((N * H, L, S)\)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出：\((..., L, N * H, E / H)\)，\((N * H, L, S)\)
- en: 'Note: It’s optional to have the query/key/value inputs with more than three
    dimensions (for broadcast purpose).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：具有超过三个维度的查询/键/值输入是可选的（用于广播目的）。
- en: The ScaledDotProduct module will operate on the last three dimensions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ScaledDotProduct模块将在最后三个维度上操作。
- en: where L is the target length, S is the source length, H is the number of attention
    heads, N is the batch size, and E is the embedding dimension.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中L是目标长度，S是源长度，H是注意力头的数量，N是批量大小，E是嵌入维度。
