- en: torch.nn.functional
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torch.nn.functional
- en: 原文：[https://pytorch.org/docs/stable/nn.functional.html](https://pytorch.org/docs/stable/nn.functional.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/nn.functional.html](https://pytorch.org/docs/stable/nn.functional.html)
- en: Convolution functions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积函数
- en: '| [`conv1d`](generated/torch.nn.functional.conv1d.html#torch.nn.functional.conv1d
    "torch.nn.functional.conv1d") | Applies a 1D convolution over an input signal
    composed of several input planes. |'
  id: totrans-3
  prefs: []
  type: TYPE_TB
  zh: '| [`conv1d`](generated/torch.nn.functional.conv1d.html#torch.nn.functional.conv1d
    "torch.nn.functional.conv1d") | 对由多个输入平面组成的输入信号应用1D卷积。 |'
- en: '| [`conv2d`](generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d
    "torch.nn.functional.conv2d") | Applies a 2D convolution over an input image composed
    of several input planes. |'
  id: totrans-4
  prefs: []
  type: TYPE_TB
  zh: '| [`conv2d`](generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d
    "torch.nn.functional.conv2d") | 对由多个输入平面组成的输入图像应用2D卷积。 |'
- en: '| [`conv3d`](generated/torch.nn.functional.conv3d.html#torch.nn.functional.conv3d
    "torch.nn.functional.conv3d") | Applies a 3D convolution over an input image composed
    of several input planes. |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| [`conv3d`](generated/torch.nn.functional.conv3d.html#torch.nn.functional.conv3d
    "torch.nn.functional.conv3d") | 对由多个输入平面组成的输入图像应用3D卷积。 |'
- en: '| [`conv_transpose1d`](generated/torch.nn.functional.conv_transpose1d.html#torch.nn.functional.conv_transpose1d
    "torch.nn.functional.conv_transpose1d") | Applies a 1D transposed convolution
    operator over an input signal composed of several input planes, sometimes also
    called "deconvolution". |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| [`conv_transpose1d`](generated/torch.nn.functional.conv_transpose1d.html#torch.nn.functional.conv_transpose1d
    "torch.nn.functional.conv_transpose1d") | 对由多个输入平面组成的输入信号应用1D转置卷积运算，有时也称为“反卷积”。
    |'
- en: '| [`conv_transpose2d`](generated/torch.nn.functional.conv_transpose2d.html#torch.nn.functional.conv_transpose2d
    "torch.nn.functional.conv_transpose2d") | Applies a 2D transposed convolution
    operator over an input image composed of several input planes, sometimes also
    called "deconvolution". |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| [`conv_transpose2d`](generated/torch.nn.functional.conv_transpose2d.html#torch.nn.functional.conv_transpose2d
    "torch.nn.functional.conv_transpose2d") | 对由多个输入平面组成的输入图像应用2D转置卷积运算，有时也称为“反卷积”。
    |'
- en: '| [`conv_transpose3d`](generated/torch.nn.functional.conv_transpose3d.html#torch.nn.functional.conv_transpose3d
    "torch.nn.functional.conv_transpose3d") | Applies a 3D transposed convolution
    operator over an input image composed of several input planes, sometimes also
    called "deconvolution" |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| [`conv_transpose3d`](generated/torch.nn.functional.conv_transpose3d.html#torch.nn.functional.conv_transpose3d
    "torch.nn.functional.conv_transpose3d") | 对由多个输入平面组成的输入图像应用3D转置卷积运算，有时也称为“反卷积”
    |'
- en: '| [`unfold`](generated/torch.nn.functional.unfold.html#torch.nn.functional.unfold
    "torch.nn.functional.unfold") | Extract sliding local blocks from a batched input
    tensor. |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| [`unfold`](generated/torch.nn.functional.unfold.html#torch.nn.functional.unfold
    "torch.nn.functional.unfold") | 从批量输入张量中提取滑动的局部块。 |'
- en: '| [`fold`](generated/torch.nn.functional.fold.html#torch.nn.functional.fold
    "torch.nn.functional.fold") | Combine an array of sliding local blocks into a
    large containing tensor. |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| [`fold`](generated/torch.nn.functional.fold.html#torch.nn.functional.fold
    "torch.nn.functional.fold") | 将滑动的局部块数组合并成一个包含大张量。 |'
- en: Pooling functions
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化函数
- en: '| [`avg_pool1d`](generated/torch.nn.functional.avg_pool1d.html#torch.nn.functional.avg_pool1d
    "torch.nn.functional.avg_pool1d") | Applies a 1D average pooling over an input
    signal composed of several input planes. |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| [`avg_pool1d`](generated/torch.nn.functional.avg_pool1d.html#torch.nn.functional.avg_pool1d
    "torch.nn.functional.avg_pool1d") | 对由多个输入平面组成的输入信号应用1D平均池化。 |'
- en: '| [`avg_pool2d`](generated/torch.nn.functional.avg_pool2d.html#torch.nn.functional.avg_pool2d
    "torch.nn.functional.avg_pool2d") | Applies 2D average-pooling operation in $kH
    \times kW$kH×kW regions by step size $sH \times sW$sH×sW steps. |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| [`avg_pool2d`](generated/torch.nn.functional.avg_pool2d.html#torch.nn.functional.avg_pool2d
    "torch.nn.functional.avg_pool2d") | 通过步长$sH \times sW$在$kH \times kW$区域内应用2D平均池化操作。
    |'
- en: '| [`avg_pool3d`](generated/torch.nn.functional.avg_pool3d.html#torch.nn.functional.avg_pool3d
    "torch.nn.functional.avg_pool3d") | Applies 3D average-pooling operation in $kT
    \times kH \times kW$kT×kH×kW regions by step size $sT \times sH \times sW$sT×sH×sW
    steps. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| [`avg_pool3d`](generated/torch.nn.functional.avg_pool3d.html#torch.nn.functional.avg_pool3d
    "torch.nn.functional.avg_pool3d") | 通过步长$sT \times sH \times sW$在$kT \times kH
    \times kW$区域内应用3D平均池化操作。 |'
- en: '| [`max_pool1d`](generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d
    "torch.nn.functional.max_pool1d") | Applies a 1D max pooling over an input signal
    composed of several input planes. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| [`max_pool1d`](generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d
    "torch.nn.functional.max_pool1d") | 对由多个输入平面组成的输入信号应用1D最大池化。 |'
- en: '| [`max_pool2d`](generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d
    "torch.nn.functional.max_pool2d") | Applies a 2D max pooling over an input signal
    composed of several input planes. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| [`max_pool2d`](generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d
    "torch.nn.functional.max_pool2d") | 对由多个输入平面组成的输入信号应用2D最大池化。 |'
- en: '| [`max_pool3d`](generated/torch.nn.functional.max_pool3d.html#torch.nn.functional.max_pool3d
    "torch.nn.functional.max_pool3d") | Applies a 3D max pooling over an input signal
    composed of several input planes. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| [`max_pool3d`](generated/torch.nn.functional.max_pool3d.html#torch.nn.functional.max_pool3d
    "torch.nn.functional.max_pool3d") | 对由多个输入平面组成的输入信号应用3D最大池化。 |'
- en: '| [`max_unpool1d`](generated/torch.nn.functional.max_unpool1d.html#torch.nn.functional.max_unpool1d
    "torch.nn.functional.max_unpool1d") | Compute a partial inverse of `MaxPool1d`.
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| [`max_unpool1d`](generated/torch.nn.functional.max_unpool1d.html#torch.nn.functional.max_unpool1d
    "torch.nn.functional.max_unpool1d") | 计算`MaxPool1d`的部分逆。 |'
- en: '| [`max_unpool2d`](generated/torch.nn.functional.max_unpool2d.html#torch.nn.functional.max_unpool2d
    "torch.nn.functional.max_unpool2d") | Compute a partial inverse of `MaxPool2d`.
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| [`max_unpool2d`](generated/torch.nn.functional.max_unpool2d.html#torch.nn.functional.max_unpool2d
    "torch.nn.functional.max_unpool2d") | 计算`MaxPool2d`的部分逆。 |'
- en: '| [`max_unpool3d`](generated/torch.nn.functional.max_unpool3d.html#torch.nn.functional.max_unpool3d
    "torch.nn.functional.max_unpool3d") | Compute a partial inverse of `MaxPool3d`.
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| [`max_unpool3d`](generated/torch.nn.functional.max_unpool3d.html#torch.nn.functional.max_unpool3d
    "torch.nn.functional.max_unpool3d") | 计算`MaxPool3d`的部分逆。 |'
- en: '| [`lp_pool1d`](generated/torch.nn.functional.lp_pool1d.html#torch.nn.functional.lp_pool1d
    "torch.nn.functional.lp_pool1d") | Apply a 1D power-average pooling over an input
    signal composed of several input planes. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| [`lp_pool1d`](生成/torch.nn.functional.lp_pool1d.html#torch.nn.functional.lp_pool1d
    "torch.nn.functional.lp_pool1d") | 对由多个输入平面组成的输入信号应用1D幂平均池化。 |'
- en: '| [`lp_pool2d`](generated/torch.nn.functional.lp_pool2d.html#torch.nn.functional.lp_pool2d
    "torch.nn.functional.lp_pool2d") | Apply a 2D power-average pooling over an input
    signal composed of several input planes. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| [`lp_pool2d`](生成/torch.nn.functional.lp_pool2d.html#torch.nn.functional.lp_pool2d
    "torch.nn.functional.lp_pool2d") | 对由多个输入平面组成的输入信号应用2D幂平均池化。 |'
- en: '| [`adaptive_max_pool1d`](generated/torch.nn.functional.adaptive_max_pool1d.html#torch.nn.functional.adaptive_max_pool1d
    "torch.nn.functional.adaptive_max_pool1d") | Applies a 1D adaptive max pooling
    over an input signal composed of several input planes. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| [`adaptive_max_pool1d`](生成/torch.nn.functional.adaptive_max_pool1d.html#torch.nn.functional.adaptive_max_pool1d
    "torch.nn.functional.adaptive_max_pool1d") | 对由多个输入平面组成的输入信号应用1D自适应最大池化。 |'
- en: '| [`adaptive_max_pool2d`](generated/torch.nn.functional.adaptive_max_pool2d.html#torch.nn.functional.adaptive_max_pool2d
    "torch.nn.functional.adaptive_max_pool2d") | Applies a 2D adaptive max pooling
    over an input signal composed of several input planes. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [`adaptive_max_pool2d`](生成/torch.nn.functional.adaptive_max_pool2d.html#torch.nn.functional.adaptive_max_pool2d
    "torch.nn.functional.adaptive_max_pool2d") | 对由多个输入平面组成的输入信号应用2D自适应最大池化。 |'
- en: '| [`adaptive_max_pool3d`](generated/torch.nn.functional.adaptive_max_pool3d.html#torch.nn.functional.adaptive_max_pool3d
    "torch.nn.functional.adaptive_max_pool3d") | Applies a 3D adaptive max pooling
    over an input signal composed of several input planes. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| [`adaptive_max_pool3d`](生成/torch.nn.functional.adaptive_max_pool3d.html#torch.nn.functional.adaptive_max_pool3d
    "torch.nn.functional.adaptive_max_pool3d") | 对由多个输入平面组成的输入信号应用3D自适应最大池化。 |'
- en: '| [`adaptive_avg_pool1d`](generated/torch.nn.functional.adaptive_avg_pool1d.html#torch.nn.functional.adaptive_avg_pool1d
    "torch.nn.functional.adaptive_avg_pool1d") | Applies a 1D adaptive average pooling
    over an input signal composed of several input planes. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| [`adaptive_avg_pool1d`](生成/torch.nn.functional.adaptive_avg_pool1d.html#torch.nn.functional.adaptive_avg_pool1d
    "torch.nn.functional.adaptive_avg_pool1d") | 对由多个输入平面组成的输入信号应用1D自适应平均池化。 |'
- en: '| [`adaptive_avg_pool2d`](generated/torch.nn.functional.adaptive_avg_pool2d.html#torch.nn.functional.adaptive_avg_pool2d
    "torch.nn.functional.adaptive_avg_pool2d") | Apply a 2D adaptive average pooling
    over an input signal composed of several input planes. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| [`adaptive_avg_pool2d`](生成/torch.nn.functional.adaptive_avg_pool2d.html#torch.nn.functional.adaptive_avg_pool2d
    "torch.nn.functional.adaptive_avg_pool2d") | 对由多个输入平面组成的输入信号应用2D自适应平均池化。 |'
- en: '| [`adaptive_avg_pool3d`](generated/torch.nn.functional.adaptive_avg_pool3d.html#torch.nn.functional.adaptive_avg_pool3d
    "torch.nn.functional.adaptive_avg_pool3d") | Apply a 3D adaptive average pooling
    over an input signal composed of several input planes. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| [`adaptive_avg_pool3d`](生成/torch.nn.functional.adaptive_avg_pool3d.html#torch.nn.functional.adaptive_avg_pool3d
    "torch.nn.functional.adaptive_avg_pool3d") | 对由多个输入平面组成的输入信号应用3D自适应平均池化。 |'
- en: '| [`fractional_max_pool2d`](generated/torch.nn.functional.fractional_max_pool2d.html#torch.nn.functional.fractional_max_pool2d
    "torch.nn.functional.fractional_max_pool2d") | Applies 2D fractional max pooling
    over an input signal composed of several input planes. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| [`fractional_max_pool2d`](生成/torch.nn.functional.fractional_max_pool2d.html#torch.nn.functional.fractional_max_pool2d
    "torch.nn.functional.fractional_max_pool2d") | 对由多个输入平面组成的输入信号应用2D分数最大池化。 |'
- en: '| [`fractional_max_pool3d`](generated/torch.nn.functional.fractional_max_pool3d.html#torch.nn.functional.fractional_max_pool3d
    "torch.nn.functional.fractional_max_pool3d") | Applies 3D fractional max pooling
    over an input signal composed of several input planes. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| [`fractional_max_pool3d`](生成/torch.nn.functional.fractional_max_pool3d.html#torch.nn.functional.fractional_max_pool3d
    "torch.nn.functional.fractional_max_pool3d") | 对由多个输入平面组成的输入信号应用3D分数最大池化。 |'
- en: Attention Mechanisms
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力机制
- en: '| [`scaled_dot_product_attention`](generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention
    "torch.nn.functional.scaled_dot_product_attention") | Computes scaled dot product
    attention on query, key and value tensors, using an optional attention mask if
    passed, and applying dropout if a probability greater than 0.0 is specified. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| [`scaled_dot_product_attention`](生成/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention
    "torch.nn.functional.scaled_dot_product_attention") | 在查询、键和值张量上计算缩放点积注意力，如果传递了可选的注意力掩码，则应用dropout，如果指定了大于0.0的概率。
    |'
- en: Non-linear activation functions[](#non-linear-activation-functions "Permalink
    to this heading")
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性激活函数[](#non-linear-activation-functions "跳转到此标题")
- en: '| [`threshold`](generated/torch.nn.functional.threshold.html#torch.nn.functional.threshold
    "torch.nn.functional.threshold") | Apply a threshold to each element of the input
    Tensor. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| [`threshold`](生成/torch.nn.functional.threshold.html#torch.nn.functional.threshold
    "torch.nn.functional.threshold") | 对输入张量的每个元素应用阈值。 |'
- en: '| [`threshold_`](generated/torch.nn.functional.threshold_.html#torch.nn.functional.threshold_
    "torch.nn.functional.threshold_") | In-place version of [`threshold()`](generated/torch.nn.functional.threshold.html#torch.nn.functional.threshold
    "torch.nn.functional.threshold"). |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| [`threshold_`](生成/torch.nn.functional.threshold_.html#torch.nn.functional.threshold_
    "torch.nn.functional.threshold_") | [`threshold()`](生成/torch.nn.functional.threshold.html#torch.nn.functional.threshold
    "torch.nn.functional.threshold")的原地版本。 |'
- en: '| [`relu`](generated/torch.nn.functional.relu.html#torch.nn.functional.relu
    "torch.nn.functional.relu") | Applies the rectified linear unit function element-wise.
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| [`relu`](生成/torch.nn.functional.relu.html#torch.nn.functional.relu "torch.nn.functional.relu")
    | 逐元素应用修正线性单元函数。 |'
- en: '| [`relu_`](generated/torch.nn.functional.relu_.html#torch.nn.functional.relu_
    "torch.nn.functional.relu_") | In-place version of [`relu()`](generated/torch.nn.functional.relu.html#torch.nn.functional.relu
    "torch.nn.functional.relu"). |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| [`relu_`](生成/torch.nn.functional.relu_.html#torch.nn.functional.relu_ "torch.nn.functional.relu_")
    | [`relu()`](生成/torch.nn.functional.relu.html#torch.nn.functional.relu "torch.nn.functional.relu")的原地版本。
    |'
- en: '| [`hardtanh`](generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh
    "torch.nn.functional.hardtanh") | Applies the HardTanh function element-wise.
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| [`hardtanh`](generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh
    "torch.nn.functional.hardtanh") | 逐元素应用HardTanh函数。 |'
- en: '| [`hardtanh_`](generated/torch.nn.functional.hardtanh_.html#torch.nn.functional.hardtanh_
    "torch.nn.functional.hardtanh_") | In-place version of [`hardtanh()`](generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh
    "torch.nn.functional.hardtanh"). |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| [`hardtanh_`](generated/torch.nn.functional.hardtanh_.html#torch.nn.functional.hardtanh_
    "torch.nn.functional.hardtanh_") | [`hardtanh()`](generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh
    "torch.nn.functional.hardtanh")的原地版本。 |'
- en: '| [`hardswish`](generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish
    "torch.nn.functional.hardswish") | Apply hardswish function, element-wise. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| [`hardswish`](generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish
    "torch.nn.functional.hardswish") | 应用硬swish函数，逐元素。 |'
- en: '| [`relu6`](generated/torch.nn.functional.relu6.html#torch.nn.functional.relu6
    "torch.nn.functional.relu6") | Applies the element-wise function $\text{ReLU6}(x)
    = \min(\max(0,x), 6)$ReLU6(x)=min(max(0,x),6). |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| [`relu6`](generated/torch.nn.functional.relu6.html#torch.nn.functional.relu6
    "torch.nn.functional.relu6") | 应用逐元素函数 $\text{ReLU6}(x) = \min(\max(0,x), 6)$。
    |'
- en: '| [`elu`](generated/torch.nn.functional.elu.html#torch.nn.functional.elu "torch.nn.functional.elu")
    | Apply the Exponential Linear Unit (ELU) function element-wise. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| [`elu`](generated/torch.nn.functional.elu.html#torch.nn.functional.elu "torch.nn.functional.elu")
    | 逐元素应用指数线性单元（ELU）函数。 |'
- en: '| [`elu_`](generated/torch.nn.functional.elu_.html#torch.nn.functional.elu_
    "torch.nn.functional.elu_") | In-place version of [`elu()`](generated/torch.nn.functional.elu.html#torch.nn.functional.elu
    "torch.nn.functional.elu"). |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| [`elu_`](generated/torch.nn.functional.elu_.html#torch.nn.functional.elu_
    "torch.nn.functional.elu_") | [`elu()`](generated/torch.nn.functional.elu.html#torch.nn.functional.elu
    "torch.nn.functional.elu")的原地版本。 |'
- en: '| [`selu`](generated/torch.nn.functional.selu.html#torch.nn.functional.selu
    "torch.nn.functional.selu") | Applies element-wise, $\text{SELU}(x) = scale *
    (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))$SELU(x)=scale∗(max(0,x)+min(0,α∗(exp(x)−1))),
    with $\alpha=1.6732632423543772848170429916717$α=1.6732632423543772848170429916717
    and $scale=1.0507009873554804934193349852946$scale=1.0507009873554804934193349852946.
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| [`selu`](generated/torch.nn.functional.selu.html#torch.nn.functional.selu
    "torch.nn.functional.selu") | 逐元素应用，$\text{SELU}(x) = scale * (\max(0,x) + \min(0,
    \alpha * (\exp(x) - 1)))$，其中 $\alpha=1.6732632423543772848170429916717$ 和 $scale=1.0507009873554804934193349852946$。
    |'
- en: '| [`celu`](generated/torch.nn.functional.celu.html#torch.nn.functional.celu
    "torch.nn.functional.celu") | Applies element-wise, $\text{CELU}(x) = \max(0,x)
    + \min(0, \alpha * (\exp(x/\alpha) - 1))$CELU(x)=max(0,x)+min(0,α∗(exp(x/α)−1)).
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| [`celu`](generated/torch.nn.functional.celu.html#torch.nn.functional.celu
    "torch.nn.functional.celu") | 逐元素应用，$\text{CELU}(x) = \max(0,x) + \min(0, \alpha
    * (\exp(x/\alpha) - 1))$。 |'
- en: '| [`leaky_relu`](generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu
    "torch.nn.functional.leaky_relu") | Applies element-wise, $\text{LeakyReLU}(x)
    = \max(0, x) + \text{negative\_slope} * \min(0, x)$LeakyReLU(x)=max(0,x)+negative_slope∗min(0,x)
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| [`leaky_relu`](generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu
    "torch.nn.functional.leaky_relu") | 逐元素应用，$\text{LeakyReLU}(x) = \max(0, x) +
    \text{negative\_slope} * \min(0, x)$。 |'
- en: '| [`leaky_relu_`](generated/torch.nn.functional.leaky_relu_.html#torch.nn.functional.leaky_relu_
    "torch.nn.functional.leaky_relu_") | In-place version of [`leaky_relu()`](generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu
    "torch.nn.functional.leaky_relu"). |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| [`leaky_relu_`](generated/torch.nn.functional.leaky_relu_.html#torch.nn.functional.leaky_relu_
    "torch.nn.functional.leaky_relu_") | [`leaky_relu()`](generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu
    "torch.nn.functional.leaky_relu")的原地版本。 |'
- en: '| [`prelu`](generated/torch.nn.functional.prelu.html#torch.nn.functional.prelu
    "torch.nn.functional.prelu") | Applies element-wise the function $\text{PReLU}(x)
    = \max(0,x) + \text{weight} * \min(0,x)$PReLU(x)=max(0,x)+weight∗min(0,x) where
    weight is a learnable parameter. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| [`prelu`](generated/torch.nn.functional.prelu.html#torch.nn.functional.prelu
    "torch.nn.functional.prelu") | 逐元素应用函数 $\text{PReLU}(x) = \max(0,x) + \text{weight}
    * \min(0,x)$，其中weight是可学习参数。 |'
- en: '| [`rrelu`](generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu
    "torch.nn.functional.rrelu") | Randomized leaky ReLU. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| [`rrelu`](generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu
    "torch.nn.functional.rrelu") | 随机泄漏的ReLU。 |'
- en: '| [`rrelu_`](generated/torch.nn.functional.rrelu_.html#torch.nn.functional.rrelu_
    "torch.nn.functional.rrelu_") | In-place version of [`rrelu()`](generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu
    "torch.nn.functional.rrelu"). |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| [`rrelu_`](generated/torch.nn.functional.rrelu_.html#torch.nn.functional.rrelu_
    "torch.nn.functional.rrelu_") | [`rrelu()`](generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu
    "torch.nn.functional.rrelu")的原地版本。 |'
- en: '| [`glu`](generated/torch.nn.functional.glu.html#torch.nn.functional.glu "torch.nn.functional.glu")
    | The gated linear unit. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| [`glu`](generated/torch.nn.functional.glu.html#torch.nn.functional.glu "torch.nn.functional.glu")
    | 门控线性单元。 |'
- en: '| [`gelu`](generated/torch.nn.functional.gelu.html#torch.nn.functional.gelu
    "torch.nn.functional.gelu") | When the approximate argument is ''none'', it applies
    element-wise the function $\text{GELU}(x) = x * \Phi(x)$GELU(x)=x∗Φ(x) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| [`gelu`](generated/torch.nn.functional.gelu.html#torch.nn.functional.gelu
    "torch.nn.functional.gelu") | 当近似参数为''none''时，逐元素应用函数 $\text{GELU}(x) = x * \Phi(x)$。
    |'
- en: '| [`logsigmoid`](generated/torch.nn.functional.logsigmoid.html#torch.nn.functional.logsigmoid
    "torch.nn.functional.logsigmoid") | Applies element-wise $\text{LogSigmoid}(x_i)
    = \log \left(\frac{1}{1 + \exp(-x_i)}\right)$LogSigmoid(xi​)=log(1+exp(−xi​)1​)
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| [`logsigmoid`](generated/torch.nn.functional.logsigmoid.html#torch.nn.functional.logsigmoid
    "torch.nn.functional.logsigmoid") | 逐元素应用 $\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1
    + \exp(-x_i)}\right)$。 |'
- en: '| [`hardshrink`](generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink
    "torch.nn.functional.hardshrink") | Applies the hard shrinkage function element-wise
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| [`hardshrink`](generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink
    "torch.nn.functional.hardshrink") | 逐元素应用硬收缩函数。 |'
- en: '| [`tanhshrink`](generated/torch.nn.functional.tanhshrink.html#torch.nn.functional.tanhshrink
    "torch.nn.functional.tanhshrink") | Applies element-wise, $\text{Tanhshrink}(x)
    = x - \text{Tanh}(x)$Tanhshrink(x)=x−Tanh(x) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| [`tanhshrink`](generated/torch.nn.functional.tanhshrink.html#torch.nn.functional.tanhshrink
    "torch.nn.functional.tanhshrink") | 应用函数$\text{Tanhshrink}(x) = x - \text{Tanh}(x)$
    |'
- en: '| [`softsign`](generated/torch.nn.functional.softsign.html#torch.nn.functional.softsign
    "torch.nn.functional.softsign") | Applies element-wise, the function $\text{SoftSign}(x)
    = \frac{x}{1 + &#124;x&#124;}$SoftSign(x)=1+∣x∣x​ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| [`softsign`](generated/torch.nn.functional.softsign.html#torch.nn.functional.softsign
    "torch.nn.functional.softsign") | 对每个元素应用函数$\text{SoftSign}(x) = \frac{x}{1 +
    |x|}$ |'
- en: '| [`softplus`](generated/torch.nn.functional.softplus.html#torch.nn.functional.softplus
    "torch.nn.functional.softplus") | Applies element-wise, the function $\text{Softplus}(x)
    = \frac{1}{\beta} * \log(1 + \exp(\beta * x))$Softplus(x)=β1​∗log(1+exp(β∗x)).
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| [`softplus`](generated/torch.nn.functional.softplus.html#torch.nn.functional.softplus
    "torch.nn.functional.softplus") | 对每个元素应用函数$\text{Softplus}(x) = \frac{1}{\beta}
    * \log(1 + \exp(\beta * x))$ |'
- en: '| [`softmin`](generated/torch.nn.functional.softmin.html#torch.nn.functional.softmin
    "torch.nn.functional.softmin") | Apply a softmin function. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| [`softmin`](generated/torch.nn.functional.softmin.html#torch.nn.functional.softmin
    "torch.nn.functional.softmin") | 应用softmin函数。 |'
- en: '| [`softmax`](generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax
    "torch.nn.functional.softmax") | Apply a softmax function. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| [`softmax`](generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax
    "torch.nn.functional.softmax") | 应用softmax函数。 |'
- en: '| [`softshrink`](generated/torch.nn.functional.softshrink.html#torch.nn.functional.softshrink
    "torch.nn.functional.softshrink") | Applies the soft shrinkage function elementwise
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| [`softshrink`](generated/torch.nn.functional.softshrink.html#torch.nn.functional.softshrink
    "torch.nn.functional.softshrink") | 对每个元素应用软收缩函数。 |'
- en: '| [`gumbel_softmax`](generated/torch.nn.functional.gumbel_softmax.html#torch.nn.functional.gumbel_softmax
    "torch.nn.functional.gumbel_softmax") | Sample from the Gumbel-Softmax distribution
    ([Link 1](https://arxiv.org/abs/1611.00712) [Link 2](https://arxiv.org/abs/1611.01144))
    and optionally discretize. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| [`gumbel_softmax`](generated/torch.nn.functional.gumbel_softmax.html#torch.nn.functional.gumbel_softmax
    "torch.nn.functional.gumbel_softmax") | 从Gumbel-Softmax分布中采样，并可选择离散化。 |'
- en: '| [`log_softmax`](generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax
    "torch.nn.functional.log_softmax") | Apply a softmax followed by a logarithm.
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| [`log_softmax`](generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax
    "torch.nn.functional.log_softmax") | 应用softmax后再取对数。 |'
- en: '| [`tanh`](generated/torch.nn.functional.tanh.html#torch.nn.functional.tanh
    "torch.nn.functional.tanh") | Applies element-wise, $\text{Tanh}(x) = \tanh(x)
    = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}$Tanh(x)=tanh(x)=exp(x)+exp(−x)exp(x)−exp(−x)​
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| [`tanh`](generated/torch.nn.functional.tanh.html#torch.nn.functional.tanh
    "torch.nn.functional.tanh") | 对每个元素应用$\text{Tanh}(x) = \tanh(x) = \frac{\exp(x)
    - \exp(-x)}{\exp(x) + \exp(-x)}$ |'
- en: '| [`sigmoid`](generated/torch.nn.functional.sigmoid.html#torch.nn.functional.sigmoid
    "torch.nn.functional.sigmoid") | Applies the element-wise function $\text{Sigmoid}(x)
    = \frac{1}{1 + \exp(-x)}$Sigmoid(x)=1+exp(−x)1​ |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| [`sigmoid`](generated/torch.nn.functional.sigmoid.html#torch.nn.functional.sigmoid
    "torch.nn.functional.sigmoid") | 对每个元素应用函数$\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}$
    |'
- en: '| [`hardsigmoid`](generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid
    "torch.nn.functional.hardsigmoid") | Apply the Hardsigmoid function element-wise.
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [`hardsigmoid`](generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid
    "torch.nn.functional.hardsigmoid") | 对每个元素应用Hardsigmoid函数。 |'
- en: '| [`silu`](generated/torch.nn.functional.silu.html#torch.nn.functional.silu
    "torch.nn.functional.silu") | Apply the Sigmoid Linear Unit (SiLU) function, element-wise.
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [`silu`](generated/torch.nn.functional.silu.html#torch.nn.functional.silu
    "torch.nn.functional.silu") | 对每个元素应用Sigmoid线性单元（SiLU）函数。 |'
- en: '| [`mish`](generated/torch.nn.functional.mish.html#torch.nn.functional.mish
    "torch.nn.functional.mish") | Apply the Mish function, element-wise. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| [`mish`](generated/torch.nn.functional.mish.html#torch.nn.functional.mish
    "torch.nn.functional.mish") | 对每个元素应用Mish函数。 |'
- en: '| [`batch_norm`](generated/torch.nn.functional.batch_norm.html#torch.nn.functional.batch_norm
    "torch.nn.functional.batch_norm") | Apply Batch Normalization for each channel
    across a batch of data. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| [`batch_norm`](generated/torch.nn.functional.batch_norm.html#torch.nn.functional.batch_norm
    "torch.nn.functional.batch_norm") | 对数据批次中的每个通道应用批次归一化。 |'
- en: '| [`group_norm`](generated/torch.nn.functional.group_norm.html#torch.nn.functional.group_norm
    "torch.nn.functional.group_norm") | Apply Group Normalization for last certain
    number of dimensions. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| [`group_norm`](generated/torch.nn.functional.group_norm.html#torch.nn.functional.group_norm
    "torch.nn.functional.group_norm") | 对最后若干维度应用组归一化。 |'
- en: '| [`instance_norm`](generated/torch.nn.functional.instance_norm.html#torch.nn.functional.instance_norm
    "torch.nn.functional.instance_norm") | Apply Instance Normalization independently
    for each channel in every data sample within a batch. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| [`instance_norm`](generated/torch.nn.functional.instance_norm.html#torch.nn.functional.instance_norm
    "torch.nn.functional.instance_norm") | 对每个数据样本中的每个通道独立应用实例归一化。 |'
- en: '| [`layer_norm`](generated/torch.nn.functional.layer_norm.html#torch.nn.functional.layer_norm
    "torch.nn.functional.layer_norm") | Apply Layer Normalization for last certain
    number of dimensions. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| [`layer_norm`](generated/torch.nn.functional.layer_norm.html#torch.nn.functional.layer_norm
    "torch.nn.functional.layer_norm") | 对最后若干维度应用层归一化。 |'
- en: '| [`local_response_norm`](generated/torch.nn.functional.local_response_norm.html#torch.nn.functional.local_response_norm
    "torch.nn.functional.local_response_norm") | Apply local response normalization
    over an input signal. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| [`local_response_norm`](generated/torch.nn.functional.local_response_norm.html#torch.nn.functional.local_response_norm
    "torch.nn.functional.local_response_norm") | 对输入信号应用局部响应归一化。 |'
- en: '| [`normalize`](generated/torch.nn.functional.normalize.html#torch.nn.functional.normalize
    "torch.nn.functional.normalize") | Perform $L_p$Lp​ normalization of inputs over
    specified dimension. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| [`normalize`](generated/torch.nn.functional.normalize.html#torch.nn.functional.normalize
    "torch.nn.functional.normalize") | 在指定维度上对输入进行$L_p$规范化。 |'
- en: Linear functions
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性函数
- en: '| [`linear`](generated/torch.nn.functional.linear.html#torch.nn.functional.linear
    "torch.nn.functional.linear") | Applies a linear transformation to the incoming
    data: $y = xA^T + b$y=xAT+b. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| [`linear`](生成/torch.nn.functional.linear.html#torch.nn.functional.linear
    "torch.nn.functional.linear") | 对传入数据应用线性变换：$y = xA^T + b$y=xAT+b。 |'
- en: '| [`bilinear`](generated/torch.nn.functional.bilinear.html#torch.nn.functional.bilinear
    "torch.nn.functional.bilinear") | Applies a bilinear transformation to the incoming
    data: $y = x_1^T A x_2 + b$y=x1T​Ax2​+b |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| [`bilinear`](生成/torch.nn.functional.bilinear.html#torch.nn.functional.bilinear
    "torch.nn.functional.bilinear") | 对传入数据应用双线性变换：$y = x_1^T A x_2 + b$y=x1T​Ax2​+b
    |'
- en: Dropout functions
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout函数
- en: '| [`dropout`](generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout
    "torch.nn.functional.dropout") | During training, randomly zeroes some elements
    of the input tensor with probability `p`. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| [`dropout`](生成/torch.nn.functional.dropout.html#torch.nn.functional.dropout
    "torch.nn.functional.dropout") | 在训练期间，以概率`p`随机将输入张量的一些元素置零。 |'
- en: '| [`alpha_dropout`](generated/torch.nn.functional.alpha_dropout.html#torch.nn.functional.alpha_dropout
    "torch.nn.functional.alpha_dropout") | Apply alpha dropout to the input. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| [`alpha_dropout`](生成/torch.nn.functional.alpha_dropout.html#torch.nn.functional.alpha_dropout
    "torch.nn.functional.alpha_dropout") | 对输入应用alpha dropout。 |'
- en: '| [`feature_alpha_dropout`](generated/torch.nn.functional.feature_alpha_dropout.html#torch.nn.functional.feature_alpha_dropout
    "torch.nn.functional.feature_alpha_dropout") | Randomly masks out entire channels
    (a channel is a feature map). |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| [`feature_alpha_dropout`](生成/torch.nn.functional.feature_alpha_dropout.html#torch.nn.functional.feature_alpha_dropout
    "torch.nn.functional.feature_alpha_dropout") | 随机屏蔽整个通道（通道是一个特征图）。 |'
- en: '| [`dropout1d`](generated/torch.nn.functional.dropout1d.html#torch.nn.functional.dropout1d
    "torch.nn.functional.dropout1d") | Randomly zero out entire channels (a channel
    is a 1D feature map). |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| [`dropout1d`](生成/torch.nn.functional.dropout1d.html#torch.nn.functional.dropout1d
    "torch.nn.functional.dropout1d") | 随机将整个通道置零（通道是一个1D特征图）。 |'
- en: '| [`dropout2d`](generated/torch.nn.functional.dropout2d.html#torch.nn.functional.dropout2d
    "torch.nn.functional.dropout2d") | Randomly zero out entire channels (a channel
    is a 2D feature map). |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| [`dropout2d`](生成/torch.nn.functional.dropout2d.html#torch.nn.functional.dropout2d
    "torch.nn.functional.dropout2d") | 随机将整个通道置零（通道是一个2D特征图）。 |'
- en: '| [`dropout3d`](generated/torch.nn.functional.dropout3d.html#torch.nn.functional.dropout3d
    "torch.nn.functional.dropout3d") | Randomly zero out entire channels (a channel
    is a 3D feature map). |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [`dropout3d`](生成/torch.nn.functional.dropout3d.html#torch.nn.functional.dropout3d
    "torch.nn.functional.dropout3d") | 随机将整个通道置零（通道是一个3D特征图）。 |'
- en: Sparse functions
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏函数
- en: '| [`embedding`](generated/torch.nn.functional.embedding.html#torch.nn.functional.embedding
    "torch.nn.functional.embedding") | Generate a simple lookup table that looks up
    embeddings in a fixed dictionary and size. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| [`embedding`](生成/torch.nn.functional.embedding.html#torch.nn.functional.embedding
    "torch.nn.functional.embedding") | 生成一个简单的查找表，在固定字典和大小中查找嵌入。 |'
- en: '| [`embedding_bag`](generated/torch.nn.functional.embedding_bag.html#torch.nn.functional.embedding_bag
    "torch.nn.functional.embedding_bag") | Compute sums, means or maxes of bags of
    embeddings. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| [`embedding_bag`](生成/torch.nn.functional.embedding_bag.html#torch.nn.functional.embedding_bag
    "torch.nn.functional.embedding_bag") | 计算嵌入包的总和、平均值或最大值。 |'
- en: '| [`one_hot`](generated/torch.nn.functional.one_hot.html#torch.nn.functional.one_hot
    "torch.nn.functional.one_hot") | Takes LongTensor with index values of shape `(*)`
    and returns a tensor of shape `(*, num_classes)` that have zeros everywhere except
    where the index of last dimension matches the corresponding value of the input
    tensor, in which case it will be 1. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| [`one_hot`](生成/torch.nn.functional.one_hot.html#torch.nn.functional.one_hot
    "torch.nn.functional.one_hot") | 接受形状为`(*)`的LongTensor索引值，并返回形状为`(*, num_classes)`的张量，除了最后一维的索引与输入张量的相应值匹配的地方为1外，其他地方都为零。
    |'
- en: Distance functions
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 距离函数
- en: '| [`pairwise_distance`](generated/torch.nn.functional.pairwise_distance.html#torch.nn.functional.pairwise_distance
    "torch.nn.functional.pairwise_distance") | See [`torch.nn.PairwiseDistance`](generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance
    "torch.nn.PairwiseDistance") for details |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| [`pairwise_distance`](生成/torch.nn.functional.pairwise_distance.html#torch.nn.functional.pairwise_distance
    "torch.nn.functional.pairwise_distance") | 详细信息请参见[`torch.nn.PairwiseDistance`](生成/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance
    "torch.nn.PairwiseDistance") |'
- en: '| [`cosine_similarity`](generated/torch.nn.functional.cosine_similarity.html#torch.nn.functional.cosine_similarity
    "torch.nn.functional.cosine_similarity") | Returns cosine similarity between `x1`
    and `x2`, computed along dim. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| [`cosine_similarity`](生成/torch.nn.functional.cosine_similarity.html#torch.nn.functional.cosine_similarity
    "torch.nn.functional.cosine_similarity") | 返回沿着维度计算的`x1`和`x2`之间的余弦相似度。 |'
- en: '| [`pdist`](generated/torch.nn.functional.pdist.html#torch.nn.functional.pdist
    "torch.nn.functional.pdist") | Computes the p-norm distance between every pair
    of row vectors in the input. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| [`pdist`](生成/torch.nn.functional.pdist.html#torch.nn.functional.pdist "torch.nn.functional.pdist")
    | 计算输入中每对行向量之间的p-范数距离。 |'
- en: Loss functions
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: '| [`binary_cross_entropy`](generated/torch.nn.functional.binary_cross_entropy.html#torch.nn.functional.binary_cross_entropy
    "torch.nn.functional.binary_cross_entropy") | Measure Binary Cross Entropy between
    the target and input probabilities. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| [`binary_cross_entropy`](生成/torch.nn.functional.binary_cross_entropy.html#torch.nn.functional.binary_cross_entropy
    "torch.nn.functional.binary_cross_entropy") | 计算目标和输入概率之间的二元交叉熵。 |'
- en: '| [`binary_cross_entropy_with_logits`](generated/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits
    "torch.nn.functional.binary_cross_entropy_with_logits") | Calculate Binary Cross
    Entropy between target and input logits. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| [`binary_cross_entropy_with_logits`](生成/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits
    "torch.nn.functional.binary_cross_entropy_with_logits") | 计算目标和输入logits之间的二元交叉熵。
    |'
- en: '| [`poisson_nll_loss`](generated/torch.nn.functional.poisson_nll_loss.html#torch.nn.functional.poisson_nll_loss
    "torch.nn.functional.poisson_nll_loss") | Poisson negative log likelihood loss.
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| [`poisson_nll_loss`](生成/torch.nn.functional.poisson_nll_loss.html#torch.nn.functional.poisson_nll_loss
    "torch.nn.functional.poisson_nll_loss") | 泊松负对数似然损失。 |'
- en: '| [`cosine_embedding_loss`](generated/torch.nn.functional.cosine_embedding_loss.html#torch.nn.functional.cosine_embedding_loss
    "torch.nn.functional.cosine_embedding_loss") | See [`CosineEmbeddingLoss`](generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss
    "torch.nn.CosineEmbeddingLoss") for details. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| [`cosine_embedding_loss`](generated/torch.nn.functional.cosine_embedding_loss.html#torch.nn.functional.cosine_embedding_loss
    "torch.nn.functional.cosine_embedding_loss") | 详细信息请参阅[`CosineEmbeddingLoss`](generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss
    "torch.nn.CosineEmbeddingLoss")。 |'
- en: '| [`cross_entropy`](generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy
    "torch.nn.functional.cross_entropy") | Compute the cross entropy loss between
    input logits and target. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| [`cross_entropy`](generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy
    "torch.nn.functional.cross_entropy") | 计算输入logits和目标之间的交叉熵损失。 |'
- en: '| [`ctc_loss`](generated/torch.nn.functional.ctc_loss.html#torch.nn.functional.ctc_loss
    "torch.nn.functional.ctc_loss") | Apply the Connectionist Temporal Classification
    loss. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [`ctc_loss`](generated/torch.nn.functional.ctc_loss.html#torch.nn.functional.ctc_loss
    "torch.nn.functional.ctc_loss") | 应用连接主义时间分类损失。 |'
- en: '| [`gaussian_nll_loss`](generated/torch.nn.functional.gaussian_nll_loss.html#torch.nn.functional.gaussian_nll_loss
    "torch.nn.functional.gaussian_nll_loss") | Gaussian negative log likelihood loss.
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| [`gaussian_nll_loss`](generated/torch.nn.functional.gaussian_nll_loss.html#torch.nn.functional.gaussian_nll_loss
    "torch.nn.functional.gaussian_nll_loss") | 高斯负对数似然损失。 |'
- en: '| [`hinge_embedding_loss`](generated/torch.nn.functional.hinge_embedding_loss.html#torch.nn.functional.hinge_embedding_loss
    "torch.nn.functional.hinge_embedding_loss") | See [`HingeEmbeddingLoss`](generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss
    "torch.nn.HingeEmbeddingLoss") for details. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [`hinge_embedding_loss`](generated/torch.nn.functional.hinge_embedding_loss.html#torch.nn.functional.hinge_embedding_loss
    "torch.nn.functional.hinge_embedding_loss") | 详细信息请参阅[`HingeEmbeddingLoss`](generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss
    "torch.nn.HingeEmbeddingLoss")。 |'
- en: '| [`kl_div`](generated/torch.nn.functional.kl_div.html#torch.nn.functional.kl_div
    "torch.nn.functional.kl_div") | Compute the KL Divergence loss. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| [`kl_div`](generated/torch.nn.functional.kl_div.html#torch.nn.functional.kl_div
    "torch.nn.functional.kl_div") | 计算KL散度损失。 |'
- en: '| [`l1_loss`](generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss
    "torch.nn.functional.l1_loss") | Function that takes the mean element-wise absolute
    value difference. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| [`l1_loss`](generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss
    "torch.nn.functional.l1_loss") | 计算元素间绝对值差的均值。 |'
- en: '| [`mse_loss`](generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss
    "torch.nn.functional.mse_loss") | Measures the element-wise mean squared error.
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| [`mse_loss`](generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss
    "torch.nn.functional.mse_loss") | 计算元素间均方误差。 |'
- en: '| [`margin_ranking_loss`](generated/torch.nn.functional.margin_ranking_loss.html#torch.nn.functional.margin_ranking_loss
    "torch.nn.functional.margin_ranking_loss") | See [`MarginRankingLoss`](generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss
    "torch.nn.MarginRankingLoss") for details. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| [`margin_ranking_loss`](generated/torch.nn.functional.margin_ranking_loss.html#torch.nn.functional.margin_ranking_loss
    "torch.nn.functional.margin_ranking_loss") | 详细信息请参阅[`MarginRankingLoss`](generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss
    "torch.nn.MarginRankingLoss")。 |'
- en: '| [`multilabel_margin_loss`](generated/torch.nn.functional.multilabel_margin_loss.html#torch.nn.functional.multilabel_margin_loss
    "torch.nn.functional.multilabel_margin_loss") | See [`MultiLabelMarginLoss`](generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss
    "torch.nn.MultiLabelMarginLoss") for details. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| [`multilabel_margin_loss`](generated/torch.nn.functional.multilabel_margin_loss.html#torch.nn.functional.multilabel_margin_loss
    "torch.nn.functional.multilabel_margin_loss") | 详细信息请参阅[`MultiLabelMarginLoss`](generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss
    "torch.nn.MultiLabelMarginLoss")。 |'
- en: '| [`multilabel_soft_margin_loss`](generated/torch.nn.functional.multilabel_soft_margin_loss.html#torch.nn.functional.multilabel_soft_margin_loss
    "torch.nn.functional.multilabel_soft_margin_loss") | See [`MultiLabelSoftMarginLoss`](generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss
    "torch.nn.MultiLabelSoftMarginLoss") for details. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| [`multilabel_soft_margin_loss`](generated/torch.nn.functional.multilabel_soft_margin_loss.html#torch.nn.functional.multilabel_soft_margin_loss
    "torch.nn.functional.multilabel_soft_margin_loss") | 详细信息请参阅[`MultiLabelSoftMarginLoss`](generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss
    "torch.nn.MultiLabelSoftMarginLoss")。 |'
- en: '| [`multi_margin_loss`](generated/torch.nn.functional.multi_margin_loss.html#torch.nn.functional.multi_margin_loss
    "torch.nn.functional.multi_margin_loss") | See [`MultiMarginLoss`](generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss
    "torch.nn.MultiMarginLoss") for details. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| [`multi_margin_loss`](generated/torch.nn.functional.multi_margin_loss.html#torch.nn.functional.multi_margin_loss
    "torch.nn.functional.multi_margin_loss") | 详细信息请参阅[`MultiMarginLoss`](generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss
    "torch.nn.MultiMarginLoss")。 |'
- en: '| [`nll_loss`](generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss
    "torch.nn.functional.nll_loss") | Compute the negative log likelihood loss. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| [`nll_loss`](generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss
    "torch.nn.functional.nll_loss") | 计算负对数似然损失。 |'
- en: '| [`huber_loss`](generated/torch.nn.functional.huber_loss.html#torch.nn.functional.huber_loss
    "torch.nn.functional.huber_loss") | Compute the Huber loss. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| [`huber_loss`](generated/torch.nn.functional.huber_loss.html#torch.nn.functional.huber_loss
    "torch.nn.functional.huber_loss") | 计算Huber损失。 |'
- en: '| [`smooth_l1_loss`](generated/torch.nn.functional.smooth_l1_loss.html#torch.nn.functional.smooth_l1_loss
    "torch.nn.functional.smooth_l1_loss") | Compute the Smooth L1 loss. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| [`smooth_l1_loss`](generated/torch.nn.functional.smooth_l1_loss.html#torch.nn.functional.smooth_l1_loss
    "torch.nn.functional.smooth_l1_loss") | 计算平滑L1损失。 |'
- en: '| [`soft_margin_loss`](generated/torch.nn.functional.soft_margin_loss.html#torch.nn.functional.soft_margin_loss
    "torch.nn.functional.soft_margin_loss") | See [`SoftMarginLoss`](generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss
    "torch.nn.SoftMarginLoss") for details. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| [`soft_margin_loss`](generated/torch.nn.functional.soft_margin_loss.html#torch.nn.functional.soft_margin_loss
    "torch.nn.functional.soft_margin_loss") | 详细信息请参阅[`SoftMarginLoss`](generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss
    "torch.nn.SoftMarginLoss")。 |'
- en: '| [`triplet_margin_loss`](generated/torch.nn.functional.triplet_margin_loss.html#torch.nn.functional.triplet_margin_loss
    "torch.nn.functional.triplet_margin_loss") | Compute the triplet loss between
    given input tensors and a margin greater than 0. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| [`triplet_margin_loss`](generated/torch.nn.functional.triplet_margin_loss.html#torch.nn.functional.triplet_margin_loss
    "torch.nn.functional.triplet_margin_loss") | 计算给定输入张量之间的三元组损失，边距大于0。 |'
- en: '| [`triplet_margin_with_distance_loss`](generated/torch.nn.functional.triplet_margin_with_distance_loss.html#torch.nn.functional.triplet_margin_with_distance_loss
    "torch.nn.functional.triplet_margin_with_distance_loss") | Compute the triplet
    margin loss for input tensors using a custom distance function. |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| [`triplet_margin_with_distance_loss`](generated/torch.nn.functional.triplet_margin_with_distance_loss.html#torch.nn.functional.triplet_margin_with_distance_loss
    "torch.nn.functional.triplet_margin_with_distance_loss") | 使用自定义距离函数计算输入张量的三元组边距损失。
    |'
- en: Vision functions
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vision functions
- en: '| [`pixel_shuffle`](generated/torch.nn.functional.pixel_shuffle.html#torch.nn.functional.pixel_shuffle
    "torch.nn.functional.pixel_shuffle") | Rearranges elements in a tensor of shape
    $(*, C \times r^2, H, W)$(∗,C×r2,H,W) to a tensor of shape $(*, C, H \times r,
    W \times r)$(∗,C,H×r,W×r), where r is the `upscale_factor`. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| [`pixel_shuffle`](generated/torch.nn.functional.pixel_shuffle.html#torch.nn.functional.pixel_shuffle
    "torch.nn.functional.pixel_shuffle") | 将形状为$(*, C \times r^2, H, W)$(∗,C×r2,H,W)的张量重新排列为形状为$(*,
    C, H \times r, W \times r)$(∗,C,H×r,W×r)的张量，其中r是`upscale_factor`。 |'
- en: '| [`pixel_unshuffle`](generated/torch.nn.functional.pixel_unshuffle.html#torch.nn.functional.pixel_unshuffle
    "torch.nn.functional.pixel_unshuffle") | Reverses the [`PixelShuffle`](generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle
    "torch.nn.PixelShuffle") operation by rearranging elements in a tensor of shape
    $(*, C, H \times r, W \times r)$(∗,C,H×r,W×r) to a tensor of shape $(*, C \times
    r^2, H, W)$(∗,C×r2,H,W), where r is the `downscale_factor`. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| [`pixel_unshuffle`](generated/torch.nn.functional.pixel_unshuffle.html#torch.nn.functional.pixel_unshuffle
    "torch.nn.functional.pixel_unshuffle") | 通过将形状为$(*, C, H \times r, W \times r)$(∗,C,H×r,W×r)的张量重新排列为形状为$(*,
    C \times r^2, H, W)$(∗,C×r2,H,W)的张量，其中r是`downscale_factor`，来反转[`PixelShuffle`](generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle
    "torch.nn.PixelShuffle")操作。 |'
- en: '| [`pad`](generated/torch.nn.functional.pad.html#torch.nn.functional.pad "torch.nn.functional.pad")
    | Pads tensor. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| [`pad`](generated/torch.nn.functional.pad.html#torch.nn.functional.pad "torch.nn.functional.pad")
    | 填充张量。 |'
- en: '| [`interpolate`](generated/torch.nn.functional.interpolate.html#torch.nn.functional.interpolate
    "torch.nn.functional.interpolate") | Down/up samples the input. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| [`interpolate`](generated/torch.nn.functional.interpolate.html#torch.nn.functional.interpolate
    "torch.nn.functional.interpolate") | 对输入进行下采样/上采样。 |'
- en: '| [`upsample`](generated/torch.nn.functional.upsample.html#torch.nn.functional.upsample
    "torch.nn.functional.upsample") | Upsample input. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| [`upsample`](generated/torch.nn.functional.upsample.html#torch.nn.functional.upsample
    "torch.nn.functional.upsample") | 上采样输入。 |'
- en: '| [`upsample_nearest`](generated/torch.nn.functional.upsample_nearest.html#torch.nn.functional.upsample_nearest
    "torch.nn.functional.upsample_nearest") | Upsamples the input, using nearest neighbours''
    pixel values. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| [`upsample_nearest`](generated/torch.nn.functional.upsample_nearest.html#torch.nn.functional.upsample_nearest
    "torch.nn.functional.upsample_nearest") | 使用最近邻像素值对输入进行上采样。 |'
- en: '| [`upsample_bilinear`](generated/torch.nn.functional.upsample_bilinear.html#torch.nn.functional.upsample_bilinear
    "torch.nn.functional.upsample_bilinear") | Upsamples the input, using bilinear
    upsampling. |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| [`upsample_bilinear`](generated/torch.nn.functional.upsample_bilinear.html#torch.nn.functional.upsample_bilinear
    "torch.nn.functional.upsample_bilinear") | 使用双线性上采样对输入进行上采样。 |'
- en: '| [`grid_sample`](generated/torch.nn.functional.grid_sample.html#torch.nn.functional.grid_sample
    "torch.nn.functional.grid_sample") | Compute grid sample. |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| [`grid_sample`](generated/torch.nn.functional.grid_sample.html#torch.nn.functional.grid_sample
    "torch.nn.functional.grid_sample") | 计算网格采样。 |'
- en: '| [`affine_grid`](generated/torch.nn.functional.affine_grid.html#torch.nn.functional.affine_grid
    "torch.nn.functional.affine_grid") | Generate 2D or 3D flow field (sampling grid),
    given a batch of affine matrices `theta`. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| [`affine_grid`](generated/torch.nn.functional.affine_grid.html#torch.nn.functional.affine_grid
    "torch.nn.functional.affine_grid") | 给定一批仿射矩阵`theta`，生成2D或3D流场（采样网格）。 |'
- en: DataParallel functions (multi-GPU, distributed)[](#dataparallel-functions-multi-gpu-distributed
    "Permalink to this heading")
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataParallel functions (multi-GPU, distributed)[](#dataparallel-functions-multi-gpu-distributed
    "Permalink to this heading")
- en: data_parallel
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: data_parallel
- en: '| `torch.nn.parallel.data_parallel` | Evaluate module(input) in parallel across
    the GPUs given in device_ids. |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| `torch.nn.parallel.data_parallel` | 在给定的device_ids上并行评估模块(input)。 |'
