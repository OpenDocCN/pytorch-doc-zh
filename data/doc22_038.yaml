- en: Automatic differentiation package - torch.autograd
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动微分包 - torch.autograd
- en: 原文：[https://pytorch.org/docs/stable/autograd.html](https://pytorch.org/docs/stable/autograd.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/autograd.html](https://pytorch.org/docs/stable/autograd.html)
- en: '`torch.autograd` provides classes and functions implementing automatic differentiation
    of arbitrary scalar valued functions. It requires minimal changes to the existing
    code - you only need to declare `Tensor` s for which gradients should be computed
    with the `requires_grad=True` keyword. As of now, we only support autograd for
    floating point `Tensor` types ( half, float, double and bfloat16) and complex
    `Tensor` types (cfloat, cdouble).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.autograd` 提供了实现任意标量值函数自动微分的类和函数。它对现有代码的更改很小 - 您只需要声明应计算梯度的 `Tensor`，并使用
    `requires_grad=True` 关键字。目前，我们仅支持浮点 `Tensor` 类型（half、float、double 和 bfloat16）和复数
    `Tensor` 类型（cfloat、cdouble）的自动微分。'
- en: '| [`backward`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") | Computes the sum of gradients of given tensors with
    respect to graph leaves. |'
  id: totrans-3
  prefs: []
  type: TYPE_TB
  zh: '| [`backward`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") | 计算给定张量相对于图叶子节点的梯度之和。|'
- en: '| [`grad`](generated/torch.autograd.grad.html#torch.autograd.grad "torch.autograd.grad")
    | Computes and returns the sum of gradients of outputs with respect to the inputs.
    |'
  id: totrans-4
  prefs: []
  type: TYPE_TB
  zh: '| [`grad`](generated/torch.autograd.grad.html#torch.autograd.grad "torch.autograd.grad")
    | 计算并返回输出相对于输入的梯度之和。|'
- en: '## Forward-mode Automatic Differentiation[](#forward-mode-automatic-differentiation
    "Permalink to this heading")'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '## 前向模式自动微分[](#forward-mode-automatic-differentiation "跳转到此标题")'
- en: Warning
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This API is in beta. Even though the function signatures are very unlikely to
    change, improved operator coverage is planned before we consider this stable.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此 API 处于 beta 版本。尽管函数签名很可能不会更改，但在我们将其视为稳定之前，计划增加更多操作符的覆盖范围。
- en: Please see the [forward-mode AD tutorial](https://pytorch.org/tutorials/intermediate/forward_ad_usage.html)
    for detailed steps on how to use this API.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看[前向模式 AD 教程](https://pytorch.org/tutorials/intermediate/forward_ad_usage.html)以获取如何使用此
    API 的详细步骤。
- en: '| [`forward_ad.dual_level`](generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level
    "torch.autograd.forward_ad.dual_level") | Context-manager for forward AD, where
    all forward AD computation must occur within the `dual_level` context. |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| [`forward_ad.dual_level`](generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level
    "torch.autograd.forward_ad.dual_level") | 用于前向 AD 的上下文管理器，在 `dual_level` 上下文中必须进行所有前向
    AD 计算。|'
- en: '| [`forward_ad.make_dual`](generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual
    "torch.autograd.forward_ad.make_dual") | Associate a tensor value with its tangent
    to create a "dual tensor" for forward AD gradient computation. |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| [`forward_ad.make_dual`](generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual
    "torch.autograd.forward_ad.make_dual") | 将张量值与其切线关联起来，创建一个用于前向 AD 梯度计算的“双重张量”。|'
- en: '| [`forward_ad.unpack_dual`](generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual
    "torch.autograd.forward_ad.unpack_dual") | Unpack a "dual tensor" to get both
    its Tensor value and its forward AD gradient. |  ## Functional higher level API[](#functional-higher-level-api
    "Permalink to this heading")'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '| [`forward_ad.unpack_dual`](generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual
    "torch.autograd.forward_ad.unpack_dual") | 解包“双重张量”，获取其张量值和前向 AD 梯度。|  ## 功能高级
    API[](#functional-higher-level-api "跳转到此标题")'
- en: Warning
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This API is in beta. Even though the function signatures are very unlikely to
    change, major improvements to performances are planned before we consider this
    stable.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此 API 处于 beta 版本。尽管函数签名很可能不会更改，但在我们将其视为稳定之前，计划对性能进行重大改进。
- en: This section contains the higher level API for the autograd that builds on the
    basic API above and allows you to compute jacobians, hessians, etc.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含了构建在基本 API 基础上的 autograd 高级 API，允许您计算雅可比矩阵、Hessian 矩阵等。
- en: 'This API works with user-provided functions that take only Tensors as input
    and return only Tensors. If your function takes other arguments that are not Tensors
    or Tensors that don’t have requires_grad set, you can use a lambda to capture
    them. For example, for a function `f` that takes three inputs, a Tensor for which
    we want the jacobian, another tensor that should be considered constant and a
    boolean flag as `f(input, constant, flag=flag)` you can use it as `functional.jacobian(lambda
    x: f(x, constant, flag=flag), input)`.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '此 API 适用于用户提供的仅以张量作为输入并返回张量的函数。如果您的函数接受其他非张量参数或未设置 requires_grad 的张量，您可以使用
    lambda 来捕获它们。例如，对于一个接受三个输入的函数 `f`，一个张量用于计算雅可比矩阵，另一个应被视为常数的张量，以及一个布尔标志作为 `f(input,
    constant, flag=flag)`，您可以使用 `functional.jacobian(lambda x: f(x, constant, flag=flag),
    input)`。'
- en: '| [`functional.jacobian`](generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian
    "torch.autograd.functional.jacobian") | Compute the Jacobian of a given function.
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| [`functional.jacobian`](generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian
    "torch.autograd.functional.jacobian") | 计算给定函数的雅可比矩阵。|'
- en: '| [`functional.hessian`](generated/torch.autograd.functional.hessian.html#torch.autograd.functional.hessian
    "torch.autograd.functional.hessian") | Compute the Hessian of a given scalar function.
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| [`functional.hessian`](generated/torch.autograd.functional.hessian.html#torch.autograd.functional.hessian
    "torch.autograd.functional.hessian") | 计算给定标量函数的 Hessian 矩阵。|'
- en: '| [`functional.vjp`](generated/torch.autograd.functional.vjp.html#torch.autograd.functional.vjp
    "torch.autograd.functional.vjp") | Compute the dot product between a vector `v`
    and the Jacobian of the given function at the point given by the inputs. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| [`functional.vjp`](generated/torch.autograd.functional.vjp.html#torch.autograd.functional.vjp
    "torch.autograd.functional.vjp") | 计算给定函数在输入点处的 Jacobian 矩阵与向量 `v` 的点积。|'
- en: '| [`functional.jvp`](generated/torch.autograd.functional.jvp.html#torch.autograd.functional.jvp
    "torch.autograd.functional.jvp") | Compute the dot product between the Jacobian
    of the given function at the point given by the inputs and a vector `v`. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| [`functional.jvp`](generated/torch.autograd.functional.jvp.html#torch.autograd.functional.jvp
    "torch.autograd.functional.jvp") | 计算给定函数在输入点处的 Jacobian 矩阵与向量 `v` 的点积。|'
- en: '| [`functional.vhp`](generated/torch.autograd.functional.vhp.html#torch.autograd.functional.vhp
    "torch.autograd.functional.vhp") | Compute the dot product between vector `v`
    and Hessian of a given scalar function at a specified point. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| [`functional.vhp`](generated/torch.autograd.functional.vhp.html#torch.autograd.functional.vhp
    "torch.autograd.functional.vhp") | 计算向量`v`与给定标量函数在指定点处的Hessian的点积。 |'
- en: '| [`functional.hvp`](generated/torch.autograd.functional.hvp.html#torch.autograd.functional.hvp
    "torch.autograd.functional.hvp") | Compute the dot product between the scalar
    function''s Hessian and a vector `v` at a specified point. |  ## Locally disabling
    gradient computation[](#locally-disabling-gradient-computation "Permalink to this
    heading")'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '| [`functional.hvp`](generated/torch.autograd.functional.hvp.html#torch.autograd.functional.hvp
    "torch.autograd.functional.hvp") | 计算标量函数的Hessian和向量`v`在指定点处的点积。 |## 本地禁用梯度计算[](#locally-disabling-gradient-computation
    "跳转到此标题的永久链接")'
- en: 'See [Locally disabling gradient computation](notes/autograd.html#locally-disable-grad-doc)
    for more information on the differences between no-grad and inference mode as
    well as other related mechanisms that may be confused with the two. Also see [Locally
    disabling gradient computation](torch.html#torch-rst-local-disable-grad) for a
    list of functions that can be used to locally disable gradients.  ## Default gradient
    layouts'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有关无梯度和推断模式之间的区别以及可能与两者混淆的其他相关机制的更多信息，请参阅[本地禁用梯度计算](notes/autograd.html#locally-disable-grad-doc)。还请参阅[本地禁用梯度计算](torch.html#torch-rst-local-disable-grad)以获取可用于本地禁用梯度的函数列表。##
    默认梯度布局
- en: When a non-sparse `param` receives a non-sparse gradient during [`torch.autograd.backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") or [`torch.Tensor.backward()`](generated/torch.Tensor.backward.html#torch.Tensor.backward
    "torch.Tensor.backward") `param.grad` is accumulated as follows.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当非稀疏`param`在[`torch.autograd.backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward")或[`torch.Tensor.backward()`](generated/torch.Tensor.backward.html#torch.Tensor.backward
    "torch.Tensor.backward")期间接收到非稀疏梯度时，`param.grad`将按以下方式累积。
- en: 'If `param.grad` is initially `None`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`param.grad`最初为`None`：
- en: If `param`’s memory is non-overlapping and dense, `.grad` is created with strides
    matching `param` (thus matching `param`’s layout).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`param`的内存不重叠且密集，`.grad`将以匹配`param`的步幅创建（从而匹配`param`的布局）。
- en: Otherwise, `.grad` is created with rowmajor-contiguous strides.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，`.grad` 将以行优先连续步幅创建。
- en: 'If `param` already has a non-sparse `.grad` attribute:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`param`已经具有非稀疏的`.grad`属性：
- en: If `create_graph=False`, `backward()` accumulates into `.grad` in-place, which
    preserves its strides.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`create_graph=False`，`backward()`会原地累积到`.grad`中，从而保留其步幅。
- en: If `create_graph=True`, `backward()` replaces `.grad` with a new tensor `.grad
    + new grad`, which attempts (but does not guarantee) matching the preexisting
    `.grad`’s strides.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`create_graph=True`，`backward()`将用新张量`.grad + new grad`替换`.grad`，尝试（但不保证）匹配现有`.grad`的步幅。
- en: The default behavior (letting `.grad`s be `None` before the first `backward()`,
    such that their layout is created according to 1 or 2, and retained over time
    according to 3 or 4) is recommended for best performance. Calls to `model.zero_grad()`
    or `optimizer.zero_grad()` will not affect `.grad` layouts.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐默认行为（在第一次`backward()`之前让`.grad`为`None`，使其布局根据1或2创建，并根据3或4保留）以获得最佳性能。调用`model.zero_grad()`或`optimizer.zero_grad()`不会影响`.grad`的布局。
- en: 'In fact, resetting all `.grad`s to `None` before each accumulation phase, e.g.:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在每个累积阶段之前将所有的`.grad`重置为`None`，例如：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: such that they’re recreated according to 1 or 2 every time, is a valid alternative
    to `model.zero_grad()` or `optimizer.zero_grad()` that may improve performance
    for some networks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每次根据1或2重新创建它们，是`model.zero_grad()`或`optimizer.zero_grad()`的有效替代方法，可能会提高某些网络的性能。
- en: Manual gradient layouts
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动梯度布局
- en: If you need manual control over `.grad`’s strides, assign `param.grad =` a zeroed
    tensor with desired strides before the first `backward()`, and never reset it
    to `None`. 3 guarantees your layout is preserved as long as `create_graph=False`.
    4 indicates your layout is *likely* preserved even if `create_graph=True`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要手动控制`.grad`的步幅，在第一次`backward()`之前将`param.grad =`分配为具有所需步幅的零张量，并永远不要将其重置为`None`。3保证只要`create_graph=False`，您的布局就会被保留。4表明即使`create_graph=True`，您的布局也*可能*会被保留。
- en: In-place operations on Tensors[](#in-place-operations-on-tensors "Permalink
    to this heading")
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量的原地操作[](#in-place-operations-on-tensors "跳转到此标题的永久链接")
- en: Supporting in-place operations in autograd is a hard matter, and we discourage
    their use in most cases. Autograd’s aggressive buffer freeing and reuse makes
    it very efficient and there are very few occasions when in-place operations actually
    lower memory usage by any significant amount. Unless you’re operating under heavy
    memory pressure, you might never need to use them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在autograd中支持原地操作是一个棘手的问题，我们不鼓励在大多数情况下使用它们。Autograd的积极缓冲区释放和重用使其非常高效，只有在极度内存压力下，才有很少的情况下原地操作实际上会显著降低内存使用量。除非您在极度内存压力下操作，否则您可能永远不需要使用它们。
- en: In-place correctness checks[](#in-place-correctness-checks "Permalink to this
    heading")
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原地正确性检查[](#in-place-correctness-checks "跳转到此标题的永久链接")
- en: All `Tensor` s keep track of in-place operations applied to them, and if the
    implementation detects that a tensor was saved for backward in one of the functions,
    but it was modified in-place afterwards, an error will be raised once backward
    pass is started. This ensures that if you’re using in-place functions and not
    seeing any errors, you can be sure that the computed gradients are correct.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有`Tensor`都会跟踪应用于它们的原地操作，如果实现检测到一个张量在其中一个函数中保存以进行反向传播，但之后在原地进行修改，那么一旦开始反向传播，就会引发错误。这确保了如果您使用原地函数而没有看到任何错误，您可以确信计算的梯度是正确的。
- en: Variable (deprecated)
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量（已弃用）
- en: Warning
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: 'The Variable API has been deprecated: Variables are no longer necessary to
    use autograd with tensors. Autograd automatically supports Tensors with `requires_grad`
    set to `True`. Below please find a quick guide on what has changed:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Variable API 已被弃用：不再需要使用 Variables 来使用带有 `requires_grad` 设置为 `True` 的张量进行自动求导。自动求导自动支持
    `requires_grad` 设置为 `True` 的张量。以下是有关已更改内容的快速指南：
- en: '`Variable(tensor)` and `Variable(tensor, requires_grad)` still work as expected,
    but they return Tensors instead of Variables.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Variable(tensor)` 和 `Variable(tensor, requires_grad)` 仍然按预期工作，但它们返回张量而不是 Variables。'
- en: '`var.data` is the same thing as `tensor.data`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`var.data` 和 `tensor.data` 是相同的。'
- en: Methods such as `var.backward(), var.detach(), var.register_hook()` now work
    on tensors with the same method names.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，`var.backward()、var.detach()、var.register_hook()` 等方法在具有相同方法名称的张量上也可以使用。
- en: 'In addition, one can now create tensors with `requires_grad=True` using factory
    methods such as [`torch.randn()`](generated/torch.randn.html#torch.randn "torch.randn"),
    [`torch.zeros()`](generated/torch.zeros.html#torch.zeros "torch.zeros"), [`torch.ones()`](generated/torch.ones.html#torch.ones
    "torch.ones"), and others like the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，现在可以使用工厂方法（如 [`torch.randn()`](generated/torch.randn.html#torch.randn "torch.randn")、[`torch.zeros()`](generated/torch.zeros.html#torch.zeros
    "torch.zeros")、[`torch.ones()`](generated/torch.ones.html#torch.ones "torch.ones")
    等）创建 `requires_grad=True` 的张量，如下所示：
- en: '`autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)`'
- en: Tensor autograd functions
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量自动求导函数
- en: '| `torch.Tensor.grad` | This attribute is `None` by default and becomes a Tensor
    the first time a call to [`backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") computes gradients for `self`. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| `torch.Tensor.grad` | 默认情况下，此属性为 `None`，并且在第一次调用 [`backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") 为 `self` 计算梯度时变为张量。 |'
- en: '| `torch.Tensor.requires_grad` | Is `True` if gradients need to be computed
    for this Tensor, `False` otherwise. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| `torch.Tensor.requires_grad` | 如果需要为此张量计算梯度，则为 `True`，否则为 `False`。 |'
- en: '| `torch.Tensor.is_leaf` | All Tensors that have `requires_grad` which is `False`
    will be leaf Tensors by convention. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| `torch.Tensor.is_leaf` | 所有 `requires_grad` 为 `False` 的张量将按照惯例成为叶子张量。 |'
- en: '| `torch.Tensor.backward`([gradient, ...]) | Computes the gradient of current
    tensor wrt graph leaves. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| `torch.Tensor.backward`([gradient, ...]) | 计算当前张量相对于图叶的梯度。 |'
- en: '| `torch.Tensor.detach` | Returns a new Tensor, detached from the current graph.
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| `torch.Tensor.detach` | 返回一个从当前图中分离的新张量。 |'
- en: '| `torch.Tensor.detach_` | Detaches the Tensor from the graph that created
    it, making it a leaf. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `torch.Tensor.detach_` | 从创建它的图中分离张量，使其成为叶子。 |'
- en: '| `torch.Tensor.register_hook`(hook) | Registers a backward hook. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `torch.Tensor.register_hook`(hook) | 注册一个反向钩子。 |'
- en: '| `torch.Tensor.register_post_accumulate_grad_hook`(hook) | Registers a backward
    hook that runs after grad accumulation. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `torch.Tensor.register_post_accumulate_grad_hook`(hook) | 注册一个在梯度累积后运行的反向钩子。
    |'
- en: '| `torch.Tensor.retain_grad`() | Enables this Tensor to have their [`grad`](generated/torch.autograd.grad.html#torch.autograd.grad
    "torch.autograd.grad") populated during [`backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward"). |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `torch.Tensor.retain_grad`() | 允许此张量在 [`backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") 运行时填充其 [`grad`](generated/torch.autograd.grad.html#torch.autograd.grad
    "torch.autograd.grad")。 |'
- en: Function
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Function
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Base class to create custom autograd.Function.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建自定义自动求导函数的基类。
- en: To create a custom autograd.Function, subclass this class and implement the
    [`forward()`](generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") and [`backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") static methods. Then, to use your custom op in the
    forward pass, call the class method `apply`. Do not call [`forward()`](generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") directly.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建自定义自动求导函数，请继承此类并实现 [`forward()`](generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") 和 [`backward()`](generated/torch.autograd.backward.html#torch.autograd.backward
    "torch.autograd.backward") 静态方法。然后，在前向传播中使用您的自定义操作，调用类方法 `apply`。不要直接调用 [`forward()`](generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward")。
- en: To ensure correctness and best performance, make sure you are calling the correct
    methods on `ctx` and validating your backward function using [`torch.autograd.gradcheck()`](#module-torch.autograd.gradcheck
    "torch.autograd.gradcheck").
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保正确性和最佳性能，请确保在 `ctx` 上调用正确的方法，并使用 [`torch.autograd.gradcheck()`](#module-torch.autograd.gradcheck
    "torch.autograd.gradcheck") 验证您的反向函数。
- en: See [Extending torch.autograd](notes/extending.html#extending-autograd) for
    more details on how to use this class.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何使用此类的更多详细信息，请参阅 [扩展 torch.autograd](notes/extending.html#extending-autograd)。
- en: 'Examples:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '| [`Function.forward`](generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") | Define the forward of the custom autograd
    Function. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [`Function.forward`](generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward
    "torch.autograd.Function.forward") | 定义自定义自动求导函数的前向传播。 |'
- en: '| [`Function.backward`](generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward") | Define a formula for differentiating the
    operation with backward mode automatic differentiation. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| [`Function.backward`](generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward") | 定义使用反向模式自动微分操作的微分公式。 |'
- en: '| [`Function.jvp`](generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") | Define a formula for differentiating the operation
    with forward mode automatic differentiation. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| [`Function.jvp`](generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp
    "torch.autograd.Function.jvp") | 定义使用前向模式自动微分操作的微分公式。 |'
- en: '| [`Function.vmap`](generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap") | Define the behavior for this autograd.Function
    underneath [`torch.vmap()`](generated/torch.vmap.html#torch.vmap "torch.vmap").
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| [`Function.vmap`](generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap
    "torch.autograd.Function.vmap") | 定义此autograd.Function在[`torch.vmap()`](generated/torch.vmap.html#torch.vmap
    "torch.vmap")下的行为。 |'
- en: Context method mixins
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文方法混合
- en: When creating a new [`Function`](#torch.autograd.Function "torch.autograd.Function"),
    the following methods are available to ctx.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新的[`Function`](#torch.autograd.Function "torch.autograd.Function")时，以下方法可用于ctx。
- en: '| [`function.FunctionCtx.mark_dirty`](generated/torch.autograd.function.FunctionCtx.mark_dirty.html#torch.autograd.function.FunctionCtx.mark_dirty
    "torch.autograd.function.FunctionCtx.mark_dirty") | Mark given tensors as modified
    in an in-place operation. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| [`function.FunctionCtx.mark_dirty`](generated/torch.autograd.function.FunctionCtx.mark_dirty.html#torch.autograd.function.FunctionCtx.mark_dirty
    "torch.autograd.function.FunctionCtx.mark_dirty") | 将给定张量标记为原位操作中修改的张量。 |'
- en: '| [`function.FunctionCtx.mark_non_differentiable`](generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html#torch.autograd.function.FunctionCtx.mark_non_differentiable
    "torch.autograd.function.FunctionCtx.mark_non_differentiable") | Mark outputs
    as non-differentiable. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| [`function.FunctionCtx.mark_non_differentiable`](generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html#torch.autograd.function.FunctionCtx.mark_non_differentiable
    "torch.autograd.function.FunctionCtx.mark_non_differentiable") | 将输出标记为不可微分。 |'
- en: '| [`function.FunctionCtx.save_for_backward`](generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward
    "torch.autograd.function.FunctionCtx.save_for_backward") | Save given tensors
    for a future call to [`backward()`](generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward"). |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| [`function.FunctionCtx.save_for_backward`](generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward
    "torch.autograd.function.FunctionCtx.save_for_backward") | 保存给定张量，以便将来调用[`backward()`](generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward
    "torch.autograd.Function.backward")。 |'
- en: '| [`function.FunctionCtx.set_materialize_grads`](generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html#torch.autograd.function.FunctionCtx.set_materialize_grads
    "torch.autograd.function.FunctionCtx.set_materialize_grads") | Set whether to
    materialize grad tensors. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| [`function.FunctionCtx.set_materialize_grads`](generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html#torch.autograd.function.FunctionCtx.set_materialize_grads
    "torch.autograd.function.FunctionCtx.set_materialize_grads") | 设置是否实现梯度张量。 |'
- en: '## Numerical gradient checking[](#module-torch.autograd.gradcheck "Permalink
    to this heading")'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '## 数值梯度检查[](#module-torch.autograd.gradcheck "Permalink to this heading")'
- en: '| [`gradcheck`](generated/torch.autograd.gradcheck.gradcheck.html#torch.autograd.gradcheck.gradcheck
    "torch.autograd.gradcheck.gradcheck") | Check gradients computed via small finite
    differences against analytical gradients wrt tensors in `inputs` that are of floating
    point or complex type and with `requires_grad=True`. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| [`gradcheck`](generated/torch.autograd.gradcheck.gradcheck.html#torch.autograd.gradcheck.gradcheck
    "torch.autograd.gradcheck.gradcheck") | 检查通过小的有限差分计算的梯度与`inputs`中的浮点或复数类型张量相对于解析梯度的梯度，这些张量具有`requires_grad=True`。
    |'
- en: '| [`gradgradcheck`](generated/torch.autograd.gradcheck.gradgradcheck.html#torch.autograd.gradcheck.gradgradcheck
    "torch.autograd.gradcheck.gradgradcheck") | Check gradients of gradients computed
    via small finite differences against analytical gradients wrt tensors in `inputs`
    and `grad_outputs` that are of floating point or complex type and with `requires_grad=True`.
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| [`gradgradcheck`](generated/torch.autograd.gradcheck.gradgradcheck.html#torch.autograd.gradcheck.gradgradcheck
    "torch.autograd.gradcheck.gradgradcheck") | 检查通过小的有限差分计算的梯度的梯度与`inputs`和`grad_outputs`中的浮点或复数类型张量相对于解析梯度的梯度，这些张量具有`requires_grad=True`。
    |'
- en: Profiler
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能分析器
- en: Autograd includes a profiler that lets you inspect the cost of different operators
    inside your model - both on the CPU and GPU. There are three modes implemented
    at the moment - CPU-only using [`profile`](#torch.autograd.profiler.profile "torch.autograd.profiler.profile").
    nvprof based (registers both CPU and GPU activity) using [`emit_nvtx`](#torch.autograd.profiler.emit_nvtx
    "torch.autograd.profiler.emit_nvtx"). and vtune profiler based using [`emit_itt`](#torch.autograd.profiler.emit_itt
    "torch.autograd.profiler.emit_itt").
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Autograd包括一个性能分析器，让您可以检查模型内不同运算符的成本 - 包括在CPU和GPU上。目前实现了三种模式 - 仅CPU使用[`profile`](#torch.autograd.profiler.profile
    "torch.autograd.profiler.profile")。基于nvprof的（同时注册CPU和GPU活动）使用[`emit_nvtx`](#torch.autograd.profiler.emit_nvtx
    "torch.autograd.profiler.emit_nvtx")。基于vtune性能分析器的使用[`emit_itt`](#torch.autograd.profiler.emit_itt
    "torch.autograd.profiler.emit_itt")。
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Context manager that manages autograd profiler state and holds a summary of
    results.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文管理器，管理自动求导分析器状态并保存结果摘要。
- en: 'Under the hood it just records events of functions being executed in C++ and
    exposes those events to Python. You can wrap any code into it and it will only
    report runtime of PyTorch functions. Note: profiler is thread local and is automatically
    propagated into the async tasks'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，它只记录在C++中执行的函数事件，并将这些事件暴露给Python。您可以将任何代码包装在其中，它只会报告PyTorch函数的运行时。注意：性能分析器是线程本地的，并自动传播到异步任务中
- en: Parameters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**enabled** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Setting this to False makes this context
    manager a no-op.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**enabled**（*bool*，可选）- 将其设置为False会使此上下文管理器无效。'
- en: '**use_cuda** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Enables timing of CUDA events as well using
    the cudaEvent API. Adds approximately 4us of overhead to each tensor operation.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**use_cuda**（*bool*，可选）- 启用使用cudaEvent API对CUDA事件进行计时。每个张量操作大约增加4微秒的开销。'
- en: '**record_shapes** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If shapes recording is set, information
    about input dimensions will be collected. This allows one to see which dimensions
    have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True).
    Please note that shape recording might skew your profiling data. It is recommended
    to use separate runs with and without shape recording to validate the timing.
    Most likely the skew will be negligible for bottom most events (in a case of nested
    function calls). But for higher level functions the total self cpu time might
    be artificially increased because of the shape collection.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**record_shapes**（*bool*） – 如果设置了形状记录，将收集有关输入维度的信息。这允许查看在幕后使用了哪些维度，并进一步通过使用prof.key_averages(group_by_input_shape=True)按它们进行分组。请注意，形状记录可能会使您的分析数据产生偏差。建议使用分别具有和不具有形状记录的运行来验证时间。在嵌套函数调用的情况下，最底层事件的偏差可能可以忽略不计。但对于更高级别的函数，总自身CPU时间可能会因为形状收集而人为增加。'
- en: '**with_flops** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If with_flops is set, the profiler will
    estimate the FLOPs (floating point operations) value using the operator’s input
    shape. This allows one to estimate the hardware performance. Currently, this option
    only works for the matrix multiplication and 2D convolution operators.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**with_flops**（*bool*） – 如果设置了with_flops，分析器将使用操作符的输入形状来估计FLOPs（浮点运算）值。这允许估计硬件性能。目前，此选项仅适用于矩阵乘法和2D卷积操作。'
- en: '**profile_memory** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – track tensor memory allocation/deallocation.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**profile_memory**（*bool*） – 跟踪张量的内存分配/释放。'
- en: '**with_stack** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – record source information (file and line
    number) for the ops.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**with_stack**（*bool*） – 记录操作的源信息（文件和行号）。'
- en: '**with_modules** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – record module hierarchy (including function names) corresponding
    to the callstack of the op. e.g. If module A’s forward call’s module B’s forward
    which contains an aten::add op, then aten::add’s module hierarchy is A.B Note
    that this support exist, at the moment, only for TorchScript models and not eager
    mode models.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**with_modules**（*bool*） – 记录与操作的调用堆栈对应的模块层次结构（包括函数名）。例如，如果模块A的前向调用模块B的前向，其中包含一个aten::add操作，那么aten::add的模块层次结构是A.B。请注意，目前此支持仅适用于TorchScript模型，而不适用于急切模式模型。'
- en: '**use_kineto** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – experimental, enable profiling with Kineto
    profiler.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**use_kineto**（*bool*） – 实验性的，启用使用Kineto分析器进行分析。'
- en: '**use_cpu** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – profile CPU events; setting to `False` requires
    `use_kineto=True` and can be used to lower the overhead for GPU-only profiling.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**use_cpu**（*bool*） – 分析CPU事件；将其设置为`False`需要`use_kineto=True`，可用于降低仅针对GPU的分析的开销。'
- en: '**experimental_config** (*_ExperimentalConfig*) – A set of experimental options
    used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**experimental_config**（*_ExperimentalConfig*） – 由像Kineto这样的分析器库使用的一组实验选项。请注意，不保证向后兼容性。'
- en: Example
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '| [`profiler.profile.export_chrome_trace`](generated/torch.autograd.profiler.profile.export_chrome_trace.html#torch.autograd.profiler.profile.export_chrome_trace
    "torch.autograd.profiler.profile.export_chrome_trace") | Export an EventList as
    a Chrome tracing tools file. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| [`profiler.profile.export_chrome_trace`](generated/torch.autograd.profiler.profile.export_chrome_trace.html#torch.autograd.profiler.profile.export_chrome_trace
    "torch.autograd.profiler.profile.export_chrome_trace") | 将事件列表导出为Chrome跟踪工具文件。
    |'
- en: '| [`profiler.profile.key_averages`](generated/torch.autograd.profiler.profile.key_averages.html#torch.autograd.profiler.profile.key_averages
    "torch.autograd.profiler.profile.key_averages") | Averages all function events
    over their keys. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [`profiler.profile.key_averages`](generated/torch.autograd.profiler.profile.key_averages.html#torch.autograd.profiler.profile.key_averages
    "torch.autograd.profiler.profile.key_averages") | 对所有函数事件按其键进行平均。 |'
- en: '| [`profiler.profile.self_cpu_time_total`](generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total
    "torch.autograd.profiler.profile.self_cpu_time_total") | Returns total time spent
    on CPU. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| [`profiler.profile.self_cpu_time_total`](generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total
    "torch.autograd.profiler.profile.self_cpu_time_total") | 返回在CPU上花费的总时间。 |'
- en: '| [`profiler.profile.total_average`](generated/torch.autograd.profiler.profile.total_average.html#torch.autograd.profiler.profile.total_average
    "torch.autograd.profiler.profile.total_average") | Averages all events. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [`profiler.profile.total_average`](generated/torch.autograd.profiler.profile.total_average.html#torch.autograd.profiler.profile.total_average
    "torch.autograd.profiler.profile.total_average") | 对所有事件进行平均。 |'
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Context manager that makes every autograd operation emit an NVTX range.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使每个自动求导操作发出一个NVTX范围的上下文管理器。
- en: 'It is useful when running the program under nvprof:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用nvprof运行程序时很有用：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Unfortunately, there’s no way to force nvprof to flush the data it collected
    to disk, so for CUDA profiling one has to use this context manager to annotate
    nvprof traces and wait for the process to exit before inspecting them. Then, either
    NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or [`torch.autograd.profiler.load_nvprof()`](generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof
    "torch.autograd.profiler.load_nvprof") can load the results for inspection e.g.
    in Python REPL.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，没有办法强制nvprof将其收集的数据刷新到磁盘上，因此对于CUDA分析，人们必须使用此上下文管理器来注释nvprof跟踪，并等待进程退出后再进行检查。然后，可以使用NVIDIA
    Visual Profiler（nvvp）来可视化时间轴，或者[`torch.autograd.profiler.load_nvprof()`](generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof
    "torch.autograd.profiler.load_nvprof")可以加载结果以供在Python REPL中检查。
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**enabled** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Setting `enabled=False` makes this context
    manager a no-op. Default: `True`.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**enabled**（*bool*，可选）- 设置`enabled=False`会使此上下文管理器无效。默认值：`True`。'
- en: '**record_shapes** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If `record_shapes=True`, the nvtx range
    wrapping each autograd op will append information about the sizes of Tensor arguments
    received by that op, in the following format: `[[arg0.size(0), arg0.size(1), ...],
    [arg1.size(0), arg1.size(1), ...], ...]` Non-tensor arguments will be represented
    by `[]`. Arguments will be listed in the order they are received by the backend
    op. Please note that this order may not match the order in which those arguments
    were passed on the Python side. Also note that shape recording may increase the
    overhead of nvtx range creation. Default: `False`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**record_shapes**（*bool*，可选）- 如果`record_shapes=True`，则每个自动求导操作包装的nvtx范围将附加有关该操作接收的Tensor参数大小的信息，格式如下：`[[arg0.size(0),
    arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]` 非张量参数将用`[]`表示。参数将按照后端操作接收它们的顺序列出。请注意，此顺序可能与这些参数在Python端传递的顺序不匹配。还请注意，形状记录可能会增加nvtx范围创建的开销。默认值：`False`'
- en: Example
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Forward-backward correlation**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向-反向相关性**'
- en: When viewing a profile created using [`emit_nvtx`](#torch.autograd.profiler.emit_nvtx
    "torch.autograd.profiler.emit_nvtx") in the Nvidia Visual Profiler, correlating
    each backward-pass op with the corresponding forward-pass op can be difficult.
    To ease this task, [`emit_nvtx`](#torch.autograd.profiler.emit_nvtx "torch.autograd.profiler.emit_nvtx")
    appends sequence number information to the ranges it generates.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Nvidia Visual Profiler创建的配置文件时，将每个反向传递操作与相应的前向传递操作进行关联可能会很困难。为了简化这个任务，[`emit_nvtx`](#torch.autograd.profiler.emit_nvtx
    "torch.autograd.profiler.emit_nvtx")会向其生成的范围附加序列号信息。
- en: During the forward pass, each function range is decorated with `seq=<N>`. `seq`
    is a running counter, incremented each time a new backward Function object is
    created and stashed for backward. Thus, the `seq=<N>` annotation associated with
    each forward function range tells you that if a backward Function object is created
    by this forward function, the backward object will receive sequence number N.
    During the backward pass, the top-level range wrapping each C++ backward Function’s
    `apply()` call is decorated with `stashed seq=<M>`. `M` is the sequence number
    that the backward object was created with. By comparing `stashed seq` numbers
    in backward with `seq` numbers in forward, you can track down which forward op
    created each backward Function.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传递期间，每个函数范围都被装饰为`seq=<N>`。`seq`是一个运行计数器，每次创建一个新的反向Function对象并存储用于反向传递时递增。因此，与每个前向函数范围相关联的`seq=<N>`注释告诉您，如果此前向函数创建了一个反向Function对象，那么反向对象将接收序列号N。在反向传递期间，包装每个C++反向Function的`apply()`调用的顶级范围都被装饰为`stashed
    seq=<M>`。`M`是创建反向对象时的序列号。通过在反向传递中比较反向中的`stashed seq`号和前向中的`seq`号，您可以追踪哪个前向操作创建了每个反向Function。
- en: Any functions executed during the backward pass are also decorated with `seq=<N>`.
    During default backward (with `create_graph=False`) this information is irrelevant,
    and in fact, `N` may simply be 0 for all such functions. Only the top-level ranges
    associated with backward Function objects’ `apply()` methods are useful, as a
    way to correlate these Function objects with the earlier forward pass.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传递期间执行的任何函数也都被装饰为`seq=<N>`。在默认反向传递（`create_graph=False`）中，此信息是无关紧要的，实际上，对于所有这样的函数，`N`可能只是0。只有与反向Function对象的`apply()`方法相关联的顶级范围是有用的，作为将这些Function对象与之前的前向传递相关联的一种方式。
- en: '**Double-backward**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**双向传递**'
- en: 'If, on the other hand, a backward pass with `create_graph=True` is underway
    (in other words, if you are setting up for a double-backward), each function’s
    execution during backward is given a nonzero, useful `seq=<N>`. Those functions
    may themselves create Function objects to be executed later during double-backward,
    just as the original functions in the forward pass did. The relationship between
    backward and double-backward is conceptually the same as the relationship between
    forward and backward: The functions still emit current-sequence-number-tagged
    ranges, the Function objects they create still stash those sequence numbers, and
    during the eventual double-backward, the Function objects’ `apply()` ranges are
    still tagged with `stashed seq` numbers, which can be compared to seq numbers
    from the backward pass.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果正在进行带有`create_graph=True`的反向传递（换句话说，如果您正在为双向传递做准备），则在反向期间执行的每个函数都会获得一个非零、有用的`seq=<N>`。这些函数本身可能会创建Function对象，以便稍后在双向传递期间执行，就像前向传递中的原始函数一样。反向和双向之间的关系在概念上与前向和反向之间的关系相同：函数仍然发出当前序列号标记的范围，它们创建的Function对象仍然存储这些序列号，并且在最终的双向传递期间，Function对象的`apply()`范围仍然标记有`stashed
    seq`号，可以与反向传递中的seq号进行比较。
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Context manager that makes every autograd operation emit an ITT range.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文管理器，使每个自动求导操作都发出一个ITT范围。
- en: 'It is useful when running the program under Intel(R) VTune Profiler:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行程序时在英特尔(R) VTune Profiler下很有用：
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The Instrumentation and Tracing Technology (ITT) API enables your application
    to generate and control the collection of trace data during its execution across
    different Intel tools. This context manager is to annotate Intel(R) VTune Profiling
    trace. With help of this context manager, you will be able to see labled ranges
    in Intel(R) VTune Profiler GUI.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 仪器和跟踪技术（ITT）API使您的应用程序能够在其执行过程中生成和控制跨不同英特尔工具的跟踪数据的收集。这个上下文管理器是用来注释英特尔(R) VTune
    Profiling跟踪的。借助这个上下文管理器，您将能够在英特尔(R) VTune Profiler GUI 中看到标记的范围。
- en: Parameters
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**enabled** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – Setting `enabled=False` makes this context
    manager a no-op. Default: `True`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**enabled** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在 Python v3.12 中)")*,* *可选*) – 设置 `enabled=False` 会使这个上下文管理器无效。默认值：`True`。'
- en: '**record_shapes** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")*,* *optional*) – If `record_shapes=True`, the itt range wrapping
    each autograd op will append information about the sizes of Tensor arguments received
    by that op, in the following format: `[[arg0.size(0), arg0.size(1), ...], [arg1.size(0),
    arg1.size(1), ...], ...]` Non-tensor arguments will be represented by `[]`. Arguments
    will be listed in the order they are received by the backend op. Please note that
    this order may not match the order in which those arguments were passed on the
    Python side. Also note that shape recording may increase the overhead of itt range
    creation. Default: `False`'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**record_shapes** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在 Python v3.12 中)")*,* *可选*) – 如果 `record_shapes=True`，则每个自动求导操作周围的 itt 范围将附加关于该操作接收到的张量参数大小的信息，格式如下：`[[arg0.size(0),
    arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]` 非张量参数将用 `[]` 表示。参数将按照它们被后端操作接收的顺序列出。请注意，这个顺序可能与这些参数在
    Python 端传递的顺序不匹配。还请注意，形状记录可能会增加 itt 范围创建的开销。默认值：`False`'
- en: Example
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '| [`profiler.load_nvprof`](generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof
    "torch.autograd.profiler.load_nvprof") | Open an nvprof trace file and parses
    autograd annotations. |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| [`profiler.load_nvprof`](generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof
    "torch.autograd.profiler.load_nvprof") | 打开一个 nvprof 跟踪文件并解析自动求导注释。 |'
- en: Anomaly detection
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常检测
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Context-manager that enable anomaly detection for the autograd engine.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文管理器，用于启用自动求导引擎的异常检测。
- en: 'This does two things:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做两件事：
- en: Running the forward pass with detection enabled will allow the backward pass
    to print the traceback of the forward operation that created the failing backward
    function.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在启用检测的情况下运行前向传递将允许反向传递打印创建失败的反向函数的前向操作的回溯。
- en: If `check_nan` is `True`, any backward computation that generate “nan” value
    will raise an error. Default `True`.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `check_nan` 为 `True`，任何生成“nan”值的反向计算都将引发错误。默认为 `True`。
- en: Warning
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This mode should be enabled only for debugging as the different tests will slow
    down your program execution.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此模式应仅用于调试，因为不同的测试会减慢程序执行速度。
- en: Example
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Context-manager that sets the anomaly detection for the autograd engine on or
    off.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文管理器，用于设置自动求导引擎的异常检测开启或关闭。
- en: '`set_detect_anomaly` will enable or disable the autograd anomaly detection
    based on its argument `mode`. It can be used as a context-manager or as a function.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`set_detect_anomaly` 将根据其参数 `mode` 启用或禁用自动求导异常检测。它可以作为上下文管理器或函数使用。'
- en: See `detect_anomaly` above for details of the anomaly detection behaviour.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 有关异常检测行为的详细信息，请参阅上面的 `detect_anomaly`。
- en: Parameters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**mode** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in
    Python v3.12)")) – Flag whether to enable anomaly detection (`True`), or disable
    (`False`).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mode** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(在
    Python v3.12 中)")*) – 标志，指示是否启用异常检测 (`True`) 或禁用 (`False`)。'
- en: '**check_nan** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – Flag whether to raise an error when the backward generate
    “nan”'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**check_nan** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(在 Python v3.12 中)")) – 标志，指示在反向传播生成“nan”时是否引发错误。'
- en: Autograd graph
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动求导图
- en: Autograd exposes methods that allow one to inspect the graph and interpose behavior
    during the backward pass.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 自动求导公开了允许检查图并在反向传播期间插入行为的方法。
- en: The `grad_fn` attribute of a [`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor")
    holds a `torch.autograd.graph.Node` if the tensor is the output of a operation
    that was recorded by autograd (i.e., grad_mode is enabled and at least one of
    the inputs required gradients), or `None` otherwise.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果张量是由自动求导记录的操作的输出（即启用了 grad_mode 并且至少一个输入需要梯度），则 [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor") 的 `grad_fn` 属性将保存一个 `torch.autograd.graph.Node`，否则为 `None`。
- en: '| [`graph.Node.name`](generated/torch.autograd.graph.Node.name.html#torch.autograd.graph.Node.name
    "torch.autograd.graph.Node.name") | Return the name. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| [`graph.Node.name`](generated/torch.autograd.graph.Node.name.html#torch.autograd.graph.Node.name
    "torch.autograd.graph.Node.name") | 返回名称。 |'
- en: '| [`graph.Node.metadata`](generated/torch.autograd.graph.Node.metadata.html#torch.autograd.graph.Node.metadata
    "torch.autograd.graph.Node.metadata") | Return the metadata. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| [`graph.Node.metadata`](generated/torch.autograd.graph.Node.metadata.html#torch.autograd.graph.Node.metadata
    "torch.autograd.graph.Node.metadata") | 返回元数据。 |'
- en: '| [`graph.Node.next_functions`](generated/torch.autograd.graph.Node.next_functions.html#torch.autograd.graph.Node.next_functions
    "torch.autograd.graph.Node.next_functions") |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| [`graph.Node.next_functions`](generated/torch.autograd.graph.Node.next_functions.html#torch.autograd.graph.Node.next_functions
    "torch.autograd.graph.Node.next_functions") |  |'
- en: '| [`graph.Node.register_hook`](generated/torch.autograd.graph.Node.register_hook.html#torch.autograd.graph.Node.register_hook
    "torch.autograd.graph.Node.register_hook") | Register a backward hook. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| [`graph.Node.register_hook`](generated/torch.autograd.graph.Node.register_hook.html#torch.autograd.graph.Node.register_hook
    "torch.autograd.graph.Node.register_hook") | 注册一个反向钩子。 |'
- en: '| [`graph.Node.register_prehook`](generated/torch.autograd.graph.Node.register_prehook.html#torch.autograd.graph.Node.register_prehook
    "torch.autograd.graph.Node.register_prehook") | Register a backward pre-hook.
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '[`graph.Node.register_prehook`](generated/torch.autograd.graph.Node.register_prehook.html#torch.autograd.graph.Node.register_prehook
    "torch.autograd.graph.Node.register_prehook") | 注册一个反向预钩子。'
- en: 'Some operations need intermediary results to be saved during the forward pass
    in order to execute the backward pass. These intermediary results are saved as
    attributes on the `grad_fn` and can be accessed. For example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一些操作需要在前向传递期间保存中间结果以执行反向传递。这些中间结果保存为`grad_fn`上的属性，并可以访问。例如：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can also define how these saved tensors should be packed / unpacked using
    hooks. A common application is to trade compute for memory by saving those intermediary
    results to disk or to CPU instead of leaving them on the GPU. This is especially
    useful if you notice your model fits on GPU during evaluation, but not training.
    Also see [Hooks for saved tensors](notes/autograd.html#saved-tensors-hooks-doc).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用钩子定义这些保存的张量应如何打包/解包。一个常见的应用是通过将这些中间结果保存到磁盘或 CPU 来交换计算和内存，而不是将它们留在 GPU
    上。如果您注意到您的模型在评估期间适合 GPU，但在训练期间不适合，则这是非常有用的。另请参阅[保存张量的钩子](notes/autograd.html#saved-tensors-hooks-doc)。
- en: '[PRE15]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Context-manager that sets a pair of pack / unpack hooks for saved tensors.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为保存的张量设置一对 pack / unpack 钩子的上下文管理器。
- en: Use this context-manager to define how intermediary results of an operation
    should be packed before saving, and unpacked on retrieval.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个上下文管理器来定义操作的中间结果在保存前应如何打包，并在检索时解包。
- en: In that context, the `pack_hook` function will be called everytime an operation
    saves a tensor for backward (this includes intermediary results saved using `save_for_backward()`
    but also those recorded by a PyTorch-defined operation). The output of `pack_hook`
    is then stored in the computation graph instead of the original tensor.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每次操作保存张量进行反向传递时（包括使用`save_for_backward()`保存的中间结果以及由 PyTorch 定义的操作记录的结果），都会调用`pack_hook`函数。然后，`pack_hook`的输出将存储在计算图中，而不是原始张量。
- en: The `unpack_hook` is called when the saved tensor needs to be accessed, namely
    when executing [`torch.Tensor.backward()`](generated/torch.Tensor.backward.html#torch.Tensor.backward
    "torch.Tensor.backward") or [`torch.autograd.grad()`](generated/torch.autograd.grad.html#torch.autograd.grad
    "torch.autograd.grad"). It takes as argument the *packed* object returned by `pack_hook`
    and should return a tensor which has the same content as the original tensor (passed
    as input to the corresponding `pack_hook`).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要访问保存的张量时，即在执行[`torch.Tensor.backward()`](generated/torch.Tensor.backward.html#torch.Tensor.backward
    "torch.Tensor.backward")或[`torch.autograd.grad()`](generated/torch.autograd.grad.html#torch.autograd.grad
    "torch.autograd.grad")时，将调用`unpack_hook`。它以`pack_hook`返回的*packed*对象作为参数，并应返回一个与原始张量内容相同的张量（作为输入传递给相应的`pack_hook`）。
- en: 'The hooks should have the following signatures:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 钩子应具有以下签名：
- en: 'pack_hook(tensor: Tensor) -> Any'
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'pack_hook(tensor: Tensor) -> Any'
- en: ''
  id: totrans-163
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: unpack_hook(Any) -> Tensor
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: unpack_hook(Any) -> Tensor
- en: where the return value of `pack_hook` is a valid input to `unpack_hook`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`pack_hook`的返回值是`unpack_hook`的有效输入。
- en: In general, you want `unpack_hook(pack_hook(t))` to be equal to `t` in terms
    of value, size, dtype and device.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，您希望`unpack_hook(pack_hook(t))`在值、大小、dtype 和设备方面等于`t`。
- en: 'Example:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Warning
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Performing an inplace operation on the input to either hooks may lead to undefined
    behavior.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对输入执行就地操作到任一钩子可能导致未定义行为。
- en: Warning
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Only one pair of hooks is allowed at a time. When recursively nesting this context-manager,
    only the inner-most pair of hooks will be applied.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一次只允许一个钩子对。当递归嵌套这个上下文管理器时，只有最内层的一对钩子将被应用。
- en: '[PRE17]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Context manager under which tensors saved by the forward pass will be stored
    on cpu, then retrieved for backward.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传递期间保存的张量将存储在 CPU 上，然后在反向传递时检索。
- en: When performing operations within this context manager, intermediary results
    saved in the graph during the forward pass will be moved to CPU, then copied back
    to the original device when needed for the backward pass. If the graph was already
    on CPU, no tensor copy is performed.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文管理器中执行操作时，在前向传递期间保存在图中的中间结果将被移动到 CPU，然后在需要进行反向传递时复制回原始设备。如果图已经在 CPU 上，则不执行张量复制。
- en: Use this context-manager to trade compute for GPU memory usage (e.g. when your
    model doesn’t fit in GPU memory during training).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个上下文管理器来在计算和 GPU 内存使用之间进行交换（例如，在训练期间模型不适合 GPU 内存时）。
- en: Parameters
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**pin_memory** ([*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")) – If `True` tensors will be saved to CPU pinned memory during
    packing and copied to GPU asynchronously during unpacking. Defaults to `False`.
    Also see [Use pinned memory buffers](notes/cuda.html#cuda-memory-pinning).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**pin_memory**（[*bool*](https://docs.python.org/3/library/functions.html#bool
    "(in Python v3.12)")） - 如果为`True`，张量将在打包期间保存到 CPU 固定内存，并在解包期间异步复制到 GPU。默认为`False`。另请参阅[使用固定内存缓冲区](notes/cuda.html#cuda-memory-pinning)。'
- en: 'Example:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Context-manager that disables the saved tensors default hooks feature.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用保存的张量默认钩子功能的上下文管理器。
- en: Useful for if you are creating a feature that does not work with saved tensors
    default hooks.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在创建一个与保存的张量默认钩子不兼容的特性，则此功能很有用。
- en: Parameters
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**error_message** ([*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")) – When saved tensors default hooks are used when they have
    been are disabled, a RuntimeError with this error message gets raised.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**error_message**（[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")） - 当使用保存的张量默认钩子时，当它们被禁用时，会引发带有此错误消息的 RuntimeError。'
- en: 'Example:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE20]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Register a multi-grad backward hook.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注册一个多梯度反向钩子。
- en: The hook will be called after gradients with respect to every tensor in `tensors`
    have been computed. If a tensor is in `tensors` but is not part of the graph,
    or if a tensor is not needed to compute the gradients for any `inputs` specified
    for the current `.backward()` or `.grad()` call, this tensor will be ignored and
    the hook will not wait for its gradient to be computed.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算完与`tensors`中的每个张量相关的梯度后，将调用钩子。如果一个张量在`tensors`中但不是图的一部分，或者一个张量不需要计算当前`.backward()`或`.grad()`调用中指定的任何`inputs`的梯度，那么这个张量将被忽略，钩子将不会等待其梯度被计算。
- en: After every non-ignored tensor’s gradient has been computed, `fn` will be called
    with those gradients. `None` will be passed for tensors that did not have their
    gradients computed.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算完每个未被忽略的张量的梯度后，将调用`fn`并传递这些梯度。对于那些没有计算梯度的张量，将传递`None`。
- en: The hook should not modify its arguments.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 钩子不应修改其参数。
- en: This function returns a handle with a method `handle.remove()` that removes
    the hook.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数返回一个具有`handle.remove()`方法的句柄，用于移除钩子。
- en: Note
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: See [Backward Hooks execution](notes/autograd.html#backward-hooks-execution)
    for more information on how when this hook is executed, and how its execution
    is ordered relative to other hooks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此钩子的执行时间以及其执行顺序与其他钩子的关系的更多信息，请参见[反向钩子执行](notes/autograd.html#backward-hooks-execution)。
- en: 'Example:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE22]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Context manager under which mutating tensors saved for backward is allowed.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 允许变异保存用于反向传播的张量的上下文管理器。
- en: Under this context manager, tensors saved for backward are cloned on mutation,
    so the original version can still be used during backward. Normally, mutating
    a tensor saved for backward will result in an error raised when it’s used during
    backward.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文管理器下，保存用于反向传播的张量在变异时会被克隆，因此原始版本仍然可以在反向传播期间使用。通常，对保存用于反向传播的张量进行变异会导致在反向传播期间使用时引发错误。
- en: To ensure the correct behavior, both the forward and backward should be run
    under the same context manager.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保正确的行为，前向和后向都应该在相同的上下文管理器下运行。
- en: Returns
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: An _AllowMutationOnSavedContext object storing the state managed by this context
    manager. This object can be useful for debugging purposes. The state managed by
    the context manager is automatically cleared upon exiting.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一个存储由这个上下文管理器管理的状态的 _AllowMutationOnSavedContext 对象。这个对象对于调试目的很有用。上下文管理器管理的状态在退出时会自动清除。
- en: 'Example:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE24]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Object representing a given gradient edge within the autograd graph. To get
    the gradient edge where a given Tensor gradient will be computed, you can do `edge
    = autograd.graph.get_gradient_edge(tensor)`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 表示autograd图中给定梯度边缘的对象。要获取将计算给定张量梯度的梯度边缘，可以执行`edge = autograd.graph.get_gradient_edge(tensor)`。
- en: '[PRE26]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Get the gradient edge for computing the gradient of the given Tensor.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 获取给定张量的梯度的梯度边缘。
- en: In particular, it is equivalent to call `g = autograd.grad(loss, input)` and
    `g = autograd.grad(loss, get_gradient_edge(input))`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，调用`g = autograd.grad(loss, input)`和`g = autograd.grad(loss, get_gradient_edge(input))`是等价的。
