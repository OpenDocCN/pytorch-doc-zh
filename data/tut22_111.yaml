- en: Distributed and Parallel Training Tutorials
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式和并行训练教程
- en: 原文：[https://pytorch.org/tutorials/distributed/home.html](https://pytorch.org/tutorials/distributed/home.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/distributed/home.html](https://pytorch.org/tutorials/distributed/home.html)
- en: Distributed training is a model training paradigm that involves spreading training
    workload across multiple worker nodes, therefore significantly improving the speed
    of training and model accuracy. While distributed training can be used for any
    type of ML model training, it is most beneficial to use it for large models and
    compute demanding tasks as deep learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练是一种模型训练范式，涉及将训练工作负载分布到多个工作节点，从而显著提高训练速度和模型准确性。虽然分布式训练可用于任何类型的ML模型训练，但对于大型模型和计算密集型任务（如深度学习）使用它最为有益。
- en: 'There are a few ways you can perform distributed training in PyTorch with each
    method having their advantages in certain use cases:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中有几种方法可以进行分布式训练，每种方法在特定用例中都有其优势：
- en: '[DistributedDataParallel (DDP)](#learn-ddp)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DistributedDataParallel (DDP)](#learn-ddp)'
- en: '[Fully Sharded Data Parallel (FSDP)](#learn-fsdp)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[完全分片数据并行（FSDP）](#learn-fsdp)'
- en: '[Device Mesh](#device-mesh)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[设备网格](#device-mesh)'
- en: '[Remote Procedure Call (RPC) distributed training](#learn-rpc)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[远程过程调用（RPC）分布式训练](#learn-rpc)'
- en: '[Custom Extensions](#custom-extensions)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自定义扩展](#custom-extensions)'
- en: Read more about these options in [Distributed Overview](../beginner/dist_overview.html).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在[分布式概述](../beginner/dist_overview.html)中了解更多关于这些选项的信息。
- en: '## Learn DDP'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '## 学习DDP'
- en: DDP Intro Video Tutorials
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DDP简介视频教程
- en: A step-by-step video series on how to get started with DistributedDataParallel
    and advance to more complex topics
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列逐步视频教程，介绍如何开始使用DistributedDataParallel，并逐步深入更复杂的主题
- en: Code Video
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 代码视频
- en: Getting Started with Distributed Data Parallel
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用分布式数据并行处理
- en: This tutorial provides a short and gentle intro to the PyTorch DistributedData
    Parallel.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程为PyTorch DistributedData Parallel提供了简短而温和的介绍。
- en: Code
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 代码
- en: Distributed Training with Uneven Inputs Using the Join Context Manager
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Join上下文管理器进行不均匀输入的分布式训练
- en: This tutorial describes the Join context manager and demonstrates it’s use with
    DistributedData Parallel.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程描述了Join上下文管理器，并演示了如何与DistributedData Parallel一起使用。
- en: 'Code  ## Learn FSDP'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '代码  ## 学习FSDP'
- en: Getting Started with FSDP
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用FSDP
- en: This tutorial demonstrates how you can perform distributed training with FSDP
    on a MNIST dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程演示了如何在MNIST数据集上使用FSDP进行分布式训练。
- en: Code
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 代码
- en: FSDP Advanced
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: FSDP 高级
- en: In this tutorial, you will learn how to fine-tune a HuggingFace (HF) T5 model
    with FSDP for text summarization.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将学习如何使用FSDP对HuggingFace（HF）T5模型进行微调，用于文本摘要。
- en: 'Code  ## Learn DeviceMesh'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '代码  ## 学习DeviceMesh'
- en: Getting Started with DeviceMesh
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用DeviceMesh
- en: In this tutorial you will learn about DeviceMesh and how it can help with distributed
    training.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将了解DeviceMesh以及它如何帮助进行分布式训练。
- en: 'Code  ## Learn RPC'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '代码  ## 学习RPC'
- en: Getting Started with Distributed RPC Framework
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用分布式RPC框架
- en: This tutorial demonstrates how to get started with RPC-based distributed training.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程演示了如何开始使用基于RPC的分布式训练。
- en: Code
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 代码
- en: Implementing a Parameter Server Using Distributed RPC Framework
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分布式RPC框架实现参数服务器
- en: This tutorial walks you through a simple example of implementing a parameter
    server using PyTorch’s Distributed RPC framework.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将带您完成一个简单的示例，使用PyTorch的分布式RPC框架实现参数服务器。
- en: Code
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 代码
- en: Implementing Batch RPC Processing Using Asynchronous Executions
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用异步执行实现批处理RPC处理
- en: In this tutorial you will build batch-processing RPC applications with the @rpc.functions.async_execution
    decorator.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将使用@rpc.functions.async_execution装饰器构建批处理RPC应用程序。
- en: Code
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 代码
- en: Combining Distributed DataParallel with Distributed RPC Framework
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将分布式DataParallel与分布式RPC框架结合
- en: In this tutorial you will learn how to combine distributed data parallelism
    with distributed model parallelism.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将学习如何将分布式数据并行性与分布式模型并行性结合起来。
- en: 'Code  ## Custom Extensions'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '代码  ## 自定义扩展'
- en: Customize Process Group Backends Using Cpp Extensions
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Cpp扩展自定义Process Group后端
- en: In this tutorial you will learn to implement a custom ProcessGroup backend and
    plug that into PyTorch distributed package using cpp extensions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将学习如何实现自定义的ProcessGroup后端，并将其插入到PyTorch分布式包中使用cpp扩展。
- en: Code
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 代码
