- en: Modules
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块
- en: 原文：[https://pytorch.org/docs/stable/notes/modules.html](https://pytorch.org/docs/stable/notes/modules.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/notes/modules.html](https://pytorch.org/docs/stable/notes/modules.html)
- en: 'PyTorch uses modules to represent neural networks. Modules are:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch使用模块来表示神经网络。模块是：
- en: '**Building blocks of stateful computation.** PyTorch provides a robust library
    of modules and makes it simple to define new custom modules, allowing for easy
    construction of elaborate, multi-layer neural networks.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有状态计算的构建块。** PyTorch提供了一个强大的模块库，并且可以轻松定义新的自定义模块，从而可以轻松构建复杂的多层神经网络。'
- en: '**Tightly integrated with PyTorch’s** [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
    **system.** Modules make it simple to specify learnable parameters for PyTorch’s
    Optimizers to update.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与PyTorch的** [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
    **系统紧密集成。** 模块使得为PyTorch的优化器指定可学习参数变得简单。'
- en: '**Easy to work with and transform.** Modules are straightforward to save and
    restore, transfer between CPU / GPU / TPU devices, prune, quantize, and more.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于使用和转换。** 模块易于保存和恢复，在CPU / GPU / TPU设备之间传输，修剪，量化等。'
- en: This note describes modules, and is intended for all PyTorch users. Since modules
    are so fundamental to PyTorch, many topics in this note are elaborated on in other
    notes or tutorials, and links to many of those documents are provided here as
    well.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本说明描述了模块，并适用于所有PyTorch用户。由于模块对于PyTorch非常重要，因此本说明中的许多主题在其他说明或教程中有详细介绍，并在此处提供了许多文档的链接。
- en: '[A Simple Custom Module](#a-simple-custom-module)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个简单的自定义模块](#a-simple-custom-module)'
- en: '[Modules as Building Blocks](#modules-as-building-blocks)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模块作为构建块](#modules-as-building-blocks)'
- en: '[Neural Network Training with Modules](#neural-network-training-with-modules)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用模块进行神经网络训练](#neural-network-training-with-modules)'
- en: '[Module State](#module-state)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模块状态](#module-state)'
- en: '[Module Initialization](#module-initialization)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模块初始化](#module-initialization)'
- en: '[Module Hooks](#module-hooks)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模块钩子](#module-hooks)'
- en: '[Advanced Features](#advanced-features)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高级功能](#advanced-features)'
- en: '[Distributed Training](#distributed-training)'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分布式训练](#distributed-training)'
- en: '[Profiling Performance](#profiling-performance)'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[性能分析](#profiling-performance)'
- en: '[Improving Performance with Quantization](#improving-performance-with-quantization)'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用量化改善性能](#improving-performance-with-quantization)'
- en: '[Improving Memory Usage with Pruning](#improving-memory-usage-with-pruning)'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用修剪改善内存使用情况](#improving-memory-usage-with-pruning)'
- en: '[Parametrizations](#parametrizations)'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[参数化](#parametrizations)'
- en: '[Transforming Modules with FX](#transforming-modules-with-fx)'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用FX转换模块](#transforming-modules-with-fx)'
- en: '[A Simple Custom Module](#id4)[](#a-simple-custom-module "Permalink to this
    heading")'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[一个简单的自定义模块](#id4)[](#a-simple-custom-module "跳转到此标题")'
- en: To get started, let’s look at a simpler, custom version of PyTorch’s [`Linear`](../generated/torch.nn.Linear.html#torch.nn.Linear
    "torch.nn.Linear") module. This module applies an affine transformation to its
    input.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，让我们看一个更简单的自定义版本PyTorch的[`Linear`](../generated/torch.nn.Linear.html#torch.nn.Linear
    "torch.nn.Linear")模块。该模块对其输入应用仿射变换。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This simple module has the following fundamental characteristics of modules:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的模块具有以下模块的基本特征：
- en: '**It inherits from the base Module class.** All modules should subclass [`Module`](../generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module") for composability with other modules.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它继承自基本的Module类。** 所有模块都应该子类化[`Module`](../generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")以便与其他模块组合。'
- en: '**It defines some “state” that is used in computation.** Here, the state consists
    of randomly-initialized `weight` and `bias` tensors that define the affine transformation.
    Because each of these is defined as a [`Parameter`](../generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter
    "torch.nn.parameter.Parameter"), they are *registered* for the module and will
    automatically be tracked and returned from calls to [`parameters()`](../generated/torch.nn.Module.html#torch.nn.Module.parameters
    "torch.nn.Module.parameters"). Parameters can be considered the “learnable” aspects
    of the module’s computation (more on this later). Note that modules are not required
    to have state, and can also be stateless.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它定义了在计算中使用的一些“状态”。** 在这里，状态包括随机初始化的`weight`和`bias`张量，用于定义仿射变换。因为每个都被定义为[`Parameter`](../generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter
    "torch.nn.parameter.Parameter")，它们被*注册*到模块中，并将自动跟踪并从调用[`parameters()`](../generated/torch.nn.Module.html#torch.nn.Module.parameters
    "torch.nn.Module.parameters")返回。参数可以被认为是模块计算的“可学习”部分（稍后会详细介绍）。请注意，模块不需要具有状态，也可以是无状态的。'
- en: '**It defines a forward() function that performs the computation.** For this
    affine transformation module, the input is matrix-multiplied with the `weight`
    parameter (using the `@` short-hand notation) and added to the `bias` parameter
    to produce the output. More generally, the `forward()` implementation for a module
    can perform arbitrary computation involving any number of inputs and outputs.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它定义了一个执行计算的forward()函数。** 对于这个仿射变换模块，输入与`weight`参数进行矩阵乘法（使用`@`简写符号）并加上`bias`参数以产生输出。更一般地，模块的`forward()`实现可以执行涉及任意数量输入和输出的任意计算。'
- en: 'This simple module demonstrates how modules package state and computation together.
    Instances of this module can be constructed and called:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的模块演示了模块如何将状态和计算打包在一起。可以构建并调用此模块的实例：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that the module itself is callable, and that calling it invokes its `forward()`
    function. This name is in reference to the concepts of “forward pass” and “backward
    pass”, which apply to each module. The “forward pass” is responsible for applying
    the computation represented by the module to the given input(s) (as shown in the
    above snippet). The “backward pass” computes gradients of module outputs with
    respect to its inputs, which can be used for “training” parameters through gradient
    descent methods. PyTorch’s autograd system automatically takes care of this backward
    pass computation, so it is not required to manually implement a `backward()` function
    for each module. The process of training module parameters through successive
    forward / backward passes is covered in detail in [Neural Network Training with
    Modules](#neural-network-training-with-modules).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模块本身是可调用的，调用它会调用其`forward()`函数。这个名称是指“前向传播”和“反向传播”的概念，这些概念适用于每个模块。 “前向传播”负责将模块表示的计算应用于给定的输入（如上面的代码片段所示）。
    “反向传播”计算模块输出相对于其输入的梯度，这些梯度可以用于通过梯度下降方法“训练”参数。PyTorch的自动求导系统会自动处理这个反向传播计算，因此不需要为每个模块手动实现`backward()`函数。通过连续的前向/反向传播训练模块参数的过程在[使用模块进行神经网络训练](#neural-network-training-with-modules)中有详细介绍。
- en: 'The full set of parameters registered by the module can be iterated through
    via a call to [`parameters()`](../generated/torch.nn.Module.html#torch.nn.Module.parameters
    "torch.nn.Module.parameters") or [`named_parameters()`](../generated/torch.nn.Module.html#torch.nn.Module.named_parameters
    "torch.nn.Module.named_parameters"), where the latter includes each parameter’s
    name:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过调用[`parameters()`](../generated/torch.nn.Module.html#torch.nn.Module.parameters
    "torch.nn.Module.parameters")或[`named_parameters()`](../generated/torch.nn.Module.html#torch.nn.Module.named_parameters
    "torch.nn.Module.named_parameters")来迭代模块注册的完整参数集，后者包括每个参数的名称：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In general, the parameters registered by a module are aspects of the module’s
    computation that should be “learned”. A later section of this note shows how to
    update these parameters using one of PyTorch’s Optimizers. Before we get to that,
    however, let’s first examine how modules can be composed with one another.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模块注册的参数是模块计算的方面，应该是“可学习的”。本笔记的后面部分展示了如何使用PyTorch的优化器更新这些参数。然而，在此之前，让我们首先看一下模块如何与其他模块组合。
- en: '[Modules as Building Blocks](#id5)[](#modules-as-building-blocks "Permalink
    to this heading")'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[模块作为构建模块](#id5)[](#modules-as-building-blocks "跳转到此标题")'
- en: 'Modules can contain other modules, making them useful building blocks for developing
    more elaborate functionality. The simplest way to do this is using the [`Sequential`](../generated/torch.nn.Sequential.html#torch.nn.Sequential
    "torch.nn.Sequential") module. It allows us to chain together multiple modules:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 模块可以包含其他模块，使它们成为开发更复杂功能的有用构建模块。最简单的方法是使用[`Sequential`](../generated/torch.nn.Sequential.html#torch.nn.Sequential
    "torch.nn.Sequential")模块。它允许我们将多个模块链接在一起：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that [`Sequential`](../generated/torch.nn.Sequential.html#torch.nn.Sequential
    "torch.nn.Sequential") automatically feeds the output of the first `MyLinear`
    module as input into the [`ReLU`](../generated/torch.nn.ReLU.html#torch.nn.ReLU
    "torch.nn.ReLU"), and the output of that as input into the second `MyLinear` module.
    As shown, it is limited to in-order chaining of modules with a single input and
    output.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[`Sequential`](../generated/torch.nn.Sequential.html#torch.nn.Sequential
    "torch.nn.Sequential")会自动将第一个`MyLinear`模块的输出作为输入传递给[`ReLU`](../generated/torch.nn.ReLU.html#torch.nn.ReLU
    "torch.nn.ReLU")，并将其输出作为输入传递给第二个`MyLinear`模块。如所示，它仅限于按顺序链接具有单个输入和输出的模块。
- en: In general, it is recommended to define a custom module for anything beyond
    the simplest use cases, as this gives full flexibility on how submodules are used
    for a module’s computation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，建议为超出最简单用例的任何内容定义自定义模块，因为这样可以完全灵活地使用子模块进行模块的计算。
- en: 'For example, here’s a simple neural network implemented as a custom module:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里是一个简单的神经网络，实现为一个自定义模块：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This module is composed of two “children” or “submodules” (`l0` and `l1`) that
    define the layers of the neural network and are utilized for computation within
    the module’s `forward()` method. Immediate children of a module can be iterated
    through via a call to [`children()`](../generated/torch.nn.Module.html#torch.nn.Module.children
    "torch.nn.Module.children") or [`named_children()`](../generated/torch.nn.Module.html#torch.nn.Module.named_children
    "torch.nn.Module.named_children"):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模块由两个“子模块”（`l0`和`l1`）组成，它们定义了神经网络的层，并在模块的`forward()`方法中用于计算。通过调用[`children()`](../generated/torch.nn.Module.html#torch.nn.Module.children
    "torch.nn.Module.children")或[`named_children()`](../generated/torch.nn.Module.html#torch.nn.Module.named_children
    "torch.nn.Module.named_children")可以迭代模块的直接子模块：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To go deeper than just the immediate children, [`modules()`](../generated/torch.nn.Module.html#torch.nn.Module.modules
    "torch.nn.Module.modules") and [`named_modules()`](../generated/torch.nn.Module.html#torch.nn.Module.named_modules
    "torch.nn.Module.named_modules") *recursively* iterate through a module and its
    child modules:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入到不仅仅是直接子模块，[`modules()`](../generated/torch.nn.Module.html#torch.nn.Module.modules
    "torch.nn.Module.modules")和[`named_modules()`](../generated/torch.nn.Module.html#torch.nn.Module.named_modules
    "torch.nn.Module.named_modules") *递归*迭代一个模块及其子模块：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Sometimes, it’s necessary for a module to dynamically define submodules. The
    [`ModuleList`](../generated/torch.nn.ModuleList.html#torch.nn.ModuleList "torch.nn.ModuleList")
    and [`ModuleDict`](../generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict "torch.nn.ModuleDict")
    modules are useful here; they register submodules from a list or dict:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，模块需要动态定义子模块。[`ModuleList`](../generated/torch.nn.ModuleList.html#torch.nn.ModuleList
    "torch.nn.ModuleList")和[`ModuleDict`](../generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict
    "torch.nn.ModuleDict")模块在这里很有用；它们从列表或字典中注册子模块：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For any given module, its parameters consist of its direct parameters as well
    as the parameters of all submodules. This means that calls to [`parameters()`](../generated/torch.nn.Module.html#torch.nn.Module.parameters
    "torch.nn.Module.parameters") and [`named_parameters()`](../generated/torch.nn.Module.html#torch.nn.Module.named_parameters
    "torch.nn.Module.named_parameters") will recursively include child parameters,
    allowing for convenient optimization of all parameters within the network:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码片段后，请注意网络的参数已经发生了变化。特别是，检查`l1`的`weight`参数的值，现在它的值更接近0（这是可以预期的）：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It’s also easy to move all parameters to a different device or change their
    precision using [`to()`](../generated/torch.nn.Module.html#torch.nn.Module.to
    "torch.nn.Module.to"):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用[`to()`](../generated/torch.nn.Module.html#torch.nn.Module.to "torch.nn.Module.to")将所有参数移动到不同的设备或更改它们的精度：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'More generally, an arbitrary function can be applied to a module and its submodules
    recursively by using the [`apply()`](../generated/torch.nn.Module.html#torch.nn.Module.apply
    "torch.nn.Module.apply") function. For example, to apply custom initialization
    to parameters of a module and its submodules:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，可以使用[`apply()`](../generated/torch.nn.Module.html#torch.nn.Module.apply
    "torch.nn.Module.apply")函数将任意函数递归地应用到模块及其子模块上。例如，要对模块及其子模块的参数应用自定义初始化：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: These examples show how elaborate neural networks can be formed through module
    composition and conveniently manipulated. To allow for quick and easy construction
    of neural networks with minimal boilerplate, PyTorch provides a large library
    of performant modules within the [`torch.nn`](../nn.html#module-torch.nn "torch.nn")
    namespace that perform common neural network operations like pooling, convolutions,
    loss functions, etc.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例展示了如何通过模块组合和方便操作来构建复杂的神经网络。为了快速简便地构建神经网络而减少样板代码，PyTorch提供了一个大型的高性能模块库，位于[`torch.nn`](../nn.html#module-torch.nn
    "torch.nn")命名空间中，执行常见的神经网络操作，如池化、卷积、损失函数等。
- en: In the next section, we give a full example of training a neural network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将给出一个完整的训练神经网络的示例。
- en: 'For more information, check out:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请查看：
- en: 'Library of PyTorch-provided modules: [torch.nn](https://pytorch.org/docs/stable/nn.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于任何给定的模块，其参数包括其直接参数以及所有子模块的参数。这意味着调用[`parameters()`](../generated/torch.nn.Module.html#torch.nn.Module.parameters
    "torch.nn.Module.parameters")和[`named_parameters()`](../generated/torch.nn.Module.html#torch.nn.Module.named_parameters
    "torch.nn.Module.named_parameters")将递归包括子参数，方便地优化网络中的所有参数：
- en: 'Defining neural net modules: [https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html](https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义神经网络模块：[https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html](https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html)
- en: '## [Neural Network Training with Modules](#id6)[](#neural-network-training-with-modules
    "Permalink to this heading")'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '## [使用模块进行神经网络训练](#id6)[](#neural-network-training-with-modules "跳转到此标题的永久链接")'
- en: 'Once a network is built, it has to be trained, and its parameters can be easily
    optimized with one of PyTorch’s Optimizers from [`torch.optim`](../optim.html#module-torch.optim
    "torch.optim"):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 构建完网络后，需要对其进行训练，并且可以使用PyTorch的优化器之一从[`torch.optim`](../optim.html#module-torch.optim
    "torch.optim")中轻松优化其参数：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this simplified example, the network learns to simply output zero, as any
    non-zero output is “penalized” according to its absolute value by employing [`torch.abs()`](../generated/torch.abs.html#torch.abs
    "torch.abs") as a loss function. While this is not a very interesting task, the
    key parts of training are present:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简化的示例中，网络学习简单地输出零，因为任何非零输出都会根据其绝对值“受到惩罚”，使用[`torch.abs()`](../generated/torch.abs.html#torch.abs
    "torch.abs")作为损失函数。虽然这不是一个非常有趣的任务，但训练的关键部分都在其中：
- en: A network is created.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个网络。
- en: An optimizer (in this case, a stochastic gradient descent optimizer) is created,
    and the network’s parameters are associated with it.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch提供的模块库：[torch.nn](https://pytorch.org/docs/stable/nn.html)
- en: A training loop…
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个训练循环…
- en: acquires an input,
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取一个输入，
- en: runs the network,
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个优化器（在本例中是随机梯度下降优化器），并将网络的参数与之关联。
- en: computes a loss,
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失，
- en: zeros the network’s parameters’ gradients,
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将网络的参数梯度置零，
- en: calls loss.backward() to update the parameters’ gradients,
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用loss.backward()来更新参数的梯度，
- en: calls optimizer.step() to apply the gradients to the parameters.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用optimizer.step()将梯度应用到参数上。
- en: 'After the above snippet has been run, note that the network’s parameters have
    changed. In particular, examining the value of `l1`‘s `weight` parameter shows
    that its values are now much closer to 0 (as may be expected):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 运行网络，
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Note that the above process is done entirely while the network module is in
    “training mode”. Modules default to training mode and can be switched between
    training and evaluation modes using [`train()`](../generated/torch.nn.Module.html#torch.nn.Module.train
    "torch.nn.Module.train") and [`eval()`](../generated/torch.nn.Module.html#torch.nn.Module.eval
    "torch.nn.Module.eval"). They can behave differently depending on which mode they
    are in. For example, the `BatchNorm` module maintains a running mean and variance
    during training that are not updated when the module is in evaluation mode. In
    general, modules should be in training mode during training and only switched
    to evaluation mode for inference or evaluation. Below is an example of a custom
    module that behaves differently between the two modes:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述过程完全在网络模块处于“训练模式”时完成。模块默认为训练模式，并可以使用[`train()`](../generated/torch.nn.Module.html#torch.nn.Module.train
    "torch.nn.Module.train")和[`eval()`](../generated/torch.nn.Module.html#torch.nn.Module.eval
    "torch.nn.Module.eval")在训练和评估模式之间切换。它们在不同模式下的行为可能不同。例如，`BatchNorm`模块在训练期间维护一个运行时均值和方差，在评估模式下不会更新。通常，在训练期间模块应该处于训练模式，并且只有在推断或评估时才切换到评估模式。下面是一个在两种模式之间表现不同的自定义模块的示例：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Training neural networks can often be tricky. For more information, check out:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络通常会很棘手。更多信息，请查看：
- en: 'Using Optimizers: [https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用优化器：[https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html)。
- en: 'Neural network training: [https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络训练：[https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)
- en: 'Introduction to autograd: [https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动求导简介：[https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
- en: '[Module State](#id7)'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[模块状态](#id7)'
- en: 'In the previous section, we demonstrated training a module’s “parameters”,
    or learnable aspects of computation. Now, if we want to save the trained model
    to disk, we can do so by saving its `state_dict` (i.e. “state dictionary”):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '在前一节中，我们演示了训练模块的“参数”或计算的可学习部分。现在，如果我们想将训练好的模型保存到磁盘，可以通过保存其`state_dict`（即“状态字典”）来实现： '
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A module’s `state_dict` contains state that affects its computation. This includes,
    but is not limited to, the module’s parameters. For some modules, it may be useful
    to have state beyond parameters that affects module computation but is not learnable.
    For such cases, PyTorch provides the concept of “buffers”, both “persistent” and
    “non-persistent”. Following is an overview of the various types of state a module
    can have:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 模块的`state_dict`包含影响其计算的状态。这包括但不限于模块的参数。对于某些模块，除了参数之外还有影响模块计算但不可学习的状态可能很有用。对于这种情况，PyTorch提供了“缓冲区”的概念，包括“持久性”和“非持久性”。以下是模块可能具有的各种类型状态的概述：
- en: '**Parameters**: learnable aspects of computation; contained within the `state_dict`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数**：计算的可学习部分；包含在`state_dict`中'
- en: '**Buffers**: non-learnable aspects of computation'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓冲区**：计算的非可学习部分'
- en: '**Persistent** buffers: contained within the `state_dict` (i.e. serialized
    when saving & loading)'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持久性**缓冲区：包含在`state_dict`中（即在保存和加载时被序列化）'
- en: '**Non-persistent** buffers: not contained within the `state_dict` (i.e. left
    out of serialization)'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非持久性**缓冲区：不包含在`state_dict`中（即在序列化时被排除）'
- en: 'As a motivating example for the use of buffers, consider a simple module that
    maintains a running mean. We want the current value of the running mean to be
    considered part of the module’s `state_dict` so that it will be restored when
    loading a serialized form of the module, but we don’t want it to be learnable.
    This snippet shows how to use [`register_buffer()`](../generated/torch.nn.Module.html#torch.nn.Module.register_buffer
    "torch.nn.Module.register_buffer") to accomplish this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个简单的维护运行均值的模块为例，我们希望运行均值的当前值被视为模块的`state_dict`的一部分，以便在加载模块的序列化形式时恢复，但我们不希望它是可学习的。以下代码片段展示了如何使用[`register_buffer()`](../generated/torch.nn.Module.html#torch.nn.Module.register_buffer
    "torch.nn.Module.register_buffer")来实现这一点：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, the current value of the running mean is considered part of the module’s
    `state_dict` and will be properly restored when loading the module from disk:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行均值的当前值被视为模块的`state_dict`的一部分，并且在从磁盘加载模块时将被正确恢复：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As mentioned previously, buffers can be left out of the module’s `state_dict`
    by marking them as non-persistent:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，通过将其标记为非持久性，可以将缓冲区排除在模块的`state_dict`之外：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Both persistent and non-persistent buffers are affected by model-wide device
    / dtype changes applied with [`to()`](../generated/torch.nn.Module.html#torch.nn.Module.to
    "torch.nn.Module.to"):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 模型范围内使用[`to()`](../generated/torch.nn.Module.html#torch.nn.Module.to "torch.nn.Module.to")应用的设备/数据类型更改会影响持久性和非持久性缓冲区：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Buffers of a module can be iterated over using [`buffers()`](../generated/torch.nn.Module.html#torch.nn.Module.buffers
    "torch.nn.Module.buffers") or [`named_buffers()`](../generated/torch.nn.Module.html#torch.nn.Module.named_buffers
    "torch.nn.Module.named_buffers").
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用[`buffers()`](../generated/torch.nn.Module.html#torch.nn.Module.buffers
    "torch.nn.Module.buffers")或[`named_buffers()`](../generated/torch.nn.Module.html#torch.nn.Module.named_buffers
    "torch.nn.Module.named_buffers")迭代模块的缓冲区。
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following class demonstrates the various ways of registering parameters
    and buffers within a module:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下类演示了在模块内注册参数和缓冲区的各种方法：
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For more information, check out:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请查看：
- en: 'Saving and loading: [https://pytorch.org/tutorials/beginner/saving_loading_models.html](https://pytorch.org/tutorials/beginner/saving_loading_models.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和加载：[https://pytorch.org/tutorials/beginner/saving_loading_models.html](https://pytorch.org/tutorials/beginner/saving_loading_models.html)
- en: 'Serialization semantics: [https://pytorch.org/docs/main/notes/serialization.html](https://pytorch.org/docs/main/notes/serialization.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列化语义：[https://pytorch.org/docs/main/notes/serialization.html](https://pytorch.org/docs/main/notes/serialization.html)
- en: What is a state dict? [https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html)
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是状态字典？[https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html)
- en: '[Module Initialization](#id8)[](#module-initialization "Permalink to this heading")'
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[模块初始化](#id8)[](#module-initialization "跳转到此标题")'
- en: By default, parameters and floating-point buffers for modules provided by [`torch.nn`](../nn.html#module-torch.nn
    "torch.nn") are initialized during module instantiation as 32-bit floating point
    values on the CPU using an initialization scheme determined to perform well historically
    for the module type. For certain use cases, it may be desired to initialize with
    a different dtype, device (e.g. GPU), or initialization technique.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，由[`torch.nn`](../nn.html#module-torch.nn "torch.nn")提供的模块的参数和浮点缓冲区在模块实例化时作为32位浮点值在CPU上初始化，使用的初始化方案是根据模块类型的历史表现确定的。对于某些用例，可能希望使用不同的数据类型、设备（例如GPU）或初始化技术进行初始化。
- en: 'Examples:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Note that the device and dtype options demonstrated above also apply to any
    floating-point buffers registered for the module:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上面演示的设备和数据类型选项也适用于为模块注册的任何浮点缓冲区：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'While module writers can use any device or dtype to initialize parameters in
    their custom modules, good practice is to use `dtype=torch.float` and `device=''cpu''`
    by default as well. Optionally, you can provide full flexibility in these areas
    for your custom module by conforming to the convention demonstrated above that
    all [`torch.nn`](../nn.html#module-torch.nn "torch.nn") modules follow:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模块编写者可以使用任何设备或dtype来初始化其自定义模块中的参数，但通常做法是默认使用`dtype=torch.float`和`device='cpu'`。您还可以通过遵循上述示例中展示的约定为自定义模块提供这些领域的完全灵活性，所有[`torch.nn`](../nn.html#module-torch.nn
    "torch.nn")模块都遵循这一约定：
- en: Provide a `device` constructor kwarg that applies to any parameters / buffers
    registered by the module.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模块注册的任何参数/缓冲提供一个`device`构造函数kwarg。
- en: Provide a `dtype` constructor kwarg that applies to any parameters / floating-point
    buffers registered by the module.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模块注册的任何参数/浮点缓冲提供一个`dtype`构造函数kwarg。
- en: Only use initialization functions (i.e. functions from [`torch.nn.init`](../nn.html#module-torch.nn.init
    "torch.nn.init")) on parameters and buffers within the module’s constructor. Note
    that this is only required to use [`skip_init()`](../generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init
    "torch.nn.utils.skip_init"); see [this page](https://pytorch.org/tutorials/prototype/skip_param_init.html#updating-modules-to-support-skipping-initialization)
    for an explanation.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅在模块的构造函数中对参数和缓冲使用初始化函数（即来自[`torch.nn.init`](../nn.html#module-torch.nn.init
    "torch.nn.init")的函数）。请注意，这仅在使用[`skip_init()`](../generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init
    "torch.nn.utils.skip_init")时才需要；请参阅[此页面](https://pytorch.org/tutorials/prototype/skip_param_init.html#updating-modules-to-support-skipping-initialization)以获取解释。
- en: 'For more information, check out:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请查看：
- en: 'Skipping module parameter initialization: [https://pytorch.org/tutorials/prototype/skip_param_init.html](https://pytorch.org/tutorials/prototype/skip_param_init.html)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳过模块参数初始化：[https://pytorch.org/tutorials/prototype/skip_param_init.html](https://pytorch.org/tutorials/prototype/skip_param_init.html)
- en: '[Module Hooks](#id9)'
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[模块Hooks](#id9)'
- en: In [Neural Network Training with Modules](#neural-network-training-with-modules),
    we demonstrated the training process for a module, which iteratively performs
    forward and backward passes, updating module parameters each iteration. For more
    control over this process, PyTorch provides “hooks” that can perform arbitrary
    computation during a forward or backward pass, even modifying how the pass is
    done if desired. Some useful examples for this functionality include debugging,
    visualizing activations, examining gradients in-depth, etc. Hooks can be added
    to modules you haven’t written yourself, meaning this functionality can be applied
    to third-party or PyTorch-provided modules.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在[使用模块进行神经网络训练](#neural-network-training-with-modules)中，我们演示了模块的训练过程，该过程迭代地执行前向和后向传递，更新模块参数。为了更好地控制这个过程，PyTorch提供了“hooks”，可以在前向或后向传递过程中执行任意计算，甚至在需要时修改传递的方式。这种功能的一些有用示例包括调试、可视化激活、深入检查梯度等。可以将hooks添加到您自己没有编写的模块中，这意味着这种功能可以应用于第三方或PyTorch提供的模块。
- en: 'PyTorch provides two types of hooks for modules:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch为模块提供了两种类型的hooks：
- en: '**Forward hooks** are called during the forward pass. They can be installed
    for a given module with [`register_forward_pre_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook
    "torch.nn.Module.register_forward_pre_hook") and [`register_forward_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook
    "torch.nn.Module.register_forward_hook"). These hooks will be called respectively
    just before the forward function is called and just after it is called. Alternatively,
    these hooks can be installed globally for all modules with the analogous [`register_module_forward_pre_hook()`](../generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook
    "torch.nn.modules.module.register_module_forward_pre_hook") and [`register_module_forward_hook()`](../generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook
    "torch.nn.modules.module.register_module_forward_hook") functions.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前向hooks**在前向传递期间调用。可以使用[`register_forward_pre_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook
    "torch.nn.Module.register_forward_pre_hook")和[`register_forward_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook
    "torch.nn.Module.register_forward_hook")为给定模块安装这些hooks。这些hooks将分别在调用前向函数之前和之后调用。或者，可以使用类似的[`register_module_forward_pre_hook()`](../generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook
    "torch.nn.modules.module.register_module_forward_pre_hook")和[`register_module_forward_hook()`](../generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook
    "torch.nn.modules.module.register_module_forward_hook")函数全局安装这些hooks。'
- en: '**Backward hooks** are called during the backward pass. They can be installed
    with [`register_full_backward_pre_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook
    "torch.nn.Module.register_full_backward_pre_hook") and [`register_full_backward_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook
    "torch.nn.Module.register_full_backward_hook"). These hooks will be called when
    the backward for this Module has been computed. [`register_full_backward_pre_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook
    "torch.nn.Module.register_full_backward_pre_hook") will allow the user to access
    the gradients for outputs while [`register_full_backward_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook
    "torch.nn.Module.register_full_backward_hook") will allow the user to access the
    gradients both the inputs and outputs. Alternatively, they can be installed globally
    for all modules with [`register_module_full_backward_hook()`](../generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook
    "torch.nn.modules.module.register_module_full_backward_hook") and [`register_module_full_backward_pre_hook()`](../generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook
    "torch.nn.modules.module.register_module_full_backward_pre_hook").'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向钩子**在反向传播过程中被调用。可以使用[`register_full_backward_pre_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook
    "torch.nn.Module.register_full_backward_pre_hook")和[`register_full_backward_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook
    "torch.nn.Module.register_full_backward_hook")来安装这些钩子。这些钩子将在该模块的反向传播计算完成时被调用。[`register_full_backward_pre_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook
    "torch.nn.Module.register_full_backward_pre_hook")允许用户访问输出的梯度，而[`register_full_backward_hook()`](../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook
    "torch.nn.Module.register_full_backward_hook")允许用户访问输入和输出的梯度。另外，可以使用[`register_module_full_backward_hook()`](../generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook
    "torch.nn.modules.module.register_module_full_backward_hook")和[`register_module_full_backward_pre_hook()`](../generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook
    "torch.nn.modules.module.register_module_full_backward_pre_hook")来全局安装所有模块的钩子。'
- en: All hooks allow the user to return an updated value that will be used throughout
    the remaining computation. Thus, these hooks can be used to either execute arbitrary
    code along the regular module forward/backward or modify some inputs/outputs without
    having to change the module’s `forward()` function.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的钩子都允许用户返回一个更新后的值，该值将在剩余的计算过程中使用。因此，这些钩子可以用来在常规模块的前向/后向过程中执行任意代码，或者修改一些输入/输出而无需更改模块的`forward()`函数。
- en: 'Below is an example demonstrating usage of forward and backward hooks:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，演示了前向和反向钩子的用法：
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[Advanced Features](#id10)'
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[高级功能](#id10)'
- en: PyTorch also provides several more advanced features that are designed to work
    with modules. All these functionalities are available for custom-written modules,
    with the small caveat that certain features may require modules to conform to
    particular constraints in order to be supported. In-depth discussion of these
    features and the corresponding requirements can be found in the links below.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还提供了几个更高级的功能，旨在与模块一起使用。所有这些功能都适用于自定义编写的模块，只是有一个小小的注意事项，即某些功能可能需要模块符合特定约束才能得到支持。这些功能的深入讨论以及相应的要求可以在下面的链接中找到。
- en: '[Distributed Training](#id11)'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[分布式训练](#id11)'
- en: Various methods for distributed training exist within PyTorch, both for scaling
    up training using multiple GPUs as well as training across multiple machines.
    Check out the [distributed training overview page](https://pytorch.org/tutorials/beginner/dist_overview.html)
    for detailed information on how to utilize these.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中存在各种分布式训练方法，既可以使用多个GPU进行训练，也可以跨多台机器进行训练。查看[分布式训练概述页面](https://pytorch.org/tutorials/beginner/dist_overview.html)以获取有关如何利用这些功能的详细信息。
- en: '[Profiling Performance](#id12)[](#profiling-performance "Permalink to this
    heading")'
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[性能分析](#id12)[](#profiling-performance "跳转到此标题的永久链接")'
- en: The [PyTorch Profiler](https://pytorch.org/tutorials/beginner/profiler.html)
    can be useful for identifying performance bottlenecks within your models. It measures
    and outputs performance characteristics for both memory usage and time spent.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyTorch性能分析器](https://pytorch.org/tutorials/beginner/profiler.html)可用于识别模型中的性能瓶颈。它可以测量和输出内存使用和时间消耗的性能特征。'
- en: '[Improving Performance with Quantization](#id13)[](#improving-performance-with-quantization
    "Permalink to this heading")'
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[通过量化提高性能](#id13)[](#improving-performance-with-quantization "跳转到此标题的永久链接")'
- en: Applying quantization techniques to modules can improve performance and memory
    usage by utilizing lower bitwidths than floating-point precision. Check out the
    various PyTorch-provided mechanisms for quantization [here](https://pytorch.org/docs/stable/quantization.html).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 将量化技术应用于模块可以通过使用比浮点精度更低的位宽来提高性能和内存使用。查看PyTorch提供的各种量化机制[这里](https://pytorch.org/docs/stable/quantization.html)。
- en: '[Improving Memory Usage with Pruning](#id14)[](#improving-memory-usage-with-pruning
    "Permalink to this heading")'
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[通过修剪改善内存使用](#id14)[](#improving-memory-usage-with-pruning "跳转到此标题的永久链接")'
- en: Large deep learning models are often over-parametrized, resulting in high memory
    usage. To combat this, PyTorch provides mechanisms for model pruning, which can
    help reduce memory usage while maintaining task accuracy. The [Pruning tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)
    describes how to utilize the pruning techniques PyTorch provides or define custom
    pruning techniques as necessary.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 大型深度学习模型通常存在过度参数化的问题，导致内存使用量很高。为了解决这个问题，PyTorch提供了模型修剪的机制，可以帮助减少内存使用量同时保持任务准确性。[修剪教程](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)描述了如何利用PyTorch提供的修剪技术或根据需要定义自定义修剪技术。
- en: '[Parametrizations](#id15)'
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[参数化](#id15)'
- en: For certain applications, it can be beneficial to constrain the parameter space
    during model training. For example, enforcing orthogonality of the learned parameters
    can improve convergence for RNNs. PyTorch provides a mechanism for applying [parametrizations](https://pytorch.org/tutorials/intermediate/parametrizations.html)
    such as this, and further allows for custom constraints to be defined.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些应用程序，在模型训练过程中约束参数空间可能是有益的。例如，强制学习参数的正交性可以改善RNN的收敛性。PyTorch提供了一种应用[参数化](https://pytorch.org/tutorials/intermediate/parametrizations.html)的机制，还允许定义自定义约束。
- en: '[Transforming Modules with FX](#id16)[](#transforming-modules-with-fx "Permalink
    to this heading")'
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[使用FX转换模块](#id16)[](#transforming-modules-with-fx "跳转到此标题")'
- en: The [FX](https://pytorch.org/docs/stable/fx.html) component of PyTorch provides
    a flexible way to transform modules by operating directly on module computation
    graphs. This can be used to programmatically generate or manipulate modules for
    a broad array of use cases. To explore FX, check out these examples of using FX
    for [convolution + batch norm fusion](https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html)
    and [CPU performance analysis](https://pytorch.org/tutorials/intermediate/fx_profiling_tutorial.html).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的[FX](https://pytorch.org/docs/stable/fx.html)组件提供了一种灵活的方式来通过直接操作模块计算图来转换模块。这可以用于以编程方式生成或操作各种用例的模块。要探索FX，请查看使用FX进行[卷积+批量归一化融合](https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html)和[CPU性能分析](https://pytorch.org/tutorials/intermediate/fx_profiling_tutorial.html)的示例。
