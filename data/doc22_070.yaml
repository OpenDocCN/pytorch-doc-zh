- en: torch.optim
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torch.optim
- en: 原文：[https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)'
- en: '[`torch.optim`](#module-torch.optim "torch.optim") is a package implementing
    various optimization algorithms.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[`torch.optim`](#module-torch.optim "torch.optim")是一个实现各种优化算法的包。'
- en: Most commonly used methods are already supported, and the interface is general
    enough, so that more sophisticated ones can also be easily integrated in the future.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的方法已经得到支持，接口足够通用，以便将来也可以轻松集成更复杂的方法。
- en: How to use an optimizer
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用优化器
- en: To use [`torch.optim`](#module-torch.optim "torch.optim") you have to construct
    an optimizer object that will hold the current state and will update the parameters
    based on the computed gradients.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用[`torch.optim`](#module-torch.optim "torch.optim")，您必须构建一个优化器对象，该对象将保存当前状态，并根据计算出的梯度更新参数。
- en: Constructing it
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建它
- en: To construct an [`Optimizer`](#torch.optim.Optimizer "torch.optim.Optimizer")
    you have to give it an iterable containing the parameters (all should be `Variable`
    s) to optimize. Then, you can specify optimizer-specific options such as the learning
    rate, weight decay, etc.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个[`Optimizer`](#torch.optim.Optimizer "torch.optim.Optimizer")，您必须给它一个包含要优化的参数（都应该是`Variable`）的可迭代对象。然后，您可以指定特定于优化器的选项，如学习率、权重衰减等。
- en: 'Example:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Per-parameter options
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个参数的选项
- en: '[`Optimizer`](#torch.optim.Optimizer "torch.optim.Optimizer") s also support
    specifying per-parameter options. To do this, instead of passing an iterable of
    `Variable` s, pass in an iterable of [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)") s. Each of them will define a separate parameter group, and
    should contain a `params` key, containing a list of parameters belonging to it.
    Other keys should match the keyword arguments accepted by the optimizers, and
    will be used as optimization options for this group.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[`Optimizer`](#torch.optim.Optimizer "torch.optim.Optimizer")还支持指定每个参数的选项。为此，不要传递`Variable`的可迭代对象，而是传递一个[`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)")的可迭代对象。每个字典将定义一个单独的参数组，并应包含一个`params`键，其中包含属于该组的参数列表。其他键应与优化器接受的关键字参数匹配，并将用作该组的优化选项。'
- en: Note
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can still pass options as keyword arguments. They will be used as defaults,
    in the groups that didn’t override them. This is useful when you only want to
    vary a single option, while keeping all others consistent between parameter groups.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您仍然可以将选项作为关键字参数传递。它们将被用作默认值，在未覆盖它们的组中。当您只想要改变单个选项，同时保持所有其他参数组之间一致时，这是很有用的。
- en: 'For example, this is very useful when one wants to specify per-layer learning
    rates:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当想要指定每层的学习率时，这是非常有用的：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This means that `model.base`’s parameters will use the default learning rate
    of `1e-2`, `model.classifier`’s parameters will use a learning rate of `1e-3`,
    and a momentum of `0.9` will be used for all parameters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着`model.base`的参数将使用学习率的默认值`1e-2`，`model.classifier`的参数将使用学习率`1e-3`，并且所有参数将使用动量`0.9`。
- en: Taking an optimization step[](#taking-an-optimization-step "Permalink to this
    heading")
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行优化步骤[](#taking-an-optimization-step "Permalink to this heading")
- en: 'All optimizers implement a [`step()`](generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step
    "torch.optim.Optimizer.step") method, that updates the parameters. It can be used
    in two ways:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有优化器都实现了一个[`step()`](generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step
    "torch.optim.Optimizer.step")方法，用于更新参数。它可以以两种方式使用：
- en: '`optimizer.step()`'
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`optimizer.step()`'
- en: This is a simplified version supported by most optimizers. The function can
    be called once the gradients are computed using e.g. `backward()`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是大多数优化器支持的简化版本。一旦使用`backward()`计算出梯度，就可以调用该函数。
- en: 'Example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`optimizer.step(closure)`'
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`optimizer.step(closure)`'
- en: Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate
    the function multiple times, so you have to pass in a closure that allows them
    to recompute your model. The closure should clear the gradients, compute the loss,
    and return it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一些优化算法，如共轭梯度和LBFGS，需要多次重新评估函数，因此您必须传递一个闭包，允许它们重新计算您的模型。闭包应清除梯度，计算损失并返回它。
- en: 'Example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '## Base class'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '## 基类'
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Base class for all optimizers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有优化器的基类。
- en: Warning
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Parameters need to be specified as collections that have a deterministic ordering
    that is consistent between runs. Examples of objects that don’t satisfy those
    properties are sets and iterators over values of dictionaries.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 需要将参数指定为具有确定性顺序的集合，该顺序在运行之间保持一致。不满足这些属性的对象的示例包括集合和字典值的迭代器。
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '**params** (*iterable*) – an iterable of [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor") s or [`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)") s. Specifies what Tensors should be optimized.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params**（*可迭代对象*）- 一个包含[`torch.Tensor`](tensors.html#torch.Tensor "torch.Tensor")或[`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(in Python v3.12)")的可迭代对象。指定应该优化哪些张量。'
- en: '**defaults** ([*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict
    "(in Python v3.12)")*[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* [*Any*](https://docs.python.org/3/library/typing.html#typing.Any
    "(in Python v3.12)")*]*) – (dict): a dict containing default values of optimization
    options (used when a parameter group doesn’t specify them).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**defaults**（[*Dict*](https://docs.python.org/3/library/typing.html#typing.Dict
    "(in Python v3.12)")*[*[*str*](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)")*,* [*Any*](https://docs.python.org/3/library/typing.html#typing.Any
    "(in Python v3.12)")*]*) - （字典）：包含优化选项的默认值的字典（当参数组未指定它们时使用）。'
- en: '| [`Optimizer.add_param_group`](generated/torch.optim.Optimizer.add_param_group.html#torch.optim.Optimizer.add_param_group
    "torch.optim.Optimizer.add_param_group") | Add a param group to the [`Optimizer`](#torch.optim.Optimizer
    "torch.optim.Optimizer") s param_groups. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| [`Optimizer.add_param_group`](generated/torch.optim.Optimizer.add_param_group.html#torch.optim.Optimizer.add_param_group
    "torch.optim.Optimizer.add_param_group") | 向[`Optimizer`](#torch.optim.Optimizer
    "torch.optim.Optimizer")的param_groups中添加一个参数组。 |'
- en: '| [`Optimizer.load_state_dict`](generated/torch.optim.Optimizer.load_state_dict.html#torch.optim.Optimizer.load_state_dict
    "torch.optim.Optimizer.load_state_dict") | Loads the optimizer state. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| [`Optimizer.load_state_dict`](generated/torch.optim.Optimizer.load_state_dict.html#torch.optim.Optimizer.load_state_dict
    "torch.optim.Optimizer.load_state_dict") | 加载优化器状态。 |'
- en: '| [`Optimizer.state_dict`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict") | Returns the state of the optimizer as a
    [`dict`](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)").
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| [`Optimizer.state_dict`](generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict
    "torch.optim.Optimizer.state_dict") | 返回优化器状态的[`dict`](https://docs.python.org/3/library/stdtypes.html#dict
    "(在Python v3.12中)")。 |'
- en: '| [`Optimizer.step`](generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step
    "torch.optim.Optimizer.step") | Performs a single optimization step (parameter
    update). |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| [`Optimizer.step`](generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step
    "torch.optim.Optimizer.step") | 执行单个优化步骤（参数更新）。 |'
- en: '| [`Optimizer.zero_grad`](generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad
    "torch.optim.Optimizer.zero_grad") | Resets the gradients of all optimized [`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor") s. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| [`Optimizer.zero_grad`](generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad
    "torch.optim.Optimizer.zero_grad") | 重置所有优化过的[`torch.Tensor`](tensors.html#torch.Tensor
    "torch.Tensor")的梯度。 |'
- en: Algorithms
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法
- en: '| [`Adadelta`](generated/torch.optim.Adadelta.html#torch.optim.Adadelta "torch.optim.Adadelta")
    | Implements Adadelta algorithm. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| [`Adadelta`](generated/torch.optim.Adadelta.html#torch.optim.Adadelta "torch.optim.Adadelta")
    | 实现了Adadelta算法。 |'
- en: '| [`Adagrad`](generated/torch.optim.Adagrad.html#torch.optim.Adagrad "torch.optim.Adagrad")
    | Implements Adagrad algorithm. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| [`Adagrad`](generated/torch.optim.Adagrad.html#torch.optim.Adagrad "torch.optim.Adagrad")
    | 实现了Adagrad算法。 |'
- en: '| [`Adam`](generated/torch.optim.Adam.html#torch.optim.Adam "torch.optim.Adam")
    | Implements Adam algorithm. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| [`Adam`](generated/torch.optim.Adam.html#torch.optim.Adam "torch.optim.Adam")
    | 实现了Adam算法。 |'
- en: '| [`AdamW`](generated/torch.optim.AdamW.html#torch.optim.AdamW "torch.optim.AdamW")
    | Implements AdamW algorithm. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| [`AdamW`](generated/torch.optim.AdamW.html#torch.optim.AdamW "torch.optim.AdamW")
    | 实现了AdamW算法。 |'
- en: '| [`SparseAdam`](generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam
    "torch.optim.SparseAdam") | SparseAdam implements a masked version of the Adam
    algorithm suitable for sparse gradients. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| [`SparseAdam`](generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam
    "torch.optim.SparseAdam") | SparseAdam 实现了适用于稀疏梯度的掩码版本的Adam算法。 |'
- en: '| [`Adamax`](generated/torch.optim.Adamax.html#torch.optim.Adamax "torch.optim.Adamax")
    | Implements Adamax algorithm (a variant of Adam based on infinity norm). |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| [`Adamax`](generated/torch.optim.Adamax.html#torch.optim.Adamax "torch.optim.Adamax")
    | 实现了Adamax算法（基于无穷范数的Adam变体）。 |'
- en: '| [`ASGD`](generated/torch.optim.ASGD.html#torch.optim.ASGD "torch.optim.ASGD")
    | Implements Averaged Stochastic Gradient Descent. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| [`ASGD`](generated/torch.optim.ASGD.html#torch.optim.ASGD "torch.optim.ASGD")
    | 实现了平均随机梯度下降算法。 |'
- en: '| [`LBFGS`](generated/torch.optim.LBFGS.html#torch.optim.LBFGS "torch.optim.LBFGS")
    | Implements L-BFGS algorithm. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| [`LBFGS`](generated/torch.optim.LBFGS.html#torch.optim.LBFGS "torch.optim.LBFGS")
    | 实现了L-BFGS算法。 |'
- en: '| [`NAdam`](generated/torch.optim.NAdam.html#torch.optim.NAdam "torch.optim.NAdam")
    | Implements NAdam algorithm. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| [`NAdam`](generated/torch.optim.NAdam.html#torch.optim.NAdam "torch.optim.NAdam")
    | 实现了NAdam算法。 |'
- en: '| [`RAdam`](generated/torch.optim.RAdam.html#torch.optim.RAdam "torch.optim.RAdam")
    | Implements RAdam algorithm. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| [`RAdam`](generated/torch.optim.RAdam.html#torch.optim.RAdam "torch.optim.RAdam")
    | 实现了RAdam算法。 |'
- en: '| [`RMSprop`](generated/torch.optim.RMSprop.html#torch.optim.RMSprop "torch.optim.RMSprop")
    | Implements RMSprop algorithm. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| [`RMSprop`](generated/torch.optim.RMSprop.html#torch.optim.RMSprop "torch.optim.RMSprop")
    | 实现了RMSprop算法。 |'
- en: '| [`Rprop`](generated/torch.optim.Rprop.html#torch.optim.Rprop "torch.optim.Rprop")
    | Implements the resilient backpropagation algorithm. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| [`Rprop`](generated/torch.optim.Rprop.html#torch.optim.Rprop "torch.optim.Rprop")
    | 实现了弹性反向传播算法。 |'
- en: '| [`SGD`](generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD")
    | Implements stochastic gradient descent (optionally with momentum). |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| [`SGD`](generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD")
    | 实现了随机梯度下降（可选带有动量）。 |'
- en: Many of our algorithms have various implementations optimized for performance,
    readability and/or generality, so we attempt to default to the generally fastest
    implementation for the current device if no particular implementation has been
    specified by the user.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的许多算法都有各种实现，针对性能、可读性和/或通用性进行了优化，因此如果用户没有指定特定的实现，我们会尝试默认选择当前设备上通常最快的实现。
- en: 'We have 3 major categories of implementations: for-loop, foreach (multi-tensor),
    and fused. The most straightforward implementations are for-loops over the parameters
    with big chunks of computation. For-looping is usually slower than our foreach
    implementations, which combine parameters into a multi-tensor and run the big
    chunks of computation all at once, thereby saving many sequential kernel calls.
    A few of our optimizers have even faster fused implementations, which fuse the
    big chunks of computation into one kernel. We can think of foreach implementations
    as fusing horizontally and fused implementations as fusing vertically on top of
    that.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有3个主要类别的实现：for循环、foreach（多张量）和融合。最直接的实现是对参数进行for循环，并进行大量计算。for循环通常比我们的foreach实现慢，后者将参数组合成多个张量，并一次性运行大量计算，从而节省许多顺序内核调用。我们的一些优化器甚至有更快的融合实现，将大量计算融合成一个内核。我们可以将foreach实现看作是在水平方向融合，将融合实现看作是在此基础上在垂直方向融合。
- en: In general, the performance ordering of the 3 implementations is fused > foreach
    > for-loop. So when applicable, we default to foreach over for-loop. Applicable
    means the foreach implementation is available, the user has not specified any
    implementation-specific kwargs (e.g., fused, foreach, differentiable), and all
    tensors are native and on CUDA. Note that while fused should be even faster than
    foreach, the implementations are newer and we would like to give them more bake-in
    time before flipping the switch everywhere. You are welcome to try them out though!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这3种实现的性能排序是融合 > foreach > for-loop。因此，在适用的情况下，我们默认选择foreach而不是for-loop。适用的意思是foreach实现可用，用户没有指定任何特定于实现的kwargs（例如，融合，foreach，可微分），并且所有张量都是本地的且在CUDA上。请注意，尽管融合应该比foreach更快，但这些实现是较新的，我们希望在完全切换之前给它们更多的时间来烘烤。您可以尝试它们！
- en: 'Below is a table showing the available and default implementations of each
    algorithm:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个表格，显示了每种算法的可用和默认实现：
- en: '| Algorithm | Default | Has foreach? | Has fused? |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 默认 | 有foreach？ | 有融合？ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| [`Adadelta`](generated/torch.optim.Adadelta.html#torch.optim.Adadelta "torch.optim.Adadelta")
    | foreach | yes | no |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| [`Adadelta`](generated/torch.optim.Adadelta.html#torch.optim.Adadelta "torch.optim.Adadelta")
    | foreach | yes | no |'
- en: '| [`Adagrad`](generated/torch.optim.Adagrad.html#torch.optim.Adagrad "torch.optim.Adagrad")
    | foreach | yes | no |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| [`Adagrad`](generated/torch.optim.Adagrad.html#torch.optim.Adagrad "torch.optim.Adagrad")
    | foreach | yes | no |'
- en: '| [`Adam`](generated/torch.optim.Adam.html#torch.optim.Adam "torch.optim.Adam")
    | foreach | yes | yes |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| [`Adam`](generated/torch.optim.Adam.html#torch.optim.Adam "torch.optim.Adam")
    | foreach | yes | yes |'
- en: '| [`AdamW`](generated/torch.optim.AdamW.html#torch.optim.AdamW "torch.optim.AdamW")
    | foreach | yes | yes |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| [`AdamW`](generated/torch.optim.AdamW.html#torch.optim.AdamW "torch.optim.AdamW")
    | foreach | yes | yes |'
- en: '| [`SparseAdam`](generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam
    "torch.optim.SparseAdam") | for-loop | no | no |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| [`SparseAdam`](generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam
    "torch.optim.SparseAdam") | for-loop | no | no |'
- en: '| [`Adamax`](generated/torch.optim.Adamax.html#torch.optim.Adamax "torch.optim.Adamax")
    | foreach | yes | no |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [`Adamax`](generated/torch.optim.Adamax.html#torch.optim.Adamax "torch.optim.Adamax")
    | foreach | yes | no |'
- en: '| [`ASGD`](generated/torch.optim.ASGD.html#torch.optim.ASGD "torch.optim.ASGD")
    | foreach | yes | no |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [`ASGD`](generated/torch.optim.ASGD.html#torch.optim.ASGD "torch.optim.ASGD")
    | foreach | yes | no |'
- en: '| [`LBFGS`](generated/torch.optim.LBFGS.html#torch.optim.LBFGS "torch.optim.LBFGS")
    | for-loop | no | no |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| [`LBFGS`](generated/torch.optim.LBFGS.html#torch.optim.LBFGS "torch.optim.LBFGS")
    | for-loop | no | no |'
- en: '| [`NAdam`](generated/torch.optim.NAdam.html#torch.optim.NAdam "torch.optim.NAdam")
    | foreach | yes | no |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| [`NAdam`](generated/torch.optim.NAdam.html#torch.optim.NAdam "torch.optim.NAdam")
    | foreach | yes | no |'
- en: '| [`RAdam`](generated/torch.optim.RAdam.html#torch.optim.RAdam "torch.optim.RAdam")
    | foreach | yes | no |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| [`RAdam`](generated/torch.optim.RAdam.html#torch.optim.RAdam "torch.optim.RAdam")
    | foreach | yes | no |'
- en: '| [`RMSprop`](generated/torch.optim.RMSprop.html#torch.optim.RMSprop "torch.optim.RMSprop")
    | foreach | yes | no |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| [`RMSprop`](generated/torch.optim.RMSprop.html#torch.optim.RMSprop "torch.optim.RMSprop")
    | foreach | yes | no |'
- en: '| [`Rprop`](generated/torch.optim.Rprop.html#torch.optim.Rprop "torch.optim.Rprop")
    | foreach | yes | no |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| [`Rprop`](generated/torch.optim.Rprop.html#torch.optim.Rprop "torch.optim.Rprop")
    | foreach | yes | no |'
- en: '| [`SGD`](generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD")
    | foreach | yes | no |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| [`SGD`](generated/torch.optim.SGD.html#torch.optim.SGD "torch.optim.SGD")
    | foreach | yes | no |'
- en: How to adjust learning rate[](#how-to-adjust-learning-rate "Permalink to this
    heading")
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何调整学习率[](#how-to-adjust-learning-rate "Permalink to this heading")
- en: '[`torch.optim.lr_scheduler`](#module-torch.optim.lr_scheduler "torch.optim.lr_scheduler")
    provides several methods to adjust the learning rate based on the number of epochs.
    [`torch.optim.lr_scheduler.ReduceLROnPlateau`](generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau
    "torch.optim.lr_scheduler.ReduceLROnPlateau") allows dynamic learning rate reducing
    based on some validation measurements.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[`torch.optim.lr_scheduler`](#module-torch.optim.lr_scheduler "torch.optim.lr_scheduler")
    提供了几种根据周期数调整学习率的方法。[`torch.optim.lr_scheduler.ReduceLROnPlateau`](generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau
    "torch.optim.lr_scheduler.ReduceLROnPlateau") 允许根据一些验证测量动态减少学习率。'
- en: 'Learning rate scheduling should be applied after optimizer’s update; e.g.,
    you should write your code this way:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调度应该在优化器更新之后应用；例如，您应该按照以下方式编写代码：
- en: 'Example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Most learning rate schedulers can be called back-to-back (also referred to as
    chaining schedulers). The result is that each scheduler is applied one after the
    other on the learning rate obtained by the one preceding it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数学习率调度器可以被连续调用（也称为链接调度器）。结果是，每个调度器都会在前一个调度器获得的学习率上依次应用。
- en: 'Example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In many places in the documentation, we will use the following template to refer
    to schedulers algorithms.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档的许多地方，我们将使用以下模板来引用调度器算法。
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Warning
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called
    before the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way.
    If you use the learning rate scheduler (calling `scheduler.step()`) before the
    optimizer’s update (calling `optimizer.step()`), this will skip the first value
    of the learning rate schedule. If you are unable to reproduce results after upgrading
    to PyTorch 1.1.0, please check if you are calling `scheduler.step()` at the wrong
    time.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch 1.1.0之前，预期应在优化器更新之前调用学习率调度器；1.1.0以一种破坏向后兼容的方式改变了这种行为。如果您在优化器更新之前（调用`scheduler.step()`）使用学习率调度器（调用`optimizer.step()`），这将跳过学习率调度表的第一个值。如果您在升级到PyTorch
    1.1.0后无法复现结果，请检查是否在错误的时间调用了`scheduler.step()`。
- en: '| [`lr_scheduler.LambdaLR`](generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR
    "torch.optim.lr_scheduler.LambdaLR") | Sets the learning rate of each parameter
    group to the initial lr times a given function. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.LambdaLR`](generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR
    "torch.optim.lr_scheduler.LambdaLR") | 将每个参数组的学习率设置为初始学习率乘以给定的函数。 |'
- en: '| [`lr_scheduler.MultiplicativeLR`](generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR
    "torch.optim.lr_scheduler.MultiplicativeLR") | Multiply the learning rate of each
    parameter group by the factor given in the specified function. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.MultiplicativeLR`](generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR
    "torch.optim.lr_scheduler.MultiplicativeLR") | 将每个参数组的学习率乘以指定函数中给定的因子。 |'
- en: '| [`lr_scheduler.StepLR`](generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR
    "torch.optim.lr_scheduler.StepLR") | Decays the learning rate of each parameter
    group by gamma every step_size epochs. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.StepLR`](generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR
    "torch.optim.lr_scheduler.StepLR") | 每隔step_size个epoch通过gamma减少每个参数组的学习率。 |'
- en: '| [`lr_scheduler.MultiStepLR`](generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR
    "torch.optim.lr_scheduler.MultiStepLR") | Decays the learning rate of each parameter
    group by gamma once the number of epoch reaches one of the milestones. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.MultiStepLR`](generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR
    "torch.optim.lr_scheduler.MultiStepLR") | 当epoch数达到里程碑之一时，通过gamma减少每个参数组的学习率。
    |'
- en: '| [`lr_scheduler.ConstantLR`](generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR
    "torch.optim.lr_scheduler.ConstantLR") | Decays the learning rate of each parameter
    group by a small constant factor until the number of epoch reaches a pre-defined
    milestone: total_iters. |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.ConstantLR`](generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR
    "torch.optim.lr_scheduler.ConstantLR") | 通过一个小的常数因子逐渐减少每个参数组的学习率，直到epoch数达到预定义的里程碑：total_iters。
    |'
- en: '| [`lr_scheduler.LinearLR`](generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR
    "torch.optim.lr_scheduler.LinearLR") | Decays the learning rate of each parameter
    group by linearly changing small multiplicative factor until the number of epoch
    reaches a pre-defined milestone: total_iters. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.LinearLR`](generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR
    "torch.optim.lr_scheduler.LinearLR") | 通过线性改变小的乘法因子逐渐减少每个参数组的学习率，直到epoch数达到预定义的里程碑：total_iters。
    |'
- en: '| [`lr_scheduler.ExponentialLR`](generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR
    "torch.optim.lr_scheduler.ExponentialLR") | Decays the learning rate of each parameter
    group by gamma every epoch. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.ExponentialLR`](generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR
    "torch.optim.lr_scheduler.ExponentialLR") | 每个epoch通过gamma减少每个参数组的学习率。 |'
- en: '| [`lr_scheduler.PolynomialLR`](generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR
    "torch.optim.lr_scheduler.PolynomialLR") | Decays the learning rate of each parameter
    group using a polynomial function in the given total_iters. |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.PolynomialLR`](generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR
    "torch.optim.lr_scheduler.PolynomialLR") | 使用给定的total_iters中的多项式函数逐渐减少每个参数组的学习率。
    |'
- en: '| [`lr_scheduler.CosineAnnealingLR`](generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR
    "torch.optim.lr_scheduler.CosineAnnealingLR") | Set the learning rate of each
    parameter group using a cosine annealing schedule, where $\eta_{max}$ηmax​ is
    set to the initial lr and $T_{cur}$Tcur​ is the number of epochs since the last
    restart in SGDR: |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.CosineAnnealingLR`](generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR
    "torch.optim.lr_scheduler.CosineAnnealingLR") | 使用余弦退火调度设置每个参数组的学习率，其中$ \eta_{max}
    $设置为初始lr，$ T_{cur} $是自上次SGDR重新启动以来的epoch数： |'
- en: '| [`lr_scheduler.ChainedScheduler`](generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler
    "torch.optim.lr_scheduler.ChainedScheduler") | Chains list of learning rate schedulers.
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.ChainedScheduler`](generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler
    "torch.optim.lr_scheduler.ChainedScheduler") | 链接学习率调度器列表。 |'
- en: '| [`lr_scheduler.SequentialLR`](generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR
    "torch.optim.lr_scheduler.SequentialLR") | Receives the list of schedulers that
    is expected to be called sequentially during optimization process and milestone
    points that provides exact intervals to reflect which scheduler is supposed to
    be called at a given epoch. |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.SequentialLR`](generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR
    "torch.optim.lr_scheduler.SequentialLR") | 接收预期在优化过程中按顺序调用的调度器列表和提供确切间隔的里程碑点，以反映在给定epoch时应调用哪个调度器。
    |'
- en: '| [`lr_scheduler.ReduceLROnPlateau`](generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau
    "torch.optim.lr_scheduler.ReduceLROnPlateau") | Reduce learning rate when a metric
    has stopped improving. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.ReduceLROnPlateau`](generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau
    "torch.optim.lr_scheduler.ReduceLROnPlateau") | 当指标停止改善时减少学习率。 |'
- en: '| [`lr_scheduler.CyclicLR`](generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR
    "torch.optim.lr_scheduler.CyclicLR") | Sets the learning rate of each parameter
    group according to cyclical learning rate policy (CLR). |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.CyclicLR`](generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR
    "torch.optim.lr_scheduler.CyclicLR") | 根据循环学习率策略（CLR）设置每个参数组的学习率。 |'
- en: '| [`lr_scheduler.OneCycleLR`](generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR
    "torch.optim.lr_scheduler.OneCycleLR") | Sets the learning rate of each parameter
    group according to the 1cycle learning rate policy. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.OneCycleLR`](generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR
    "torch.optim.lr_scheduler.OneCycleLR") | 根据1cycle学习率策略设置每个参数组的学习率。 |'
- en: '| [`lr_scheduler.CosineAnnealingWarmRestarts`](generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts") | Set the learning rate
    of each parameter group using a cosine annealing schedule, where $\eta_{max}$ηmax​
    is set to the initial lr, $T_{cur}$Tcur​ is the number of epochs since the last
    restart and $T_{i}$Ti​ is the number of epochs between two warm restarts in SGDR:
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| [`lr_scheduler.CosineAnnealingWarmRestarts`](generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts") | 使用余弦退火调度设置每个参数组的学习率，其中$\eta_{max}$设置为初始lr，$T_{cur}$是自上次重启以来的时代数，$T_{i}$是SGDR中两次热重启之间的时代数：
    |'
- en: Weight Averaging (SWA and EMA)[](#weight-averaging-swa-and-ema "Permalink to
    this heading")
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重平均（SWA和EMA）[](#weight-averaging-swa-and-ema "跳转到此标题的永久链接")
- en: '[`torch.optim.swa_utils`](#module-torch.optim.swa_utils "torch.optim.swa_utils")
    implements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA).
    In particular, the `torch.optim.swa_utils.AveragedModel` class implements SWA
    and EMA models, `torch.optim.swa_utils.SWALR` implements the SWA learning rate
    scheduler and `torch.optim.swa_utils.update_bn()` is a utility function used to
    update SWA/EMA batch normalization statistics at the end of training.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[`torch.optim.swa_utils`](#module-torch.optim.swa_utils "torch.optim.swa_utils")实现了随机权重平均（SWA）和指数移动平均（EMA）。特别是，`torch.optim.swa_utils.AveragedModel`类实现了SWA和EMA模型，`torch.optim.swa_utils.SWALR`实现了SWA学习率调度程序，`torch.optim.swa_utils.update_bn()`是一个实用函数，用于在训练结束时更新SWA/EMA批归一化统计数据。'
- en: SWA has been proposed in [Averaging Weights Leads to Wider Optima and Better
    Generalization](https://arxiv.org/abs/1803.05407).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SWA已经在[Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407)中提出。
- en: EMA is a widely known technique to reduce the training time by reducing the
    number of weight updates needed. It is a variation of [Polyak averaging](https://paperswithcode.com/method/polyak-averaging),
    but using exponential weights instead of equal weights across iterations.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: EMA是一种广泛知晓的技术，通过减少所需的权重更新次数来减少训练时间。它是[Polyak平均](https://paperswithcode.com/method/polyak-averaging)的一种变体，但是使用指数权重而不是在迭代中使用相等权重。
- en: Constructing averaged models[](#constructing-averaged-models "Permalink to this
    heading")
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建平均模型[](#constructing-averaged-models "跳转到此标题的永久链接")
- en: The AveragedModel class serves to compute the weights of the SWA or EMA model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: AveragedModel类用于计算SWA或EMA模型的权重。
- en: 'You can create an SWA averaged model by running:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行以下命令创建一个SWA平均模型：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'EMA models are constructed by specifying the `multi_avg_fn` argument as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定`multi_avg_fn`参数来构建EMA模型，如下所示：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Decay is a parameter between 0 and 1 that controls how fast the averaged parameters
    are decayed. If not provided to `get_ema_multi_avg_fn`, the default is 0.999.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 衰减是一个介于0和1之间的参数，控制平均参数衰减的速度。如果未提供给`get_ema_multi_avg_fn`，则默认值为0.999。
- en: '`get_ema_multi_avg_fn` returns a function that applies the following EMA equation
    to the weights:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_ema_multi_avg_fn`返回一个函数，该函数将以下EMA方程应用于权重：'
- en: $W^\textrm{EMA}_{t+1} = \alpha W^\textrm{EMA}_{t} + (1 - \alpha) W^\textrm{model}_t$
    Wt+1EMA​=αWtEMA​+(1−α)Wtmodel​
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $W^\textrm{EMA}_{t+1} = \alpha W^\textrm{EMA}_{t} + (1 - \alpha) W^\textrm{model}_t$
- en: where alpha is the EMA decay.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中alpha是EMA衰减。
- en: 'Here the model `model` can be an arbitrary [`torch.nn.Module`](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module") object. `averaged_model` will keep track of the running averages
    of the parameters of the `model`. To update these averages, you should use the
    `update_parameters()` function after the optimizer.step():'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型`model`可以是任意[`torch.nn.Module`](generated/torch.nn.Module.html#torch.nn.Module
    "torch.nn.Module")对象。`averaged_model`将跟踪`model`的参数的运行平均值。要更新这些平均值，您应该在optimizer.step()之后使用`update_parameters()`函数：
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For SWA and EMA, this call is usually done right after the optimizer `step()`.
    In the case of SWA, this is usually skipped for some numbers of steps at the beginning
    of the training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SWA和EMA，这个调用通常在optimizer `step()`之后立即执行。在SWA的情况下，通常在训练开始时跳过一些步骤。
- en: Custom averaging strategies[](#custom-averaging-strategies "Permalink to this
    heading")
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义平均策略[](#custom-averaging-strategies "跳转到此标题的永久链接")
- en: 'By default, `torch.optim.swa_utils.AveragedModel` computes a running equal
    average of the parameters that you provide, but you can also use custom averaging
    functions with the `avg_fn` or `multi_avg_fn` parameters:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`torch.optim.swa_utils.AveragedModel`计算您提供的参数的运行平均值，但您也可以使用`avg_fn`或`multi_avg_fn`参数使用自定义平均函数：
- en: '`avg_fn` allows defining a function operating on each parameter tuple (averaged
    parameter, model parameter) and should return the new averaged parameter.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`avg_fn`允许定义一个操作在每个参数元组（平均参数，模型参数）上，并应返回新的平均参数。'
- en: '`multi_avg_fn` allows defining more efficient operations acting on a tuple
    of parameter lists, (averaged parameter list, model parameter list), at the same
    time, for example using the `torch._foreach*` functions. This function must update
    the averaged parameters in-place.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multi_avg_fn`允许定义更高效的操作，同时作用于参数列表的元组（平均参数列表，模型参数列表），例如使用`torch._foreach*`函数。此函数必须原地更新平均参数。'
- en: 'In the following example `ema_model` computes an exponential moving average
    using the `avg_fn` parameter:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，`ema_model`使用`avg_fn`参数计算指数移动平均值：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the following example `ema_model` computes an exponential moving average
    using the more efficient `multi_avg_fn` parameter:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，`ema_model`使用更高效的`multi_avg_fn`参数计算指数移动平均值：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: SWA learning rate schedules[](#swa-learning-rate-schedules "Permalink to this
    heading")
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SWA学习率调度[](#swa-learning-rate-schedules "跳转到此标题的永久链接")
- en: 'Typically, in SWA the learning rate is set to a high constant value. `SWALR`
    is a learning rate scheduler that anneals the learning rate to a fixed value,
    and then keeps it constant. For example, the following code creates a scheduler
    that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs
    within each parameter group:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在SWA中，学习率设置为一个较高的恒定值。`SWALR`是一个学习率调度程序，它将学习率退火到一个固定值，然后保持恒定。例如，以下代码创建一个调度程序，它在每个参数组内将学习率从初始值线性退火到0.05，共5个时期：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can also use cosine annealing to a fixed value instead of linear annealing
    by setting `anneal_strategy="cos"`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过设置`anneal_strategy="cos"`来使用余弦退火到固定值，而不是线性退火。
- en: Taking care of batch normalization[](#taking-care-of-batch-normalization "Permalink
    to this heading")
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理批归一化[](#taking-care-of-batch-normalization "Permalink to this heading")
- en: '`update_bn()` is a utility function that allows to compute the batchnorm statistics
    for the SWA model on a given dataloader `loader` at the end of training:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_bn()`是一个实用函数，允许在训练结束时计算给定数据加载器`loader`上SWA模型的批归一化统计信息：'
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`update_bn()` applies the `swa_model` to every element in the dataloader and
    computes the activation statistics for each batch normalization layer in the model.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_bn()`将`swa_model`应用于数据加载器中的每个元素，并计算模型中每个批归一化层的激活统计信息。'
- en: Warning
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: '`update_bn()` assumes that each batch in the dataloader `loader` is either
    a tensors or a list of tensors where the first element is the tensor that the
    network `swa_model` should be applied to. If your dataloader has a different structure,
    you can update the batch normalization statistics of the `swa_model` by doing
    a forward pass with the `swa_model` on each element of the dataset.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_bn()`假设数据加载器`loader`中的每个批次都是张量或张量列表，其中第一个元素是应用于网络`swa_model`的张量。如果您的数据加载器具有不同的结构，您可以通过在数据集的每个元素上使用`swa_model`进行前向传递来更新`swa_model`的批归一化统计信息。'
- en: 'Putting it all together: SWA[](#putting-it-all-together-swa "Permalink to this
    heading")'
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容放在一起：SWA[](#putting-it-all-together-swa "Permalink to this heading")
- en: 'In the example below, `swa_model` is the SWA model that accumulates the averages
    of the weights. We train the model for a total of 300 epochs and we switch to
    the SWA learning rate schedule and start to collect SWA averages of the parameters
    at epoch 160:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，`swa_model`是累积权重平均值的SWA模型。我们总共训练模型300个时期，并切换到SWA学习率计划，并开始在第160个时期收集参数的SWA平均值：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Putting it all together: EMA[](#putting-it-all-together-ema "Permalink to this
    heading")'
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容放在一起：EMA[](#putting-it-all-together-ema "Permalink to this heading")
- en: In the example below, `ema_model` is the EMA model that accumulates the exponentially-decayed
    averages of the weights with a decay rate of 0.999. We train the model for a total
    of 300 epochs and start to collect EMA averages immediately.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，`ema_model`是EMA模型，它累积权重的指数衰减平均值，衰减率为0.999。我们总共训练模型300个时期，并立即开始收集EMA平均值。
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
