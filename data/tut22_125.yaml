- en: Training Transformer models using Pipeline Parallelism
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用管道并行性训练Transformer模型
- en: 原文：[https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html](https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html](https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-intermediate-pipeline-tutorial-py) to download
    the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-intermediate-pipeline-tutorial-py)下载完整示例代码
- en: '**Author**: [Pritam Damania](https://github.com/pritamdamania87)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：[Pritam Damania](https://github.com/pritamdamania87)'
- en: This tutorial demonstrates how to train a large Transformer model across multiple
    GPUs using pipeline parallelism. This tutorial is an extension of the [Sequence-to-Sequence
    Modeling with nn.Transformer and TorchText](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
    tutorial and scales up the same model to demonstrate how pipeline parallelism
    can be used to train Transformer models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程演示了如何使用管道并行性在多个GPU上训练大型Transformer模型。本教程是[使用nn.Transformer和TorchText进行序列到序列建模](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)教程的延伸，并扩展了相同的模型，以演示如何使用管道并行性来训练Transformer模型。
- en: 'Prerequisites:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 先决条件：
- en: '[Pipeline Parallelism](https://pytorch.org/docs/stable/pipeline.html)'
  id: totrans-7
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管道并行性](https://pytorch.org/docs/stable/pipeline.html)'
- en: ''
  id: totrans-8
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Sequence-to-Sequence Modeling with nn.Transformer and TorchText](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)'
  id: totrans-10
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用nn.Transformer和TorchText进行序列到序列建模](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)'
- en: Define the model
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义模型
- en: In this tutorial, we will split a Transformer model across two GPUs and use
    pipeline parallelism to train the model. The model is exactly the same model used
    in the [Sequence-to-Sequence Modeling with nn.Transformer and TorchText](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
    tutorial, but is split into two stages. The largest number of parameters belong
    to the [nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)
    layer. The [nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)
    itself consists of `nlayers` of [nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html).
    As a result, our focus is on `nn.TransformerEncoder` and we split the model such
    that half of the `nn.TransformerEncoderLayer` are on one GPU and the other half
    are on another. To do this, we pull out the `Encoder` and `Decoder` sections into
    separate modules and then build an `nn.Sequential` representing the original Transformer
    module.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将把一个Transformer模型分成两个GPU，并使用管道并行性来训练模型。该模型与[使用nn.Transformer和TorchText进行序列到序列建模](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)教程中使用的模型完全相同，但被分成两个阶段。最大数量的参数属于[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)层。[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)本身由`nlayers`个[nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)组成。因此，我们的重点是`nn.TransformerEncoder`，我们将模型分成一半的`nn.TransformerEncoderLayer`在一个GPU上，另一半在另一个GPU上。为此，我们将`Encoder`和`Decoder`部分提取到单独的模块中，然后构建一个代表原始Transformer模块的`nn.Sequential`。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`PositionalEncoding` module injects some information about the relative or
    absolute position of the tokens in the sequence. The positional encodings have
    the same dimension as the embeddings so that the two can be summed. Here, we use
    `sine` and `cosine` functions of different frequencies.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`PositionalEncoding`模块注入了关于序列中标记的相对或绝对位置的一些信息。位置编码与嵌入具有相同的维度，因此可以将两者相加。在这里，我们使用不同频率的`sine`和`cosine`函数。'
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load and batch data
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和批处理数据
- en: The training process uses Wikitext-2 dataset from `torchtext`. To access torchtext
    datasets, please install torchdata following instructions at [https://github.com/pytorch/data](https://github.com/pytorch/data).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程使用了来自`torchtext`的Wikitext-2数据集。要访问torchtext数据集，请按照[https://github.com/pytorch/data](https://github.com/pytorch/data)上的说明安装torchdata。
- en: 'The vocab object is built based on the train dataset and is used to numericalize
    tokens into tensors. Starting from sequential data, the `batchify()` function
    arranges the dataset into columns, trimming off any tokens remaining after the
    data has been divided into batches of size `batch_size`. For instance, with the
    alphabet as the sequence (total length of 26) and a batch size of 4, we would
    divide the alphabet into 4 sequences of length 6:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: vocab对象是基于训练数据集构建的，并用于将标记数值化为张量。从顺序数据开始，`batchify()`函数将数据集排列成列，将数据分成大小为`batch_size`的批次后，修剪掉任何剩余的标记。例如，以字母表作为序列（总长度为26）和批次大小为4，我们将字母表分成长度为6的4个序列：
- en: \[\begin{bmatrix} \text{A} & \text{B} & \text{C} & \ldots & \text{X} & \text{Y}
    & \text{Z} \end{bmatrix} \Rightarrow \begin{bmatrix} \begin{bmatrix}\text{A} \\
    \text{B} \\ \text{C} \\ \text{D} \\ \text{E} \\ \text{F}\end{bmatrix} & \begin{bmatrix}\text{G}
    \\ \text{H} \\ \text{I} \\ \text{J} \\ \text{K} \\ \text{L}\end{bmatrix} & \begin{bmatrix}\text{M}
    \\ \text{N} \\ \text{O} \\ \text{P} \\ \text{Q} \\ \text{R}\end{bmatrix} & \begin{bmatrix}\text{S}
    \\ \text{T} \\ \text{U} \\ \text{V} \\ \text{W} \\ \text{X}\end{bmatrix} \end{bmatrix}\]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{bmatrix} \text{A} & \text{B} & \text{C} & \ldots & \text{X} & \text{Y}
    & \text{Z} \end{bmatrix} \Rightarrow \begin{bmatrix} \begin{bmatrix}\text{A} \\
    \text{B} \\ \text{C} \\ \text{D} \\ \text{E} \\ \text{F}\end{bmatrix} & \begin{bmatrix}\text{G}
    \\ \text{H} \\ \text{I} \\ \text{J} \\ \text{K} \\ \text{L}\end{bmatrix} & \begin{bmatrix}\text{M}
    \\ \text{N} \\ \text{O} \\ \text{P} \\ \text{Q} \\ \text{R}\end{bmatrix} & \begin{bmatrix}\text{S}
    \\ \text{T} \\ \text{U} \\ \text{V} \\ \text{W} \\ \text{X}\end{bmatrix} \end{bmatrix}\]
- en: These columns are treated as independent by the model, which means that the
    dependence of `G` and `F` can not be learned, but allows more efficient batch
    processing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将这些列视为独立的，这意味着无法学习`G`和`F`之间的依赖关系，但可以实现更高效的批处理。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Functions to generate input and target sequence
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成输入和目标序列的函数
- en: '`get_batch()` function generates the input and target sequence for the transformer
    model. It subdivides the source data into chunks of length `bptt`. For the language
    modeling task, the model needs the following words as `Target`. For example, with
    a `bptt` value of 2, we’d get the following two Variables for `i` = 0:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_batch()`函数为transformer模型生成输入和目标序列。它将源数据细分为长度为`bptt`的块。对于语言建模任务，模型需要以下单词作为`Target`。例如，对于`bptt`值为2，我们会得到`i`
    = 0时的以下两个变量：'
- en: '![../_images/transformer_input_target.png](../Images/20ef8681366b44461cf49d1ab98ab8f2.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/transformer_input_target.png](../Images/20ef8681366b44461cf49d1ab98ab8f2.png)'
- en: It should be noted that the chunks are along dimension 0, consistent with the
    `S` dimension in the Transformer model. The batch dimension `N` is along dimension
    1.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意到，块沿着维度0，与Transformer模型中的`S`维度一致。批量维度`N`沿着维度1。
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Model scale and Pipe initialization
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型规模和Pipe初始化
- en: To demonstrate training large Transformer models using pipeline parallelism,
    we scale up the Transformer layers appropriately. We use an embedding dimension
    of 4096, hidden size of 4096, 16 attention heads and 12 total transformer layers
    (`nn.TransformerEncoderLayer`). This creates a model with **~1.4 billion** parameters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示使用管道并行性训练大型Transformer模型，我们适当地扩展了Transformer层。我们使用了4096的嵌入维度，4096的隐藏大小，16个注意力头和12个总的Transformer层（`nn.TransformerEncoderLayer`）。这创建了一个拥有**~14亿**参数的模型。
- en: We need to initialize the [RPC Framework](https://pytorch.org/docs/stable/rpc.html)
    since Pipe depends on the RPC framework via [RRef](https://pytorch.org/docs/stable/rpc.html#rref)
    which allows for future expansion to cross host pipelining. We need to initialize
    the RPC framework with only a single worker since we’re using a single process
    to drive multiple GPUs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要初始化[RPC框架](https://pytorch.org/docs/stable/rpc.html)，因为Pipe依赖于RPC框架通过[RRef](https://pytorch.org/docs/stable/rpc.html#rref)进行跨主机流水线扩展。我们需要仅使用单个worker初始化RPC框架，因为我们使用单个进程来驱动多个GPU。
- en: The pipeline is then initialized with 8 transformer layers on one GPU and 8
    transformer layers on the other GPU.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在一个GPU上初始化8个transformer层，并在另一个GPU上初始化8个transformer层。
- en: Note
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For efficiency purposes we ensure that the `nn.Sequential` passed to `Pipe`
    only consists of two elements (corresponding to two GPUs), this allows the Pipe
    to work with only two partitions and avoid any cross-partition overheads.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高效率，我们确保传递给`Pipe`的`nn.Sequential`只包含两个元素（对应两个GPU），这允许Pipe仅使用两个分区并避免任何跨分区的开销。
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Run the model
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行模型
- en: '[CrossEntropyLoss](https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)
    is applied to track the loss and [SGD](https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD)
    implements stochastic gradient descent method as the optimizer. The initial learning
    rate is set to 5.0\. [StepLR](https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR)
    is applied to adjust the learn rate through epochs. During the training, we use
    [nn.utils.clip_grad_norm_](https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_)
    function to scale all the gradient together to prevent exploding.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[CrossEntropyLoss](https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)用于跟踪损失，[SGD](https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD)实现随机梯度下降方法作为优化器。初始学习率设置为5.0。[StepLR](https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR)用于通过epoch调整学习率。在训练期间，我们使用[nn.utils.clip_grad_norm_](https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_)函数将所有梯度一起缩放，以防止梯度爆炸。'
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Loop over epochs. Save the model if the validation loss is the best we’ve seen
    so far. Adjust the learning rate after each epoch.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 循环迭代。如果验证损失是迄今为止最好的，则保存模型。每个epoch后调整学习率。
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Evaluate the model with the test dataset
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用测试数据集评估模型
- en: Apply the best model to check the result with the test dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 应用最佳模型来检查与测试数据集的结果。
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Total running time of the script:** ( 8 minutes 5.064 seconds)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（8分钟5.064秒）'
- en: '[`Download Python source code: pipeline_tutorial.py`](../_downloads/b4afbcfb1c1ac5f5cd7da108c2236f09/pipeline_tutorial.py)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：pipeline_tutorial.py`](../_downloads/b4afbcfb1c1ac5f5cd7da108c2236f09/pipeline_tutorial.py)'
- en: '[`Download Jupyter notebook: pipeline_tutorial.ipynb`](../_downloads/4cefa4723023eb5d85ed047dadc7f491/pipeline_tutorial.ipynb)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：pipeline_tutorial.ipynb`](../_downloads/4cefa4723023eb5d85ed047dadc7f491/pipeline_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[由Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)'
