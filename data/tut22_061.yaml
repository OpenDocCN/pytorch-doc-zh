- en: 'Pendulum: Writing your environment and transforms with TorchRL'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pendulum：使用TorchRL编写您的环境和转换
- en: 原文：[https://pytorch.org/tutorials/advanced/pendulum.html](https://pytorch.org/tutorials/advanced/pendulum.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/advanced/pendulum.html](https://pytorch.org/tutorials/advanced/pendulum.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-advanced-pendulum-py) to download the full example
    code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-advanced-pendulum-py)下载完整示例代码
- en: '**Author**: [Vincent Moens](https://github.com/vmoens)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：[Vincent Moens](https://github.com/vmoens)'
- en: Creating an environment (a simulator or an interface to a physical control system)
    is an integrative part of reinforcement learning and control engineering.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 创建环境（模拟器或物理控制系统的接口）是强化学习和控制工程的一个整合部分。
- en: TorchRL provides a set of tools to do this in multiple contexts. This tutorial
    demonstrates how to use PyTorch and TorchRL code a pendulum simulator from the
    ground up. It is freely inspired by the Pendulum-v1 implementation from [OpenAI-Gym/Farama-Gymnasium
    control library](https://github.com/Farama-Foundation/Gymnasium).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: TorchRL提供了一套工具在多种情境下实现这一点。本教程演示了如何使用PyTorch和TorchRL从头开始编写一个摆模拟器。它受到了[OpenAI-Gym/Farama-Gymnasium控制库](https://github.com/Farama-Foundation/Gymnasium)中Pendulum-v1实现的启发。
- en: '![Pendulum](../Images/6be04eb745758aea3a2462c7d6adf03a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![Pendulum](../Images/6be04eb745758aea3a2462c7d6adf03a.png)'
- en: Simple Pendulum
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 简单摆
- en: 'Key learnings:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 关键收获：
- en: 'How to design an environment in TorchRL: - Writing specs (input, observation
    and reward); - Implementing behavior: seeding, reset and step.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在TorchRL中设计环境：- 编写规格（输入、观察和奖励）；- 实现行为：种子、重置和步骤。
- en: Transforming your environment inputs and outputs, and writing your own transforms;
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换您的环境输入和输出，并编写您自己的转换；
- en: How to use [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") to carry arbitrary data structures through the
    `codebase`.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用[`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(在tensordict vmain (0.4.0 )版本)")通过`codebase`传递任意数据结构。
- en: 'In the process, we will touch three crucial components of TorchRL:'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个过程中，我们将涉及TorchRL的三个关键组件：
- en: '[environments](https://pytorch.org/rl/reference/envs.html)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[environments](https://pytorch.org/rl/reference/envs.html)'
- en: '[transforms](https://pytorch.org/rl/reference/envs.html#transforms)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[transforms](https://pytorch.org/rl/reference/envs.html#transforms)'
- en: '[models (policy and value function)](https://pytorch.org/rl/reference/modules.html)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[models（策略和值函数）](https://pytorch.org/rl/reference/modules.html)'
- en: To give a sense of what can be achieved with TorchRL’s environments, we will
    be designing a *stateless* environment. While stateful environments keep track
    of the latest physical state encountered and rely on this to simulate the state-to-state
    transition, stateless environments expect the current state to be provided to
    them at each step, along with the action undertaken. TorchRL supports both types
    of environments, but stateless environments are more generic and hence cover a
    broader range of features of the environment API in TorchRL.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示TorchRL环境可以实现的功能，我们将设计一个*无状态*环境。有状态的环境会跟踪最新遇到的物理状态，并依赖于此来模拟状态之间的转换，而无状态的环境期望在每一步提供当前状态，以及采取的动作。TorchRL支持这两种类型的环境，但无状态环境更通用，因此涵盖了TorchRL环境API中更广泛的功能特性。
- en: 'Modeling stateless environments gives users full control over the input and
    outputs of the simulator: one can reset an experiment at any stage or actively
    modify the dynamics from the outside. However, it assumes that we have some control
    over a task, which may not always be the case: solving a problem where we cannot
    control the current state is more challenging but has a much wider set of applications.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 建模无状态环境使用户完全控制模拟器的输入和输出：可以在任何阶段重置实验或从外部主动修改动态。然而，这假设我们对任务有一定控制，这并不总是情况：解决一个我们无法控制当前状态的问题更具挑战性，但具有更广泛的应用范围。
- en: Another advantage of stateless environments is that they can enable batched
    execution of transition simulations. If the backend and the implementation allow
    it, an algebraic operation can be executed seamlessly on scalars, vectors, or
    tensors. This tutorial gives such examples.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态环境的另一个优点是可以实现批量执行转换模拟。如果后端和实现允许，可以在标量、向量或张量上无缝执行代数操作。本教程提供了这样的示例。
- en: 'This tutorial will be structured as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将按以下结构展开：
- en: 'We will first get acquainted with the environment properties: its shape (`batch_size`),
    its methods (mainly [`step()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id4
    "(in torchrl vmain (0.4.0 ))"), [`reset()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id1
    "(in torchrl vmain (0.4.0 ))") and [`set_seed()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id3
    "(in torchrl vmain (0.4.0 ))")) and finally its specs.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将首先熟悉环境的属性：其形状（`batch_size`），其方法（主要是[`step()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id4
    "(在torchrl vmain (0.4.0 )版本)")、[`reset()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id1
    "(在torchrl vmain (0.4.0 )版本)")和[`set_seed()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id3
    "(在torchrl vmain (0.4.0 )版本)")）以及最后的规格。
- en: After having coded our simulator, we will demonstrate how it can be used during
    training with transforms.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编写完我们的模拟器后，我们将演示如何在训练过程中使用转换。
- en: 'We will explore new avenues that follow from the TorchRL’s API, including:
    the possibility of transforming inputs, the vectorized execution of the simulation
    and the possibility of backpropagation through the simulation graph.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将探索从TorchRL的API中产生的新途径，包括：转换输入的可能性，模拟的向量化执行以及通过模拟图进行反向传播的可能性。
- en: Finally, we will train a simple policy to solve the system we implemented.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将训练一个简单的策略来解决我们实现的系统。
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'There are four things you must take care of when designing a new environment
    class:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 设计新环境类时需要注意的四个方面：
- en: '`EnvBase._reset()`, which codes for the resetting of the simulator at a (potentially
    random) initial state;'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EnvBase._reset()`，这段代码用于在（可能是随机的）初始状态下重置模拟器；'
- en: '`EnvBase._step()` which codes for the state transition dynamic;'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EnvBase._step()`编码了状态转移动态；'
- en: '`EnvBase._set_seed`()` which implements the seeding mechanism;'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EnvBase._set_seed`()`实现种子机制；'
- en: the environment specs.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境规范。
- en: 'Let us first describe the problem at hand: we would like to model a simple
    pendulum over which we can control the torque applied on its fixed point. Our
    goal is to place the pendulum in upward position (angular position at 0 by convention)
    and having it standing still in that position. To design our dynamic system, we
    need to define two equations: the motion equation following an action (the torque
    applied) and the reward equation that will constitute our objective function.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先描述手头的问题：我们想要建模一个简单的摆锤，我们可以控制施加在其固定点上的扭矩。我们的目标是将摆锤放在向上位置（按照惯例，角位置为0）并使其保持在该位置静止。为了设计我们的动态系统，我们需要定义两个方程：遵循动作（施加的扭矩）的运动方程和构成我们目标函数的奖励方程。
- en: 'For the motion equation, we will update the angular velocity following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于运动方程，我们将根据以下方式更新角速度：
- en: \[\dot{\theta}_{t+1} = \dot{\theta}_t + (3 * g / (2 * L) * \sin(\theta_t) +
    3 / (m * L^2) * u) * dt\]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \[\dot{\theta}_{t+1} = \dot{\theta}_t + (3 * g / (2 * L) * \sin(\theta_t) +
    3 / (m * L^2) * u) * dt\]
- en: where \(\dot{\theta}\) is the angular velocity in rad/sec, \(g\) is the gravitational
    force, \(L\) is the pendulum length, \(m\) is its mass, \(\theta\) is its angular
    position and \(u\) is the torque. The angular position is then updated according
    to
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中\(\dot{\theta}\)是角速度（弧度/秒），\(g\)是重力，\(L\)是摆长，\(m\)是质量，\(\theta\)是角位置，\(u\)是扭矩。然后根据以下方式更新角位置
- en: \[\theta_{t+1} = \theta_{t} + \dot{\theta}_{t+1} dt\]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \[\theta_{t+1} = \theta_{t} + \dot{\theta}_{t+1} dt\]
- en: We define our reward as
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将奖励定义为
- en: \[r = -(\theta^2 + 0.1 * \dot{\theta}^2 + 0.001 * u^2)\]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \[r = -(\theta^2 + 0.1 * \dot{\theta}^2 + 0.001 * u^2)\]
- en: which will be maximized when the angle is close to 0 (pendulum in upward position),
    the angular velocity is close to 0 (no motion) and the torque is 0 too.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当角度接近0（摆锤向上位置）、角速度接近0（无运动）且扭矩也为0时，将最大化奖励。
- en: 'Coding the effect of an action: `_step()`'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码一个动作的效果：`_step()`
- en: The step method is the first thing to consider, as it will encode the simulation
    that is of interest to us. In TorchRL, the [`EnvBase`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase
    "(in torchrl vmain (0.4.0 ))") class has a `EnvBase.step()` method that receives
    a [`tensordict.TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") instance with an `"action"` entry indicating
    what action is to be taken.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤方法是首要考虑的事项，因为它将编码我们感兴趣的模拟。在TorchRL中，[`EnvBase`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase)类有一个`EnvBase.step()`方法，接收一个带有`"action"`条目的[`tensordict.TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict)实例，指示要执行的操作。
- en: To facilitate the reading and writing from that `tensordict` and to make sure
    that the keys are consistent with what’s expected from the library, the simulation
    part has been delegated to a private abstract method `_step()` which reads input
    data from a `tensordict`, and writes a *new* `tensordict` with the output data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便从`tensordict`中读取和写入数据，并确保键与库期望的一致，模拟部分已被委托给一个私有的抽象方法`_step()`，该方法从`tensordict`中读取输入数据，并写入一个*新的*`tensordict`，其中包含输出数据。
- en: 'The `_step()` method should do the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`_step()`方法应该执行以下操作：'
- en: Read the input keys (such as `"action"`) and execute the simulation based on
    these;
  id: totrans-43
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取输入键（如`"action"`）并根据这些执行模拟；
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-45
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Retrieve observations, done state and reward;
  id: totrans-46
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索观察值、完成状态和奖励；
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Write the set of observation values along with the reward and done state at
    the corresponding entries in a new `TensorDict`.
  id: totrans-49
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一组观察值以及对应条目中的奖励和完成状态写入新的`TensorDict`。
- en: Next, the [`step()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id4
    "(in torchrl vmain (0.4.0 ))") method will merge the output of [`step()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id4
    "(in torchrl vmain (0.4.0 ))") in the input `tensordict` to enforce input/output
    consistency.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，[`step()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id4)方法将合并[`step()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id4)的输出到输入的`tensordict`中，以强制执行输入/输出的一致性。
- en: 'Typically, for stateful environments, this will look like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有状态的环境，通常会是这样的：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that the root `tensordict` has not changed, the only modification is
    the appearance of a new `"next"` entry that contains the new information.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，根`tensordict`没有改变，唯一的修改是出现了一个包含新信息的新`"next"`条目。
- en: In the Pendulum example, our `_step()` method will read the relevant entries
    from the input `tensordict` and compute the position and velocity of the pendulum
    after the force encoded by the `"action"` key has been applied onto it. We compute
    the new angular position of the pendulum `"new_th"` as the result of the previous
    position `"th"` plus the new velocity `"new_thdot"` over a time interval `dt`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在摆锤示例中，我们的`_step()`方法将从输入的`tensordict`中读取相关条目，并在施加了由`"action"`键编码的力后计算摆锤的位置和速度。我们计算摆锤的新角位置`"new_th"`为前一个位置`"th"`加上新速度`"new_thdot"`乘以时间间隔`dt`的结果。
- en: Since our goal is to turn the pendulum up and maintain it still in that position,
    our `cost` (negative reward) function is lower for positions close to the target
    and low speeds. Indeed, we want to discourage positions that are far from being
    “upward” and/or speeds that are far from 0.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的目标是将摆锤竖起并保持在那个位置静止，我们的`cost`（负奖励）函数对于接近目标和低速度的位置具有较低的值。实际上，我们希望阻止远离“向上”位置和/或速度远离0的位置。
- en: In our example, `EnvBase._step()` is encoded as a static method since our environment
    is stateless. In stateful settings, the `self` argument is needed as the state
    needs to be read from the environment.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，`EnvBase._step()`被编码为静态方法，因为我们的环境是无状态的。在有状态的设置中，需要`self`参数，因为需要从环境中读取状态。
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Resetting the simulator: `_reset()`'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重置模拟器：`_reset()`
- en: The second method we need to care about is the `_reset()` method. Like `_step()`,
    it should write the observation entries and possibly a done state in the `tensordict`
    it outputs (if the done state is omitted, it will be filled as `False` by the
    parent method [`reset()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id1
    "(in torchrl vmain (0.4.0 ))")). In some contexts, it is required that the `_reset`
    method receives a command from the function that called it (for example, in multi-agent
    settings we may want to indicate which agents need to be reset). This is why the
    `_reset()` method also expects a `tensordict` as input, albeit it may perfectly
    be empty or `None`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要关注的第二个方法是 `_reset()` 方法。与 `_step()` 一样，它应该在输出的 `tensordict` 中写入观察条目，并可能包含一个完成状态（如果省略完成状态，则父方法
    [`reset()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id1
    "(在 torchrl vmain (0.4.0 )") 将填充为 `False`）。在某些情况下，要求 `_reset` 方法接收来自调用它的函数的命令（例如，在多代理设置中，我们可能希望指示需要重置哪些代理）。这就是为什么
    `_reset()` 方法也期望一个 `tensordict` 作为输入，尽管它可以完全为空或为 `None`。
- en: The parent `EnvBase.reset()` does some simple checks like the `EnvBase.step()`
    does, such as making sure that a `"done"` state is returned in the output `tensordict`
    and that the shapes match what is expected from the specs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 父类 `EnvBase.reset()` 进行一些简单的检查，就像 `EnvBase.step()` 一样，例如确保在输出 `tensordict` 中返回一个
    `"done"` 状态，并且形状与规格期望的匹配。
- en: For us, the only important thing to consider is whether `EnvBase._reset()` contains
    all the expected observations. Once more, since we are working with a stateless
    environment, we pass the configuration of the pendulum in a nested `tensordict`
    named `"params"`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，唯一需要考虑的是 `EnvBase._reset()` 是否包含所有预期的观察结果。再次强调，由于我们正在处理无状态环境，我们将摆锤的配置传递给名为
    `"params"` 的嵌套 `tensordict`。
- en: In this example, we do not pass a done state as this is not mandatory for `_reset()`
    and our environment is non-terminating, so we always expect it to be `False`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们不传递完成状态，因为这对于 `_reset()` 不是强制性的，而且我们的环境是非终止的，因此我们总是期望它为 `False`。
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Environment metadata: `env.*_spec`'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境元数据：`env.*_spec`
- en: The specs define the input and output domain of the environment. It is important
    that the specs accurately define the tensors that will be received at runtime,
    as they are often used to carry information about environments in multiprocessing
    and distributed settings. They can also be used to instantiate lazily defined
    neural networks and test scripts without actually querying the environment (which
    can be costly with real-world physical systems for instance).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 规格定义了环境的输入和输出域。重要的是，规格准确定义了在运行时将接收到的张量，因为它们经常用于在多进程和分布式设置中携带有关环境的信息。它们还可以用于实例化懒惰定义的神经网络和测试脚本，而无需实际查询环境（例如，对于真实世界的物理系统来说，这可能是昂贵的）。
- en: 'There are four specs that we must code in our environment:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的环境中必须编码的四个规格：
- en: '`EnvBase.observation_spec`: This will be a [`CompositeSpec`](https://pytorch.org/rl/reference/generated/torchrl.data.CompositeSpec.html#torchrl.data.CompositeSpec
    "(in torchrl vmain (0.4.0 ))") instance where each key is an observation (a `CompositeSpec`
    can be viewed as a dictionary of specs).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EnvBase.observation_spec`: 这将是一个 [`CompositeSpec`](https://pytorch.org/rl/reference/generated/torchrl.data.CompositeSpec.html#torchrl.data.CompositeSpec
    "(在 torchrl vmain (0.4.0 )") 实例，其中每个键都是一个观察（`CompositeSpec` 可以被视为规格字典）。'
- en: '`EnvBase.action_spec`: It can be any type of spec, but it is required that
    it corresponds to the `"action"` entry in the input `tensordict`;'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EnvBase.action_spec`: 它可以是任何类型的规格，但要求它对应于输入 `tensordict` 中的 `"action"` 条目;'
- en: '`EnvBase.reward_spec`: provides information about the reward space;'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EnvBase.reward_spec`: 提供有关奖励空间的信息;'
- en: '`EnvBase.done_spec`: provides information about the space of the done flag.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EnvBase.done_spec`: 提供有关完成标志空间的信息。'
- en: 'TorchRL specs are organized in two general containers: `input_spec` which contains
    the specs of the information that the step function reads (divided between `action_spec`
    containing the action and `state_spec` containing all the rest), and `output_spec`
    which encodes the specs that the step outputs (`observation_spec`, `reward_spec`
    and `done_spec`). In general, you should not interact directly with `output_spec`
    and `input_spec` but only with their content: `observation_spec`, `reward_spec`,
    `done_spec`, `action_spec` and `state_spec`. The reason if that the specs are
    organized in a non-trivial way within `output_spec` and `input_spec` and neither
    of these should be directly modified.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TorchRL 规格分为两个通用容器：`input_spec` 包含步骤函数读取的信息的规格（分为包含动作的 `action_spec` 和包含其余所有内容的
    `state_spec`），以及 `output_spec` 编码步骤输出的规格（`observation_spec`、`reward_spec` 和 `done_spec`）。一般来说，您不应直接与
    `output_spec` 和 `input_spec` 交互，而只应与它们的内容交互：`observation_spec`、`reward_spec`、`done_spec`、`action_spec`
    和 `state_spec`。原因是规格在 `output_spec` 和 `input_spec` 中以非平凡的方式组织，这两者都不应直接修改。
- en: In other words, the `observation_spec` and related properties are convenient
    shortcuts to the content of the output and input spec containers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，`observation_spec` 和相关属性是输出和输入规格容器内容的便捷快捷方式。
- en: TorchRL offers multiple [`TensorSpec`](https://pytorch.org/rl/reference/generated/torchrl.data.TensorSpec.html#torchrl.data.TensorSpec
    "(in torchrl vmain (0.4.0 ))") [subclasses](https://pytorch.org/rl/reference/data.html#tensorspec)
    to encode the environment’s input and output characteristics.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: TorchRL 提供多个 [`TensorSpec`](https://pytorch.org/rl/reference/generated/torchrl.data.TensorSpec.html#torchrl.data.TensorSpec
    "(在 torchrl vmain (0.4.0 ))") [子类](https://pytorch.org/rl/reference/data.html#tensorspec)
    来编码环境的输入和输出特征。
- en: Specs shape
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规格形状
- en: The environment specs leading dimensions must match the environment batch-size.
    This is done to enforce that every component of an environment (including its
    transforms) have an accurate representation of the expected input and output shapes.
    This is something that should be accurately coded in stateful settings.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 环境规格的主要维度必须与环境批处理大小匹配。这是为了强制确保环境的每个组件（包括其转换）都具有预期输入和输出形状的准确表示。这是在有状态设置中应准确编码的内容。
- en: For non batch-locked environments, such as the one in our example (see below),
    this is irrelevant as the environment batch size will most likely be empty.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非`batch-locked`环境，例如我们示例中的环境（见下文），这是无关紧要的，因为环境批处理大小很可能为空。
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Reproducible experiments: seeding'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可重现的实验：种子
- en: Seeding an environment is a common operation when initializing an experiment.
    The only goal of `EnvBase._set_seed()` is to set the seed of the contained simulator.
    If possible, this operation should not call `reset()` or interact with the environment
    execution. The parent `EnvBase.set_seed()` method incorporates a mechanism that
    allows seeding multiple environments with a different pseudo-random and reproducible
    seed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对环境进行种子操作是初始化实验时的常见操作。`EnvBase._set_seed()`的唯一目的是设置包含的模拟器的种子。如果可能的话，这个操作不应该调用`reset()`或与环境执行交互。父类`EnvBase.set_seed()`方法包含一个机制，允许使用不同的伪随机和可重现种子对多个环境进行种子化。
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Wrapping things together: the [`EnvBase`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase
    "(in torchrl vmain (0.4.0 ))") class'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将事物包装在一起：[`EnvBase`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase
    "(在torchrl vmain (0.4.0)")类
- en: We can finally put together the pieces and design our environment class. The
    specs initialization needs to be performed during the environment construction,
    so we must take care of calling the `_make_spec()` method within `PendulumEnv.__init__()`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以组合这些部分并设计我们的环境类。规格初始化需要在环境构建过程中执行，因此我们必须确保在`PendulumEnv.__init__()`内调用`_make_spec()`方法。
- en: 'We add a static method `PendulumEnv.gen_params()` which deterministically generates
    a set of hyperparameters to be used during execution:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了一个静态方法`PendulumEnv.gen_params()`，它确定性地生成一组在执行过程中使用的超参数：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We define the environment as non-`batch_locked` by turning the `homonymous`
    attribute to `False`. This means that we will **not** enforce the input `tensordict`
    to have a `batch-size` that matches the one of the environment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`homonymous`属性设置为`False`，我们将环境定义为非`batch_locked`。这意味着我们**不会**强制输入的`tensordict`具有与环境相匹配的`batch-size`。
- en: The following code will just put together the pieces we have coded above.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将组合我们上面编码的部分。
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Testing our environment
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试我们的环境
- en: 'TorchRL provides a simple function [`check_env_specs()`](https://pytorch.org/rl/reference/generated/torchrl.envs.utils.check_env_specs.html#torchrl.envs.utils.check_env_specs
    "(in torchrl vmain (0.4.0 ))") to check that a (transformed) environment has an
    input/output structure that matches the one dictated by its specs. Let us try
    it out:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: TorchRL提供了一个简单的函数[`check_env_specs()`](https://pytorch.org/rl/reference/generated/torchrl.envs.utils.check_env_specs.html#torchrl.envs.utils.check_env_specs
    "(在torchrl vmain (0.4.0))")来检查一个（转换后的）环境是否具有与其规格所规定的输入/输出结构相匹配的结构。让我们试一试：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can have a look at our specs to have a visual representation of the environment
    signature:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看我们的规格，以便对环境签名进行可视化表示：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can execute a couple of commands too to check that the output structure matches
    what is expected.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以执行一些命令来检查输出结构是否符合预期。
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can run the `env.rand_step()` to generate an action randomly from the `action_spec`
    domain. A `tensordict` containing the hyperparameters and the current state **must**
    be passed since our environment is stateless. In stateful contexts, `env.rand_step()`
    works perfectly too.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行`env.rand_step()`来从`action_spec`域中随机生成一个动作。由于我们的环境是无状态的，**必须**传递一个包含超参数和当前状态的`tensordict`。在有状态的情况下，`env.rand_step()`也可以完美运行。
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Transforming an environment
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换环境
- en: 'Writing environment transforms for stateless simulators is slightly more complicated
    than for stateful ones: transforming an output entry that needs to be read at
    the following iteration requires to apply the inverse transform before calling
    `meth.step()` at the next step. This is an ideal scenario to showcase all the
    features of TorchRL’s transforms!'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为无状态模拟器编写环境转换比为有状态模拟器稍微复杂一些：转换需要在下一次迭代时读取的输出条目需要在下一步调用`meth.step()`之前应用逆转换。这是展示TorchRL转换所有功能的理想场景！
- en: For instance, in the following transformed environment we `unsqueeze` the entries
    `["th", "thdot"]` to be able to stack them along the last dimension. We also pass
    them as `in_keys_inv` to squeeze them back to their original shape once they are
    passed as input in the next iteration.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在以下转换后的环境中，我们对`["th", "thdot"]`条目进行`unsqueeze`操作，以便能够沿着最后一个维度堆叠它们。我们还将它们作为`in_keys_inv`传递，以便在下一次迭代中将它们作为输入传递时将它们压缩回原始形状。
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Writing custom transforms
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写自定义转换
- en: 'TorchRL’s transforms may not cover all the operations one wants to execute
    after an environment has been executed. Writing a transform does not require much
    effort. As for the environment design, there are two steps in writing a transform:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: TorchRL的转换可能不涵盖所有希望在环境执行后执行的操作。编写一个转换并不需要太多的努力。与环境设计一样，编写转换有两个步骤：
- en: Getting the dynamics right (forward and inverse);
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确获取动态（正向和反向）；
- en: Adapting the environment specs.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整环境规格。
- en: 'A transform can be used in two settings: on its own, it can be used as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module
    "(in PyTorch v2.2)"). It can also be used appended to a [`TransformedEnv`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv
    "(in torchrl vmain (0.4.0 ))"). The structure of the class allows to customize
    the behavior in the different contexts.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 转换可以在两种设置中使用：独立使用时，它可以作为一个[`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module
    "(在PyTorch v2.2)")。它也可以附加到一个[`TransformedEnv`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv
    "(在torchrl vmain (0.4.0))")。类的结构允许在不同上下文中自定义行为。
- en: 'A [`Transform`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.Transform.html#torchrl.envs.transforms.Transform
    "(in torchrl vmain (0.4.0 ))") skeleton can be summarized as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[`Transform`](https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.Transform.html#torchrl.envs.transforms.Transform
    "(in torchrl vmain (0.4.0 ))")的框架可以总结如下：'
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There are three entry points (`forward()`, `_step()` and `inv()`) which all
    receive [`tensordict.TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") instances. The first two will eventually go
    through the keys indicated by `in_keys` and call `_apply_transform()` to each
    of these. The results will be written in the entries pointed by `Transform.out_keys`
    if provided (if not the `in_keys` will be updated with the transformed values).
    If inverse transforms need to be executed, a similar data flow will be executed
    but with the `Transform.inv()` and `Transform._inv_apply_transform()` methods
    and across the `in_keys_inv` and `out_keys_inv` list of keys. The following figure
    summarized this flow for environments and replay buffers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个入口点（`forward()`、`_step()`和`inv()`），它们都接收[`tensordict.TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))")实例。前两个最终将通过`in_keys`指示的键，并对每个键调用`_apply_transform()`。如果提供了`Transform.out_keys`，结果将写入由`Transform.out_keys`指向的条目（如果没有，则`in_keys`将使用转换后的值进行更新）。如果需要执行逆转换，将执行类似的数据流，但使用`Transform.inv()`和`Transform._inv_apply_transform()`方法，并跨`in_keys_inv`和`out_keys_inv`键列表。以下图总结了环境和重放缓冲区的这种流程。
- en: Transform API
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 转换API
- en: In some cases, a transform will not work on a subset of keys in a unitary manner,
    but will execute some operation on the parent environment or work with the entire
    input `tensordict`. In those cases, the `_call()` and `forward()` methods should
    be re-written, and the `_apply_transform()` method can be skipped.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，一个转换不会以单元方式在一部分键上工作，而是会在父环境上执行一些操作或者与整个输入的`tensordict`一起工作。在这些情况下，应重新编写`_call()`和`forward()`方法，可以跳过`_apply_transform()`方法。
- en: 'Let us code new transforms that will compute the `sine` and `cosine` values
    of the position angle, as these values are more useful to us to learn a policy
    than the raw angle value:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写新的转换，计算位置角的`sine`和`cosine`值，因为这些值对我们学习策略比原始角度值更有用：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Concatenates the observations onto an “observation” entry. `del_keys=False`
    ensures that we keep these values for the next iteration.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 将观察结果连接到“observation”条目上。`del_keys=False`确保我们保留这些值供下一次迭代使用。
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once more, let us check that our environment specs match what is received:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们检查一下我们的环境规格是否与接收到的一致：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Executing a rollout
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行一个轨迹
- en: 'Executing a rollout is a succession of simple steps:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 执行一个轨迹是一系列简单的步骤：
- en: reset the environment
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重置环境
- en: 'while some condition is not met:'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要某个条件未满足：
- en: compute an action given a policy
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据策略计算一个动作
- en: execute a step given this action
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行给定此动作的步骤
- en: collect the data
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集数据
- en: make a `MDP` step
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行`MDP`步骤
- en: gather the data and return
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集数据并返回
- en: These operations have been conveniently wrapped in the [`rollout()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id2
    "(in torchrl vmain (0.4.0 ))") method, from which we provide a simplified version
    here below.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作已经方便地包装在[`rollout()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id2
    "(in torchrl vmain (0.4.0 ))")方法中，我们在下面提供一个简化版本。
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Batching computations
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量计算
- en: 'The last unexplored end of our tutorial is the ability that we have to batch
    computations in TorchRL. Because our environment does not make any assumptions
    regarding the input data shape, we can seamlessly execute it over batches of data.
    Even better: for non-batch-locked environments such as our Pendulum, we can change
    the batch size on the fly without recreating the environment. To do this, we just
    generate parameters with the desired shape.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们教程的最后一个未探索的部分是我们在TorchRL中批量计算的能力。因为我们的环境对输入数据形状没有任何假设，所以我们可以无缝地在数据批次上执行它。更好的是：对于像我们的摆锤这样的非批量锁定环境，我们可以在不重新创建环境的情况下即时更改批量大小。为此，我们只需生成所需形状的参数。
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Executing a rollout with a batch of data requires us to reset the environment
    out of the rollout function, since we need to define the batch_size dynamically
    and this is not supported by [`rollout()`](https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#id2
    "(in torchrl vmain (0.4.0 ))"):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一批数据执行一个轨迹需要我们在轨迹函数之外重置环境，因为我们需要动态定义批量大小，而`rollout()`不支持这一点：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Training a simple policy
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个简单的策略
- en: In this example, we will train a simple policy using the reward as a differentiable
    objective, such as a negative loss. We will take advantage of the fact that our
    dynamic system is fully differentiable to backpropagate through the trajectory
    return and adjust the weights of our policy to maximize this value directly. Of
    course, in many settings many of the assumptions we make do not hold, such as
    differentiable system and full access to the underlying mechanics.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用奖励作为可微目标来训练一个简单的策略，比如一个负损失。我们将利用我们的动态系统是完全可微的这一事实，通过轨迹返回反向传播并调整我们的策略权重，以直接最大化这个值。当然，在许多情况下，我们所做的假设并不成立，比如可微系统和对底层机制的完全访问。
- en: Still, this is a very simple example that showcases how a training loop can
    be coded with a custom environment in TorchRL.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这只是一个非常简单的例子，展示了如何在TorchRL中使用自定义环境编写训练循环。
- en: 'Let us first write the policy network:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先编写策略网络：
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'and our optimizer:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 和我们的优化器：
- en: '[PRE30]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Training loop
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练循环
- en: 'We will successively:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依次：
- en: generate a trajectory
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个轨迹
- en: sum the rewards
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对奖励求和
- en: backpropagate through the graph defined by these operations
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过这些操作定义的图进行反向传播
- en: clip the gradient norm and make an optimization step
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 裁剪梯度范数并进行优化步骤
- en: repeat
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复
- en: At the end of the training loop, we should have a final reward close to 0 which
    demonstrates that the pendulum is upward and still as desired.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环结束时，我们应该有一个接近0的最终奖励，这表明摆锤向上并保持静止。
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![returns, last reward](../Images/9f96ad35f80739a0aa9e5611f81b2cd0.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![returns, last reward](../Images/9f96ad35f80739a0aa9e5611f81b2cd0.png)'
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Conclusion
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this tutorial, we have learned how to code a stateless environment from
    scratch. We touched the subjects of:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们学习了如何从头开始编码一个无状态环境。我们涉及了以下主题：
- en: The four essential components that need to be taken care of when coding an environment
    (`step`, `reset`, seeding and building specs). We saw how these methods and classes
    interact with the [`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(in tensordict vmain (0.4.0 ))") class;
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码环境时需要注意的四个基本组件（`step`、`reset`、种子和构建规范）。我们看到这些方法和类如何与[`TensorDict`](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict
    "(在tensordict vmain (0.4.0))")类交互；
- en: How to test that an environment is properly coded using [`check_env_specs()`](https://pytorch.org/rl/reference/generated/torchrl.envs.utils.check_env_specs.html#torchrl.envs.utils.check_env_specs
    "(in torchrl vmain (0.4.0 ))");
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何测试环境是否正确编码使用[`check_env_specs()`](https://pytorch.org/rl/reference/generated/torchrl.envs.utils.check_env_specs.html#torchrl.envs.utils.check_env_specs
    "(在torchrl vmain (0.4.0))");
- en: How to append transforms in the context of stateless environments and how to
    write custom transformations;
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在无状态环境的上下文中追加转换以及如何编写自定义转换；
- en: How to train a policy on a fully differentiable simulator.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在完全可微分的模拟器上训练策略。
- en: '**Total running time of the script:** ( 2 minutes 30.147 seconds)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的总运行时间：（2分钟30.147秒）
- en: '[`Download Python source code: pendulum.py`](../_downloads/0c4dc681209d8c964aae9de5e477d280/pendulum.py)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：pendulum.py`](../_downloads/0c4dc681209d8c964aae9de5e477d280/pendulum.py)'
- en: '[`Download Jupyter notebook: pendulum.ipynb`](../_downloads/8016e5cfa285bd92b9684c45552fffcc/pendulum.ipynb)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：pendulum.ipynb`](../_downloads/8016e5cfa285bd92b9684c45552fffcc/pendulum.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery生成的画廊](https://sphinx-gallery.github.io)'
