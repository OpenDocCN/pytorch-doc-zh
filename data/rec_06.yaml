- en: torchrec.distributed.sharding
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torchrec.distributed.sharding
- en: 原文：[https://pytorch.org/torchrec/torchrec.distributed.sharding.html](https://pytorch.org/torchrec/torchrec.distributed.sharding.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/torchrec/torchrec.distributed.sharding.html](https://pytorch.org/torchrec/torchrec.distributed.sharding.html)
- en: '## torchrec.distributed.sharding.cw_sharding[](#module-torchrec.distributed.sharding.cw_sharding
    "Permalink to this heading")'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '## torchrec.distributed.sharding.cw_sharding[](#module-torchrec.distributed.sharding.cw_sharding
    "Permalink to this heading")'
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Bases: [`BaseTwEmbeddingSharding`](#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding
    "torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding")[`C`, `F`,
    `T`, `W`]'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseTwEmbeddingSharding`](#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding
    "torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding")[`C`, `F`,
    `T`, `W`]
- en: Base class for column-wise sharding.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 列式分片的基类。
- en: '[PRE1]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Bases: [`BaseCwEmbeddingSharding`](#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding
    "torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseCwEmbeddingSharding`](#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding
    "torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]
- en: Shards embedding bags column-wise, i.e.. a given embedding table is partitioned
    along its columns and placed on specified ranks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 按列切分嵌入包，即。给定的嵌入表沿其列进行分区，并放置在指定的秩上。
- en: '[PRE6]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]
- en: '[PRE10]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Define the computation performed at every call.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类重写。
- en: Note
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行注册的钩子，而后者会默默地忽略它们。
- en: '[PRE11]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]
- en: '[PRE13]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Define the computation performed at every call.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类重写。
- en: Note
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行注册的钩子，而后者会默默地忽略它们。
- en: '[PRE14]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Bases: [`BaseCwEmbeddingSharding`](#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding
    "torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), [`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Tensor`]'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseCwEmbeddingSharding`](#torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding
    "torchrec.distributed.sharding.cw_sharding.BaseCwEmbeddingSharding")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), [`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Tensor`]
- en: '[PRE16]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]  ## torchrec.distributed.dist_data[](#module-torchrec.distributed.dist_data
    "Permalink to this heading")'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE18]  ## torchrec.distributed.dist_data[](#module-torchrec.distributed.dist_data
    "Permalink to this heading")'
- en: '[PRE19]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Bases: `Module`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Merges the pooled/sequence embedding tensor on each device into single tensor.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个设备上的池化/序列嵌入张量合并为单个张量。
- en: 'Parameters:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**device** (*torch.device*) – device on which buffer will be allocated.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 将分配缓冲区的设备。'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size** (*int*) – 拓扑中的设备数量。'
- en: '**cat_dim** (*int*) – which dimension you would like to concatenate on. For
    pooled embedding it is 1; for sequence embedding it is 0.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cat_dim** (*int*) – 您希望在哪个维度上进行连接。对于池化嵌入，它是1；对于序列嵌入，它是0。'
- en: '[PRE20]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Performs AlltoOne operation on pooled/sequence embeddings tensors.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对池化/序列嵌入张量执行AlltoOne操作。
- en: 'Parameters:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of embedding tensors.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**tensors** (*List**[**torch.Tensor**]*) – 嵌入张量的列表。'
- en: 'Returns:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of the merged embeddings.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 合并嵌入的可等待对象。
- en: 'Return type:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE21]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Bases: `Module`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Merges the pooled embedding tensor on each device into single tensor.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个设备上的池化嵌入张量合并为单个张量。
- en: 'Parameters:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**device** (*torch.device*) – device on which buffer will be allocated.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*）- 将分配缓冲区的设备。'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size**（*int*）- 拓扑中的设备数量。'
- en: '[PRE24]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Performs AlltoOne operation with Reduce on pooled embeddings tensors.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Reduce对汇总嵌入张量执行AlltoOne操作。
- en: 'Parameters:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of embedding tensors.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**tensors**（*List**[**torch.Tensor**]*）- 嵌入张量的列表。'
- en: 'Returns:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of the reduced embeddings.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 减少嵌入的等待。
- en: 'Return type:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE25]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Bases: `Module`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基础：`Module`
- en: Redistributes KeyedJaggedTensor to a ProcessGroup according to splits.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据拆分将KeyedJaggedTensor重新分发到ProcessGroup。
- en: Implementation utilizes AlltoAll collective as part of torch.distributed.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实现利用torch.distributed中的AlltoAll集体。
- en: The input provides the necessary tensors and input splits to distribute. The
    first collective call in KJTAllToAllSplitsAwaitable will transmit output splits
    (to allocate correct space for tensors) and batch size per rank. The following
    collective calls in KJTAllToAllTensorsAwaitable will transmit the actual tensors
    asynchronously.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输入提供了必要的张量和输入拆分以进行分发。KJTAllToAllSplitsAwaitable中的第一个集体调用将传输输出拆分（以为张量分配正确的空间）和每个等级的批量大小。KJTAllToAllTensorsAwaitable中的后续集体调用将异步传输实际张量。
- en: 'Parameters:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于AlltoAll通信的ProcessGroup。'
- en: '**splits** (*List**[**int**]*) – List of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**splits**（*List**[**int**]*）- 长度为pg.size()的列表，指示要发送到每个pg.rank()的特征数量。假定KeyedJaggedTensor按目标等级排序。对所有等级都是相同的。'
- en: '**stagger** (*int*) – stagger value to apply to recat tensor, see _get_recat
    function for more detail.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**stagger**（*int*）- 要应用于recat张量的间隔值，详细信息请参见_get_recat函数。'
- en: 'Example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE28]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Sends input to relevant ProcessGroup ranks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入发送到相关的ProcessGroup等级。
- en: The first wait will get the output splits for the provided tensors and issue
    tensors AlltoAll. The second wait will get the tensors.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个等待将获取所提供张量的输出拆分并发出张量AlltoAll。第二个等待将获取张量。
- en: 'Parameters:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – KeyedJaggedTensor of values
    to distribute.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**input**（[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")）- 要分发的值的KeyedJaggedTensor。'
- en: 'Returns:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of a KJTAllToAllTensorsAwaitable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一个KJTAllToAllTensorsAwaitable的等待。
- en: 'Return type:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KJTAllToAllTensorsAwaitable](#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KJTAllToAllTensorsAwaitable](#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]'
- en: '[PRE30]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Bases: [`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`KJTAllToAllTensorsAwaitable`](#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基础：[`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`KJTAllToAllTensorsAwaitable`](#torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable
    "torchrec.distributed.dist_data.KJTAllToAllTensorsAwaitable")]
- en: Awaitable for KJT tensors splits AlltoAll.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: KJT张量拆分AlltoAll的等待。
- en: 'Parameters:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于AlltoAll通信的ProcessGroup。'
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – input KJT.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input**（[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")）- 输入KJT。'
- en: '**splits** (*List**[**int**]*) – list of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**splits**（*List**[**int**]*）- 长度为pg.size()的列表，指示要发送到每个pg.rank()的特征数量。假定KeyedJaggedTensor按目标等级排序。对所有等级都是相同的。'
- en: '**tensor_splits** (*Dict**[**str**,* *List**[**int**]**]*) – tensor splits
    provided by input KJT.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tensor_splits**（*Dict**[**str**,* *List**[**int**]**]*）- 输入KJT提供的张量拆分。'
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – provided KJT tensors (ie.
    lengths, values) to redistribute according to splits.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_tensors**（*List**[**torch.Tensor**]*）- 根据拆分提供的KJT张量（即长度、值）进行重新分发。'
- en: '**keys** (*List**[**str**]*) – KJT keys after AlltoAll.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keys**（*List**[**str**]*）- AlltoAll后的KJT键。'
- en: '**device** (*torch.device*) – device on which buffers will be allocated.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*）- 将分配缓冲区的设备。'
- en: '**stagger** (*int*) – stagger value to apply to recat tensor.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**stagger**（*int*）- 要应用于recat张量的间隔值。'
- en: '[PRE32]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Bases: [`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 基础：[`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]
- en: Awaitable for KJT tensors AlltoAll.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: KJT张量AlltoAll的等待。
- en: 'Parameters:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于AlltoAll通信的ProcessGroup。'
- en: '**input** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – input KJT.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input**（[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")）- 输入KJT。'
- en: '**splits** (*List**[**int**]*) – list of len(pg.size()) which indicates how
    many features to send to each pg.rank(). It is assumed the KeyedJaggedTensor is
    ordered by destination rank. Same for all ranks.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**splits**（*List**[**int**]**）- 每个 pg.rank() 发送多少特征的长度列表。假定 KeyedJaggedTensor
    按目标排名排序。对所有排名都相同。'
- en: '**input_splits** (*List**[**List**[**int**]**]*) – input splits (number of
    values each rank will get) for each tensor in AlltoAll.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_splits**（*List**[**List**[**int**]**]*）- 每个 AlltoAll 中张量的输入拆分（每个排名将获得的值数量）。'
- en: '**output_splits** (*List**[**List**[**int**]**]*) – output splits (number of
    values per rank in output) for each tensor in AlltoAll.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_splits**（*List**[**List**[**int**]**]*）- 每个 AlltoAll 中张量的输出拆分（输出中每个排名的值数量）。'
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – provided KJT tensors (ie.
    lengths, values) to redistribute according to splits.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_tensors**（*List**[**torch.Tensor**]*）- 提供的 KJT 张量（即长度、值），根据拆分重新分配。'
- en: '**labels** (*List**[**str**]*) – labels for each provided tensor.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**labels**（*List**[**str**]*）- 每个提供的张量的标签。'
- en: '**keys** (*List**[**str**]*) – KJT keys after AlltoAll.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**keys**（*List**[**str**]*）- AlltoAll 后的 KJT 键。'
- en: '**device** (*torch.device*) – device on which buffers will be allocated.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*）- 将分配缓冲区的设备。'
- en: '**stagger** (*int*) – stagger value to apply to recat tensor.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**stagger**（*int*）- 应用于 recat 张量的间隔值。'
- en: '**stride_per_rank** (*Optional**[**List**[**int**]**]*) – stride per rank in
    the non variable batch per feature case.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**stride_per_rank**（*可选**[**List**[**int**]**]*）- 在非变量批次特征情况下每个排名的步幅。'
- en: '[PRE33]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Bases: `Module`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Redistributes KeyedJaggedTensor to all devices.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将 KeyedJaggedTensor 重新分配到所有设备。
- en: Implementation utilizes OnetoAll function, which essentially P2P copies the
    feature to the devices.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 实现利用 OnetoAll 函数，该函数本质上将特征复制到设备。
- en: 'Parameters:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**splits** (*List**[**int**]*) – lengths of features to split the KeyJaggedTensor
    features into before copying them.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**splits**（*List**[**int**]**）- 将 KeyJaggedTensor 特征拆分成副本之前的特征长度。'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size**（*int*）- 拓扑中的设备数量。'
- en: '**device** (*torch.device*) – the device on which the KJTs will be allocated.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*）- 将分配 KJTs 的设备。'
- en: '[PRE34]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Splits features first and then sends the slices to the corresponding devices.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先拆分特征，然后将切片发送到相应的设备。
- en: 'Parameters:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**kjt** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – the input features.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**kjt**（[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")）- 输入特征。'
- en: 'Returns:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of KeyedJaggedTensor splits.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: KeyedJaggedTensor 拆分的可等待对象。
- en: 'Return type:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[List[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[List[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
- en: '[PRE35]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Bases: `Module`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: The module class that wraps the all-gather communication primitive for pooled
    embedding communication.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 包装池化嵌入通信的全收集通信原语的模块类。
- en: Provided a local input tensor with a layout of [batch_size, dimension], we want
    to gather input tensors from all ranks into a flattened output tensor.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 提供具有 [batch_size, dimension] 布局的本地输入张量，我们希望从所有排名收集输入张量到一个扁平化的输出张量中。
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    all-gather is only available for NCCL backend.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 该类返回池化嵌入张量的异步可等待句柄。全收集仅适用于 NCCL 后端。
- en: 'Parameters:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – the process group that the all-gather communication
    happens within.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 发生全收集通信的进程组。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs**（*可选**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*）- 量化通信编解码器。'
- en: 'Example:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE37]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Performs reduce scatter operation on pooled embeddings tensor.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在池化嵌入张量上执行减少散布操作。
- en: 'Parameters:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_emb** (*torch.Tensor*) – tensor of shape [num_buckets x batch_size,
    dimension].'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**local_emb**（*torch.Tensor*）- 形状为 [num_buckets x batch_size, dimension] 的张量。'
- en: 'Returns:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为 [batch_size, dimension] 的张量的池化嵌入的可等待对象。
- en: 'Return type:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
- en: '[PRE39]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Bases: `Module`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Shards batches and collects keys of tensor with a ProcessGroup according to
    dim_sum_per_rank.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 dim_sum_per_rank 使用 ProcessGroup 对张量进行分片和收集键。
- en: Implementation utilizes alltoall_pooled operation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 实现利用 alltoall_pooled 操作。
- en: 'Parameters:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于全收集通信的 ProcessGroup。'
- en: '**dim_sum_per_rank** (*List**[**int**]*) – number of features (sum of dimensions)
    of the embedding in each rank.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dim_sum_per_rank**（*List**[**int**]*）- 每个排名中嵌入的特征数量（维度之和）。'
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*可选**[**torch.device**]*）- 将分配缓冲区的设备。'
- en: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    – callback functions.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**callbacks**（*可选**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*）-
    回调函数。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs**（*可选**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*）- 量化通信编解码器。'
- en: 'Example:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE41]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Performs AlltoAll pooled operation on pooled embeddings tensor.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在池化嵌入张量上执行全收集操作。
- en: 'Parameters:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs**（*torch.Tensor*）- 要分发的值的张量。'
- en: '**batch_size_per_rank** (*Optional**[**List**[**int**]**]*) – batch size per
    rank, to support variable batch size.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_rank**（*可选**[**List**[**int**]**]*）- 每个排名的批量大小，以支持可变批量大小。'
- en: 'Returns:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总嵌入的可等待。
- en: 'Return type:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
- en: '[PRE44]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Bases: [`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[`Tensor`]'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[`Tensor`]
- en: Awaitable for pooled embeddings after collective operation.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 集体操作后的汇总嵌入的可等待。
- en: 'Parameters:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tensor_awaitable** ([*Awaitable*](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*) – awaitable of concatenated
    tensors from all the processes in the group after collective.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**tensor_awaitable**（[*Awaitable*](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*）- 集体后来自组中所有进程的张量的连接张量的可等待。'
- en: '[PRE46]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Bases: `Module`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: The module class that wraps reduce-scatter communication primitives for pooled
    embedding communication in row-wise and twrw sharding.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 包装了用于行级和twrw分片中的汇总嵌入通信的reduce-scatter通信原语的模块类。
- en: For pooled embeddings, we have a local model-parallel output tensor with a layout
    of [num_buckets x batch_size, dimension]. We need to sum over num_buckets dimension
    across batches. We split the tensor along the first dimension into unequal chunks
    (tensor slices of different buckets) according to input_splits and reduce them
    into the output tensor and scatter the results for corresponding ranks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于汇总嵌入，我们有一个本地模型并行输出张量，布局为[num_buckets x batch_size，维度]。我们需要跨批次对num_buckets维度求和。我们根据input_splits将张量沿第一维拆分为不均匀的块（不同桶的张量切片），将它们减少到输出张量并将结果分散到相应的排名。
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    reduce-scatter-v operation is only available for NCCL backend.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 该类返回汇总嵌入张量的异步Awaitable句柄。reduce-scatter-v操作仅适用于NCCL后端。
- en: 'Parameters:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – the process group that the reduce-scatter communication
    happens within.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 减少散列通信发生在其中的进程组。'
- en: '**codecs** – quantized communication codecs.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs** - 量化通信编解码器。'
- en: '[PRE48]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Performs reduce scatter operation on pooled embeddings tensor.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在汇总嵌入张量上执行减少散列操作。
- en: 'Parameters:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – tensor of shape [num_buckets * batch_size,
    dimension].'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs**（*torch.Tensor*）- 形状为[num_buckets * batch_size，维度]的张量。'
- en: '**input_splits** (*Optional**[**List**[**int**]**]*) – list of splits for local_embs
    dim 0.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_splits**（*可选**[**List**[**int**]**]*）- 用于local_embs维度0的拆分列表。'
- en: 'Returns:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的形状为[batch_size，维度]的汇总嵌入的可等待。
- en: 'Return type:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
- en: '[PRE49]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Bases: `Module`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Merges the pooled/sequence embedding tensor on each device into single tensor.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个设备上的汇总/序列嵌入张量合并为单个张量。
- en: 'Parameters:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**device** (*torch.device*) – device on which buffer will be allocated'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*torch.device*）- 将分配缓冲区的设备'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size**（*int*）- 拓扑中的设备数量。'
- en: '**cat_dim** (*int*) – which dimension you like to concate on. For pooled embedding
    it is 1; for sequence embedding it is 0.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cat_dim**（*int*）- 您希望在其上连接的维度。对于汇总嵌入，它是1；对于序列嵌入，它是0。'
- en: '[PRE51]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Performs AlltoOne operation on pooled embeddings tensors.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在汇总嵌入张量上执行AlltoOne操作。
- en: 'Parameters:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tensors** (*List**[**torch.Tensor**]*) – list of pooled embedding tensors.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**tensors**（*List**[**torch.Tensor**]*）- 汇总嵌入张量的列表。'
- en: 'Returns:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of the merged pooled embeddings.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 合并汇总嵌入的可等待。
- en: 'Return type:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE52]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Bases: `Module`'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Redistributes sequence embedding to a ProcessGroup according to splits.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 根据分片将序列嵌入重新分配到ProcessGroup。
- en: 'Parameters:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – the process group that the AlltoAll communication
    happens within.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- AlltoAll通信发生在其中的进程组。'
- en: '**features_per_rank** (*List**[**int**]*) – list of number of features per
    rank.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**features_per_rank**（*List**[**int**]*）- 每个排名的特征数量列表。'
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*可选**[**torch.device**]*）- 将分配缓冲区的设备。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs**（*可选**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) - 量化通信编解码器。'
- en: 'Example:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE55]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Performs AlltoAll operation on sequence embeddings tensor.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列嵌入张量上执行AlltoAll操作。
- en: 'Parameters:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – input embeddings tensor.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs**（*torch.Tensor*）- 输入嵌入张量。'
- en: '**lengths** (*torch.Tensor*) – lengths of sparse features after AlltoAll.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lengths**（*torch.Tensor*）- AlltoAll后稀疏特征的长度。'
- en: '**input_splits** (*List**[**int**]*) – input splits of AlltoAll.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_splits**（*List**[**int**]*）- AlltoAll的输入分片。'
- en: '**output_splits** (*List**[**int**]*) – output splits of AlltoAll.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**output_splits**（*List**[**int**]*）- AlltoAll的输出分片。'
- en: '**unbucketize_permute_tensor** (*Optional**[**torch.Tensor**]*) – stores the
    permute order of the KJT bucketize (for row-wise sharding only).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unbucketize_permute_tensor**（*可选**[**torch.Tensor**]*）- 存储KJT bucketize的排列顺序（仅适用于行级分片）。'
- en: '**batch_size_per_rank** – (Optional[List[int]]): batch size per rank.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_rank** - （可选[List[int]]）：每个rank的批量大小。'
- en: '**sparse_features_recat** (*Optional**[**torch.Tensor**]*) – recat tensor used
    for sparse feature input dist. Must be provided if using variable batch size.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sparse_features_recat**（*Optional**[**torch.Tensor**]*）- 用于稀疏特征输入分布的recat张量。如果使用可变批量大小，则必须提供。'
- en: 'Returns:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of sequence embeddings.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 序列嵌入的可等待对象。
- en: 'Return type:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[SequenceEmbeddingsAwaitable](#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable
    "torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable")'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[SequenceEmbeddingsAwaitable](#torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable
    "torchrec.distributed.dist_data.SequenceEmbeddingsAwaitable")'
- en: '[PRE57]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Bases: [`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[`Tensor`]'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[`Tensor`]
- en: Awaitable for sequence embeddings after collective operation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在集体操作后的序列嵌入之后的可等待对象。
- en: 'Parameters:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**tensor_awaitable** ([*Awaitable*](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*) – awaitable of concatenated
    tensors from all the processes in the group after collective.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tensor_awaitable**（[*Awaitable*](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")*[**torch.Tensor**]*) - 集体操作后来自组内所有进程的连接张量的可等待对象。'
- en: '**unbucketize_permute_tensor** (*Optional**[**torch.Tensor**]*) – stores the
    permute order of KJT bucketize (for row-wise sharding only).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unbucketize_permute_tensor**（*Optional**[**torch.Tensor**]*）- 存储KJT桶化的排列顺序（仅适用于逐行分片）。'
- en: '**embedding_dim** (*int*) – embedding dimension.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**embedding_dim**（*int*）- 嵌入维度。'
- en: '[PRE59]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Bases: [`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[`List`[`List`[`int`]]]'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`Awaitable`](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[`List`[`List`[`int`]]]
- en: Awaitable for splits AlltoAll.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 拆分AlltoAll的可等待对象。
- en: 'Parameters:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**input_tensors** (*List**[**torch.Tensor**]*) – tensor of splits to redistribute.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_tensors**（*List**[**torch.Tensor**]*）- 要重新分配的拆分张量。'
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于AlltoAll通信的ProcessGroup。'
- en: '[PRE60]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Bases: `Module`'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: Shards batches and collects keys of tensor with a ProcessGroup according to
    dim_sum_per_rank.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 根据dim_sum_per_rank对张量的批次进行分片并收集键与ProcessGroup一起。
- en: Implementation utilizes variable_batch_alltoall_pooled operation.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 实现利用variable_batch_alltoall_pooled操作。
- en: 'Parameters:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于AlltoAll通信的ProcessGroup。'
- en: '**emb_dim_per_rank_per_feature** (*List**[**List**[**int**]**]*) – embedding
    dimensions per rank per feature.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**emb_dim_per_rank_per_feature**（*List**[**List**[**int**]**]*）- 每个特征的每个rank的嵌入维度。'
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*Optional**[**torch.device**]*）- 将分配缓冲区的设备。'
- en: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    – callback functions.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**callbacks**（*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*）-
    回调函数。'
- en: '**codecs** (*Optional**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*) – quantized communication
    codecs.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs**（*Optional**[*[*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]*）- 量化通信编解码器。'
- en: 'Example:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE61]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Performs AlltoAll pooled operation with variable batch size per feature on a
    pooled embeddings tensor.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对池化嵌入张量进行具有每个特征可变批量大小的AlltoAll池化操作。
- en: 'Parameters:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs**（*torch.Tensor*）- 要分发的值的张量。'
- en: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – batch
    size per rank per feature, post a2a. Used to get the input splits.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_rank_per_feature**（*List**[**List**[**int**]**]*）- 每个特征的每个rank的批量大小，a2a后。用于获取输入拆分。'
- en: '**batch_size_per_feature_pre_a2a** (*List**[**int**]*) – local batch size before
    scattering, used to get the output splits. Ordered by rank_0 feature, rank_1 feature,
    …'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_feature_pre_a2a**（*List**[**int**]*）- 分散之前的本地批量大小，用于获取输出拆分。按rank_0特征，rank_1特征排序，...'
- en: 'Returns:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 池化嵌入的可等待对象。
- en: 'Return type:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
- en: '[PRE64]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Bases: `Module`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：`Module`
- en: The module class that wraps reduce-scatter communication primitives for pooled
    embedding communication of variable batch in rw and twrw sharding.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 包装可变批量池化嵌入通信的reduce-scatter通信原语的模块类，rw和twrw分片。
- en: For variable batch per feature pooled embeddings, we have a local model-parallel
    output tensor with a 1d layout of the total sum of batch sizes per rank per feature
    multiplied by corresponding embedding dim [batch_size_r0_f0 * emb_dim_f0 + …)].
    We split the tensor into unequal chunks by rank according to batch_size_per_rank_per_feature
    and corresponding embedding_dims and reduce them into the output tensor and scatter
    the results for corresponding ranks.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个特征池化嵌入的可变批量，我们有一个本地模型并行输出张量，其布局为每个特征的每个rank的批量大小总和乘以相应的嵌入维度的1d布局[batch_size_r0_f0
    * emb_dim_f0 + …)]。我们根据batch_size_per_rank_per_feature和相应的embedding_dims将张量分割成不均匀的块，并将它们减少到输出张量并将结果分散到相应的rank。
- en: The class returns the async Awaitable handle for pooled embeddings tensor. The
    reduce-scatter-v operation is only available for NCCL backend.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 该类返回用于池化嵌入张量的异步Awaitable句柄。reduce-scatter-v操作仅适用于NCCL后端。
- en: 'Parameters:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – the process group that the reduce-scatter communication
    happens within.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- reduce-scatter通信发生在其中的进程组。'
- en: '**codecs** – quantized communication codecs.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**codecs** - 量化通信编解码器。'
- en: '[PRE66]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Performs reduce scatter operation on pooled embeddings tensor.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对池化嵌入张量执行reduce scatter操作。
- en: 'Parameters:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – tensor of shape [num_buckets * batch_size,
    dimension].'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs** (*torch.Tensor*) - 形状为[num_buckets * batch_size, dimension]的张量。'
- en: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) – batch
    size per rank per feature used to determine input splits.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size_per_rank_per_feature** (*List**[**List**[**int**]**]*) - 每个特征的每个等级的批量大小，用于确定输入拆分。'
- en: '**embedding_dims** (*List**[**int**]*) – embedding dimensions per feature used
    to determine input splits.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**embedding_dims** (*List**[**int**]*) - 每个特征的嵌入维度，用于确定输入拆分。'
- en: 'Returns:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 等待池化张量的嵌入，形状为[batch_size, dimension]。
- en: 'Return type:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[PooledEmbeddingsAwaitable](#torchrec.distributed.dist_data.PooledEmbeddingsAwaitable
    "torchrec.distributed.dist_data.PooledEmbeddingsAwaitable")'
- en: '[PRE67]  ## torchrec.distributed.sharding.dp_sharding[](#module-torchrec.distributed.sharding.dp_sharding
    "Permalink to this heading")'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE67]  ## torchrec.distributed.sharding.dp_sharding[](#module-torchrec.distributed.sharding.dp_sharding
    "跳转到此标题")'
- en: '[PRE68]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Bases: [`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]
- en: Base class for data-parallel sharding.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 基类用于数据并行分片。
- en: '[PRE69]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]
- en: Distributes pooled embeddings to be data-parallel.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 将池化嵌入分发为数据并行。
- en: '[PRE76]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: No-op as pooled embeddings are already distributed in data-parallel fashion.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 由于池化嵌入已经以数据并行方式分布，因此无操作。
- en: 'Parameters:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – output sequence embeddings.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**local_embs** (*torch.Tensor*) - 输出序列嵌入。'
- en: 'Returns:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings tensor.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 等待池化嵌入张量。
- en: 'Return type:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE77]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Bases: [`BaseDpEmbeddingSharding`](#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding
    "torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseDpEmbeddingSharding`](#torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding
    "torchrec.distributed.sharding.dp_sharding.BaseDpEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]
- en: Shards embedding bags data-parallel, with no table sharding i.e.. a given embedding
    table is replicated across all ranks.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 将嵌入包数据并行分片，没有表分片，即给定的嵌入表在所有等级上都复制。
- en: '[PRE79]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]
- en: Distributes sparse features (input) to be data-parallel.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 将稀疏特征（输入）分发为数据并行。
- en: '[PRE83]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: No-op as sparse features are already distributed in data-parallel fashion.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 由于稀疏特征已经以数据并行方式分布，因此无操作。
- en: 'Parameters:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**sparse_features** (*SparseFeatures*) – input sparse features.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**sparse_features** (*SparseFeatures*) - 输入稀疏特征。'
- en: 'Returns:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of awaitable of SparseFeatures.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 等待稀疏特征的等待。
- en: 'Return type:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[SparseFeatures]]'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[SparseFeatures]]'
- en: '[PRE84]  ## torchrec.distributed.sharding.rw_sharding[](#module-torchrec.distributed.sharding.rw_sharding
    "Permalink to this heading")'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE84]  ## torchrec.distributed.sharding.rw_sharding[](#module-torchrec.distributed.sharding.rw_sharding
    "跳转到此标题")'
- en: '[PRE85]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Bases: [`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]
- en: Base class for row-wise sharding.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 基类用于按行分片。
- en: '[PRE86]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]
- en: Redistributes pooled embedding tensor in RW fashion with an AlltoOne operation.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 以AlltoOne操作以RW方式重新分配汇集的嵌入张量。
- en: 'Parameters:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**device** (*torch.device*) – device on which the tensors will be communicated
    to.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*torch.device*) – 将要通信的张量所在的设备。'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size** (*int*) – 拓扑中的设备数量。'
- en: '[PRE93]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Performs AlltoOne operation on sequence embeddings tensor.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列嵌入张量上执行AlltoOne操作。
- en: 'Parameters:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**local_embs** (*torch.Tensor*) – 要分发的值的张量。'
- en: 'Returns:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of sequence embeddings.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 序列嵌入的awaitable。
- en: 'Return type:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE94]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Bases: [`BaseRwEmbeddingSharding`](#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding
    "torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), [`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Tensor`]'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseRwEmbeddingSharding`](#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding
    "torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), [`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Tensor`]
- en: '[PRE96]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]
- en: '[PRE100]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Define the computation performed at every call.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 定义每次调用时执行的计算。
- en: Should be overridden by all subclasses.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被所有子类覆盖。
- en: Note
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the registered hooks while the latter silently ignores them.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行注册的钩子，而后者会默默忽略它们。
- en: '[PRE101]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]
- en: Redistributes pooled embedding tensor in RW fashion by performing a reduce-scatter
    operation.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 以RW方式执行reduce-scatter操作重新分配汇集的嵌入张量。
- en: 'Parameters:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for reduce-scatter communication.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '**pg** (*dist.ProcessGroup*) – 用于reduce-scatter通信的ProcessGroup。'
- en: '[PRE103]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Performs reduce-scatter pooled operation on pooled embeddings tensor.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在汇集的嵌入张量上执行reduce-scatter池化操作。
- en: 'Parameters:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – pooled embeddings tensor to distribute.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs** (*torch.Tensor*) – 要分发的汇集的嵌入张量。'
- en: '**sharding_ctx** (*Optional**[*[*EmbeddingShardingContext*](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext")*]*) – shared
    context from KJTAllToAll operation.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sharding_ctx** (*Optional**[*[*EmbeddingShardingContext*](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext")*]*) – 来自KJTAllToAll操作的共享上下文。'
- en: 'Returns:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings tensor.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 汇集的嵌入张量的awaitable。
- en: 'Return type:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE104]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Bases: [`BaseRwEmbeddingSharding`](#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding
    "torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseRwEmbeddingSharding`](#torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding
    "torchrec.distributed.sharding.rw_sharding.BaseRwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]
- en: Shards embedding bags row-wise, i.e.. a given embedding table is evenly distributed
    by rows and table slices are placed on all ranks.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 按行分片嵌入包，即。给定的嵌入表按行均匀分布，表切片放置在所有秩上。
- en: '[PRE106]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]
- en: Bucketizes sparse features in RW fashion and then redistributes with an AlltoAll
    collective operation.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 以RW方式对稀疏特征进行分桶，然后通过AlltoAll集体操作重新分配。
- en: 'Parameters:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg** (*dist.ProcessGroup*) - 用于AlltoAll通信的ProcessGroup。'
- en: '**intra_pg** (*dist.ProcessGroup*) – ProcessGroup within single host group
    for AlltoAll communication.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**intra_pg** (*dist.ProcessGroup*) - 单个主机组内用于AlltoAll通信的ProcessGroup。'
- en: '**num_features** (*int*) – total number of features.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_features** (*int*) - 特征总数。'
- en: '**feature_hash_sizes** (*List**[**int**]*) – hash sizes of features.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**feature_hash_sizes** (*List**[**int**]*) - 特征的哈希大小。'
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*Optional**[**torch.device**]*) - 将分配缓冲区的设备。'
- en: '**is_sequence** (*bool*) – if this is for a sequence embedding.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**is_sequence** (*bool*) - 如果这是用于序列嵌入。'
- en: '**has_feature_processor** (*bool*) – existence of feature processor (ie. position
    weighted features).'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**has_feature_processor** (*bool*) - 特征处理器的存在（即位置加权特征）。'
- en: '[PRE110]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Bucketizes sparse feature values into world size number of buckets and then
    performs AlltoAll operation.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 将稀疏特征值分桶为拓扑中设备数量的桶，然后执行AlltoAll操作。
- en: 'Parameters:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**sparse_features** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – sparse features to bucketize
    and redistribute.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '**sparse_features** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) - 要分桶和重新分配的稀疏特征。'
- en: 'Returns:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of awaitable of KeyedJaggedTensor.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 可等待的可等待的KeyedJaggedTensor。
- en: 'Return type:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
- en: '[PRE111]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]  ## torchrec.distributed.sharding.tw_sharding[](#module-torchrec.distributed.sharding.tw_sharding
    "Permalink to this heading")'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE113]  ## torchrec.distributed.sharding.tw_sharding[](#module-torchrec.distributed.sharding.tw_sharding
    "Permalink to this heading")'
- en: '[PRE114]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Bases: [`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]
- en: Base class for table wise sharding.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 表格智能分片的基类。
- en: '[PRE115]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'Bases: [`BaseTwEmbeddingSharding`](#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding
    "torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), [`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Tensor`]'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseTwEmbeddingSharding`](#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding
    "torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), [`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList"), `List`[`Tensor`], `Tensor`]
- en: Shards embedding bags table-wise for inference
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 为推断分片嵌入包表格
- en: '[PRE124]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`NullShardingContext`](torchrec.distributed.html#torchrec.distributed.types.NullShardingContext
    "torchrec.distributed.types.NullShardingContext"), `List`[`Tensor`], `Tensor`]
- en: Merges pooled embedding tensor from each device for inference.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 合并每个设备的汇总嵌入张量以进行推断。
- en: 'Parameters:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffer will be
    allocated.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*Optional**[**torch.device**]*) - 将分配缓冲区的设备。'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size** (*int*) - 拓扑中设备的数量。'
- en: '[PRE128]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: Performs AlltoOne operation on pooled embedding tensors.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 对汇总嵌入张量执行AlltoOne操作。
- en: 'Parameters:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*List**[**torch.Tensor**]*) – pooled embedding tensors with
    len(local_embs) == world_size.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '**local_embs** (*List**[**torch.Tensor**]*) - 具有len(local_embs) == world_size的汇总嵌入张量。'
- en: 'Returns:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of merged pooled embedding tensor.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 可等待的合并汇总嵌入张量。
- en: 'Return type:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE129]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KJTList`](torchrec.distributed.html#torchrec.distributed.embedding_types.KJTList
    "torchrec.distributed.embedding_types.KJTList")]
- en: Redistributes sparse features to all devices for inference.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 将稀疏特征重新分配到所有设备进行推断。
- en: 'Parameters:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**features_per_rank** (*List**[**int**]*) – number of features to send to each
    rank.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**features_per_rank** (*List**[**int**]*) - 发送到每个等级的特征数。'
- en: '**world_size** (*int*) – number of devices in the topology.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**world_size** (*int*) - 拓扑中设备的数量。'
- en: '**fused_params** (*Dict**[**str**,* *Any**]*) – fused parameters of the model.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fused_params** (*Dict**[**str**,* *Any**]*) - 模型的融合参数。'
- en: '[PRE131]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: Performs OnetoAll operation on sparse features.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 对稀疏特征执行OnetoAll操作。
- en: 'Parameters:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**sparse_features** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – sparse features to redistribute.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '**sparse_features**（[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")）- 要重新分配的稀疏特征。'
- en: 'Returns:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of awaitable of KeyedJaggedTensor.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 可等待的KeyedJaggedTensor的可等待。
- en: 'Return type:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
- en: '[PRE132]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]
- en: Redistributes pooled embedding tensor with an AlltoAll collective operation
    for table wise sharding.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AlltoAll集体操作重新分配池化的嵌入张量，以进行表格划分。
- en: 'Parameters:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于AlltoAll通信的ProcessGroup。'
- en: '**dim_sum_per_rank** (*List**[**int**]*) – number of features (sum of dimensions)
    of the embedding in each rank.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dim_sum_per_rank**（*List**[**int**]*）- 每个rank中嵌入的特征数量（维度之和）。'
- en: '**emb_dim_per_rank_per_feature** (*List**[**List**[**int**]**]*) – embedding
    dimension per rank per feature, used for variable batch per feature.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**emb_dim_per_rank_per_feature**（*List**[**List**[**int**]**]*）- 每个特征的每个rank的嵌入维度，用于每个特征的可变批处理。'
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device**（*Optional**[**torch.device**]*）- 将分配缓冲区的设备。'
- en: '**callbacks** (*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*)
    –'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**callbacks**（*Optional**[**List**[**Callable**[**[**torch.Tensor**]**,* *torch.Tensor**]**]**]*）-'
- en: '**qcomm_codecs_registry** (*Optional**[**Dict**[**str**,* [*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]**]*) –'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**qcomm_codecs_registry**（*Optional**[**Dict**[**str**,* [*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]**]*）-'
- en: '[PRE134]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Performs AlltoAll operation on pooled embeddings tensor.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 对池化的嵌入张量执行AlltoAll操作。
- en: 'Parameters:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – tensor of values to distribute.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**local_embs**（*torch.Tensor*）- 要分发的值的张量。'
- en: '**sharding_ctx** (*Optional**[*[*EmbeddingShardingContext*](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext")*]*) – shared
    context from KJTAllToAll operation.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sharding_ctx**（*Optional**[*[*EmbeddingShardingContext*](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext")*]*）- 来自KJTAllToAll操作的共享上下文。'
- en: 'Returns:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 池化的嵌入的可等待。
- en: 'Return type:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE135]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Bases: [`BaseTwEmbeddingSharding`](#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding
    "torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseTwEmbeddingSharding`](#torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding
    "torchrec.distributed.sharding.tw_sharding.BaseTwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]
- en: Shards embedding bags table-wise, i.e.. a given embedding table is entirely
    placed on a selected rank.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 按表格划分嵌入包，即。给定的嵌入表完全放置在选定的rank上。
- en: '[PRE137]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]
- en: Redistributes sparse features with an AlltoAll collective operation for table
    wise sharding.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AlltoAll集体操作重新分配稀疏特征，以进行表格划分。
- en: 'Parameters:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg**（*dist.ProcessGroup*）- 用于AlltoAll通信的ProcessGroup。'
- en: '**features_per_rank** (*List**[**int**]*) – number of features to send to each
    rank.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**features_per_rank**（*List**[**int**]*）- 发送到每个rank的特征数量。'
- en: '[PRE141]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: Performs AlltoAll operation on sparse features.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 对稀疏特征执行AlltoAll操作。
- en: 'Parameters:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**sparse_features** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – sparse features to redistribute.'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '**sparse_features**（[*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")）- 要重新分配的稀疏特征。'
- en: 'Returns:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of awaitable of KeyedJaggedTensor.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 可等待的KeyedJaggedTensor的可等待。
- en: 'Return type:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '[可等待对象](torchrec.distributed.html#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[[可等待对象](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]]'
- en: '[PRE142]  ## torchrec.distributed.sharding.twcw_sharding[](#module-torchrec.distributed.sharding.twcw_sharding
    "Permalink to this heading")'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE142]  ## torchrec.distributed.sharding.twcw_sharding[](#module-torchrec.distributed.sharding.twcw_sharding
    "Permalink to this heading")'
- en: '[PRE143]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Bases: [`CwPooledEmbeddingSharding`](#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding
    "torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding")'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`CwPooledEmbeddingSharding`](#torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding
    "torchrec.distributed.sharding.cw_sharding.CwPooledEmbeddingSharding")
- en: 'Shards embedding bags table-wise column-wise, i.e.. a given embedding table
    is partitioned along its columns and the table slices are placed on all ranks
    within a host group.  ## torchrec.distributed.sharding.twrw_sharding[](#module-torchrec.distributed.sharding.twrw_sharding
    "Permalink to this heading")'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '按表格方式分片嵌入包，即给定的嵌入表按列进行分区，并将表切片放置在主机组内的所有秩上。  ## torchrec.distributed.sharding.twrw_sharding[](#module-torchrec.distributed.sharding.twrw_sharding
    "Permalink to this heading")'
- en: '[PRE144]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'Bases: [`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`EmbeddingSharding`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingSharding
    "torchrec.distributed.embedding_sharding.EmbeddingSharding")[`C`, `F`, `T`, `W`]
- en: Base class for table wise row wise sharding.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 表格智能行智能分片的基类。
- en: '[PRE145]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Bases: [`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseEmbeddingDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseEmbeddingDist
    "torchrec.distributed.embedding_sharding.BaseEmbeddingDist")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), `Tensor`,
    `Tensor`]
- en: Redistributes pooled embedding tensor in TWRW fashion by performing a reduce-scatter
    operation row wise on the host level and then an AlltoAll operation table wise
    on the global level.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在主机级别逐行执行reduce-scatter操作，然后在全局级别逐表执行全对全操作，以TWRW方式重新分配池化嵌入张量。
- en: 'Parameters:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**cross_pg** (*dist.ProcessGroup*) – global level ProcessGroup for AlltoAll
    communication.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cross_pg** (*dist.ProcessGroup*) – 用于全对全通信的全局级ProcessGroup。'
- en: '**intra_pg** (*dist.ProcessGroup*) – host level ProcessGroup for reduce-scatter
    communication.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**intra_pg** (*dist.ProcessGroup*) – 用于reduce-scatter通信的主机级ProcessGroup。'
- en: '**dim_sum_per_node** (*List**[**int**]*) – number of features (sum of dimensions)
    of the embedding for each host.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dim_sum_per_node** (*列表**[**int**]*) – 每个主机的嵌入特征的数量（维度之和）。'
- en: '**emb_dim_per_node_per_feature** (*List**[**List**[**int**]**]*) –'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**emb_dim_per_node_per_feature** (*列表**[**列表**[**int**]**]*) –'
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*可选**[**torch.device**]*) – 将分配缓冲区的设备。'
- en: '**qcomm_codecs_registry** (*Optional**[**Dict**[**str**,* [*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]**]*) –'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**qcomm_codecs_registry** (*可选**[**Dict**[**str**,* [*QuantizedCommCodecs*](torchrec.distributed.html#torchrec.distributed.types.QuantizedCommCodecs
    "torchrec.distributed.types.QuantizedCommCodecs")*]**]*) –'
- en: '[PRE151]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: Performs reduce-scatter pooled operation on pooled embeddings tensor followed
    by AlltoAll pooled operation.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 对池化嵌入张量执行reduce-scatter池化操作，然后进行全对全池化操作。
- en: 'Parameters:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**local_embs** (*torch.Tensor*) – pooled embeddings tensor to distribute.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '**local_embs** (*torch.Tensor*) – 要分发的池化嵌入张量。'
- en: 'Returns:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of pooled embeddings tensor.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 池化嵌入张量的可等待对象。
- en: 'Return type:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[torch.Tensor]'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '[可等待对象](torchrec.distributed.html#torchrec.distributed.types.Awaitable "torchrec.distributed.types.Awaitable")[torch.Tensor]'
- en: '[PRE152]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'Bases: [`BaseTwRwEmbeddingSharding`](#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding
    "torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseTwRwEmbeddingSharding`](#torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding
    "torchrec.distributed.sharding.twrw_sharding.BaseTwRwEmbeddingSharding")[[`EmbeddingShardingContext`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.EmbeddingShardingContext
    "torchrec.distributed.embedding_sharding.EmbeddingShardingContext"), [`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor"), `Tensor`, `Tensor`]
- en: Shards embedding bags table-wise then row-wise.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 按表格方式分片嵌入包，然后按行方式分片。
- en: '[PRE154]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: 'Bases: [`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 基类：[`BaseSparseFeaturesDist`](torchrec.distributed.html#torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist
    "torchrec.distributed.embedding_sharding.BaseSparseFeaturesDist")[[`KeyedJaggedTensor`](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]
- en: Bucketizes sparse features in TWRW fashion and then redistributes with an AlltoAll
    collective operation.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 以TWRW方式对稀疏特征进行分桶，然后通过全对全集体操作重新分配。
- en: 'Parameters:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**pg** (*dist.ProcessGroup*) – ProcessGroup for AlltoAll communication.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pg** (*dist.ProcessGroup*) – 用于全对全通信的ProcessGroup。'
- en: '**intra_pg** (*dist.ProcessGroup*) – ProcessGroup within single host group
    for AlltoAll communication.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**intra_pg** (*dist.ProcessGroup*) – 单个主机组内用于AlltoAll通信的ProcessGroup。'
- en: '**id_list_features_per_rank** (*List**[**int**]*) – number of id list features
    to send to each rank.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**id_list_features_per_rank** (*List**[**int**]*) – 发送到每个排名的id列表特征的数量。'
- en: '**id_score_list_features_per_rank** (*List**[**int**]*) – number of id score
    list features to send to each rank.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**id_score_list_features_per_rank** (*List**[**int**]*) – 发送到每个排名的id分数列表特征的数量。'
- en: '**id_list_feature_hash_sizes** (*List**[**int**]*) – hash sizes of id list
    features.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**id_list_feature_hash_sizes** (*List**[**int**]*) – id列表特征的哈希大小。'
- en: '**id_score_list_feature_hash_sizes** (*List**[**int**]*) – hash sizes of id
    score list features.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**id_score_list_feature_hash_sizes** (*List**[**int**]*) – id分数列表特征的哈希大小。'
- en: '**device** (*Optional**[**torch.device**]*) – device on which buffers will
    be allocated.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**device** (*Optional**[**torch.device**]*) – 将分配缓冲区的设备。'
- en: '**has_feature_processor** (*bool*) – existence of a feature processor (ie.
    position weighted features).'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**has_feature_processor** (*bool*) – 特征处理器的存在（即位置加权特征）。'
- en: 'Example:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE158]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: Bucketizes sparse feature values into local world size number of buckets, performs
    staggered shuffle on the sparse features, and then performs AlltoAll operation.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 将稀疏特征值分桶为本地世界大小的桶数，对稀疏特征执行交错洗牌，然后执行AlltoAll操作。
- en: 'Parameters:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '**sparse_features** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – sparse features to bucketize
    and redistribute.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '**sparse_features** ([*KeyedJaggedTensor*](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")) – 要进行分桶和重新分配的稀疏特征。'
- en: 'Returns:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: awaitable of KeyedJaggedTensor.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: KeyedJaggedTensor的可等待对象。
- en: 'Return type:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 返回类型：
- en: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '[Awaitable](torchrec.distributed.html#torchrec.distributed.types.Awaitable
    "torchrec.distributed.types.Awaitable")[[KeyedJaggedTensor](torchrec.sparse.html#torchrec.sparse.jagged_tensor.KeyedJaggedTensor
    "torchrec.sparse.jagged_tensor.KeyedJaggedTensor")]'
- en: '[PRE160]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
