- en: Introduction to PyTorch Tensors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch张量介绍
- en: 原文：[https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[原文](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)'
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-beginner-introyt-tensors-deeper-tutorial-py)
    to download the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-beginner-introyt-tensors-deeper-tutorial-py)下载完整的示例代码
- en: '[Introduction](introyt1_tutorial.html) || **Tensors** || [Autograd](autogradyt_tutorial.html)
    || [Building Models](modelsyt_tutorial.html) || [TensorBoard Support](tensorboardyt_tutorial.html)
    || [Training Models](trainingyt.html) || [Model Understanding](captumyt.html)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[介绍](introyt1_tutorial.html) || **张量** || [自动求导](autogradyt_tutorial.html)
    || [构建模型](modelsyt_tutorial.html) || [TensorBoard支持](tensorboardyt_tutorial.html)
    || [训练模型](trainingyt.html) || [模型理解](captumyt.html)'
- en: Follow along with the video below or on [youtube](https://www.youtube.com/watch?v=r7QDUPb2dCM).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 请跟随下面的视频或[YouTube](https://www.youtube.com/watch?v=r7QDUPb2dCM)。
- en: '[https://www.youtube.com/embed/r7QDUPb2dCM](https://www.youtube.com/embed/r7QDUPb2dCM)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/r7QDUPb2dCM](https://www.youtube.com/embed/r7QDUPb2dCM)'
- en: Tensors are the central data abstraction in PyTorch. This interactive notebook
    provides an in-depth introduction to the `torch.Tensor` class.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是PyTorch中的中心数据抽象。这个交互式笔记本提供了对`torch.Tensor`类的深入介绍。
- en: First things first, let’s import the PyTorch module. We’ll also add Python’s
    math module to facilitate some of the examples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入PyTorch模块。我们还将添加Python的math模块以便于一些示例。
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating Tensors
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建张量
- en: 'The simplest way to create a tensor is with the `torch.empty()` call:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 创建张量的最简单方法是使用`torch.empty()`调用：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s unpack what we just did:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解开刚才做的事情：
- en: We created a tensor using one of the numerous factory methods attached to the
    `torch` module.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`torch`模块附加的众多工厂方法之一创建了一个张量。
- en: The tensor itself is 2-dimensional, having 3 rows and 4 columns.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量本身是二维的，有3行和4列。
- en: The type of the object returned is `torch.Tensor`, which is an alias for `torch.FloatTensor`;
    by default, PyTorch tensors are populated with 32-bit floating point numbers.
    (More on data types below.)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回对象的类型是`torch.Tensor`，它是`torch.FloatTensor`的别名；默认情况下，PyTorch张量由32位浮点数填充。（有关数据类型的更多信息请参见下文。）
- en: You will probably see some random-looking values when printing your tensor.
    The `torch.empty()` call allocates memory for the tensor, but does not initialize
    it with any values - so what you’re seeing is whatever was in memory at the time
    of allocation.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打印张量时，您可能会看到一些看起来随机的值。`torch.empty()`调用为张量分配内存，但不会用任何值初始化它 - 所以您看到的是在分配时内存中的内容。
- en: 'A brief note about tensors and their number of dimensions, and terminology:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 关于张量及其维度和术语的简要说明：
- en: You will sometimes see a 1-dimensional tensor called a *vector.*
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时您会看到一个称为*向量*的一维张量。
- en: Likewise, a 2-dimensional tensor is often referred to as a *matrix.*
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，一个二维张量通常被称为*矩阵*。
- en: Anything with more than two dimensions is generally just called a tensor.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超过两个维度的任何内容通常都被称为张量。
- en: 'More often than not, you’ll want to initialize your tensor with some value.
    Common cases are all zeros, all ones, or random values, and the `torch` module
    provides factory methods for all of these:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 往往您会希望用某个值初始化张量。常见情况是全零、全一或随机值，`torch`模块为所有这些情况提供了工厂方法：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The factory methods all do just what you’d expect - we have a tensor full of
    zeros, another full of ones, and another with random values between 0 and 1.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 工厂方法都只是做您期望的事情 - 我们有一个全是零的张量，另一个全是一的张量，还有一个介于0和1之间的随机值的张量。
- en: Random Tensors and Seeding
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机张量和种子
- en: 'Speaking of the random tensor, did you notice the call to `torch.manual_seed()`
    immediately preceding it? Initializing tensors, such as a model’s learning weights,
    with random values is common but there are times - especially in research settings
    - where you’ll want some assurance of the reproducibility of your results. Manually
    setting your random number generator’s seed is the way to do this. Let’s look
    more closely:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 说到随机张量，您是否注意到在其之前立即调用了`torch.manual_seed()`？使用随机值初始化张量，例如模型的学习权重，是常见的，但有时 -
    尤其是在研究环境中 - 您会希望确保结果的可重现性。手动设置随机数生成器的种子是实现这一点的方法。让我们更仔细地看一下：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: What you should see above is that `random1` and `random3` carry identical values,
    as do `random2` and `random4`. Manually setting the RNG’s seed resets it, so that
    identical computations depending on random number should, in most settings, provide
    identical results.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到`random1`和`random3`携带相同的值，`random2`和`random4`也是如此。手动设置RNG的种子会重置它，因此在大多数情况下，依赖随机数的相同计算应该提供相同的结果。
- en: For more information, see the [PyTorch documentation on reproducibility](https://pytorch.org/docs/stable/notes/randomness.html).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅[PyTorch关于可重现性的文档](https://pytorch.org/docs/stable/notes/randomness.html)。
- en: Tensor Shapes
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量形状
- en: 'Often, when you’re performing operations on two or more tensors, they will
    need to be of the same *shape* - that is, having the same number of dimensions
    and the same number of cells in each dimension. For that, we have the `torch.*_like()`
    methods:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当您对两个或更多张量执行操作时，它们需要具有相同的*形状* - 即，具有相同数量的维度和每个维度中相同数量的单元格。为此，我们有`torch.*_like()`方法：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first new thing in the code cell above is the use of the `.shape` property
    on a tensor. This property contains a list of the extent of each dimension of
    a tensor - in our case, `x` is a three-dimensional tensor with shape 2 x 2 x 3.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码单元格中的第一个新内容是在张量上使用`.shape`属性。此属性包含张量每个维度的范围列表 - 在我们的情况下，`x`是一个形状为2 x 2 x
    3的三维张量。
- en: Below that, we call the `.empty_like()`, `.zeros_like()`, `.ones_like()`, and
    `.rand_like()` methods. Using the `.shape` property, we can verify that each of
    these methods returns a tensor of identical dimensionality and extent.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面，我们调用`.empty_like()`、`.zeros_like()`、`.ones_like()`和`.rand_like()`方法。使用`.shape`属性，我们可以验证每个方法返回的张量具有相同的维度和范围。
- en: 'The last way to create a tensor that will cover is to specify its data directly
    from a PyTorch collection:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 创建张量的最后一种方式是直接从PyTorch集合中指定其数据：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using `torch.tensor()` is the most straightforward way to create a tensor if
    you already have data in a Python tuple or list. As shown above, nesting the collections
    will result in a multi-dimensional tensor.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经有一个Python元组或列表中的数据，使用`torch.tensor()`是创建张量的最简单方式。如上所示，嵌套集合将导致一个多维张量。
- en: Note
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`torch.tensor()` creates a copy of the data.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.tensor()`会创建数据的副本。'
- en: Tensor Data Types
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量数据类型
- en: 'Setting the datatype of a tensor is possible a couple of ways:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 设置张量的数据类型有几种方式：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The simplest way to set the underlying data type of a tensor is with an optional
    argument at creation time. In the first line of the cell above, we set `dtype=torch.int16`
    for the tensor `a`. When we print `a`, we can see that it’s full of `1` rather
    than `1.` - Python’s subtle cue that this is an integer type rather than floating
    point.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 设置张量的基础数据类型的最简单方式是在创建时使用可选参数。在上面单元格的第一行中，我们为张量`a`设置了`dtype=torch.int16`。当我们打印`a`时，我们可以看到它充满了`1`而不是`1.`
    - Python微妙地暗示这是一个整数类型而不是浮点数。
- en: Another thing to notice about printing `a` is that, unlike when we left `dtype`
    as the default (32-bit floating point), printing the tensor also specifies its
    `dtype`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 关于打印`a`的另一件事是，与我们将`dtype`保留为默认值(32位浮点数)时不同，打印张量还会指定其`dtype`。
- en: You may have also spotted that we went from specifying the tensor’s shape as
    a series of integer arguments, to grouping those arguments in a tuple. This is
    not strictly necessary - PyTorch will take a series of initial, unlabeled integer
    arguments as a tensor shape - but when adding the optional arguments, it can make
    your intent more readable.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还注意到，我们从将张量的形状指定为一系列整数参数开始，到将这些参数分组在一个元组中。这并不是严格必要的 - PyTorch将一系列初始的、未标记的整数参数作为张量形状
    - 但是当添加可选参数时，可以使您的意图更易读。
- en: The other way to set the datatype is with the `.to()` method. In the cell above,
    we create a random floating point tensor `b` in the usual way. Following that,
    we create `c` by converting `b` to a 32-bit integer with the `.to()` method. Note
    that `c` contains all the same values as `b`, but truncated to integers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 设置数据类型的另一种方式是使用`.to()`方法。在上面的单元格中，我们按照通常的方式创建了一个随机浮点张量`b`。在那之后，我们通过使用`.to()`方法将`b`转换为32位整数来创建`c`。请注意，`c`包含与`b`相同的所有值，但被截断为整数。
- en: 'Available data types include:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的数据类型包括：
- en: '`torch.bool`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.bool`'
- en: '`torch.int8`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.int8`'
- en: '`torch.uint8`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.uint8`'
- en: '`torch.int16`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.int16`'
- en: '`torch.int32`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.int32`'
- en: '`torch.int64`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.int64`'
- en: '`torch.half`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.half`'
- en: '`torch.float`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.float`'
- en: '`torch.double`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.double`'
- en: '`torch.bfloat`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.bfloat`'
- en: Math & Logic with PyTorch Tensors
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch张量进行数学和逻辑运算
- en: Now that you know some of the ways to create a tensor… what can you do with
    them?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了一些创建张量的方式...您可以用它们做什么？
- en: 'Let’s look at basic arithmetic first, and how tensors interact with simple
    scalars:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看一下基本的算术运算，以及张量如何与简单标量交互：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see above, arithmetic operations between tensors and scalars, such
    as addition, subtraction, multiplication, division, and exponentiation are distributed
    over every element of the tensor. Because the output of such an operation will
    be a tensor, you can chain them together with the usual operator precedence rules,
    as in the line where we create `threes`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在上面看到的，张量和标量之间的算术运算，如加法、减法、乘法、除法和指数运算，会分布在张量的每个元素上。因为这样的操作的输出将是一个张量，您可以按照通常的运算符优先规则将它们链接在一起，就像我们创建`threes`的那一行一样。
- en: 'Similar operations between two tensors also behave like you’d intuitively expect:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 两个张量之间的类似操作也会像您直觉地期望的那样：
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It’s important to note here that all of the tensors in the previous code cell
    were of identical shape. What happens when we try to perform a binary operation
    on tensors if dissimilar shape?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是，前一个代码单元中的所有张量的形状都是相同的。当我们尝试在形状不同的张量上执行二进制操作时会发生什么？
- en: Note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The following cell throws a run-time error. This is intentional.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下单元格会抛出运行时错误。这是故意的。
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the general case, you cannot operate on tensors of different shape this way,
    even in a case like the cell above, where the tensors have an identical number
    of elements.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般情况下，您不能以这种方式操作形状不同的张量，即使在上面的单元格中，张量具有相同数量的元素。
- en: 'In Brief: Tensor Broadcasting'
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简而言之：张量广播
- en: Note
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you are familiar with broadcasting semantics in NumPy ndarrays, you’ll find
    the same rules apply here.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉NumPy ndarrays中的广播语义，您会发现这里也适用相同的规则。
- en: 'The exception to the same-shapes rule is *tensor broadcasting.* Here’s an example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与相同形状规则相悖的是*张量广播*。这里是一个例子：
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: What’s the trick here? How is it we got to multiply a 2x4 tensor by a 1x4 tensor?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的诀窍是什么？我们是如何将一个2x4的张量乘以一个1x4的张量的？
- en: Broadcasting is a way to perform an operation between tensors that have similarities
    in their shapes. In the example above, the one-row, four-column tensor is multiplied
    by *both rows* of the two-row, four-column tensor.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 广播是一种在形状相似的张量之间执行操作的方式。在上面的例子中，一行四列的张量与两行四列的张量的*每一行*相乘。
- en: This is an important operation in Deep Learning. The common example is multiplying
    a tensor of learning weights by a *batch* of input tensors, applying the operation
    to each instance in the batch separately, and returning a tensor of identical
    shape - just like our (2, 4) * (1, 4) example above returned a tensor of shape
    (2, 4).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是深度学习中的一个重要操作。一个常见的例子是将学习权重的张量乘以一个*批量*的输入张量，将操作应用于批量中的每个实例，并返回一个形状相同的张量 - 就像我们上面的(2,
    4) * (1, 4)的例子返回了一个形状为(2, 4)的张量。
- en: 'The rules for broadcasting are:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 广播的规则是：
- en: Each tensor must have at least one dimension - no empty tensors.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个张量必须至少有一个维度 - 不能是空张量。
- en: Comparing the dimension sizes of the two tensors, *going from last to first:*
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较两个张量的维度大小，*从最后到第一个维度：*
- en: Each dimension must be equal, *or*
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个维度必须相等，*或*
- en: One of the dimensions must be of size 1, *or*
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中一个维度必须为1，*或*
- en: The dimension does not exist in one of the tensors
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个张量中不存在的维度
- en: Tensors of identical shape, of course, are trivially “broadcastable”, as you
    saw earlier.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，形状相同的张量是可以“广播”的，就像您之前看到的那样。
- en: 'Here are some examples of situations that honor the above rules and allow broadcasting:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是遵守上述规则并允许广播的一些示例情况：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Look closely at the values of each tensor above:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察上面每个张量的值：
- en: The multiplication operation that created `b` was broadcast over every “layer”
    of `a`.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建`b`的乘法操作在`a`的每个“层”上进行了广播。
- en: For `c`, the operation was broadcast over every layer and row of `a` - every
    3-element column is identical.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`c`，操作在`a`的每一层和行上进行了广播 - 每个3元素列都是相同的。
- en: For `d`, we switched it around - now every *row* is identical, across layers
    and columns.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`d`，我们将其调整了一下 - 现在每个*行*在层和列之间都是相同的。
- en: For more information on broadcasting, see the [PyTorch documentation](https://pytorch.org/docs/stable/notes/broadcasting.html)
    on the topic.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有关广播的更多信息，请参阅[PyTorch文档](https://pytorch.org/docs/stable/notes/broadcasting.html)。
- en: 'Here are some examples of attempts at broadcasting that will fail:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些尝试广播但将失败的示例：
- en: Note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The following cell throws a run-time error. This is intentional.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下单元格会引发运行时错误。这是故意的。
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: More Math with Tensors
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多张量数学
- en: PyTorch tensors have over three hundred operations that can be performed on
    them.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch张量有三百多个可以对其执行的操作。
- en: 'Here is a small sample from some of the major categories of operations:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些主要类别操作的小样本：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This is a small sample of operations. For more details and the full inventory
    of math functions, have a look at the [documentation](https://pytorch.org/docs/stable/torch.html#math-operations).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一小部分操作。有关更多详细信息和数学函数的完整清单，请查看[文档](https://pytorch.org/docs/stable/torch.html#math-operations)。
- en: Altering Tensors in Place
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 就地更改张量
- en: Most binary operations on tensors will return a third, new tensor. When we say
    `c = a * b` (where `a` and `b` are tensors), the new tensor `c` will occupy a
    region of memory distinct from the other tensors.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数张量上的二进制操作将返回第三个新张量。当我们说`c = a * b`（其中`a`和`b`是张量）时，新张量`c`将占用与其他张量不同的内存区域。
- en: There are times, though, that you may wish to alter a tensor in place - for
    example, if you’re doing an element-wise computation where you can discard intermediate
    values. For this, most of the math functions have a version with an appended underscore
    (`_`) that will alter a tensor in place.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，有时您可能希望就地更改张量 - 例如，如果您正在进行可以丢弃中间值的逐元素计算。为此，大多数数学函数都有一个附加下划线（`_`）的版本，可以就地更改张量。
- en: 'For example:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For arithmetic operations, there are functions that behave similarly:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于算术操作，有类似的函数行为：
- en: '[PRE27]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that these in-place arithmetic functions are methods on the `torch.Tensor`
    object, not attached to the `torch` module like many other functions (e.g., `torch.sin()`).
    As you can see from `a.add_(b)`, *the calling tensor is the one that gets changed
    in place.*
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些就地算术函数是`torch.Tensor`对象上的方法，而不像许多其他函数（例如`torch.sin()`）附加到`torch`模块。正如您从`a.add_(b)`中看到的那样，*调用张量是在原地更改的*。
- en: 'There is another option for placing the result of a computation in an existing,
    allocated tensor. Many of the methods and functions we’ve seen so far - including
    creation methods! - have an `out` argument that lets you specify a tensor to receive
    the output. If the `out` tensor is the correct shape and `dtype`, this can happen
    without a new memory allocation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种选项，可以将计算结果放入现有的分配张量中。我们迄今为止看到的许多方法和函数 - 包括创建方法！ - 都有一个`out`参数，让您指定一个张量来接收输出。如果`out`张量具有正确的形状和`dtype`，则可以在不进行新的内存分配的情况下完成：
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Copying Tensors
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复制张量
- en: 'As with any object in Python, assigning a tensor to a variable makes the variable
    a *label* of the tensor, and does not copy it. For example:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 与Python中的任何对象一样，将张量分配给变量会使变量成为张量的*标签*，而不是复制它。例如：
- en: '[PRE31]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'But what if you want a separate copy of the data to work on? The `clone()`
    method is there for you:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果您想要一个单独的数据副本进行操作呢？`clone()`方法就是为您准备的：
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**There is an important thing to be aware of when using ``clone()``.** If your
    source tensor has autograd, enabled then so will the clone. **This will be covered
    more deeply in the video on autograd,** but if you want the light version of the
    details, continue on.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**在使用``clone()``时有一件重要的事情需要注意。**如果您的源张量启用了自动求导，那么克隆张量也会启用。**这将在自动求导的视频中更深入地介绍，但如果您想要简要了解详情，请继续阅读。'
- en: '*In many cases, this will be what you want.* For example, if your model has
    multiple computation paths in its `forward()` method, and *both* the original
    tensor and its clone contribute to the model’s output, then to enable model learning
    you want autograd turned on for both tensors. If your source tensor has autograd
    enabled (which it generally will if it’s a set of learning weights or derived
    from a computation involving the weights), then you’ll get the result you want.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*在许多情况下，这可能是您想要的。*例如，如果您的模型在其`forward()`方法中具有多个计算路径，并且*原始张量和其克隆都对模型的输出有贡献*，那么为了启用模型学习，您希望为两个张量启用自动求导。如果您的源张量已启用自动求导（如果它是一组学习权重或从涉及权重的计算派生而来，则通常会启用），那么您将获得所需的结果。'
- en: On the other hand, if you’re doing a computation where *neither* the original
    tensor nor its clone need to track gradients, then as long as the source tensor
    has autograd turned off, you’re good to go.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您进行的计算*既不需要原始张量也不需要其克隆跟踪梯度*，那么只要源张量关闭了自动求导，您就可以继续进行。
- en: '*There is a third case,* though: Imagine you’re performing a computation in
    your model’s `forward()` function, where gradients are turned on for everything
    by default, but you want to pull out some values mid-stream to generate some metrics.
    In this case, you *don’t* want the cloned copy of your source tensor to track
    gradients - performance is improved with autograd’s history tracking turned off.
    For this, you can use the `.detach()` method on the source tensor:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*然而，还有第三种情况:* 想象一下，你正在模型的`forward()`函数中执行计算，其中默认情况下为所有内容打开梯度，但你想要在中间提取一些值以生成一些指标。在这种情况下，你*不*希望克隆源张量跟踪梯度
    - 关闭自动求导历史记录跟踪可以提高性能。为此，你可以在源张量上使用`.detach()`方法：'
- en: '[PRE35]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: What’s happening here?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？
- en: We create `a` with `requires_grad=True` turned on. **We haven’t covered this
    optional argument yet, but will during the unit on autograd.**
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`requires_grad=True`创建`a`。**我们还没有涵盖这个可选参数，但在自动求导单元中会讨论。**
- en: When we print `a`, it informs us that the property `requires_grad=True` - this
    means that autograd and computation history tracking are turned on.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们打印`a`时，它告诉我们属性`requires_grad=True` - 这意味着自动求导和计算历史跟踪已打开。
- en: We clone `a` and label it `b`. When we print `b`, we can see that it’s tracking
    its computation history - it has inherited `a`’s autograd settings, and added
    to the computation history.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们克隆`a`并将其标记为`b`。当我们打印`b`时，我们可以看到它正在跟踪其计算历史 - 它继承了`a`的自动求导设置，并添加到了计算历史中。
- en: We clone `a` into `c`, but we call `detach()` first.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将`a`克隆到`c`，但首先调用`detach()`。
- en: Printing `c`, we see no computation history, and no `requires_grad=True`.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打印`c`，我们看不到计算历史，也没有`requires_grad=True`。
- en: The `detach()` method *detaches the tensor from its computation history.* It
    says, “do whatever comes next as if autograd was off.” It does this *without*
    changing `a` - you can see that when we print `a` again at the end, it retains
    its `requires_grad=True` property.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`detach()`方法*将张量与其计算历史分离*。它表示，“接下来的操作就好像自动求导已关闭一样。” 它在*不*更改`a`的情况下执行此操作 - 当我们最后再次打印`a`时，你会看到它保留了`requires_grad=True`属性。'
- en: Moving to GPU
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移到GPU
- en: One of the major advantages of PyTorch is its robust acceleration on CUDA-compatible
    Nvidia GPUs. (“CUDA” stands for *Compute Unified Device Architecture*, which is
    Nvidia’s platform for parallel computing.) So far, everything we’ve done has been
    on CPU. How do we move to the faster hardware?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的一个主要优势是其在CUDA兼容的Nvidia GPU上的强大加速。 （“CUDA”代表*Compute Unified Device Architecture*，这是Nvidia用于并行计算的平台。）到目前为止，我们所做的一切都是在CPU上进行的。我们如何转移到更快的硬件？
- en: First, we should check whether a GPU is available, with the `is_available()`
    method.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该检查GPU是否可用，使用`is_available()`方法。
- en: Note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you do not have a CUDA-compatible GPU and CUDA drivers installed, the executable
    cells in this section will not execute any GPU-related code.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有CUDA兼容的GPU和安装了CUDA驱动程序，本节中的可执行单元格将不会执行任何与GPU相关的代码。
- en: '[PRE37]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Once we’ve determined that one or more GPUs is available, we need to put our
    data someplace where the GPU can see it. Your CPU does computation on data in
    your computer’s RAM. Your GPU has dedicated memory attached to it. Whenever you
    want to perform a computation on a device, you must move *all* the data needed
    for that computation to memory accessible by that device. (Colloquially, “moving
    the data to memory accessible by the GPU” is shorted to, “moving the data to the
    GPU”.)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定有一个或多个GPU可用，我们需要将数据放在GPU可以看到的地方。你的CPU在计算时使用计算机RAM中的数据。你的GPU有专用内存附加在上面。每当你想在设备上执行计算时，你必须将进行该计算所需的*所有*数据移动到该设备可访问的内存中。
    （口语上，“将数据移动到GPU可访问的内存”缩写为“将数据移动到GPU”。）
- en: 'There are multiple ways to get your data onto your target device. You may do
    it at creation time:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以将数据放在目标设备上。你可以在创建时执行：
- en: '[PRE39]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: By default, new tensors are created on the CPU, so we have to specify when we
    want to create our tensor on the GPU with the optional `device` argument. You
    can see when we print the new tensor, PyTorch informs us which device it’s on
    (if it’s not on CPU).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，新张量是在CPU上创建的，因此我们必须在想要使用可选`device`参数在GPU上创建张量时指定。当我们打印新张量时，你可以看到PyTorch告诉我们它在哪个设备上（如果不在CPU上）。
- en: 'You can query the number of GPUs with `torch.cuda.device_count()`. If you have
    more than one GPU, you can specify them by index: `device=''cuda:0''`, `device=''cuda:1''`,
    etc.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`torch.cuda.device_count()`查询GPU的数量。如果你有多个GPU，你可以通过索引指定它们：`device='cuda:0'`，`device='cuda:1'`等。
- en: 'As a coding practice, specifying our devices everywhere with string constants
    is pretty fragile. In an ideal world, your code would perform robustly whether
    you’re on CPU or GPU hardware. You can do this by creating a device handle that
    can be passed to your tensors instead of a string:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 作为编码实践，在所有地方使用字符串常量指定设备是相当脆弱的。在理想情况下，无论你是在CPU还是GPU硬件上，你的代码都应该表现稳健。你可以通过创建一个设备句柄来实现这一点，该句柄可以传递给你的张量，而不是一个字符串：
- en: '[PRE41]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: If you have an existing tensor living on one device, you can move it to another
    with the `to()` method. The following line of code creates a tensor on CPU, and
    moves it to whichever device handle you acquired in the previous cell.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个现有张量存在于一个设备上，你可以使用`to()`方法将其移动到另一个设备上。以下代码行在CPU上创建一个张量，并将其移动到你在前一个单元格中获取的设备句柄。
- en: '[PRE43]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'It is important to know that in order to do computation involving two or more
    tensors, *all of the tensors must be on the same device*. The following code will
    throw a runtime error, regardless of whether you have a GPU device available:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要知道，为了进行涉及两个或更多张量的计算，*所有张量必须在同一设备上*。无论你是否有GPU设备可用，以下代码都会抛出运行时错误：
- en: '[PRE44]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Manipulating Tensor Shapes
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作张量形状
- en: Sometimes, you’ll need to change the shape of your tensor. Below, we’ll look
    at a few common cases, and how to handle them.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你需要改变张量的形状。下面，我们将看几种常见情况以及如何处理它们。
- en: Changing the Number of Dimensions
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改变维度数量
- en: One case where you might need to change the number of dimensions is passing
    a single instance of input to your model. PyTorch models generally expect *batches*
    of input.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种情况可能需要改变维度的数量，就是向模型传递单个输入实例。PyTorch模型通常期望输入的*批量*。
- en: For example, imagine having a model that works on 3 x 226 x 226 images - a 226-pixel
    square with 3 color channels. When you load and transform it, you’ll get a tensor
    of shape `(3, 226, 226)`. Your model, though, is expecting input of shape `(N,
    3, 226, 226)`, where `N` is the number of images in the batch. So how do you make
    a batch of one?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一个模型处理3 x 226 x 226的图像 - 一个有3个颜色通道的226像素正方形。当你加载和转换它时，你会得到一个形状为`(3, 226,
    226)`的张量。然而，你的模型期望的输入形状是`(N, 3, 226, 226)`，其中`N`是批量中图像的数量。那么如何制作一个批量为1的批次？
- en: '[PRE45]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The `unsqueeze()` method adds a dimension of extent 1. `unsqueeze(0)` adds it
    as a new zeroth dimension - now you have a batch of one!
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`unsqueeze()`方法添加一个长度为1的维度。`unsqueeze(0)`将其添加为一个新的第零维 - 现在你有一个批量为1的张量！'
- en: So if that’s *un*squeezing? What do we mean by squeezing? We’re taking advantage
    of the fact that any dimension of extent 1 *does not* change the number of elements
    in the tensor.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如果是*挤压*呢？我们所说的挤压是什么意思？我们利用了一个事实，即任何维度的长度为1 *不会*改变张量中的元素数量。
- en: '[PRE47]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Continuing the example above, let’s say the model’s output is a 20-element vector
    for each input. You would then expect the output to have shape `(N, 20)`, where
    `N` is the number of instances in the input batch. That means that for our single-input
    batch, we’ll get an output of shape `(1, 20)`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 继续上面的例子，假设模型的输出对于每个输入是一个20元素向量。那么你期望输出的形状是`(N, 20)`，其中`N`是输入批次中的实例数。这意味着对于我们的单输入批次，我们将得到一个形状为`(1,
    20)`的输出。
- en: What if you want to do some *non-batched* computation with that output - something
    that’s just expecting a 20-element vector?
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想对这个输出进行一些*非批量*计算 - 比如只期望一个20元素的向量，怎么办？
- en: '[PRE49]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: You can see from the shapes that our 2-dimensional tensor is now 1-dimensional,
    and if you look closely at the output of the cell above you’ll see that printing
    `a` shows an “extra” set of square brackets `[]` due to having an extra dimension.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从形状可以看出，我们的二维张量现在是一维的，如果你仔细看上面单元格的输出，你会发现打印`a`会显示一个“额外”的方括号`[]`，因为有一个额外的维度。
- en: You may only `squeeze()` dimensions of extent 1\. See above where we try to
    squeeze a dimension of size 2 in `c`, and get back the same shape we started with.
    Calls to `squeeze()` and `unsqueeze()` can only act on dimensions of extent 1
    because to do otherwise would change the number of elements in the tensor.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你只能`squeeze()`长度为1的维度。看上面我们尝试在`c`中挤压一个大小为2的维度，最终得到的形状与开始时相同。调用`squeeze()`和`unsqueeze()`只能作用于长度为1的维度，因为否则会改变张量中的元素数量。
- en: 'Another place you might use `unsqueeze()` is to ease broadcasting. Recall the
    example above where we had the following code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能使用`unsqueeze()`的地方是为了简化广播。回想一下我们之前的代码示例：
- en: '[PRE51]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The net effect of that was to broadcast the operation over dimensions 0 and
    2, causing the random, 3 x 1 tensor to be multiplied element-wise by every 3-element
    column in `a`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的净效果是在维度0和2上广播操作，导致随机的3 x 1张量与`a`中的每个3元素列进行逐元素相乘。
- en: 'What if the random vector had just been 3-element vector? We’d lose the ability
    to do the broadcast, because the final dimensions would not match up according
    to the broadcasting rules. `unsqueeze()` comes to the rescue:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果随机向量只是一个3元素向量怎么办？我们将失去进行广播的能力，因为最终的维度不会根据广播规则匹配。`unsqueeze()`来拯救：
- en: '[PRE52]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `squeeze()` and `unsqueeze()` methods also have in-place versions, `squeeze_()`
    and `unsqueeze_()`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`squeeze()`和`unsqueeze()`方法也有原地版本，`squeeze_()`和`unsqueeze_()`：'
- en: '[PRE54]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Sometimes you’ll want to change the shape of a tensor more radically, while
    still preserving the number of elements and their contents. One case where this
    happens is at the interface between a convolutional layer of a model and a linear
    layer of the model - this is common in image classification models. A convolution
    kernel will yield an output tensor of shape *features x width x height,* but the
    following linear layer expects a 1-dimensional input. `reshape()` will do this
    for you, provided that the dimensions you request yield the same number of elements
    as the input tensor has:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你会想要更彻底地改变张量的形状，同时仍保留元素数量和内容。一个这种情况是在模型的卷积层和线性层之间的接口处 - 这在图像分类模型中很常见。卷积核会产生一个形状为*特征
    x 宽 x 高*的输出张量，但接下来的线性层期望一个一维输入。`reshape()`会为你做这个，只要你请求的维度产生的元素数量与输入张量相同即可：
- en: '[PRE56]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `(6 * 20 * 20,)` argument in the final line of the cell above is because
    PyTorch expects a **tuple** when specifying a tensor shape - but when the shape
    is the first argument of a method, it lets us cheat and just use a series of integers.
    Here, we had to add the parentheses and comma to convince the method that this
    is really a one-element tuple.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 上面单元格最后一行的`(6 * 20 * 20,)`参数是因为PyTorch在指定张量形状时期望一个**元组** - 但当形状是方法的第一个参数时，它允许我们欺骗并只使用一系列整数。在这里，我们必须添加括号和逗号来说服方法，让它相信这实际上是一个单元素元组。
- en: When it can, `reshape()` will return a *view* on the tensor to be changed -
    that is, a separate tensor object looking at the same underlying region of memory.
    *This is important:* That means any change made to the source tensor will be reflected
    in the view on that tensor, unless you `clone()` it.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在可以的情况下，`reshape()`会返回一个*视图*，即一个查看相同底层内存区域的独立张量对象以进行更改。*这很重要：*这意味着对源张量进行的任何更改都会反映在该张量的视图中，除非你使用`clone()`。
- en: There *are* conditions, beyond the scope of this introduction, where `reshape()`
    has to return a tensor carrying a copy of the data. For more information, see
    the [docs](https://pytorch.org/docs/stable/torch.html#torch.reshape).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个介绍范围之外，`reshape()`有时必须返回一个携带数据副本的张量。更多信息请参阅[文档](https://pytorch.org/docs/stable/torch.html#torch.reshape)。
- en: NumPy Bridge
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NumPy桥接
- en: In the section above on broadcasting, it was mentioned that PyTorch’s broadcast
    semantics are compatible with NumPy’s - but the kinship between PyTorch and NumPy
    goes even deeper than that.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面关于广播的部分中提到，PyTorch的广播语义与NumPy的兼容 - 但PyTorch和NumPy之间的关系甚至比这更深。
- en: 'If you have existing ML or scientific code with data stored in NumPy ndarrays,
    you may wish to express that same data as PyTorch tensors, whether to take advantage
    of PyTorch’s GPU acceleration, or its efficient abstractions for building ML models.
    It’s easy to switch between ndarrays and PyTorch tensors:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有现有的ML或科学代码，并且数据存储在NumPy的ndarrays中，您可能希望将相同的数据表示为PyTorch张量，无论是为了利用PyTorch的GPU加速，还是为了利用其构建ML模型的高效抽象。在ndarrays和PyTorch张量之间轻松切换：
- en: '[PRE58]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: PyTorch creates a tensor of the same shape and containing the same data as the
    NumPy array, going so far as to keep NumPy’s default 64-bit float data type.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch创建一个与NumPy数组形状相同且包含相同数据的张量，甚至保留NumPy的默认64位浮点数据类型。
- en: 'The conversion can just as easily go the other way:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 转换也可以同样轻松地进行另一种方式：
- en: '[PRE60]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'It is important to know that these converted objects are using *the same underlying
    memory* as their source objects, meaning that changes to one are reflected in
    the other:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要知道，这些转换后的对象使用*相同的底层内存*作为它们的源对象，这意味着对一个对象的更改会反映在另一个对象中：
- en: '[PRE62]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '**Total running time of the script:** ( 0 minutes 0.294 seconds)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（0分钟0.294秒）'
- en: '[`Download Python source code: tensors_deeper_tutorial.py`](../../_downloads/be017e7b39198fdf668c138fd8d57abe/tensors_deeper_tutorial.py)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：tensors_deeper_tutorial.py`](../../_downloads/be017e7b39198fdf668c138fd8d57abe/tensors_deeper_tutorial.py)'
- en: '[`Download Jupyter notebook: tensors_deeper_tutorial.ipynb`](../../_downloads/63a0f0fc7b3ffb15d3a5ac8db3d521ee/tensors_deeper_tutorial.ipynb)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：tensors_deeper_tutorial.ipynb`](../../_downloads/63a0f0fc7b3ffb15d3a5ac8db3d521ee/tensors_deeper_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[由Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)'
