- en: Customize Process Group Backends Using Cpp Extensions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Cpp扩展自定义流程组后端
- en: 原文：[https://pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html](https://pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html](https://pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html)'
- en: '**Author**: Howard Huang <https://github.com/H-Huang>, [Feng Tian](https://github.com/ftian1),
    [Shen Li](https://mrshenli.github.io/), [Min Si](https://minsii.github.io/)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：Howard Huang <https://github.com/H-Huang>，[Feng Tian](https://github.com/ftian1)，[Shen
    Li](https://mrshenli.github.io/)，[Min Si](https://minsii.github.io/)'
- en: Note
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '![edit](../Images/a8aa37bcc5edbf2ba5fcf18dba1e55f9.png) View and edit this
    tutorial in [github](https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '![edit](../Images/a8aa37bcc5edbf2ba5fcf18dba1e55f9.png) 在 [github](https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst)
    上查看并编辑本教程。'
- en: 'Prerequisites:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '先决条件:'
- en: '[PyTorch Distributed Overview](../beginner/dist_overview.html)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 分布式概述](../beginner/dist_overview.html)'
- en: '[PyTorch Collective Communication Package](https://pytorch.org/docs/stable/distributed.html)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 集体通信包](https://pytorch.org/docs/stable/distributed.html)'
- en: '[PyTorch Cpp Extension](https://pytorch.org/docs/stable/cpp_extension.html)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch Cpp 扩展](https://pytorch.org/docs/stable/cpp_extension.html)'
- en: '[Writing Distributed Applications with PyTorch](https://pytorch.org/tutorials/intermediate/dist_tuto.html)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 编写分布式应用程序](https://pytorch.org/tutorials/intermediate/dist_tuto.html)'
- en: This tutorial demonstrates how to implement a custom `Backend` and plug that
    into [PyTorch distributed package](https://pytorch.org/docs/stable/distributed.html)
    using [cpp extensions](https://pytorch.org/docs/stable/cpp_extension.html). This
    is helpful when you need a specialized software stack for your hardware, or when
    you would like to experiment with new collective communication algorithms.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程演示了如何实现一个自定义的`Backend`并将其插入[PyTorch分布式包](https://pytorch.org/docs/stable/distributed.html)，使用[cpp扩展](https://pytorch.org/docs/stable/cpp_extension.html)。当您需要为硬件定制专门的软件堆栈，或者想要尝试新的集体通信算法时，这将非常有帮助。
- en: Basics
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础知识
- en: PyTorch collective communications power several widely adopted distributed training
    features, including [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html),
    [ZeroRedundancyOptimizer](https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer),
    [FullyShardedDataParallel](https://github.com/pytorch/pytorch/blob/master/torch/distributed/_fsdp/fully_sharded_data_parallel.py).
    In order to make the same collective communication API work with different communication
    backends, the distributed package abstracts collective communication operations
    into a [Backend](https://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/Backend.hpp)
    class. Different backends can then be implemented as subclasses of `Backend` using
    preferred third-party libraries. PyTorch distributed comes with three default
    backends, `ProcessGroupNCCL`, `ProcessGroupGloo`, and `ProcessGroupMPI`. However,
    beyond these three backends, there are also other communication libraries (e.g.,
    [UCC](https://github.com/openucx/ucc), [OneCCL](https://github.com/oneapi-src/oneCCL)),
    different types of hardware (e.g., [TPU](https://cloud.google.com/tpu), [Trainum](https://aws.amazon.com/machine-learning/trainium/)),
    and emerging communication algorithms (e.g., [Herring](https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud),
    [Reduction Server](https://cloud.google.com/blog/topics/developers-practitioners/optimize-training-performance-reduction-server-vertex-ai)).
    Therefore, the distributed package exposes extension APIs to allow customizing
    collective communication backends.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch集体通信支持多种广泛采用的分布式训练功能，包括[DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)，[ZeroRedundancyOptimizer](https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer)，[FullyShardedDataParallel](https://github.com/pytorch/pytorch/blob/master/torch/distributed/_fsdp/fully_sharded_data_parallel.py)。为了使相同的集体通信API能够与不同的通信后端一起工作，分布式包将集体通信操作抽象为[Backend](https://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/Backend.hpp)类。不同的后端可以作为`Backend`的子类使用首选的第三方库来实现。PyTorch分布式带有三个默认后端，`ProcessGroupNCCL`，`ProcessGroupGloo`和`ProcessGroupMPI`。然而，除了这三个后端之外，还有其他通信库（例如[UCC](https://github.com/openucx/ucc)，[OneCCL](https://github.com/oneapi-src/oneCCL)），不同类型的硬件（例如[TPU](https://cloud.google.com/tpu)，[Trainum](https://aws.amazon.com/machine-learning/trainium/)）和新兴的通信算法（例如[Herring](https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud)，[Reduction
    Server](https://cloud.google.com/blog/topics/developers-practitioners/optimize-training-performance-reduction-server-vertex-ai)）。因此，分布式包提供了扩展API来允许定制集体通信后端。
- en: The 4 steps below show how to implement a dummy `Backend` backend and use that
    in Python application code. Please note that this tutorial focuses on demonstrating
    the extension APIs, instead of developing a functioning communication backend.
    Hence, the `dummy` backend just covers a subset of the APIs (`all_reduce` and
    `all_gather`), and simply sets the values of tensors to 0.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下4个步骤展示了如何在Python应用程序代码中实现一个虚拟的`Backend`后端并使用它。请注意，本教程侧重于演示扩展API，而不是开发一个功能完善的通信后端。因此，`dummy`后端只涵盖了API的一个子集（`all_reduce`和`all_gather`），并且只是将张量的值设置为0。
- en: 'Step 1: Implement a Subclass of `Backend`'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：实现`Backend`的子类
- en: This first step is to implement a `Backend` subclass that overrides target collective
    communication APIs and runs the custom communication algorithm. The extension
    also needs to implement a `Work` subclass, which serves as a future of communication
    results and allows asynchronous execution in application code. If the extension
    uses third-party libraries, it can include the headers and call into the library
    APIs from the `BackendDummy` subclass. The two code snippets below present the
    implementation of `dummy.h` and `dummy.cpp`. See the [dummy collectives](https://github.com/H-Huang/torch_collective_extension)
    repository for the full implementation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是实现一个`Backend`子类，覆盖目标集体通信API，并运行自定义通信算法。扩展还需要实现一个`Work`子类，作为通信结果的future，并允许在应用代码中异步执行。如果扩展使用第三方库，可以在`BackendDummy`子类中包含头文件并调用库API。下面的两个代码片段展示了`dummy.h`和`dummy.cpp`的实现。请查看[dummy
    collectives](https://github.com/H-Huang/torch_collective_extension)存储库以获取完整的实现。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 2: Expose The Extension Python APIs'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：暴露扩展Python API
- en: The backend constructors are called [from Python side](https://github.com/pytorch/pytorch/blob/v1.9.0/torch/distributed/distributed_c10d.py#L643-L650),
    so the extension also needs to expose the constructor APIs to Python. This can
    be done by adding the following methods. In this example, `store` and `timeout`
    are ignored by the `BackendDummy` instantiation method, as those are not used
    in this dummy implementation. However, real-world extensions should consider using
    the `store` to perform rendezvous and supporting the `timeout` argument.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 后端构造函数是从Python端调用的，因此扩展还需要向Python公开构造函数API。这可以通过添加以下方法来实现。在这个例子中，`store`和`timeout`被`BackendDummy`实例化方法忽略，因为在这个虚拟实现中没有使用它们。然而，真实世界的扩展应该考虑使用`store`来执行会合并支持`timeout`参数。
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 3: Build The Custom Extension'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：构建自定义扩展
- en: Now, the extension source code files are ready. We can then use [cpp extensions](https://pytorch.org/docs/stable/cpp_extension.html)
    to build it. To do that, create a `setup.py` file that prepares the paths and
    commands. Then call `python setup.py develop` to install the extension.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，扩展源代码文件已经准备好。我们可以使用[cpp extensions](https://pytorch.org/docs/stable/cpp_extension.html)来构建它。为此，创建一个`setup.py`文件，准备路径和命令。然后调用`python
    setup.py develop`来安装扩展。
- en: If the extension depends on third-party libraries, you can also specify `libraries_dirs`
    and `libraries` to the cpp extension APIs. See the [torch ucc](https://github.com/openucx/torch-ucc)
    project as a real-world example.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果扩展依赖于第三方库，您还可以在cpp扩展API中指定`libraries_dirs`和`libraries`。请参考[torch ucc](https://github.com/openucx/torch-ucc)项目作为一个真实的例子。
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 4: Use The Extension in Application'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4：在应用程序中使用扩展。
- en: After installation, you can conveniently use the `dummy` backend when calling
    [init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)
    as if it is an builtin backend.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，您可以在调用[init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)时方便地使用`dummy`后端，就像它是一个内置后端一样。
- en: We can specify dispatching based on backend by changing the `backend` argument
    of `init_process_group`. We can dispatch collective with CPU tensor to `gloo`
    backend and dispatch collective with CUDA tensor to `dummy` backend by specifying
    `cpu:gloo,cuda:dummy` as the backend argument.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据后端来指定调度，方法是改变`init_process_group`的`backend`参数。我们可以通过将后端参数指定为`cpu:gloo,cuda:dummy`，将CPU张量的集体分发到`gloo`后端，将CUDA张量的集体分发到`dummy`后端。
- en: To send all tensors to `dummy` backend, we can simply specify `dummy` as the
    backend argument.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要将所有张量发送到“dummy”后端，我们可以简单地将“dummy”指定为后端参数。
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
