- en: 'NLP From Scratch: Classifying Names with a Character-Level RNN'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始的自然语言处理：使用字符级RNN对名称进行分类
- en: 原文：[https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-intermediate-char-rnn-classification-tutorial-py)
    to download the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-intermediate-char-rnn-classification-tutorial-py)下载完整的示例代码
- en: '**Author**: [Sean Robertson](https://github.com/spro)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者**：[Sean Robertson](https://github.com/spro)'
- en: 'We will be building and training a basic character-level Recurrent Neural Network
    (RNN) to classify words. This tutorial, along with two other Natural Language
    Processing (NLP) “from scratch” tutorials [NLP From Scratch: Generating Names
    with a Character-Level RNN](char_rnn_generation_tutorial.html) and [NLP From Scratch:
    Translation with a Sequence to Sequence Network and Attention](seq2seq_translation_tutorial.html),
    show how to preprocess data to model NLP. In particular these tutorials do not
    use many of the convenience functions of torchtext, so you can see how preprocessing
    to model NLP works at a low level.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将构建和训练一个基本的字符级循环神经网络（RNN）来对单词进行分类。本教程以及其他两个“从头开始”的自然语言处理（NLP）教程[NLP From
    Scratch: Generating Names with a Character-Level RNN](char_rnn_generation_tutorial.html)和[NLP
    From Scratch: Translation with a Sequence to Sequence Network and Attention](seq2seq_translation_tutorial.html)，展示了如何预处理数据以建模NLP。特别是这些教程不使用torchtext的许多便利函数，因此您可以看到如何在低级别上处理NLP以建模NLP。'
- en: A character-level RNN reads words as a series of characters - outputting a prediction
    and “hidden state” at each step, feeding its previous hidden state into each next
    step. We take the final prediction to be the output, i.e. which class the word
    belongs to.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 字符级RNN将单词作为一系列字符读取 - 在每一步输出一个预测和“隐藏状态”，将其先前的隐藏状态馈送到每个下一步。我们将最终预测视为输出，即单词属于哪个类别。
- en: 'Specifically, we’ll train on a few thousand surnames from 18 languages of origin,
    and predict which language a name is from based on the spelling:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将在来自18种语言的几千个姓氏上进行训练，并根据拼写预测名称来自哪种语言：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Recommended Preparation
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐准备工作
- en: 'Before starting this tutorial it is recommended that you have installed PyTorch,
    and have a basic understanding of Python programming language and Tensors:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本教程之前，建议您已经安装了PyTorch，并对Python编程语言和张量有基本的了解：
- en: '[https://pytorch.org/](https://pytorch.org/) For installation instructions'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/](https://pytorch.org/)获取安装说明'
- en: '[Deep Learning with PyTorch: A 60 Minute Blitz](../beginner/deep_learning_60min_blitz.html)
    to get started with PyTorch in general and learn the basics of Tensors'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyTorch进行深度学习：60分钟入门](../beginner/deep_learning_60min_blitz.html)以开始使用PyTorch并学习张量的基础知识'
- en: '[Learning PyTorch with Examples](../beginner/pytorch_with_examples.html) for
    a wide and deep overview'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过示例学习PyTorch](../beginner/pytorch_with_examples.html)提供广泛和深入的概述'
- en: '[PyTorch for Former Torch Users](../beginner/former_torchies_tutorial.html)
    if you are former Lua Torch user'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您以前是Lua Torch用户，请参阅[PyTorch for Former Torch Users](../beginner/former_torchies_tutorial.html)
- en: 'It would also be useful to know about RNNs and how they work:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 了解RNN以及它们的工作原理也会很有用：
- en: '[The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    shows a bunch of real life examples'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[循环神经网络的不合理有效性](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)展示了一堆现实生活中的例子'
- en: '[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    is about LSTMs specifically but also informative about RNNs in general'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解LSTM网络](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)专门讨论LSTMs，但也对RNNs有启发性'
- en: Preparing the Data
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Download the data from [here](https://download.pytorch.org/tutorial/data.zip)
    and extract it to the current directory.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从[这里](https://download.pytorch.org/tutorial/data.zip)下载数据并将其解压缩到当前目录。
- en: Included in the `data/names` directory are 18 text files named as `[Language].txt`.
    Each file contains a bunch of names, one name per line, mostly romanized (but
    we still need to convert from Unicode to ASCII).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`data/names`目录中包含18个名为`[Language].txt`的文本文件。每个文件包含一堆名称，每行一个名称，大多数是罗马化的（但我们仍然需要从Unicode转换为ASCII）。'
- en: 'We’ll end up with a dictionary of lists of names per language, `{language:
    [names ...]}`. The generic variables “category” and “line” (for language and name
    in our case) are used for later extensibility.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '我们最终会得到一个字典，其中包含每种语言的名称列表，`{language: [names ...]}`。通用变量“category”和“line”（在我们的案例中用于语言和名称）用于以后的可扩展性。'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we have `category_lines`, a dictionary mapping each category (language)
    to a list of lines (names). We also kept track of `all_categories` (just a list
    of languages) and `n_categories` for later reference.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有`category_lines`，一个将每个类别（语言）映射到一系列行（名称）的字典。我们还跟踪了`all_categories`（只是一个语言列表）和`n_categories`以供以后参考。
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Turning Names into Tensors
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将名称转换为张量
- en: Now that we have all the names organized, we need to turn them into Tensors
    to make any use of them.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经组织好所有的名称，我们需要将它们转换为张量以便使用。
- en: To represent a single letter, we use a “one-hot vector” of size `<1 x n_letters>`.
    A one-hot vector is filled with 0s except for a 1 at index of the current letter,
    e.g. `"b" = <0 1 0 0 0 ...>`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示单个字母，我们使用大小为`<1 x n_letters>`的“one-hot向量”。一个one-hot向量除了当前字母的索引处为1之外，其他位置都填充为0，例如，`"b"
    = <0 1 0 0 0 ...>`。
- en: To make a word we join a bunch of those into a 2D matrix `<line_length x 1 x
    n_letters>`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构成一个单词，我们将其中的一堆连接成一个2D矩阵`<line_length x 1 x n_letters>`。
- en: That extra 1 dimension is because PyTorch assumes everything is in batches -
    we’re just using a batch size of 1 here.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的1维是因为PyTorch假设一切都是批处理 - 我们这里只是使用批处理大小为1。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Creating the Network
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建网络
- en: Before autograd, creating a recurrent neural network in Torch involved cloning
    the parameters of a layer over several timesteps. The layers held hidden state
    and gradients which are now entirely handled by the graph itself. This means you
    can implement a RNN in a very “pure” way, as regular feed-forward layers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动求导之前，在Torch中创建一个循环神经网络涉及在几个时间步上克隆层的参数。这些层保存了隐藏状态和梯度，现在完全由图本身处理。这意味着您可以以非常“纯粹”的方式实现RNN，就像常规的前馈层一样。
- en: This RNN module (mostly copied from [the PyTorch for Torch users tutorial](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net))
    is just 2 linear layers which operate on an input and hidden state, with a `LogSoftmax`
    layer after the output.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个RNN模块（主要是从[PyTorch for Torch用户教程](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net)中复制的）只是在输入和隐藏状态上操作的2个线性层，输出后是一个`LogSoftmax`层。
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To run a step of this network we need to pass an input (in our case, the Tensor
    for the current letter) and a previous hidden state (which we initialize as zeros
    at first). We’ll get back the output (probability of each language) and a next
    hidden state (which we keep for the next step).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此网络的一步，我们需要传递一个输入（在我们的情况下，是当前字母的张量）和一个先前的隐藏状态（最初我们将其初始化为零）。我们将得到输出（每种语言的概率）和下一个隐藏状态（我们将其保留到下一步）。
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For the sake of efficiency we don’t want to be creating a new Tensor for every
    step, so we will use `lineToTensor` instead of `letterToTensor` and use slices.
    This could be further optimized by precomputing batches of Tensors.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高效率，我们不希望为每一步创建一个新的张量，因此我们将使用`lineToTensor`代替`letterToTensor`并使用切片。这可以通过预先计算批量张量来进一步优化。
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see the output is a `<1 x n_categories>` Tensor, where every item
    is the likelihood of that category (higher is more likely).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，输出是一个`<1 x n_categories>`张量，其中每个项目都是该类别的可能性（可能性越高，越可能）。
- en: Training
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Preparing for Training
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为训练做准备
- en: 'Before going into training we should make a few helper functions. The first
    is to interpret the output of the network, which we know to be a likelihood of
    each category. We can use `Tensor.topk` to get the index of the greatest value:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行训练之前，我们应该编写一些辅助函数。第一个是解释网络输出的函数，我们知道它是每个类别的可能性。我们可以使用`Tensor.topk`来获取最大值的索引：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will also want a quick way to get a training example (a name and its language):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望快速获取一个训练示例（一个名称及其语言）：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Training the Network
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练网络
- en: Now all it takes to train this network is show it a bunch of examples, have
    it make guesses, and tell it if it’s wrong.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练这个网络所需的全部工作就是向其展示一堆示例，让它猜测，并告诉它是否错误。
- en: For the loss function `nn.NLLLoss` is appropriate, since the last layer of the
    RNN is `nn.LogSoftmax`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于损失函数，`nn.NLLLoss`是合适的，因为RNN的最后一层是`nn.LogSoftmax`。
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Each loop of training will:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 每次训练循环将：
- en: Create input and target tensors
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建输入和目标张量
- en: Create a zeroed initial hidden state
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个初始化的零隐藏状态
- en: Read each letter in and
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐个读取每个字母
- en: Keep hidden state for next letter
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留下一个字母的隐藏状态
- en: Compare final output to target
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将最终输出与目标进行比较
- en: Back-propagate
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Return the output and loss
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回输出和损失
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we just have to run that with a bunch of examples. Since the `train` function
    returns both the output and loss we can print its guesses and also keep track
    of loss for plotting. Since there are 1000s of examples we print only every `print_every`
    examples, and take an average of the loss.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需运行一堆示例。由于`train`函数返回输出和损失，我们可以打印其猜测并跟踪损失以绘图。由于有成千上万的示例，我们仅打印每`print_every`个示例，并计算损失的平均值。
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Plotting the Results
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绘制结果
- en: 'Plotting the historical loss from `all_losses` shows the network learning:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制`all_losses`中的历史损失可以显示网络的学习情况：
- en: '[PRE19]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![char rnn classification tutorial](../Images/cc57a36a43d450df4bfc1d1d1b1ce274.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![char rnn分类教程](../Images/cc57a36a43d450df4bfc1d1d1b1ce274.png)'
- en: '[PRE20]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Evaluating the Results
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估结果
- en: To see how well the network performs on different categories, we will create
    a confusion matrix, indicating for every actual language (rows) which language
    the network guesses (columns). To calculate the confusion matrix a bunch of samples
    are run through the network with `evaluate()`, which is the same as `train()`
    minus the backprop.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看网络在不同类别上的表现如何，我们将创建一个混淆矩阵，指示对于每种实际语言（行），网络猜测的是哪种语言（列）。为了计算混淆矩阵，一堆样本通过网络运行`evaluate()`，这与`train()`相同，但没有反向传播。
- en: '[PRE21]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![char rnn classification tutorial](../Images/029a9d26725997aae97e9e3f6f10067f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![char rnn分类教程](../Images/029a9d26725997aae97e9e3f6f10067f.png)'
- en: '[PRE22]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can pick out bright spots off the main axis that show which languages it
    guesses incorrectly, e.g. Chinese for Korean, and Spanish for Italian. It seems
    to do very well with Greek, and very poorly with English (perhaps because of overlap
    with other languages).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从主轴上的亮点中挑选出显示它错误猜测的语言，例如将韩语错误猜测为中文，将意大利语错误猜测为西班牙语。它在希腊语方面表现得非常好，但在英语方面表现非常糟糕（可能是因为与其他语言的重叠）。
- en: Running on User Input
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在用户输入上运行
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The final versions of the scripts [in the Practical PyTorch repo](https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification)
    split the above code into a few files:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的最终版本在[Practical PyTorch存储库](https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification)中将上述代码拆分为几个文件：
- en: '`data.py` (loads files)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.py`（加载文件）'
- en: '`model.py` (defines the RNN)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.py`（定义RNN）'
- en: '`train.py` (runs training)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train.py`（运行训练）'
- en: '`predict.py` (runs `predict()` with command line arguments)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict.py`（使用命令行参数运行`predict()`）'
- en: '`server.py` (serve prediction as a JSON API with `bottle.py`)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`server.py`（使用`bottle.py`作为JSON API提供预测）'
- en: Run `train.py` to train and save the network.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`train.py`以训练并保存网络。
- en: 'Run `predict.py` with a name to view predictions:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`predict.py`并输入一个名称以查看预测：
- en: '[PRE25]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Run `server.py` and visit [http://localhost:5533/Yourname](http://localhost:5533/Yourname)
    to get JSON output of predictions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`server.py`并访问[http://localhost:5533/Yourname](http://localhost:5533/Yourname)以获取预测的JSON输出。
- en: Exercises
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: 'Try with a different dataset of line -> category, for example:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用不同的行 -> 类别数据集，例如：
- en: Any word -> language
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何单词 -> 语言
- en: First name -> gender
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先姓名 -> 性别
- en: Character name -> writer
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 角色名称 -> 作者
- en: Page title -> blog or subreddit
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页面标题 -> 博客或子论坛
- en: Get better results with a bigger and/or better shaped network
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一个更大和/或更好形状的网络获得更好的结果
- en: Add more linear layers
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多线性层
- en: Try the `nn.LSTM` and `nn.GRU` layers
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试`nn.LSTM`和`nn.GRU`层
- en: Combine multiple of these RNNs as a higher level network
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多个这些RNN组合成一个更高级的网络
- en: '**Total running time of the script:** ( 10 minutes 4.936 seconds)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（10分钟4.936秒）'
- en: '[`Download Python source code: char_rnn_classification_tutorial.py`](../_downloads/37c8905519d3fd3f437b783a48d06eac/char_rnn_classification_tutorial.py)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Python源代码：char_rnn_classification_tutorial.py`](../_downloads/37c8905519d3fd3f437b783a48d06eac/char_rnn_classification_tutorial.py)'
- en: '[`Download Jupyter notebook: char_rnn_classification_tutorial.ipynb`](../_downloads/13b143c2380f4768d9432d808ad50799/char_rnn_classification_tutorial.ipynb)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[`下载Jupyter笔记本：char_rnn_classification_tutorial.ipynb`](../_downloads/13b143c2380f4768d9432d808ad50799/char_rnn_classification_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)'
