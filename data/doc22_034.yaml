- en: torch.Tensor
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torch.Tensor
- en: 原文：[https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)
- en: A [`torch.Tensor`](#torch.Tensor "torch.Tensor") is a multi-dimensional matrix
    containing elements of a single data type.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[`torch.Tensor`](#torch.Tensor "torch.Tensor") 是包含单一数据类型元素的多维矩阵。'
- en: Data types
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据类型
- en: 'Torch defines 10 tensor types with CPU and GPU variants which are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Torch定义了10种张量类型，包括CPU和GPU变体，如下所示：
- en: '| Data type | dtype | CPU tensor | GPU tensor |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | dtype | CPU张量 | GPU张量 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 32-bit floating point | `torch.float32` or `torch.float` | `torch.FloatTensor`
    | `torch.cuda.FloatTensor` |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| 32位浮点数 | `torch.float32` 或 `torch.float` | `torch.FloatTensor` | `torch.cuda.FloatTensor`
    |'
- en: '| 64-bit floating point | `torch.float64` or `torch.double` | `torch.DoubleTensor`
    | `torch.cuda.DoubleTensor` |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 64位浮点数 | `torch.float64` 或 `torch.double` | `torch.DoubleTensor` | `torch.cuda.DoubleTensor`
    |'
- en: '| 16-bit floating point [1](#id4) | `torch.float16` or `torch.half` | `torch.HalfTensor`
    | `torch.cuda.HalfTensor` |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 16位浮点数 [1](#id4) | `torch.float16` 或 `torch.half` | `torch.HalfTensor` |
    `torch.cuda.HalfTensor` |'
- en: '| 16-bit floating point [2](#id5) | `torch.bfloat16` | `torch.BFloat16Tensor`
    | `torch.cuda.BFloat16Tensor` |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 16位浮点数 [2](#id5) | `torch.bfloat16` | `torch.BFloat16Tensor` | `torch.cuda.BFloat16Tensor`
    |'
- en: '| 32-bit complex | `torch.complex32` or `torch.chalf` |  |  |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 32位复数 | `torch.complex32` 或 `torch.chalf` |  |  |'
- en: '| 64-bit complex | `torch.complex64` or `torch.cfloat` |  |  |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 64位复数 | `torch.complex64` 或 `torch.cfloat` |  |  |'
- en: '| 128-bit complex | `torch.complex128` or `torch.cdouble` |  |  |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 128位复数 | `torch.complex128` 或 `torch.cdouble` |  |  |'
- en: '| 8-bit integer (unsigned) | `torch.uint8` | `torch.ByteTensor` | `torch.cuda.ByteTensor`
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 8位整数（无符号） | `torch.uint8` | `torch.ByteTensor` | `torch.cuda.ByteTensor`
    |'
- en: '| 8-bit integer (signed) | `torch.int8` | `torch.CharTensor` | `torch.cuda.CharTensor`
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 8位整数（有符号） | `torch.int8` | `torch.CharTensor` | `torch.cuda.CharTensor` |'
- en: '| 16-bit integer (signed) | `torch.int16` or `torch.short` | `torch.ShortTensor`
    | `torch.cuda.ShortTensor` |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 16位整数（有符号） | `torch.int16` 或 `torch.short` | `torch.ShortTensor` | `torch.cuda.ShortTensor`
    |'
- en: '| 32-bit integer (signed) | `torch.int32` or `torch.int` | `torch.IntTensor`
    | `torch.cuda.IntTensor` |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 32位整数（有符号） | `torch.int32` 或 `torch.int` | `torch.IntTensor` | `torch.cuda.IntTensor`
    |'
- en: '| 64-bit integer (signed) | `torch.int64` or `torch.long` | `torch.LongTensor`
    | `torch.cuda.LongTensor` |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 64位整数（有符号） | `torch.int64` 或 `torch.long` | `torch.LongTensor` | `torch.cuda.LongTensor`
    |'
- en: '| Boolean | `torch.bool` | `torch.BoolTensor` | `torch.cuda.BoolTensor` |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 布尔值 | `torch.bool` | `torch.BoolTensor` | `torch.cuda.BoolTensor` |'
- en: '| quantized 8-bit integer (unsigned) | `torch.quint8` | `torch.ByteTensor`
    | / |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 量化的8位整数（无符号） | `torch.quint8` | `torch.ByteTensor` | / |'
- en: '| quantized 8-bit integer (signed) | `torch.qint8` | `torch.CharTensor` | /
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 量化的8位整数（有符号） | `torch.qint8` | `torch.CharTensor` | / |'
- en: '| quantized 32-bit integer (signed) | `torch.qint32` | `torch.IntTensor` |
    / |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 量化的32位整数（有符号） | `torch.qint32` | `torch.IntTensor` | / |'
- en: '| quantized 4-bit integer (unsigned) [3](#id6) | `torch.quint4x2` | `torch.ByteTensor`
    | / |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 量化的4位整数（无符号）[3](#id6) | `torch.quint4x2` | `torch.ByteTensor` | / |'
- en: '[1](#id1)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](#id1)'
- en: 'Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand
    bits. Useful when precision is important at the expense of range.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有时被称为binary16：使用1个符号位，5个指数位和10个有效位。当精度重要时很有用，但会牺牲范围。
- en: '[2](#id2)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](#id2)'
- en: 'Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and
    7 significand bits. Useful when range is important, since it has the same number
    of exponent bits as `float32`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有时被称为Brain Floating Point：使用1个符号位，8个指数位和7个有效位。当范围重要时很有用，因为它具有与`float32`相同数量的指数位。
- en: '[3](#id3)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](#id3)'
- en: quantized 4-bit integer is stored as a 8-bit signed integer. Currently it’s
    only supported in EmbeddingBag operator.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 量化的4位整数存储为8位有符号整数。目前仅在EmbeddingBag运算符中支持。
- en: '[`torch.Tensor`](#torch.Tensor "torch.Tensor") is an alias for the default
    tensor type (`torch.FloatTensor`).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[`torch.Tensor`](#torch.Tensor "torch.Tensor") 是默认张量类型（`torch.FloatTensor`）的别名。'
- en: Initializing and basic operations[](#initializing-and-basic-operations "Permalink
    to this heading")
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化和基本操作[](#initializing-and-basic-operations "跳转到此标题的永久链接")
- en: 'A tensor can be constructed from a Python [`list`](https://docs.python.org/3/library/stdtypes.html#list
    "(in Python v3.12)") or sequence using the [`torch.tensor()`](generated/torch.tensor.html#torch.tensor
    "torch.tensor") constructor:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用[`torch.tensor()`](generated/torch.tensor.html#torch.tensor "torch.tensor")构造来自Python
    [`list`](https://docs.python.org/3/library/stdtypes.html#list "(在Python v3.12中)")
    或序列的张量：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Warning
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: '[`torch.tensor()`](generated/torch.tensor.html#torch.tensor "torch.tensor")
    always copies `data`. If you have a Tensor `data` and just want to change its
    `requires_grad` flag, use [`requires_grad_()`](generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_
    "torch.Tensor.requires_grad_") or [`detach()`](generated/torch.Tensor.detach.html#torch.Tensor.detach
    "torch.Tensor.detach") to avoid a copy. If you have a numpy array and want to
    avoid a copy, use [`torch.as_tensor()`](generated/torch.as_tensor.html#torch.as_tensor
    "torch.as_tensor").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[`torch.tensor()`](generated/torch.tensor.html#torch.tensor "torch.tensor")
    总是复制`data`。如果您有一个张量`data`，只想改变其`requires_grad`标志，请使用[`requires_grad_()`](generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_
    "torch.Tensor.requires_grad_")或[`detach()`](generated/torch.Tensor.detach.html#torch.Tensor.detach
    "torch.Tensor.detach")来避免复制。如果您有一个numpy数组并想避免复制，请使用[`torch.as_tensor()`](generated/torch.as_tensor.html#torch.as_tensor
    "torch.as_tensor")。'
- en: 'A tensor of specific data type can be constructed by passing a [`torch.dtype`](tensor_attributes.html#torch.dtype
    "torch.dtype") and/or a [`torch.device`](tensor_attributes.html#torch.device "torch.device")
    to a constructor or tensor creation op:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将[`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype")和/或[`torch.device`](tensor_attributes.html#torch.device
    "torch.device")传递给构造函数或张量创建操作，可以构造特定数据类型的张量：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For more information about building Tensors, see [Creation Ops](torch.html#tensor-creation-ops)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有关构建张量的更多信息，请参阅[Creation Ops](torch.html#tensor-creation-ops)
- en: 'The contents of a tensor can be accessed and modified using Python’s indexing
    and slicing notation:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Python的索引和切片表示法访问和修改张量的内容：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Use [`torch.Tensor.item()`](generated/torch.Tensor.item.html#torch.Tensor.item
    "torch.Tensor.item") to get a Python number from a tensor containing a single
    value:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [`torch.Tensor.item()`](generated/torch.Tensor.item.html#torch.Tensor.item
    "torch.Tensor.item") 从包含单个值的张量中获取一个Python数字：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For more information about indexing, see [Indexing, Slicing, Joining, Mutating
    Ops](torch.html#indexing-slicing-joining)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有关索引的更多信息，请参见 [Indexing, Slicing, Joining, Mutating Ops](torch.html#indexing-slicing-joining)。
- en: A tensor can be created with `requires_grad=True` so that [`torch.autograd`](autograd.html#module-torch.autograd
    "torch.autograd") records operations on them for automatic differentiation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 可以创建一个带有 `requires_grad=True` 的张量，以便 [`torch.autograd`](autograd.html#module-torch.autograd
    "torch.autograd") 记录对它们的操作以进行自动微分。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each tensor has an associated `torch.Storage`, which holds its data. The tensor
    class also provides multi-dimensional, [strided](https://en.wikipedia.org/wiki/Stride_of_an_array)
    view of a storage and defines numeric operations on it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 每个张量都有一个关联的 `torch.Storage`，用于保存其数据。张量类还提供了对存储的多维、[分步](https://en.wikipedia.org/wiki/Stride_of_an_array)视图，并在其上定义了数值操作。
- en: Note
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on tensor views, see [Tensor Views](tensor_view.html#tensor-view-doc).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有关张量视图的更多信息，请参见 [Tensor Views](tensor_view.html#tensor-view-doc)。
- en: Note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on the [`torch.dtype`](tensor_attributes.html#torch.dtype
    "torch.dtype"), [`torch.device`](tensor_attributes.html#torch.device "torch.device"),
    and [`torch.layout`](tensor_attributes.html#torch.layout "torch.layout") attributes
    of a [`torch.Tensor`](#torch.Tensor "torch.Tensor"), see [Tensor Attributes](tensor_attributes.html#tensor-attributes-doc).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 [`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype")、[`torch.device`](tensor_attributes.html#torch.device
    "torch.device") 和 [`torch.layout`](tensor_attributes.html#torch.layout "torch.layout")
    属性的更多信息，请参见 [`torch.Tensor`](#torch.Tensor "torch.Tensor") 的 [Tensor Attributes](tensor_attributes.html#tensor-attributes-doc)。
- en: Note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Methods which mutate a tensor are marked with an underscore suffix. For example,
    `torch.FloatTensor.abs_()` computes the absolute value in-place and returns the
    modified tensor, while `torch.FloatTensor.abs()` computes the result in a new
    tensor.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 会改变张量的方法会带有下划线后缀。例如，`torch.FloatTensor.abs_()` 在原地计算绝对值并返回修改后的张量，而 `torch.FloatTensor.abs()`
    在新张量中计算结果。
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To change an existing tensor’s [`torch.device`](tensor_attributes.html#torch.device
    "torch.device") and/or [`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype"),
    consider using [`to()`](generated/torch.Tensor.to.html#torch.Tensor.to "torch.Tensor.to")
    method on the tensor.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改现有张量的 [`torch.device`](tensor_attributes.html#torch.device "torch.device")
    和/或 [`torch.dtype`](tensor_attributes.html#torch.dtype "torch.dtype")，请考虑在张量上使用
    [`to()`](generated/torch.Tensor.to.html#torch.Tensor.to "torch.Tensor.to") 方法。
- en: Warning
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Current implementation of [`torch.Tensor`](#torch.Tensor "torch.Tensor") introduces
    memory overhead, thus it might lead to unexpectedly high memory usage in the applications
    with many tiny tensors. If this is your case, consider using one large structure.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的 [`torch.Tensor`](#torch.Tensor "torch.Tensor") 实现引入了内存开销，因此在具有许多小张量的应用程序中可能导致意外高的内存使用量。如果这是您的情况，请考虑使用一个大结构。
- en: Tensor class reference
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量类参考
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are a few main ways to create a tensor, depending on your use case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种主要方法可以创建张量，取决于您的用例。
- en: To create a tensor with pre-existing data, use [`torch.tensor()`](generated/torch.tensor.html#torch.tensor
    "torch.tensor").
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用预先存在的数据创建张量，请使用 [`torch.tensor()`](generated/torch.tensor.html#torch.tensor
    "torch.tensor")。
- en: To create a tensor with specific size, use `torch.*` tensor creation ops (see
    [Creation Ops](torch.html#tensor-creation-ops)).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要创建特定大小的张量，请使用 `torch.*` 张量创建操作（参见 [Creation Ops](torch.html#tensor-creation-ops)）。
- en: To create a tensor with the same size (and similar types) as another tensor,
    use `torch.*_like` tensor creation ops (see [Creation Ops](torch.html#tensor-creation-ops)).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要创建一个与另一个张量相同大小（和相似类型）的张量，请使用 `torch.*_like` 张量创建操作（参见 [Creation Ops](torch.html#tensor-creation-ops)）。
- en: To create a tensor with similar type but different size as another tensor, use
    `tensor.new_*` creation ops.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要创建一个与另一个张量相似类型但不同大小的张量，请使用 `tensor.new_*` 创建操作。
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Returns a view of this tensor with its dimensions reversed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个维度被颠倒的张量视图。
- en: If `n` is the number of dimensions in `x`, `x.T` is equivalent to `x.permute(n-1,
    n-2, ..., 0)`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `x` 中有 `n` 个维度，`x.T` 等同于 `x.permute(n-1, n-2, ..., 0)`。
- en: Warning
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: The use of [`Tensor.T()`](#torch.Tensor.T "torch.Tensor.T") on tensors of dimension
    other than 2 to reverse their shape is deprecated and it will throw an error in
    a future release. Consider [`mT`](#torch.Tensor.mT "torch.Tensor.mT") to transpose
    batches of matrices or x.permute(*torch.arange(x.ndim - 1, -1, -1)) to reverse
    the dimensions of a tensor.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在维度不为2的张量上使用 [`Tensor.T()`](#torch.Tensor.T "torch.Tensor.T") 来颠倒它们的形状已被弃用，并且在将来的版本中会引发错误。考虑使用
    [`mT`](#torch.Tensor.mT "torch.Tensor.mT") 来转置矩阵批次或者使用 x.permute(*torch.arange(x.ndim
    - 1, -1, -1)) 来颠倒张量的维度。
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Returns a view of a matrix (2-D tensor) conjugated and transposed.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个共轭和转置的矩阵（2-D张量）视图。
- en: '`x.H` is equivalent to `x.transpose(0, 1).conj()` for complex matrices and
    `x.transpose(0, 1)` for real matrices.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂矩阵，`x.H` 等同于 `x.transpose(0, 1).conj()`，对于实矩阵，`x.H` 等同于 `x.transpose(0,
    1)`。
- en: See also
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[`mH`](#torch.Tensor.mH "torch.Tensor.mH"): An attribute that also works on
    batches of matrices.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[`mH`](#torch.Tensor.mH "torch.Tensor.mH")：也适用于矩阵批次的属性。'
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Returns a view of this tensor with the last two dimensions transposed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个最后两个维度被转置的张量视图。
- en: '`x.mT` is equivalent to `x.transpose(-2, -1)`.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`x.mT` 等同于 `x.transpose(-2, -1)`。'
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Accessing this property is equivalent to calling [`adjoint()`](generated/torch.adjoint.html#torch.adjoint
    "torch.adjoint").
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 访问此属性等同于调用 [`adjoint()`](generated/torch.adjoint.html#torch.adjoint "torch.adjoint")。
- en: '| [`Tensor.new_tensor`](generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor
    "torch.Tensor.new_tensor") | Returns a new Tensor with `data` as the tensor data.
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.new_tensor`](generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor
    "torch.Tensor.new_tensor") | 返回一个以 `data` 为张量数据的新张量。 |'
- en: '| [`Tensor.new_full`](generated/torch.Tensor.new_full.html#torch.Tensor.new_full
    "torch.Tensor.new_full") | Returns a Tensor of size `size` filled with `fill_value`.
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.new_full`](generated/torch.Tensor.new_full.html#torch.Tensor.new_full
    "torch.Tensor.new_full") | 返回一个大小为`size`且填充为`fill_value`的张量。 |'
- en: '| [`Tensor.new_empty`](generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty
    "torch.Tensor.new_empty") | Returns a Tensor of size `size` filled with uninitialized
    data. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.new_empty`](generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty
    "torch.Tensor.new_empty") | 返回一个大小为`size`且填充为未初始化数据的张量。 |'
- en: '| [`Tensor.new_ones`](generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones
    "torch.Tensor.new_ones") | Returns a Tensor of size `size` filled with `1`. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.new_ones`](generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones
    "torch.Tensor.new_ones") | 返回一个大小为`size`且填充为`1`的张量。 |'
- en: '| [`Tensor.new_zeros`](generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros
    "torch.Tensor.new_zeros") | Returns a Tensor of size `size` filled with `0`. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.new_zeros`](generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros
    "torch.Tensor.new_zeros") | 返回一个大小为`size`且填充为`0`的张量。 |'
- en: '| [`Tensor.is_cuda`](generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda
    "torch.Tensor.is_cuda") | Is `True` if the Tensor is stored on the GPU, `False`
    otherwise. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_cuda`](generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda
    "torch.Tensor.is_cuda") | 如果张量存储在GPU上，则为`True`，否则为`False`。 |'
- en: '| [`Tensor.is_quantized`](generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized
    "torch.Tensor.is_quantized") | Is `True` if the Tensor is quantized, `False` otherwise.
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_quantized`](generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized
    "torch.Tensor.is_quantized") | 如果张量是量化的，则为`True`，否则为`False`。 |'
- en: '| [`Tensor.is_meta`](generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta
    "torch.Tensor.is_meta") | Is `True` if the Tensor is a meta tensor, `False` otherwise.
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_meta`](generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta
    "torch.Tensor.is_meta") | 如果张量是元张量，则为`True`，否则为`False`。 |'
- en: '| [`Tensor.device`](generated/torch.Tensor.device.html#torch.Tensor.device
    "torch.Tensor.device") | Is the [`torch.device`](tensor_attributes.html#torch.device
    "torch.device") where this Tensor is. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.device`](generated/torch.Tensor.device.html#torch.Tensor.device
    "torch.Tensor.device") | 此张量所在的[`torch.device`](tensor_attributes.html#torch.device
    "torch.device")。 |'
- en: '| [`Tensor.grad`](generated/torch.Tensor.grad.html#torch.Tensor.grad "torch.Tensor.grad")
    | This attribute is `None` by default and becomes a Tensor the first time a call
    to `backward()` computes gradients for `self`. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.grad`](generated/torch.Tensor.grad.html#torch.Tensor.grad "torch.Tensor.grad")
    | 默认情况下，此属性为`None`，第一次调用`backward()`计算`self`的梯度时会变成一个张量。 |'
- en: '| [`Tensor.ndim`](generated/torch.Tensor.ndim.html#torch.Tensor.ndim "torch.Tensor.ndim")
    | Alias for [`dim()`](generated/torch.Tensor.dim.html#torch.Tensor.dim "torch.Tensor.dim")
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ndim`](generated/torch.Tensor.ndim.html#torch.Tensor.ndim "torch.Tensor.ndim")
    | [`dim()`](generated/torch.Tensor.dim.html#torch.Tensor.dim "torch.Tensor.dim")的别名
    |'
- en: '| [`Tensor.real`](generated/torch.Tensor.real.html#torch.Tensor.real "torch.Tensor.real")
    | Returns a new tensor containing real values of the `self` tensor for a complex-valued
    input tensor. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.real`](generated/torch.Tensor.real.html#torch.Tensor.real "torch.Tensor.real")
    | 返回一个包含复值输入张量`self`的实部值的新张量。 |'
- en: '| [`Tensor.imag`](generated/torch.Tensor.imag.html#torch.Tensor.imag "torch.Tensor.imag")
    | Returns a new tensor containing imaginary values of the `self` tensor. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.imag`](generated/torch.Tensor.imag.html#torch.Tensor.imag "torch.Tensor.imag")
    | 返回一个包含`self`张量的虚部值的新张量。 |'
- en: '| [`Tensor.nbytes`](generated/torch.Tensor.nbytes.html#torch.Tensor.nbytes
    "torch.Tensor.nbytes") | Returns the number of bytes consumed by the "view" of
    elements of the Tensor if the Tensor does not use sparse storage layout. |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nbytes`](generated/torch.Tensor.nbytes.html#torch.Tensor.nbytes
    "torch.Tensor.nbytes") | 如果张量不使用稀疏存储布局，则返回张量元素“视图”消耗的字节数。 |'
- en: '| [`Tensor.itemsize`](generated/torch.Tensor.itemsize.html#torch.Tensor.itemsize
    "torch.Tensor.itemsize") | Alias for [`element_size()`](generated/torch.Tensor.element_size.html#torch.Tensor.element_size
    "torch.Tensor.element_size") |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.itemsize`](generated/torch.Tensor.itemsize.html#torch.Tensor.itemsize
    "torch.Tensor.itemsize") | [`element_size()`](generated/torch.Tensor.element_size.html#torch.Tensor.element_size
    "torch.Tensor.element_size")的别名 |'
- en: '| [`Tensor.abs`](generated/torch.Tensor.abs.html#torch.Tensor.abs "torch.Tensor.abs")
    | See [`torch.abs()`](generated/torch.abs.html#torch.abs "torch.abs") |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.abs`](generated/torch.Tensor.abs.html#torch.Tensor.abs "torch.Tensor.abs")
    | 参见[`torch.abs()`](generated/torch.abs.html#torch.abs "torch.abs") |'
- en: '| [`Tensor.abs_`](generated/torch.Tensor.abs_.html#torch.Tensor.abs_ "torch.Tensor.abs_")
    | In-place version of [`abs()`](generated/torch.Tensor.abs.html#torch.Tensor.abs
    "torch.Tensor.abs") |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.abs_`](generated/torch.Tensor.abs_.html#torch.Tensor.abs_ "torch.Tensor.abs_")
    | [`abs()`](generated/torch.Tensor.abs.html#torch.Tensor.abs "torch.Tensor.abs")的原地版本
    |'
- en: '| [`Tensor.absolute`](generated/torch.Tensor.absolute.html#torch.Tensor.absolute
    "torch.Tensor.absolute") | Alias for [`abs()`](generated/torch.abs.html#torch.abs
    "torch.abs") |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.absolute`](generated/torch.Tensor.absolute.html#torch.Tensor.absolute
    "torch.Tensor.absolute") | [`abs()`](generated/torch.abs.html#torch.abs "torch.abs")的别名
    |'
- en: '| [`Tensor.absolute_`](generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_
    "torch.Tensor.absolute_") | In-place version of [`absolute()`](generated/torch.Tensor.absolute.html#torch.Tensor.absolute
    "torch.Tensor.absolute") Alias for `abs_()` |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.absolute_`](generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_
    "torch.Tensor.absolute_") | [`absolute()`](generated/torch.Tensor.absolute.html#torch.Tensor.absolute
    "torch.Tensor.absolute")的原地版本，别名为`abs_()` |'
- en: '| [`Tensor.acos`](generated/torch.Tensor.acos.html#torch.Tensor.acos "torch.Tensor.acos")
    | See [`torch.acos()`](generated/torch.acos.html#torch.acos "torch.acos") |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.acos`](generated/torch.Tensor.acos.html#torch.Tensor.acos "torch.Tensor.acos")
    | 参见[`torch.acos()`](generated/torch.acos.html#torch.acos "torch.acos") |'
- en: '| [`Tensor.acos_`](generated/torch.Tensor.acos_.html#torch.Tensor.acos_ "torch.Tensor.acos_")
    | In-place version of [`acos()`](generated/torch.Tensor.acos.html#torch.Tensor.acos
    "torch.Tensor.acos") |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.acos_`](generated/torch.Tensor.acos_.html#torch.Tensor.acos_ "torch.Tensor.acos_")
    | [`acos()`](generated/torch.Tensor.acos.html#torch.Tensor.acos "torch.Tensor.acos")的原地版本
    |'
- en: '| [`Tensor.arccos`](generated/torch.Tensor.arccos.html#torch.Tensor.arccos
    "torch.Tensor.arccos") | See [`torch.arccos()`](generated/torch.arccos.html#torch.arccos
    "torch.arccos") |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arccos`](generated/torch.Tensor.arccos.html#torch.Tensor.arccos
    "torch.Tensor.arccos") | 参见[`torch.arccos()`](generated/torch.arccos.html#torch.arccos
    "torch.arccos") |'
- en: '| [`Tensor.arccos_`](generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_
    "torch.Tensor.arccos_") | In-place version of [`arccos()`](generated/torch.Tensor.arccos.html#torch.Tensor.arccos
    "torch.Tensor.arccos") |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arccos_`](generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_
    "torch.Tensor.arccos_") | [`arccos()`](generated/torch.Tensor.arccos.html#torch.Tensor.arccos
    "torch.Tensor.arccos") 的原地版本 |'
- en: '| [`Tensor.add`](generated/torch.Tensor.add.html#torch.Tensor.add "torch.Tensor.add")
    | Add a scalar or tensor to `self` tensor. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.add`](generated/torch.Tensor.add.html#torch.Tensor.add "torch.Tensor.add")
    | 将标量或张量添加到 `self` 张量中。 |'
- en: '| [`Tensor.add_`](generated/torch.Tensor.add_.html#torch.Tensor.add_ "torch.Tensor.add_")
    | In-place version of [`add()`](generated/torch.Tensor.add.html#torch.Tensor.add
    "torch.Tensor.add") |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.add_`](generated/torch.Tensor.add_.html#torch.Tensor.add_ "torch.Tensor.add_")
    | [`add()`](generated/torch.Tensor.add.html#torch.Tensor.add "torch.Tensor.add")
    的原地版本 |'
- en: '| [`Tensor.addbmm`](generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm
    "torch.Tensor.addbmm") | See [`torch.addbmm()`](generated/torch.addbmm.html#torch.addbmm
    "torch.addbmm") |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addbmm`](generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm
    "torch.Tensor.addbmm") | 查看 [`torch.addbmm()`](generated/torch.addbmm.html#torch.addbmm
    "torch.addbmm") |'
- en: '| [`Tensor.addbmm_`](generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_
    "torch.Tensor.addbmm_") | In-place version of [`addbmm()`](generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm
    "torch.Tensor.addbmm") |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addbmm_`](generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_
    "torch.Tensor.addbmm_") | [`addbmm()`](generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm
    "torch.Tensor.addbmm") 的原地版本 |'
- en: '| [`Tensor.addcdiv`](generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv
    "torch.Tensor.addcdiv") | See [`torch.addcdiv()`](generated/torch.addcdiv.html#torch.addcdiv
    "torch.addcdiv") |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addcdiv`](generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv
    "torch.Tensor.addcdiv") | 查看 [`torch.addcdiv()`](generated/torch.addcdiv.html#torch.addcdiv
    "torch.addcdiv") |'
- en: '| [`Tensor.addcdiv_`](generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_
    "torch.Tensor.addcdiv_") | In-place version of [`addcdiv()`](generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv
    "torch.Tensor.addcdiv") |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addcdiv_`](generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_
    "torch.Tensor.addcdiv_") | [`addcdiv()`](generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv
    "torch.Tensor.addcdiv") 的原地版本 |'
- en: '| [`Tensor.addcmul`](generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul
    "torch.Tensor.addcmul") | See [`torch.addcmul()`](generated/torch.addcmul.html#torch.addcmul
    "torch.addcmul") |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addcmul`](generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul
    "torch.Tensor.addcmul") | 查看 [`torch.addcmul()`](generated/torch.addcmul.html#torch.addcmul
    "torch.addcmul") |'
- en: '| [`Tensor.addcmul_`](generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_
    "torch.Tensor.addcmul_") | In-place version of [`addcmul()`](generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul
    "torch.Tensor.addcmul") |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addcmul_`](generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_
    "torch.Tensor.addcmul_") | [`addcmul()`](generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul
    "torch.Tensor.addcmul") 的原地版本 |'
- en: '| [`Tensor.addmm`](generated/torch.Tensor.addmm.html#torch.Tensor.addmm "torch.Tensor.addmm")
    | See [`torch.addmm()`](generated/torch.addmm.html#torch.addmm "torch.addmm")
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addmm`](generated/torch.Tensor.addmm.html#torch.Tensor.addmm "torch.Tensor.addmm")
    | 查看 [`torch.addmm()`](generated/torch.addmm.html#torch.addmm "torch.addmm") |'
- en: '| [`Tensor.addmm_`](generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_
    "torch.Tensor.addmm_") | In-place version of [`addmm()`](generated/torch.Tensor.addmm.html#torch.Tensor.addmm
    "torch.Tensor.addmm") |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addmm_`](generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_
    "torch.Tensor.addmm_") | [`addmm()`](generated/torch.Tensor.addmm.html#torch.Tensor.addmm
    "torch.Tensor.addmm") 的原地版本 |'
- en: '| [`Tensor.sspaddmm`](generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm
    "torch.Tensor.sspaddmm") | See [`torch.sspaddmm()`](generated/torch.sspaddmm.html#torch.sspaddmm
    "torch.sspaddmm") |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sspaddmm`](generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm
    "torch.Tensor.sspaddmm") | 查看 [`torch.sspaddmm()`](generated/torch.sspaddmm.html#torch.sspaddmm
    "torch.sspaddmm") |'
- en: '| [`Tensor.addmv`](generated/torch.Tensor.addmv.html#torch.Tensor.addmv "torch.Tensor.addmv")
    | See [`torch.addmv()`](generated/torch.addmv.html#torch.addmv "torch.addmv")
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addmv`](generated/torch.Tensor.addmv.html#torch.Tensor.addmv "torch.Tensor.addmv")
    | 查看 [`torch.addmv()`](generated/torch.addmv.html#torch.addmv "torch.addmv") |'
- en: '| [`Tensor.addmv_`](generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_
    "torch.Tensor.addmv_") | In-place version of [`addmv()`](generated/torch.Tensor.addmv.html#torch.Tensor.addmv
    "torch.Tensor.addmv") |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addmv_`](generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_
    "torch.Tensor.addmv_") | [`addmv()`](generated/torch.Tensor.addmv.html#torch.Tensor.addmv
    "torch.Tensor.addmv") 的原地版本 |'
- en: '| [`Tensor.addr`](generated/torch.Tensor.addr.html#torch.Tensor.addr "torch.Tensor.addr")
    | See [`torch.addr()`](generated/torch.addr.html#torch.addr "torch.addr") |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addr`](generated/torch.Tensor.addr.html#torch.Tensor.addr "torch.Tensor.addr")
    | 查看 [`torch.addr()`](generated/torch.addr.html#torch.addr "torch.addr") |'
- en: '| [`Tensor.addr_`](generated/torch.Tensor.addr_.html#torch.Tensor.addr_ "torch.Tensor.addr_")
    | In-place version of [`addr()`](generated/torch.Tensor.addr.html#torch.Tensor.addr
    "torch.Tensor.addr") |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.addr_`](generated/torch.Tensor.addr_.html#torch.Tensor.addr_ "torch.Tensor.addr_")
    | [`addr()`](generated/torch.Tensor.addr.html#torch.Tensor.addr "torch.Tensor.addr")
    的原地版本 |'
- en: '| [`Tensor.adjoint`](generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint
    "torch.Tensor.adjoint") | Alias for [`adjoint()`](generated/torch.adjoint.html#torch.adjoint
    "torch.adjoint") |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.adjoint`](generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint
    "torch.Tensor.adjoint") | [`adjoint()`](generated/torch.adjoint.html#torch.adjoint
    "torch.adjoint") 的别名 |'
- en: '| [`Tensor.allclose`](generated/torch.Tensor.allclose.html#torch.Tensor.allclose
    "torch.Tensor.allclose") | See [`torch.allclose()`](generated/torch.allclose.html#torch.allclose
    "torch.allclose") |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.allclose`](generated/torch.Tensor.allclose.html#torch.Tensor.allclose
    "torch.Tensor.allclose") | 查看 [`torch.allclose()`](generated/torch.allclose.html#torch.allclose
    "torch.allclose") |'
- en: '| [`Tensor.amax`](generated/torch.Tensor.amax.html#torch.Tensor.amax "torch.Tensor.amax")
    | See [`torch.amax()`](generated/torch.amax.html#torch.amax "torch.amax") |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.amax`](generated/torch.Tensor.amax.html#torch.Tensor.amax "torch.Tensor.amax")
    | 查看 [`torch.amax()`](generated/torch.amax.html#torch.amax "torch.amax") |'
- en: '| [`Tensor.amin`](generated/torch.Tensor.amin.html#torch.Tensor.amin "torch.Tensor.amin")
    | See [`torch.amin()`](generated/torch.amin.html#torch.amin "torch.amin") |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.amin`](generated/torch.Tensor.amin.html#torch.Tensor.amin "torch.Tensor.amin")
    | 查看 [`torch.amin()`](generated/torch.amin.html#torch.amin "torch.amin") |'
- en: '| [`Tensor.aminmax`](generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax
    "torch.Tensor.aminmax") | See [`torch.aminmax()`](generated/torch.aminmax.html#torch.aminmax
    "torch.aminmax") |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.aminmax`](generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax
    "torch.Tensor.aminmax") | 参见 [`torch.aminmax()`](generated/torch.aminmax.html#torch.aminmax
    "torch.aminmax") |'
- en: '| [`Tensor.angle`](generated/torch.Tensor.angle.html#torch.Tensor.angle "torch.Tensor.angle")
    | See [`torch.angle()`](generated/torch.angle.html#torch.angle "torch.angle")
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.angle`](generated/torch.Tensor.angle.html#torch.Tensor.angle "torch.Tensor.angle")
    | 参见 [`torch.angle()`](generated/torch.angle.html#torch.angle "torch.angle") |'
- en: '| [`Tensor.apply_`](generated/torch.Tensor.apply_.html#torch.Tensor.apply_
    "torch.Tensor.apply_") | Applies the function `callable` to each element in the
    tensor, replacing each element with the value returned by `callable`. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.apply_`](generated/torch.Tensor.apply_.html#torch.Tensor.apply_
    "torch.Tensor.apply_") | 将函数 `callable` 应用于张量中的每个元素，用 `callable` 返回的值替换每个元素。 |'
- en: '| [`Tensor.argmax`](generated/torch.Tensor.argmax.html#torch.Tensor.argmax
    "torch.Tensor.argmax") | See [`torch.argmax()`](generated/torch.argmax.html#torch.argmax
    "torch.argmax") |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.argmax`](generated/torch.Tensor.argmax.html#torch.Tensor.argmax
    "torch.Tensor.argmax") | 参见 [`torch.argmax()`](generated/torch.argmax.html#torch.argmax
    "torch.argmax") |'
- en: '| [`Tensor.argmin`](generated/torch.Tensor.argmin.html#torch.Tensor.argmin
    "torch.Tensor.argmin") | See [`torch.argmin()`](generated/torch.argmin.html#torch.argmin
    "torch.argmin") |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.argmin`](generated/torch.Tensor.argmin.html#torch.Tensor.argmin
    "torch.Tensor.argmin") | 参见 [`torch.argmin()`](generated/torch.argmin.html#torch.argmin
    "torch.argmin") |'
- en: '| [`Tensor.argsort`](generated/torch.Tensor.argsort.html#torch.Tensor.argsort
    "torch.Tensor.argsort") | See [`torch.argsort()`](generated/torch.argsort.html#torch.argsort
    "torch.argsort") |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.argsort`](generated/torch.Tensor.argsort.html#torch.Tensor.argsort
    "torch.Tensor.argsort") | 参见 [`torch.argsort()`](generated/torch.argsort.html#torch.argsort
    "torch.argsort") |'
- en: '| [`Tensor.argwhere`](generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere
    "torch.Tensor.argwhere") | See [`torch.argwhere()`](generated/torch.argwhere.html#torch.argwhere
    "torch.argwhere") |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.argwhere`](generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere
    "torch.Tensor.argwhere") | 参见 [`torch.argwhere()`](generated/torch.argwhere.html#torch.argwhere
    "torch.argwhere") |'
- en: '| [`Tensor.asin`](generated/torch.Tensor.asin.html#torch.Tensor.asin "torch.Tensor.asin")
    | See [`torch.asin()`](generated/torch.asin.html#torch.asin "torch.asin") |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.asin`](generated/torch.Tensor.asin.html#torch.Tensor.asin "torch.Tensor.asin")
    | 参见 [`torch.asin()`](generated/torch.asin.html#torch.asin "torch.asin") |'
- en: '| [`Tensor.asin_`](generated/torch.Tensor.asin_.html#torch.Tensor.asin_ "torch.Tensor.asin_")
    | In-place version of [`asin()`](generated/torch.Tensor.asin.html#torch.Tensor.asin
    "torch.Tensor.asin") |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.asin_`](generated/torch.Tensor.asin_.html#torch.Tensor.asin_ "torch.Tensor.asin_")
    | [`asin()`](generated/torch.Tensor.asin.html#torch.Tensor.asin "torch.Tensor.asin")
    的原地版本 |'
- en: '| [`Tensor.arcsin`](generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin
    "torch.Tensor.arcsin") | See [`torch.arcsin()`](generated/torch.arcsin.html#torch.arcsin
    "torch.arcsin") |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arcsin`](generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin
    "torch.Tensor.arcsin") | 参见 [`torch.arcsin()`](generated/torch.arcsin.html#torch.arcsin
    "torch.arcsin") |'
- en: '| [`Tensor.arcsin_`](generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_
    "torch.Tensor.arcsin_") | In-place version of [`arcsin()`](generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin
    "torch.Tensor.arcsin") |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arcsin_`](generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_
    "torch.Tensor.arcsin_") | [`arcsin()`](generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin
    "torch.Tensor.arcsin") 的原地版本 |'
- en: '| [`Tensor.as_strided`](generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided
    "torch.Tensor.as_strided") | See [`torch.as_strided()`](generated/torch.as_strided.html#torch.as_strided
    "torch.as_strided") |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.as_strided`](generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided
    "torch.Tensor.as_strided") | 参见 [`torch.as_strided()`](generated/torch.as_strided.html#torch.as_strided
    "torch.as_strided") |'
- en: '| [`Tensor.atan`](generated/torch.Tensor.atan.html#torch.Tensor.atan "torch.Tensor.atan")
    | See [`torch.atan()`](generated/torch.atan.html#torch.atan "torch.atan") |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.atan`](generated/torch.Tensor.atan.html#torch.Tensor.atan "torch.Tensor.atan")
    | 参见 [`torch.atan()`](generated/torch.atan.html#torch.atan "torch.atan") |'
- en: '| [`Tensor.atan_`](generated/torch.Tensor.atan_.html#torch.Tensor.atan_ "torch.Tensor.atan_")
    | In-place version of [`atan()`](generated/torch.Tensor.atan.html#torch.Tensor.atan
    "torch.Tensor.atan") |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.atan_`](generated/torch.Tensor.atan_.html#torch.Tensor.atan_ "torch.Tensor.atan_")
    | [`atan()`](generated/torch.Tensor.atan.html#torch.Tensor.atan "torch.Tensor.atan")
    的原地版本 |'
- en: '| [`Tensor.arctan`](generated/torch.Tensor.arctan.html#torch.Tensor.arctan
    "torch.Tensor.arctan") | See [`torch.arctan()`](generated/torch.arctan.html#torch.arctan
    "torch.arctan") |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arctan`](generated/torch.Tensor.arctan.html#torch.Tensor.arctan
    "torch.Tensor.arctan") | 参见 [`torch.arctan()`](generated/torch.arctan.html#torch.arctan
    "torch.arctan") |'
- en: '| [`Tensor.arctan_`](generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_
    "torch.Tensor.arctan_") | In-place version of [`arctan()`](generated/torch.Tensor.arctan.html#torch.Tensor.arctan
    "torch.Tensor.arctan") |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arctan_`](generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_
    "torch.Tensor.arctan_") | [`arctan()`](generated/torch.Tensor.arctan.html#torch.Tensor.arctan
    "torch.Tensor.arctan") 的原地版本 |'
- en: '| [`Tensor.atan2`](generated/torch.Tensor.atan2.html#torch.Tensor.atan2 "torch.Tensor.atan2")
    | See [`torch.atan2()`](generated/torch.atan2.html#torch.atan2 "torch.atan2")
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.atan2`](generated/torch.Tensor.atan2.html#torch.Tensor.atan2 "torch.Tensor.atan2")
    | 参见 [`torch.atan2()`](generated/torch.atan2.html#torch.atan2 "torch.atan2") |'
- en: '| [`Tensor.atan2_`](generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_
    "torch.Tensor.atan2_") | In-place version of [`atan2()`](generated/torch.Tensor.atan2.html#torch.Tensor.atan2
    "torch.Tensor.atan2") |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.atan2_`](generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_
    "torch.Tensor.atan2_") | [`atan2()`](generated/torch.Tensor.atan2.html#torch.Tensor.atan2
    "torch.Tensor.atan2") 的原地版本 |'
- en: '| [`Tensor.arctan2`](generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2
    "torch.Tensor.arctan2") | See [`torch.arctan2()`](generated/torch.arctan2.html#torch.arctan2
    "torch.arctan2") |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arctan2`](generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2
    "torch.Tensor.arctan2") | 参见 [`torch.arctan2()`](generated/torch.arctan2.html#torch.arctan2
    "torch.arctan2") |'
- en: '| [`Tensor.arctan2_`](generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_
    "torch.Tensor.arctan2_") | atan2_(other) -> Tensor |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arctan2_`](generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_
    "torch.Tensor.arctan2_") | atan2_(other) -> Tensor |'
- en: '| [`Tensor.all`](generated/torch.Tensor.all.html#torch.Tensor.all "torch.Tensor.all")
    | See [`torch.all()`](generated/torch.all.html#torch.all "torch.all") |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.all`](generated/torch.Tensor.all.html#torch.Tensor.all "torch.Tensor.all")
    | 参见 [`torch.all()`](generated/torch.all.html#torch.all "torch.all") |'
- en: '| [`Tensor.any`](generated/torch.Tensor.any.html#torch.Tensor.any "torch.Tensor.any")
    | See [`torch.any()`](generated/torch.any.html#torch.any "torch.any") |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.any`](generated/torch.Tensor.any.html#torch.Tensor.any "torch.Tensor.any")
    | 查看 [`torch.any()`](generated/torch.any.html#torch.any "torch.any") |'
- en: '| [`Tensor.backward`](generated/torch.Tensor.backward.html#torch.Tensor.backward
    "torch.Tensor.backward") | Computes the gradient of current tensor wrt graph leaves.
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.backward`](generated/torch.Tensor.backward.html#torch.Tensor.backward
    "torch.Tensor.backward") | 计算当前张量相对于图中叶子节点的梯度。 |'
- en: '| [`Tensor.baddbmm`](generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm
    "torch.Tensor.baddbmm") | See [`torch.baddbmm()`](generated/torch.baddbmm.html#torch.baddbmm
    "torch.baddbmm") |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.baddbmm`](generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm
    "torch.Tensor.baddbmm") | 查看 [`torch.baddbmm()`](generated/torch.baddbmm.html#torch.baddbmm
    "torch.baddbmm") |'
- en: '| [`Tensor.baddbmm_`](generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_
    "torch.Tensor.baddbmm_") | In-place version of [`baddbmm()`](generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm
    "torch.Tensor.baddbmm") |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.baddbmm_`](generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_
    "torch.Tensor.baddbmm_") | [`baddbmm()`](generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm
    "torch.Tensor.baddbmm") 的原地版本 |'
- en: '| [`Tensor.bernoulli`](generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli
    "torch.Tensor.bernoulli") | Returns a result tensor where each $\texttt{result[i]}$result[i]
    is independently sampled from $\text{Bernoulli}(\texttt{self[i]})$Bernoulli(self[i]).
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bernoulli`](generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli
    "torch.Tensor.bernoulli") | 返回一个结果张量，其中每个 $\texttt{result[i]}$ 从 $\text{Bernoulli}(\texttt{self[i]})$
    独立采样。 |'
- en: '| [`Tensor.bernoulli_`](generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_
    "torch.Tensor.bernoulli_") | Fills each location of `self` with an independent
    sample from $\text{Bernoulli}(\texttt{p})$Bernoulli(p). |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bernoulli_`](generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_
    "torch.Tensor.bernoulli_") | 用来自 $\text{Bernoulli}(\texttt{p})$ 的独立样本填充 `self`
    的每个位置。 |'
- en: '| [`Tensor.bfloat16`](generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16
    "torch.Tensor.bfloat16") | `self.bfloat16()` is equivalent to `self.to(torch.bfloat16)`.
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bfloat16`](generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16
    "torch.Tensor.bfloat16") | `self.bfloat16()` 等同于 `self.to(torch.bfloat16)`。 |'
- en: '| [`Tensor.bincount`](generated/torch.Tensor.bincount.html#torch.Tensor.bincount
    "torch.Tensor.bincount") | See [`torch.bincount()`](generated/torch.bincount.html#torch.bincount
    "torch.bincount") |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bincount`](generated/torch.Tensor.bincount.html#torch.Tensor.bincount
    "torch.Tensor.bincount") | 查看 [`torch.bincount()`](generated/torch.bincount.html#torch.bincount
    "torch.bincount") |'
- en: '| [`Tensor.bitwise_not`](generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not
    "torch.Tensor.bitwise_not") | See [`torch.bitwise_not()`](generated/torch.bitwise_not.html#torch.bitwise_not
    "torch.bitwise_not") |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_not`](generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not
    "torch.Tensor.bitwise_not") | 查看 [`torch.bitwise_not()`](generated/torch.bitwise_not.html#torch.bitwise_not
    "torch.bitwise_not") |'
- en: '| [`Tensor.bitwise_not_`](generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_
    "torch.Tensor.bitwise_not_") | In-place version of [`bitwise_not()`](generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not
    "torch.Tensor.bitwise_not") |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_not_`](generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_
    "torch.Tensor.bitwise_not_") | [`bitwise_not()`](generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not
    "torch.Tensor.bitwise_not") 的原地版本 |'
- en: '| [`Tensor.bitwise_and`](generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and
    "torch.Tensor.bitwise_and") | See [`torch.bitwise_and()`](generated/torch.bitwise_and.html#torch.bitwise_and
    "torch.bitwise_and") |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_and`](generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and
    "torch.Tensor.bitwise_and") | 查看 [`torch.bitwise_and()`](generated/torch.bitwise_and.html#torch.bitwise_and
    "torch.bitwise_and") |'
- en: '| [`Tensor.bitwise_and_`](generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_
    "torch.Tensor.bitwise_and_") | In-place version of [`bitwise_and()`](generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and
    "torch.Tensor.bitwise_and") |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_and_`](generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_
    "torch.Tensor.bitwise_and_") | [`bitwise_and()`](generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and
    "torch.Tensor.bitwise_and") 的原地版本 |'
- en: '| [`Tensor.bitwise_or`](generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or
    "torch.Tensor.bitwise_or") | See [`torch.bitwise_or()`](generated/torch.bitwise_or.html#torch.bitwise_or
    "torch.bitwise_or") |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_or`](generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or
    "torch.Tensor.bitwise_or") | 查看 [`torch.bitwise_or()`](generated/torch.bitwise_or.html#torch.bitwise_or
    "torch.bitwise_or") |'
- en: '| [`Tensor.bitwise_or_`](generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_
    "torch.Tensor.bitwise_or_") | In-place version of [`bitwise_or()`](generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or
    "torch.Tensor.bitwise_or") |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_or_`](generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_
    "torch.Tensor.bitwise_or_") | [`bitwise_or()`](generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or
    "torch.Tensor.bitwise_or") 的原地版本 |'
- en: '| [`Tensor.bitwise_xor`](generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor
    "torch.Tensor.bitwise_xor") | See [`torch.bitwise_xor()`](generated/torch.bitwise_xor.html#torch.bitwise_xor
    "torch.bitwise_xor") |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_xor`](generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor
    "torch.Tensor.bitwise_xor") | 查看 [`torch.bitwise_xor()`](generated/torch.bitwise_xor.html#torch.bitwise_xor
    "torch.bitwise_xor") |'
- en: '| [`Tensor.bitwise_xor_`](generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_
    "torch.Tensor.bitwise_xor_") | In-place version of [`bitwise_xor()`](generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor
    "torch.Tensor.bitwise_xor") |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_xor_`](generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_
    "torch.Tensor.bitwise_xor_") | [`bitwise_xor()`](generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor
    "torch.Tensor.bitwise_xor") 的原地版本 |'
- en: '| [`Tensor.bitwise_left_shift`](generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift
    "torch.Tensor.bitwise_left_shift") | See [`torch.bitwise_left_shift()`](generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift
    "torch.bitwise_left_shift") |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_left_shift`](generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift
    "torch.Tensor.bitwise_left_shift") | 查看 [`torch.bitwise_left_shift()`](generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift
    "torch.bitwise_left_shift") |'
- en: '| [`Tensor.bitwise_left_shift_`](generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_
    "torch.Tensor.bitwise_left_shift_") | In-place version of [`bitwise_left_shift()`](generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift
    "torch.Tensor.bitwise_left_shift") |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_left_shift_`](generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_
    "torch.Tensor.bitwise_left_shift_") | [`bitwise_left_shift()`](generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift
    "torch.Tensor.bitwise_left_shift") 的原地版本 |'
- en: '| [`Tensor.bitwise_right_shift`](generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift
    "torch.Tensor.bitwise_right_shift") | See [`torch.bitwise_right_shift()`](generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift
    "torch.bitwise_right_shift") |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_right_shift`](generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift
    "torch.Tensor.bitwise_right_shift") | 参见 [`torch.bitwise_right_shift()`](generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift
    "torch.bitwise_right_shift") |'
- en: '| [`Tensor.bitwise_right_shift_`](generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_
    "torch.Tensor.bitwise_right_shift_") | In-place version of [`bitwise_right_shift()`](generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift
    "torch.Tensor.bitwise_right_shift") |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bitwise_right_shift_`](generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_
    "torch.Tensor.bitwise_right_shift_") | [`bitwise_right_shift()`](generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift
    "torch.Tensor.bitwise_right_shift") 的原地版本 |'
- en: '| [`Tensor.bmm`](generated/torch.Tensor.bmm.html#torch.Tensor.bmm "torch.Tensor.bmm")
    | See [`torch.bmm()`](generated/torch.bmm.html#torch.bmm "torch.bmm") |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bmm`](generated/torch.Tensor.bmm.html#torch.Tensor.bmm "torch.Tensor.bmm")
    | 参见 [`torch.bmm()`](generated/torch.bmm.html#torch.bmm "torch.bmm") |'
- en: '| [`Tensor.bool`](generated/torch.Tensor.bool.html#torch.Tensor.bool "torch.Tensor.bool")
    | `self.bool()` is equivalent to `self.to(torch.bool)`. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.bool`](generated/torch.Tensor.bool.html#torch.Tensor.bool "torch.Tensor.bool")
    | `self.bool()` 等同于 `self.to(torch.bool)`。 |'
- en: '| [`Tensor.byte`](generated/torch.Tensor.byte.html#torch.Tensor.byte "torch.Tensor.byte")
    | `self.byte()` is equivalent to `self.to(torch.uint8)`. |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.byte`](generated/torch.Tensor.byte.html#torch.Tensor.byte "torch.Tensor.byte")
    | `self.byte()` 等同于 `self.to(torch.uint8)`。 |'
- en: '| [`Tensor.broadcast_to`](generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to
    "torch.Tensor.broadcast_to") | See [`torch.broadcast_to()`](generated/torch.broadcast_to.html#torch.broadcast_to
    "torch.broadcast_to"). |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.broadcast_to`](generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to
    "torch.Tensor.broadcast_to") | 参见 [`torch.broadcast_to()`](generated/torch.broadcast_to.html#torch.broadcast_to
    "torch.broadcast_to"). |'
- en: '| [`Tensor.cauchy_`](generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_
    "torch.Tensor.cauchy_") | Fills the tensor with numbers drawn from the Cauchy
    distribution: |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cauchy_`](generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_
    "torch.Tensor.cauchy_") | 用从 Cauchy 分布中抽取的数字填充张量： |'
- en: '| [`Tensor.ceil`](generated/torch.Tensor.ceil.html#torch.Tensor.ceil "torch.Tensor.ceil")
    | See [`torch.ceil()`](generated/torch.ceil.html#torch.ceil "torch.ceil") |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ceil`](generated/torch.Tensor.ceil.html#torch.Tensor.ceil "torch.Tensor.ceil")
    | 参见 [`torch.ceil()`](generated/torch.ceil.html#torch.ceil "torch.ceil") |'
- en: '| [`Tensor.ceil_`](generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_ "torch.Tensor.ceil_")
    | In-place version of [`ceil()`](generated/torch.Tensor.ceil.html#torch.Tensor.ceil
    "torch.Tensor.ceil") |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ceil_`](generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_ "torch.Tensor.ceil_")
    | [`ceil()`](generated/torch.Tensor.ceil.html#torch.Tensor.ceil "torch.Tensor.ceil")
    的原地版本 |'
- en: '| [`Tensor.char`](generated/torch.Tensor.char.html#torch.Tensor.char "torch.Tensor.char")
    | `self.char()` is equivalent to `self.to(torch.int8)`. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.char`](generated/torch.Tensor.char.html#torch.Tensor.char "torch.Tensor.char")
    | `self.char()` 等同于 `self.to(torch.int8)`。 |'
- en: '| [`Tensor.cholesky`](generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky
    "torch.Tensor.cholesky") | See [`torch.cholesky()`](generated/torch.cholesky.html#torch.cholesky
    "torch.cholesky") |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cholesky`](generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky
    "torch.Tensor.cholesky") | 参见 [`torch.cholesky()`](generated/torch.cholesky.html#torch.cholesky
    "torch.cholesky") |'
- en: '| [`Tensor.cholesky_inverse`](generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse
    "torch.Tensor.cholesky_inverse") | See [`torch.cholesky_inverse()`](generated/torch.cholesky_inverse.html#torch.cholesky_inverse
    "torch.cholesky_inverse") |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cholesky_inverse`](generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse
    "torch.Tensor.cholesky_inverse") | 参见 [`torch.cholesky_inverse()`](generated/torch.cholesky_inverse.html#torch.cholesky_inverse
    "torch.cholesky_inverse") |'
- en: '| [`Tensor.cholesky_solve`](generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve
    "torch.Tensor.cholesky_solve") | See [`torch.cholesky_solve()`](generated/torch.cholesky_solve.html#torch.cholesky_solve
    "torch.cholesky_solve") |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cholesky_solve`](generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve
    "torch.Tensor.cholesky_solve") | 参见 [`torch.cholesky_solve()`](generated/torch.cholesky_solve.html#torch.cholesky_solve
    "torch.cholesky_solve") |'
- en: '| [`Tensor.chunk`](generated/torch.Tensor.chunk.html#torch.Tensor.chunk "torch.Tensor.chunk")
    | See [`torch.chunk()`](generated/torch.chunk.html#torch.chunk "torch.chunk")
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.chunk`](generated/torch.Tensor.chunk.html#torch.Tensor.chunk "torch.Tensor.chunk")
    | 参见 [`torch.chunk()`](generated/torch.chunk.html#torch.chunk "torch.chunk") |'
- en: '| [`Tensor.clamp`](generated/torch.Tensor.clamp.html#torch.Tensor.clamp "torch.Tensor.clamp")
    | See [`torch.clamp()`](generated/torch.clamp.html#torch.clamp "torch.clamp")
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.clamp`](generated/torch.Tensor.clamp.html#torch.Tensor.clamp "torch.Tensor.clamp")
    | 参见 [`torch.clamp()`](generated/torch.clamp.html#torch.clamp "torch.clamp") |'
- en: '| [`Tensor.clamp_`](generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_
    "torch.Tensor.clamp_") | In-place version of [`clamp()`](generated/torch.Tensor.clamp.html#torch.Tensor.clamp
    "torch.Tensor.clamp") |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.clamp_`](generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_
    "torch.Tensor.clamp_") | [`clamp()`](generated/torch.Tensor.clamp.html#torch.Tensor.clamp
    "torch.Tensor.clamp") 的原地版本 |'
- en: '| [`Tensor.clip`](generated/torch.Tensor.clip.html#torch.Tensor.clip "torch.Tensor.clip")
    | Alias for [`clamp()`](generated/torch.Tensor.clamp.html#torch.Tensor.clamp "torch.Tensor.clamp").
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.clip`](generated/torch.Tensor.clip.html#torch.Tensor.clip "torch.Tensor.clip")
    | [`clamp()`](generated/torch.Tensor.clamp.html#torch.Tensor.clamp "torch.Tensor.clamp")
    的别名 |'
- en: '| [`Tensor.clip_`](generated/torch.Tensor.clip_.html#torch.Tensor.clip_ "torch.Tensor.clip_")
    | Alias for [`clamp_()`](generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_
    "torch.Tensor.clamp_"). |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.clip_`](generated/torch.Tensor.clip_.html#torch.Tensor.clip_ "torch.Tensor.clip_")
    | [`clamp_()`](generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_ "torch.Tensor.clamp_")
    的别名。 |'
- en: '| [`Tensor.clone`](generated/torch.Tensor.clone.html#torch.Tensor.clone "torch.Tensor.clone")
    | See [`torch.clone()`](generated/torch.clone.html#torch.clone "torch.clone")
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.clone`](generated/torch.Tensor.clone.html#torch.Tensor.clone "torch.Tensor.clone")
    | 查看 [`torch.clone()`](generated/torch.clone.html#torch.clone "torch.clone") |'
- en: '| [`Tensor.contiguous`](generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous
    "torch.Tensor.contiguous") | Returns a contiguous in memory tensor containing
    the same data as `self` tensor. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.contiguous`](generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous
    "torch.Tensor.contiguous") | 返回一个包含与 `self` 张量相同数据的内存连续张量 |'
- en: '| [`Tensor.copy_`](generated/torch.Tensor.copy_.html#torch.Tensor.copy_ "torch.Tensor.copy_")
    | Copies the elements from `src` into `self` tensor and returns `self`. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.copy_`](generated/torch.Tensor.copy_.html#torch.Tensor.copy_ "torch.Tensor.copy_")
    | 将 `src` 中的元素复制到 `self` 张量中并返回 `self` |'
- en: '| [`Tensor.conj`](generated/torch.Tensor.conj.html#torch.Tensor.conj "torch.Tensor.conj")
    | See [`torch.conj()`](generated/torch.conj.html#torch.conj "torch.conj") |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.conj`](generated/torch.Tensor.conj.html#torch.Tensor.conj "torch.Tensor.conj")
    | 查看 [`torch.conj()`](generated/torch.conj.html#torch.conj "torch.conj") |'
- en: '| [`Tensor.conj_physical`](generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical
    "torch.Tensor.conj_physical") | See [`torch.conj_physical()`](generated/torch.conj_physical.html#torch.conj_physical
    "torch.conj_physical") |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.conj_physical`](generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical
    "torch.Tensor.conj_physical") | 查看 [`torch.conj_physical()`](generated/torch.conj_physical.html#torch.conj_physical
    "torch.conj_physical") |'
- en: '| [`Tensor.conj_physical_`](generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_
    "torch.Tensor.conj_physical_") | In-place version of [`conj_physical()`](generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical
    "torch.Tensor.conj_physical") |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.conj_physical_`](generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_
    "torch.Tensor.conj_physical_") | [`conj_physical()`](generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical
    "torch.Tensor.conj_physical") 的原地版本 |'
- en: '| [`Tensor.resolve_conj`](generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj
    "torch.Tensor.resolve_conj") | See [`torch.resolve_conj()`](generated/torch.resolve_conj.html#torch.resolve_conj
    "torch.resolve_conj") |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.resolve_conj`](generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj
    "torch.Tensor.resolve_conj") | 查看 [`torch.resolve_conj()`](generated/torch.resolve_conj.html#torch.resolve_conj
    "torch.resolve_conj") |'
- en: '| [`Tensor.resolve_neg`](generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg
    "torch.Tensor.resolve_neg") | See [`torch.resolve_neg()`](generated/torch.resolve_neg.html#torch.resolve_neg
    "torch.resolve_neg") |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.resolve_neg`](generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg
    "torch.Tensor.resolve_neg") | 查看 [`torch.resolve_neg()`](generated/torch.resolve_neg.html#torch.resolve_neg
    "torch.resolve_neg") |'
- en: '| [`Tensor.copysign`](generated/torch.Tensor.copysign.html#torch.Tensor.copysign
    "torch.Tensor.copysign") | See [`torch.copysign()`](generated/torch.copysign.html#torch.copysign
    "torch.copysign") |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.copysign`](generated/torch.Tensor.copysign.html#torch.Tensor.copysign
    "torch.Tensor.copysign") | 查看 [`torch.copysign()`](generated/torch.copysign.html#torch.copysign
    "torch.copysign") |'
- en: '| [`Tensor.copysign_`](generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_
    "torch.Tensor.copysign_") | In-place version of [`copysign()`](generated/torch.Tensor.copysign.html#torch.Tensor.copysign
    "torch.Tensor.copysign") |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.copysign_`](generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_
    "torch.Tensor.copysign_") | [`copysign()`](generated/torch.Tensor.copysign.html#torch.Tensor.copysign
    "torch.Tensor.copysign") 的原地版本 |'
- en: '| [`Tensor.cos`](generated/torch.Tensor.cos.html#torch.Tensor.cos "torch.Tensor.cos")
    | See [`torch.cos()`](generated/torch.cos.html#torch.cos "torch.cos") |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cos`](generated/torch.Tensor.cos.html#torch.Tensor.cos "torch.Tensor.cos")
    | 查看 [`torch.cos()`](generated/torch.cos.html#torch.cos "torch.cos") |'
- en: '| [`Tensor.cos_`](generated/torch.Tensor.cos_.html#torch.Tensor.cos_ "torch.Tensor.cos_")
    | In-place version of [`cos()`](generated/torch.Tensor.cos.html#torch.Tensor.cos
    "torch.Tensor.cos") |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cos_`](generated/torch.Tensor.cos_.html#torch.Tensor.cos_ "torch.Tensor.cos_")
    | [`cos()`](generated/torch.Tensor.cos.html#torch.Tensor.cos "torch.Tensor.cos")
    的原地版本 |'
- en: '| [`Tensor.cosh`](generated/torch.Tensor.cosh.html#torch.Tensor.cosh "torch.Tensor.cosh")
    | See [`torch.cosh()`](generated/torch.cosh.html#torch.cosh "torch.cosh") |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cosh`](generated/torch.Tensor.cosh.html#torch.Tensor.cosh "torch.Tensor.cosh")
    | 查看 [`torch.cosh()`](generated/torch.cosh.html#torch.cosh "torch.cosh") |'
- en: '| [`Tensor.cosh_`](generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_ "torch.Tensor.cosh_")
    | In-place version of [`cosh()`](generated/torch.Tensor.cosh.html#torch.Tensor.cosh
    "torch.Tensor.cosh") |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cosh_`](generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_ "torch.Tensor.cosh_")
    | [`cosh()`](generated/torch.Tensor.cosh.html#torch.Tensor.cosh "torch.Tensor.cosh")
    的原地版本 |'
- en: '| [`Tensor.corrcoef`](generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef
    "torch.Tensor.corrcoef") | See [`torch.corrcoef()`](generated/torch.corrcoef.html#torch.corrcoef
    "torch.corrcoef") |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.corrcoef`](generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef
    "torch.Tensor.corrcoef") | 查看 [`torch.corrcoef()`](generated/torch.corrcoef.html#torch.corrcoef
    "torch.corrcoef") |'
- en: '| [`Tensor.count_nonzero`](generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero
    "torch.Tensor.count_nonzero") | See [`torch.count_nonzero()`](generated/torch.count_nonzero.html#torch.count_nonzero
    "torch.count_nonzero") |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.count_nonzero`](generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero
    "torch.Tensor.count_nonzero") | 查看 [`torch.count_nonzero()`](generated/torch.count_nonzero.html#torch.count_nonzero
    "torch.count_nonzero") |'
- en: '| [`Tensor.cov`](generated/torch.Tensor.cov.html#torch.Tensor.cov "torch.Tensor.cov")
    | See [`torch.cov()`](generated/torch.cov.html#torch.cov "torch.cov") |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cov`](generated/torch.Tensor.cov.html#torch.Tensor.cov "torch.Tensor.cov")
    | 查看 [`torch.cov()`](generated/torch.cov.html#torch.cov "torch.cov") |'
- en: '| [`Tensor.acosh`](generated/torch.Tensor.acosh.html#torch.Tensor.acosh "torch.Tensor.acosh")
    | See [`torch.acosh()`](generated/torch.acosh.html#torch.acosh "torch.acosh")
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.acosh`](generated/torch.Tensor.acosh.html#torch.Tensor.acosh "torch.Tensor.acosh")
    | 查看 [`torch.acosh()`](generated/torch.acosh.html#torch.acosh "torch.acosh") |'
- en: '| [`Tensor.acosh_`](generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_
    "torch.Tensor.acosh_") | In-place version of [`acosh()`](generated/torch.Tensor.acosh.html#torch.Tensor.acosh
    "torch.Tensor.acosh") |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.acosh_`](generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_
    "torch.Tensor.acosh_") | [`acosh()`](generated/torch.Tensor.acosh.html#torch.Tensor.acosh
    "torch.Tensor.acosh") 的原地版本 |'
- en: '| [`Tensor.arccosh`](generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh
    "torch.Tensor.arccosh") | acosh() -> Tensor |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arccosh`](generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh
    "torch.Tensor.arccosh") | acosh() -> Tensor |'
- en: '| [`Tensor.arccosh_`](generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_
    "torch.Tensor.arccosh_") | acosh_() -> Tensor |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arccosh_`](generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_
    "torch.Tensor.arccosh_") | acosh_() -> Tensor |'
- en: '| [`Tensor.cpu`](generated/torch.Tensor.cpu.html#torch.Tensor.cpu "torch.Tensor.cpu")
    | Returns a copy of this object in CPU memory. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cpu`](generated/torch.Tensor.cpu.html#torch.Tensor.cpu "torch.Tensor.cpu")
    | 返回此对象在CPU内存中的副本 |'
- en: '| [`Tensor.cross`](generated/torch.Tensor.cross.html#torch.Tensor.cross "torch.Tensor.cross")
    | See [`torch.cross()`](generated/torch.cross.html#torch.cross "torch.cross")
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cross`](generated/torch.Tensor.cross.html#torch.Tensor.cross "torch.Tensor.cross")
    | 参见`torch.cross()` |'
- en: '| [`Tensor.cuda`](generated/torch.Tensor.cuda.html#torch.Tensor.cuda "torch.Tensor.cuda")
    | Returns a copy of this object in CUDA memory. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cuda`](generated/torch.Tensor.cuda.html#torch.Tensor.cuda "torch.Tensor.cuda")
    | 返回此对象在CUDA内存中的副本 |'
- en: '| [`Tensor.logcumsumexp`](generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp
    "torch.Tensor.logcumsumexp") | See [`torch.logcumsumexp()`](generated/torch.logcumsumexp.html#torch.logcumsumexp
    "torch.logcumsumexp") |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logcumsumexp`](generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp
    "torch.Tensor.logcumsumexp") | 参见`torch.logcumsumexp()` |'
- en: '| [`Tensor.cummax`](generated/torch.Tensor.cummax.html#torch.Tensor.cummax
    "torch.Tensor.cummax") | See [`torch.cummax()`](generated/torch.cummax.html#torch.cummax
    "torch.cummax") |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cummax`](generated/torch.Tensor.cummax.html#torch.Tensor.cummax
    "torch.Tensor.cummax") | 参见`torch.cummax()` |'
- en: '| [`Tensor.cummin`](generated/torch.Tensor.cummin.html#torch.Tensor.cummin
    "torch.Tensor.cummin") | See [`torch.cummin()`](generated/torch.cummin.html#torch.cummin
    "torch.cummin") |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cummin`](generated/torch.Tensor.cummin.html#torch.Tensor.cummin
    "torch.Tensor.cummin") | 参见`torch.cummin()` |'
- en: '| [`Tensor.cumprod`](generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod
    "torch.Tensor.cumprod") | See [`torch.cumprod()`](generated/torch.cumprod.html#torch.cumprod
    "torch.cumprod") |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cumprod`](generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod
    "torch.Tensor.cumprod") | 参见`torch.cumprod()` |'
- en: '| [`Tensor.cumprod_`](generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_
    "torch.Tensor.cumprod_") | In-place version of [`cumprod()`](generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod
    "torch.Tensor.cumprod") |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cumprod_`](generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_
    "torch.Tensor.cumprod_") | `cumprod()`的原位版本 |'
- en: '| [`Tensor.cumsum`](generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum
    "torch.Tensor.cumsum") | See [`torch.cumsum()`](generated/torch.cumsum.html#torch.cumsum
    "torch.cumsum") |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cumsum`](generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum
    "torch.Tensor.cumsum") | 参见`torch.cumsum()` |'
- en: '| [`Tensor.cumsum_`](generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_
    "torch.Tensor.cumsum_") | In-place version of [`cumsum()`](generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum
    "torch.Tensor.cumsum") |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cumsum_`](generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_
    "torch.Tensor.cumsum_") | `cumsum()`的原位版本 |'
- en: '| [`Tensor.chalf`](generated/torch.Tensor.chalf.html#torch.Tensor.chalf "torch.Tensor.chalf")
    | `self.chalf()` is equivalent to `self.to(torch.complex32)`. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.chalf`](generated/torch.Tensor.chalf.html#torch.Tensor.chalf "torch.Tensor.chalf")
    | `self.chalf()`等同于`self.to(torch.complex32)` |'
- en: '| [`Tensor.cfloat`](generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat
    "torch.Tensor.cfloat") | `self.cfloat()` is equivalent to `self.to(torch.complex64)`.
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cfloat`](generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat
    "torch.Tensor.cfloat") | `self.cfloat()`等同于`self.to(torch.complex64)` |'
- en: '| [`Tensor.cdouble`](generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble
    "torch.Tensor.cdouble") | `self.cdouble()` is equivalent to `self.to(torch.complex128)`.
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.cdouble`](generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble
    "torch.Tensor.cdouble") | `self.cdouble()`等同于`self.to(torch.complex128)` |'
- en: '| [`Tensor.data_ptr`](generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr
    "torch.Tensor.data_ptr") | Returns the address of the first element of `self`
    tensor. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.data_ptr`](generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr
    "torch.Tensor.data_ptr") | 返回`self`张量的第一个元素的地址 |'
- en: '| [`Tensor.deg2rad`](generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad
    "torch.Tensor.deg2rad") | See [`torch.deg2rad()`](generated/torch.deg2rad.html#torch.deg2rad
    "torch.deg2rad") |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.deg2rad`](generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad
    "torch.Tensor.deg2rad") | 参见`torch.deg2rad()` |'
- en: '| [`Tensor.dequantize`](generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize
    "torch.Tensor.dequantize") | Given a quantized Tensor, dequantize it and return
    the dequantized float Tensor. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.dequantize`](generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize
    "torch.Tensor.dequantize") | 给定一个量化张量，对其进行去量化并返回去量化的浮点张量 |'
- en: '| [`Tensor.det`](generated/torch.Tensor.det.html#torch.Tensor.det "torch.Tensor.det")
    | See [`torch.det()`](generated/torch.det.html#torch.det "torch.det") |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.det`](generated/torch.Tensor.det.html#torch.Tensor.det "torch.Tensor.det")
    | 参见`torch.det()` |'
- en: '| [`Tensor.dense_dim`](generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim
    "torch.Tensor.dense_dim") | Return the number of dense dimensions in a [sparse
    tensor](sparse.html#sparse-docs) `self`. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.dense_dim`](generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim
    "torch.Tensor.dense_dim") | 返回[稀疏张量](sparse.html#sparse-docs) `self` 中的密集维度数 |'
- en: '| [`Tensor.detach`](generated/torch.Tensor.detach.html#torch.Tensor.detach
    "torch.Tensor.detach") | Returns a new Tensor, detached from the current graph.
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.detach`](generated/torch.Tensor.detach.html#torch.Tensor.detach
    "torch.Tensor.detach") | 返回一个从当前图中分离出来的新张量 |'
- en: '| [`Tensor.detach_`](generated/torch.Tensor.detach_.html#torch.Tensor.detach_
    "torch.Tensor.detach_") | Detaches the Tensor from the graph that created it,
    making it a leaf. |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.detach_`](generated/torch.Tensor.detach_.html#torch.Tensor.detach_
    "torch.Tensor.detach_") | 将张量从创建它的图中分离出来，使其成为叶子节点 |'
- en: '| [`Tensor.diag`](generated/torch.Tensor.diag.html#torch.Tensor.diag "torch.Tensor.diag")
    | See [`torch.diag()`](generated/torch.diag.html#torch.diag "torch.diag") |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.diag`](generated/torch.Tensor.diag.html#torch.Tensor.diag "torch.Tensor.diag")
    | 参见`torch.diag()` |'
- en: '| [`Tensor.diag_embed`](generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed
    "torch.Tensor.diag_embed") | See [`torch.diag_embed()`](generated/torch.diag_embed.html#torch.diag_embed
    "torch.diag_embed") |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.diag_embed`](generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed
    "torch.Tensor.diag_embed") | 查看 [`torch.diag_embed()`](generated/torch.diag_embed.html#torch.diag_embed
    "torch.diag_embed") |'
- en: '| [`Tensor.diagflat`](generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat
    "torch.Tensor.diagflat") | See [`torch.diagflat()`](generated/torch.diagflat.html#torch.diagflat
    "torch.diagflat") |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.diagflat`](generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat
    "torch.Tensor.diagflat") | 查看 [`torch.diagflat()`](generated/torch.diagflat.html#torch.diagflat
    "torch.diagflat") |'
- en: '| [`Tensor.diagonal`](generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal
    "torch.Tensor.diagonal") | See [`torch.diagonal()`](generated/torch.diagonal.html#torch.diagonal
    "torch.diagonal") |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.diagonal`](generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal
    "torch.Tensor.diagonal") | 查看 [`torch.diagonal()`](generated/torch.diagonal.html#torch.diagonal
    "torch.diagonal") |'
- en: '| [`Tensor.diagonal_scatter`](generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter
    "torch.Tensor.diagonal_scatter") | See [`torch.diagonal_scatter()`](generated/torch.diagonal_scatter.html#torch.diagonal_scatter
    "torch.diagonal_scatter") |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.diagonal_scatter`](generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter
    "torch.Tensor.diagonal_scatter") | 查看 [`torch.diagonal_scatter()`](generated/torch.diagonal_scatter.html#torch.diagonal_scatter
    "torch.diagonal_scatter") |'
- en: '| [`Tensor.fill_diagonal_`](generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_
    "torch.Tensor.fill_diagonal_") | Fill the main diagonal of a tensor that has at
    least 2-dimensions. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.fill_diagonal_`](generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_
    "torch.Tensor.fill_diagonal_") | 填充至少为2维的张量的主对角线。 |'
- en: '| [`Tensor.fmax`](generated/torch.Tensor.fmax.html#torch.Tensor.fmax "torch.Tensor.fmax")
    | See [`torch.fmax()`](generated/torch.fmax.html#torch.fmax "torch.fmax") |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.fmax`](generated/torch.Tensor.fmax.html#torch.Tensor.fmax "torch.Tensor.fmax")
    | 查看 [`torch.fmax()`](generated/torch.fmax.html#torch.fmax "torch.fmax") |'
- en: '| [`Tensor.fmin`](generated/torch.Tensor.fmin.html#torch.Tensor.fmin "torch.Tensor.fmin")
    | See [`torch.fmin()`](generated/torch.fmin.html#torch.fmin "torch.fmin") |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.fmin`](generated/torch.Tensor.fmin.html#torch.Tensor.fmin "torch.Tensor.fmin")
    | 查看 [`torch.fmin()`](generated/torch.fmin.html#torch.fmin "torch.fmin") |'
- en: '| [`Tensor.diff`](generated/torch.Tensor.diff.html#torch.Tensor.diff "torch.Tensor.diff")
    | See [`torch.diff()`](generated/torch.diff.html#torch.diff "torch.diff") |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.diff`](generated/torch.Tensor.diff.html#torch.Tensor.diff "torch.Tensor.diff")
    | 查看 [`torch.diff()`](generated/torch.diff.html#torch.diff "torch.diff") |'
- en: '| [`Tensor.digamma`](generated/torch.Tensor.digamma.html#torch.Tensor.digamma
    "torch.Tensor.digamma") | See [`torch.digamma()`](generated/torch.digamma.html#torch.digamma
    "torch.digamma") |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.digamma`](generated/torch.Tensor.digamma.html#torch.Tensor.digamma
    "torch.Tensor.digamma") | 查看 [`torch.digamma()`](generated/torch.digamma.html#torch.digamma
    "torch.digamma") |'
- en: '| [`Tensor.digamma_`](generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_
    "torch.Tensor.digamma_") | In-place version of [`digamma()`](generated/torch.Tensor.digamma.html#torch.Tensor.digamma
    "torch.Tensor.digamma") |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.digamma_`](generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_
    "torch.Tensor.digamma_") | [`digamma()`](generated/torch.Tensor.digamma.html#torch.Tensor.digamma
    "torch.Tensor.digamma") 的原地版本 |'
- en: '| [`Tensor.dim`](generated/torch.Tensor.dim.html#torch.Tensor.dim "torch.Tensor.dim")
    | Returns the number of dimensions of `self` tensor. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.dim`](generated/torch.Tensor.dim.html#torch.Tensor.dim "torch.Tensor.dim")
    | 返回 `self` 张量的维度数量。 |'
- en: '| [`Tensor.dim_order`](generated/torch.Tensor.dim_order.html#torch.Tensor.dim_order
    "torch.Tensor.dim_order") | Returns a tuple of int describing the dim order or
    physical layout of `self`. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.dim_order`](generated/torch.Tensor.dim_order.html#torch.Tensor.dim_order
    "torch.Tensor.dim_order") | 返回一个描述 `self` 张量维度顺序或物理布局的整数元组。 |'
- en: '| [`Tensor.dist`](generated/torch.Tensor.dist.html#torch.Tensor.dist "torch.Tensor.dist")
    | See [`torch.dist()`](generated/torch.dist.html#torch.dist "torch.dist") |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.dist`](generated/torch.Tensor.dist.html#torch.Tensor.dist "torch.Tensor.dist")
    | 查看 [`torch.dist()`](generated/torch.dist.html#torch.dist "torch.dist") |'
- en: '| [`Tensor.div`](generated/torch.Tensor.div.html#torch.Tensor.div "torch.Tensor.div")
    | See [`torch.div()`](generated/torch.div.html#torch.div "torch.div") |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.div`](generated/torch.Tensor.div.html#torch.Tensor.div "torch.Tensor.div")
    | 查看 [`torch.div()`](generated/torch.div.html#torch.div "torch.div") |'
- en: '| [`Tensor.div_`](generated/torch.Tensor.div_.html#torch.Tensor.div_ "torch.Tensor.div_")
    | In-place version of [`div()`](generated/torch.Tensor.div.html#torch.Tensor.div
    "torch.Tensor.div") |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.div_`](generated/torch.Tensor.div_.html#torch.Tensor.div_ "torch.Tensor.div_")
    | [`div()`](generated/torch.Tensor.div.html#torch.Tensor.div "torch.Tensor.div")
    的原地版本 |'
- en: '| [`Tensor.divide`](generated/torch.Tensor.divide.html#torch.Tensor.divide
    "torch.Tensor.divide") | See [`torch.divide()`](generated/torch.divide.html#torch.divide
    "torch.divide") |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.divide`](generated/torch.Tensor.divide.html#torch.Tensor.divide
    "torch.Tensor.divide") | 查看 [`torch.divide()`](generated/torch.divide.html#torch.divide
    "torch.divide") |'
- en: '| [`Tensor.divide_`](generated/torch.Tensor.divide_.html#torch.Tensor.divide_
    "torch.Tensor.divide_") | In-place version of [`divide()`](generated/torch.Tensor.divide.html#torch.Tensor.divide
    "torch.Tensor.divide") |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.divide_`](generated/torch.Tensor.divide_.html#torch.Tensor.divide_
    "torch.Tensor.divide_") | [`divide()`](generated/torch.Tensor.divide.html#torch.Tensor.divide
    "torch.Tensor.divide") 的原地版本 |'
- en: '| [`Tensor.dot`](generated/torch.Tensor.dot.html#torch.Tensor.dot "torch.Tensor.dot")
    | See [`torch.dot()`](generated/torch.dot.html#torch.dot "torch.dot") |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.dot`](generated/torch.Tensor.dot.html#torch.Tensor.dot "torch.Tensor.dot")
    | 查看 [`torch.dot()`](generated/torch.dot.html#torch.dot "torch.dot") |'
- en: '| [`Tensor.double`](generated/torch.Tensor.double.html#torch.Tensor.double
    "torch.Tensor.double") | `self.double()` is equivalent to `self.to(torch.float64)`.
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.double`](generated/torch.Tensor.double.html#torch.Tensor.double
    "torch.Tensor.double") | `self.double()` 等同于 `self.to(torch.float64)`。 |'
- en: '| [`Tensor.dsplit`](generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit
    "torch.Tensor.dsplit") | See [`torch.dsplit()`](generated/torch.dsplit.html#torch.dsplit
    "torch.dsplit") |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.dsplit`](generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit
    "torch.Tensor.dsplit") | 查看 [`torch.dsplit()`](generated/torch.dsplit.html#torch.dsplit
    "torch.dsplit") |'
- en: '| [`Tensor.element_size`](generated/torch.Tensor.element_size.html#torch.Tensor.element_size
    "torch.Tensor.element_size") | Returns the size in bytes of an individual element.
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.element_size`](generated/torch.Tensor.element_size.html#torch.Tensor.element_size
    "torch.Tensor.element_size") | 返回单个元素的字节大小。 |'
- en: '| [`Tensor.eq`](generated/torch.Tensor.eq.html#torch.Tensor.eq "torch.Tensor.eq")
    | See [`torch.eq()`](generated/torch.eq.html#torch.eq "torch.eq") |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.eq`](generated/torch.Tensor.eq.html#torch.Tensor.eq "torch.Tensor.eq")
    | 查看 [`torch.eq()`](generated/torch.eq.html#torch.eq "torch.eq") |'
- en: '| [`Tensor.eq_`](generated/torch.Tensor.eq_.html#torch.Tensor.eq_ "torch.Tensor.eq_")
    | In-place version of [`eq()`](generated/torch.Tensor.eq.html#torch.Tensor.eq
    "torch.Tensor.eq") |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.eq_`](generated/torch.Tensor.eq_.html#torch.Tensor.eq_ "torch.Tensor.eq_")
    | [`eq()`](generated/torch.Tensor.eq.html#torch.Tensor.eq "torch.Tensor.eq") 的原地版本
    |'
- en: '| [`Tensor.equal`](generated/torch.Tensor.equal.html#torch.Tensor.equal "torch.Tensor.equal")
    | See [`torch.equal()`](generated/torch.equal.html#torch.equal "torch.equal")
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.equal`](generated/torch.Tensor.equal.html#torch.Tensor.equal "torch.Tensor.equal")
    | 查看 [`torch.equal()`](generated/torch.equal.html#torch.equal "torch.equal") |'
- en: '| [`Tensor.erf`](generated/torch.Tensor.erf.html#torch.Tensor.erf "torch.Tensor.erf")
    | See [`torch.erf()`](generated/torch.erf.html#torch.erf "torch.erf") |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.erf`](generated/torch.Tensor.erf.html#torch.Tensor.erf "torch.Tensor.erf")
    | 查看 [`torch.erf()`](generated/torch.erf.html#torch.erf "torch.erf") |'
- en: '| [`Tensor.erf_`](generated/torch.Tensor.erf_.html#torch.Tensor.erf_ "torch.Tensor.erf_")
    | In-place version of [`erf()`](generated/torch.Tensor.erf.html#torch.Tensor.erf
    "torch.Tensor.erf") |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.erf_`](generated/torch.Tensor.erf_.html#torch.Tensor.erf_ "torch.Tensor.erf_")
    | [`erf()`](generated/torch.Tensor.erf.html#torch.Tensor.erf "torch.Tensor.erf")
    的原地版本 |'
- en: '| [`Tensor.erfc`](generated/torch.Tensor.erfc.html#torch.Tensor.erfc "torch.Tensor.erfc")
    | See [`torch.erfc()`](generated/torch.erfc.html#torch.erfc "torch.erfc") |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.erfc`](generated/torch.Tensor.erfc.html#torch.Tensor.erfc "torch.Tensor.erfc")
    | 查看 [`torch.erfc()`](generated/torch.erfc.html#torch.erfc "torch.erfc") |'
- en: '| [`Tensor.erfc_`](generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_ "torch.Tensor.erfc_")
    | In-place version of [`erfc()`](generated/torch.Tensor.erfc.html#torch.Tensor.erfc
    "torch.Tensor.erfc") |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.erfc_`](generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_ "torch.Tensor.erfc_")
    | [`erfc()`](generated/torch.Tensor.erfc.html#torch.Tensor.erfc "torch.Tensor.erfc")
    的原地版本 |'
- en: '| [`Tensor.erfinv`](generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv
    "torch.Tensor.erfinv") | See [`torch.erfinv()`](generated/torch.erfinv.html#torch.erfinv
    "torch.erfinv") |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.erfinv`](generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv
    "torch.Tensor.erfinv") | 查看 [`torch.erfinv()`](generated/torch.erfinv.html#torch.erfinv
    "torch.erfinv") |'
- en: '| [`Tensor.erfinv_`](generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_
    "torch.Tensor.erfinv_") | In-place version of [`erfinv()`](generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv
    "torch.Tensor.erfinv") |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.erfinv_`](generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_
    "torch.Tensor.erfinv_") | [`erfinv()`](generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv
    "torch.Tensor.erfinv") 的原地版本 |'
- en: '| [`Tensor.exp`](generated/torch.Tensor.exp.html#torch.Tensor.exp "torch.Tensor.exp")
    | See [`torch.exp()`](generated/torch.exp.html#torch.exp "torch.exp") |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.exp`](generated/torch.Tensor.exp.html#torch.Tensor.exp "torch.Tensor.exp")
    | 查看 [`torch.exp()`](generated/torch.exp.html#torch.exp "torch.exp") |'
- en: '| [`Tensor.exp_`](generated/torch.Tensor.exp_.html#torch.Tensor.exp_ "torch.Tensor.exp_")
    | In-place version of [`exp()`](generated/torch.Tensor.exp.html#torch.Tensor.exp
    "torch.Tensor.exp") |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.exp_`](generated/torch.Tensor.exp_.html#torch.Tensor.exp_ "torch.Tensor.exp_")
    | [`exp()`](generated/torch.Tensor.exp.html#torch.Tensor.exp "torch.Tensor.exp")
    的原地版本 |'
- en: '| [`Tensor.expm1`](generated/torch.Tensor.expm1.html#torch.Tensor.expm1 "torch.Tensor.expm1")
    | See [`torch.expm1()`](generated/torch.expm1.html#torch.expm1 "torch.expm1")
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.expm1`](generated/torch.Tensor.expm1.html#torch.Tensor.expm1 "torch.Tensor.expm1")
    | 查看 [`torch.expm1()`](generated/torch.expm1.html#torch.expm1 "torch.expm1") |'
- en: '| [`Tensor.expm1_`](generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_
    "torch.Tensor.expm1_") | In-place version of [`expm1()`](generated/torch.Tensor.expm1.html#torch.Tensor.expm1
    "torch.Tensor.expm1") |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.expm1_`](generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_
    "torch.Tensor.expm1_") | [`expm1()`](generated/torch.Tensor.expm1.html#torch.Tensor.expm1
    "torch.Tensor.expm1") 的原地版本 |'
- en: '| [`Tensor.expand`](generated/torch.Tensor.expand.html#torch.Tensor.expand
    "torch.Tensor.expand") | Returns a new view of the `self` tensor with singleton
    dimensions expanded to a larger size. |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.expand`](generated/torch.Tensor.expand.html#torch.Tensor.expand
    "torch.Tensor.expand") | 返回一个新的视图，将 `self` 张量中的单例维度扩展到更大的大小 |'
- en: '| [`Tensor.expand_as`](generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as
    "torch.Tensor.expand_as") | Expand this tensor to the same size as `other`. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.expand_as`](generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as
    "torch.Tensor.expand_as") | 将此张量扩展到与 `other` 相同的大小 |'
- en: '| [`Tensor.exponential_`](generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_
    "torch.Tensor.exponential_") | Fills `self` tensor with elements drawn from the
    PDF (probability density function): |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.exponential_`](generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_
    "torch.Tensor.exponential_") | 用从概率密度函数中抽取的元素填充 `self` 张量 |'
- en: '| [`Tensor.fix`](generated/torch.Tensor.fix.html#torch.Tensor.fix "torch.Tensor.fix")
    | See [`torch.fix()`](generated/torch.fix.html#torch.fix "torch.fix"). |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.fix`](generated/torch.Tensor.fix.html#torch.Tensor.fix "torch.Tensor.fix")
    | 查看 [`torch.fix()`](generated/torch.fix.html#torch.fix "torch.fix") |'
- en: '| [`Tensor.fix_`](generated/torch.Tensor.fix_.html#torch.Tensor.fix_ "torch.Tensor.fix_")
    | In-place version of [`fix()`](generated/torch.Tensor.fix.html#torch.Tensor.fix
    "torch.Tensor.fix") |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.fix_`](generated/torch.Tensor.fix_.html#torch.Tensor.fix_ "torch.Tensor.fix_")
    | [`fix()`](generated/torch.Tensor.fix.html#torch.Tensor.fix "torch.Tensor.fix")
    的原地版本 |'
- en: '| [`Tensor.fill_`](generated/torch.Tensor.fill_.html#torch.Tensor.fill_ "torch.Tensor.fill_")
    | Fills `self` tensor with the specified value. |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.fill_`](generated/torch.Tensor.fill_.html#torch.Tensor.fill_ "torch.Tensor.fill_")
    | 用指定值填充 `self` 张量 |'
- en: '| [`Tensor.flatten`](generated/torch.Tensor.flatten.html#torch.Tensor.flatten
    "torch.Tensor.flatten") | See [`torch.flatten()`](generated/torch.flatten.html#torch.flatten
    "torch.flatten") |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.flatten`](generated/torch.Tensor.flatten.html#torch.Tensor.flatten
    "torch.Tensor.flatten") | 查看 [`torch.flatten()`](generated/torch.flatten.html#torch.flatten
    "torch.flatten") |'
- en: '| [`Tensor.flip`](generated/torch.Tensor.flip.html#torch.Tensor.flip "torch.Tensor.flip")
    | See [`torch.flip()`](generated/torch.flip.html#torch.flip "torch.flip") |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.flip`](generated/torch.Tensor.flip.html#torch.Tensor.flip "torch.Tensor.flip")
    | 查看 [`torch.flip()`](generated/torch.flip.html#torch.flip "torch.flip") |'
- en: '| [`Tensor.fliplr`](generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr
    "torch.Tensor.fliplr") | See [`torch.fliplr()`](generated/torch.fliplr.html#torch.fliplr
    "torch.fliplr") |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.fliplr`](generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr
    "torch.Tensor.fliplr") | 查看 [`torch.fliplr()`](generated/torch.fliplr.html#torch.fliplr
    "torch.fliplr") |'
- en: '| [`Tensor.flipud`](generated/torch.Tensor.flipud.html#torch.Tensor.flipud
    "torch.Tensor.flipud") | See [`torch.flipud()`](generated/torch.flipud.html#torch.flipud
    "torch.flipud") |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.flipud`](generated/torch.Tensor.flipud.html#torch.Tensor.flipud
    "torch.Tensor.flipud") | 查看 [`torch.flipud()`](generated/torch.flipud.html#torch.flipud
    "torch.flipud") |'
- en: '| [`Tensor.float`](generated/torch.Tensor.float.html#torch.Tensor.float "torch.Tensor.float")
    | `self.float()` is equivalent to `self.to(torch.float32)`. |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.float`](generated/torch.Tensor.float.html#torch.Tensor.float "torch.Tensor.float")
    | `self.float()` 等同于 `self.to(torch.float32)` |'
- en: '| [`Tensor.float_power`](generated/torch.Tensor.float_power.html#torch.Tensor.float_power
    "torch.Tensor.float_power") | See [`torch.float_power()`](generated/torch.float_power.html#torch.float_power
    "torch.float_power") |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.float_power`](generated/torch.Tensor.float_power.html#torch.Tensor.float_power
    "torch.Tensor.float_power") | 查看 [`torch.float_power()`](generated/torch.float_power.html#torch.float_power
    "torch.float_power") |'
- en: '| [`Tensor.float_power_`](generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_
    "torch.Tensor.float_power_") | In-place version of [`float_power()`](generated/torch.Tensor.float_power.html#torch.Tensor.float_power
    "torch.Tensor.float_power") |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.float_power_`](generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_
    "torch.Tensor.float_power_") | [`float_power()`](generated/torch.Tensor.float_power.html#torch.Tensor.float_power
    "torch.Tensor.float_power") 的原地版本 |'
- en: '| [`Tensor.floor`](generated/torch.Tensor.floor.html#torch.Tensor.floor "torch.Tensor.floor")
    | See [`torch.floor()`](generated/torch.floor.html#torch.floor "torch.floor")
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.floor`](generated/torch.Tensor.floor.html#torch.Tensor.floor "torch.Tensor.floor")
    | 查看 [`torch.floor()`](generated/torch.floor.html#torch.floor "torch.floor") |'
- en: '| [`Tensor.floor_`](generated/torch.Tensor.floor_.html#torch.Tensor.floor_
    "torch.Tensor.floor_") | In-place version of [`floor()`](generated/torch.Tensor.floor.html#torch.Tensor.floor
    "torch.Tensor.floor") |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.floor_`](generated/torch.Tensor.floor_.html#torch.Tensor.floor_
    "torch.Tensor.floor_") | [`floor()`](generated/torch.Tensor.floor.html#torch.Tensor.floor
    "torch.Tensor.floor") 的原地版本 |'
- en: '| [`Tensor.floor_divide`](generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide
    "torch.Tensor.floor_divide") | See [`torch.floor_divide()`](generated/torch.floor_divide.html#torch.floor_divide
    "torch.floor_divide") |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.floor_divide`](generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide
    "torch.Tensor.floor_divide") | 查看 [`torch.floor_divide()`](generated/torch.floor_divide.html#torch.floor_divide
    "torch.floor_divide") |'
- en: '| [`Tensor.floor_divide_`](generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_
    "torch.Tensor.floor_divide_") | In-place version of [`floor_divide()`](generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide
    "torch.Tensor.floor_divide") |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.floor_divide_`](generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_
    "torch.Tensor.floor_divide_") | [`floor_divide()`](generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide
    "torch.Tensor.floor_divide") 的原地版本 |'
- en: '| [`Tensor.fmod`](generated/torch.Tensor.fmod.html#torch.Tensor.fmod "torch.Tensor.fmod")
    | See [`torch.fmod()`](generated/torch.fmod.html#torch.fmod "torch.fmod") |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.fmod`](generated/torch.Tensor.fmod.html#torch.Tensor.fmod "torch.Tensor.fmod")
    | 查看 [`torch.fmod()`](generated/torch.fmod.html#torch.fmod "torch.fmod") |'
- en: '| [`Tensor.fmod_`](generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_ "torch.Tensor.fmod_")
    | In-place version of [`fmod()`](generated/torch.Tensor.fmod.html#torch.Tensor.fmod
    "torch.Tensor.fmod") |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.fmod_`](generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_ "torch.Tensor.fmod_")
    | [`fmod()`](generated/torch.Tensor.fmod.html#torch.Tensor.fmod "torch.Tensor.fmod")
    的原地版本 |'
- en: '| [`Tensor.frac`](generated/torch.Tensor.frac.html#torch.Tensor.frac "torch.Tensor.frac")
    | See [`torch.frac()`](generated/torch.frac.html#torch.frac "torch.frac") |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.frac`](generated/torch.Tensor.frac.html#torch.Tensor.frac "torch.Tensor.frac")
    | 查看 [`torch.frac()`](generated/torch.frac.html#torch.frac "torch.frac") |'
- en: '| [`Tensor.frac_`](generated/torch.Tensor.frac_.html#torch.Tensor.frac_ "torch.Tensor.frac_")
    | In-place version of [`frac()`](generated/torch.Tensor.frac.html#torch.Tensor.frac
    "torch.Tensor.frac") |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.frac_`](generated/torch.Tensor.frac_.html#torch.Tensor.frac_ "torch.Tensor.frac_")
    | [`frac()`](generated/torch.Tensor.frac.html#torch.Tensor.frac "torch.Tensor.frac")
    的原地版本 |'
- en: '| [`Tensor.frexp`](generated/torch.Tensor.frexp.html#torch.Tensor.frexp "torch.Tensor.frexp")
    | See [`torch.frexp()`](generated/torch.frexp.html#torch.frexp "torch.frexp")
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.frexp`](generated/torch.Tensor.frexp.html#torch.Tensor.frexp "torch.Tensor.frexp")
    | 查看 [`torch.frexp()`](generated/torch.frexp.html#torch.frexp "torch.frexp") |'
- en: '| [`Tensor.gather`](generated/torch.Tensor.gather.html#torch.Tensor.gather
    "torch.Tensor.gather") | See [`torch.gather()`](generated/torch.gather.html#torch.gather
    "torch.gather") |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.gather`](generated/torch.Tensor.gather.html#torch.Tensor.gather
    "torch.Tensor.gather") | 查看 [`torch.gather()`](generated/torch.gather.html#torch.gather
    "torch.gather") |'
- en: '| [`Tensor.gcd`](generated/torch.Tensor.gcd.html#torch.Tensor.gcd "torch.Tensor.gcd")
    | See [`torch.gcd()`](generated/torch.gcd.html#torch.gcd "torch.gcd") |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.gcd`](generated/torch.Tensor.gcd.html#torch.Tensor.gcd "torch.Tensor.gcd")
    | 查看 [`torch.gcd()`](generated/torch.gcd.html#torch.gcd "torch.gcd") |'
- en: '| [`Tensor.gcd_`](generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_ "torch.Tensor.gcd_")
    | In-place version of [`gcd()`](generated/torch.Tensor.gcd.html#torch.Tensor.gcd
    "torch.Tensor.gcd") |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.gcd_`](generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_ "torch.Tensor.gcd_")
    | [`gcd()`](generated/torch.Tensor.gcd.html#torch.Tensor.gcd "torch.Tensor.gcd")
    的原地版本 |'
- en: '| [`Tensor.ge`](generated/torch.Tensor.ge.html#torch.Tensor.ge "torch.Tensor.ge")
    | See [`torch.ge()`](generated/torch.ge.html#torch.ge "torch.ge"). |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ge`](generated/torch.Tensor.ge.html#torch.Tensor.ge "torch.Tensor.ge")
    | 查看 [`torch.ge()`](generated/torch.ge.html#torch.ge "torch.ge") |'
- en: '| [`Tensor.ge_`](generated/torch.Tensor.ge_.html#torch.Tensor.ge_ "torch.Tensor.ge_")
    | In-place version of [`ge()`](generated/torch.Tensor.ge.html#torch.Tensor.ge
    "torch.Tensor.ge"). |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ge_`](generated/torch.Tensor.ge_.html#torch.Tensor.ge_ "torch.Tensor.ge_")
    | [`ge()`](generated/torch.Tensor.ge.html#torch.Tensor.ge "torch.Tensor.ge") 的原地版本
    |'
- en: '| [`Tensor.greater_equal`](generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal
    "torch.Tensor.greater_equal") | See [`torch.greater_equal()`](generated/torch.greater_equal.html#torch.greater_equal
    "torch.greater_equal"). |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.greater_equal`](generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal
    "torch.Tensor.greater_equal") | 查看 [`torch.greater_equal()`](generated/torch.greater_equal.html#torch.greater_equal
    "torch.greater_equal") |'
- en: '| [`Tensor.greater_equal_`](generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_
    "torch.Tensor.greater_equal_") | In-place version of [`greater_equal()`](generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal
    "torch.Tensor.greater_equal"). |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.greater_equal_`](generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_
    "torch.Tensor.greater_equal_") | [`greater_equal()`](generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal
    "torch.Tensor.greater_equal") 的原地版本 |'
- en: '| [`Tensor.geometric_`](generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_
    "torch.Tensor.geometric_") | Fills `self` tensor with elements drawn from the
    geometric distribution: |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.geometric_`](generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_
    "torch.Tensor.geometric_") | 用几何分布中的元素填充 `self` 张量： |'
- en: '| [`Tensor.geqrf`](generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf "torch.Tensor.geqrf")
    | See [`torch.geqrf()`](generated/torch.geqrf.html#torch.geqrf "torch.geqrf")
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.geqrf`](generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf "torch.Tensor.geqrf")
    | 参见 [`torch.geqrf()`](generated/torch.geqrf.html#torch.geqrf "torch.geqrf") |'
- en: '| [`Tensor.ger`](generated/torch.Tensor.ger.html#torch.Tensor.ger "torch.Tensor.ger")
    | See [`torch.ger()`](generated/torch.ger.html#torch.ger "torch.ger") |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ger`](generated/torch.Tensor.ger.html#torch.Tensor.ger "torch.Tensor.ger")
    | 参见 [`torch.ger()`](generated/torch.ger.html#torch.ger "torch.ger") |'
- en: '| [`Tensor.get_device`](generated/torch.Tensor.get_device.html#torch.Tensor.get_device
    "torch.Tensor.get_device") | For CUDA tensors, this function returns the device
    ordinal of the GPU on which the tensor resides. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.get_device`](generated/torch.Tensor.get_device.html#torch.Tensor.get_device
    "torch.Tensor.get_device") | 对于 CUDA 张量，此函数返回张量所在 GPU 的设备序数。 |'
- en: '| [`Tensor.gt`](generated/torch.Tensor.gt.html#torch.Tensor.gt "torch.Tensor.gt")
    | See [`torch.gt()`](generated/torch.gt.html#torch.gt "torch.gt"). |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.gt`](generated/torch.Tensor.gt.html#torch.Tensor.gt "torch.Tensor.gt")
    | 参见 [`torch.gt()`](generated/torch.gt.html#torch.gt "torch.gt") |'
- en: '| [`Tensor.gt_`](generated/torch.Tensor.gt_.html#torch.Tensor.gt_ "torch.Tensor.gt_")
    | In-place version of [`gt()`](generated/torch.Tensor.gt.html#torch.Tensor.gt
    "torch.Tensor.gt"). |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.gt_`](generated/torch.Tensor.gt_.html#torch.Tensor.gt_ "torch.Tensor.gt_")
    | [`gt()`](generated/torch.Tensor.gt.html#torch.Tensor.gt "torch.Tensor.gt") 的原地版本。
    |'
- en: '| [`Tensor.greater`](generated/torch.Tensor.greater.html#torch.Tensor.greater
    "torch.Tensor.greater") | See [`torch.greater()`](generated/torch.greater.html#torch.greater
    "torch.greater"). |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.greater`](generated/torch.Tensor.greater.html#torch.Tensor.greater
    "torch.Tensor.greater") | 参见 [`torch.greater()`](generated/torch.greater.html#torch.greater
    "torch.greater") |'
- en: '| [`Tensor.greater_`](generated/torch.Tensor.greater_.html#torch.Tensor.greater_
    "torch.Tensor.greater_") | In-place version of [`greater()`](generated/torch.Tensor.greater.html#torch.Tensor.greater
    "torch.Tensor.greater"). |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.greater_`](generated/torch.Tensor.greater_.html#torch.Tensor.greater_
    "torch.Tensor.greater_") | [`greater()`](generated/torch.Tensor.greater.html#torch.Tensor.greater
    "torch.Tensor.greater") 的原地版本。 |'
- en: '| [`Tensor.half`](generated/torch.Tensor.half.html#torch.Tensor.half "torch.Tensor.half")
    | `self.half()` is equivalent to `self.to(torch.float16)`. |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.half`](generated/torch.Tensor.half.html#torch.Tensor.half "torch.Tensor.half")
    | `self.half()` 等同于 `self.to(torch.float16)`。 |'
- en: '| [`Tensor.hardshrink`](generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink
    "torch.Tensor.hardshrink") | See [`torch.nn.functional.hardshrink()`](generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink
    "torch.nn.functional.hardshrink") |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.hardshrink`](generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink
    "torch.Tensor.hardshrink") | 参见 [`torch.nn.functional.hardshrink()`](generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink
    "torch.nn.functional.hardshrink") |'
- en: '| [`Tensor.heaviside`](generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside
    "torch.Tensor.heaviside") | See [`torch.heaviside()`](generated/torch.heaviside.html#torch.heaviside
    "torch.heaviside") |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.heaviside`](generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside
    "torch.Tensor.heaviside") | 参见 [`torch.heaviside()`](generated/torch.heaviside.html#torch.heaviside
    "torch.heaviside") |'
- en: '| [`Tensor.histc`](generated/torch.Tensor.histc.html#torch.Tensor.histc "torch.Tensor.histc")
    | See [`torch.histc()`](generated/torch.histc.html#torch.histc "torch.histc")
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.histc`](generated/torch.Tensor.histc.html#torch.Tensor.histc "torch.Tensor.histc")
    | 参见 [`torch.histc()`](generated/torch.histc.html#torch.histc "torch.histc") |'
- en: '| [`Tensor.histogram`](generated/torch.Tensor.histogram.html#torch.Tensor.histogram
    "torch.Tensor.histogram") | See [`torch.histogram()`](generated/torch.histogram.html#torch.histogram
    "torch.histogram") |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.histogram`](generated/torch.Tensor.histogram.html#torch.Tensor.histogram
    "torch.Tensor.histogram") | 参见 [`torch.histogram()`](generated/torch.histogram.html#torch.histogram
    "torch.histogram") |'
- en: '| [`Tensor.hsplit`](generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit
    "torch.Tensor.hsplit") | See [`torch.hsplit()`](generated/torch.hsplit.html#torch.hsplit
    "torch.hsplit") |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.hsplit`](generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit
    "torch.Tensor.hsplit") | 参见 [`torch.hsplit()`](generated/torch.hsplit.html#torch.hsplit
    "torch.hsplit") |'
- en: '| [`Tensor.hypot`](generated/torch.Tensor.hypot.html#torch.Tensor.hypot "torch.Tensor.hypot")
    | See [`torch.hypot()`](generated/torch.hypot.html#torch.hypot "torch.hypot")
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.hypot`](generated/torch.Tensor.hypot.html#torch.Tensor.hypot "torch.Tensor.hypot")
    | 参见 [`torch.hypot()`](generated/torch.hypot.html#torch.hypot "torch.hypot") |'
- en: '| [`Tensor.hypot_`](generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_
    "torch.Tensor.hypot_") | In-place version of [`hypot()`](generated/torch.Tensor.hypot.html#torch.Tensor.hypot
    "torch.Tensor.hypot") |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.hypot_`](generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_
    "torch.Tensor.hypot_") | [`hypot()`](generated/torch.Tensor.hypot.html#torch.Tensor.hypot
    "torch.Tensor.hypot") 的原地版本。 |'
- en: '| [`Tensor.i0`](generated/torch.Tensor.i0.html#torch.Tensor.i0 "torch.Tensor.i0")
    | See [`torch.i0()`](generated/torch.i0.html#torch.i0 "torch.i0") |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.i0`](generated/torch.Tensor.i0.html#torch.Tensor.i0 "torch.Tensor.i0")
    | 参见 [`torch.i0()`](generated/torch.i0.html#torch.i0 "torch.i0") |'
- en: '| [`Tensor.i0_`](generated/torch.Tensor.i0_.html#torch.Tensor.i0_ "torch.Tensor.i0_")
    | In-place version of [`i0()`](generated/torch.Tensor.i0.html#torch.Tensor.i0
    "torch.Tensor.i0") |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.i0_`](generated/torch.Tensor.i0_.html#torch.Tensor.i0_ "torch.Tensor.i0_")
    | [`i0()`](generated/torch.Tensor.i0.html#torch.Tensor.i0 "torch.Tensor.i0") 的原地版本。
    |'
- en: '| [`Tensor.igamma`](generated/torch.Tensor.igamma.html#torch.Tensor.igamma
    "torch.Tensor.igamma") | See [`torch.igamma()`](generated/torch.igamma.html#torch.igamma
    "torch.igamma") |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.igamma`](generated/torch.Tensor.igamma.html#torch.Tensor.igamma
    "torch.Tensor.igamma") | 参见 [`torch.igamma()`](generated/torch.igamma.html#torch.igamma
    "torch.igamma") |'
- en: '| [`Tensor.igamma_`](generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_
    "torch.Tensor.igamma_") | In-place version of [`igamma()`](generated/torch.Tensor.igamma.html#torch.Tensor.igamma
    "torch.Tensor.igamma") |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.igamma_`](generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_
    "torch.Tensor.igamma_") | [`igamma()`](generated/torch.Tensor.igamma.html#torch.Tensor.igamma
    "torch.Tensor.igamma") 的原地版本。 |'
- en: '| [`Tensor.igammac`](generated/torch.Tensor.igammac.html#torch.Tensor.igammac
    "torch.Tensor.igammac") | See [`torch.igammac()`](generated/torch.igammac.html#torch.igammac
    "torch.igammac") |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.igammac`](generated/torch.Tensor.igammac.html#torch.Tensor.igammac
    "torch.Tensor.igammac") | 参见 [`torch.igammac()`](generated/torch.igammac.html#torch.igammac
    "torch.igammac") |'
- en: '| [`Tensor.igammac_`](generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_
    "torch.Tensor.igammac_") | In-place version of [`igammac()`](generated/torch.Tensor.igammac.html#torch.Tensor.igammac
    "torch.Tensor.igammac") |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.igammac_`](generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_
    "torch.Tensor.igammac_") | [`igammac()`](generated/torch.Tensor.igammac.html#torch.Tensor.igammac
    "torch.Tensor.igammac")的原地版本。 |'
- en: '| [`Tensor.index_add_`](generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_
    "torch.Tensor.index_add_") | Accumulate the elements of `alpha` times `source`
    into the `self` tensor by adding to the indices in the order given in `index`.
    |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_add_`](generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_
    "torch.Tensor.index_add_") | 通过将`alpha`倍的`source`元素累加到`self`张量中，按照`index`中给定的顺序添加到索引中。
    |'
- en: '| [`Tensor.index_add`](generated/torch.Tensor.index_add.html#torch.Tensor.index_add
    "torch.Tensor.index_add") | Out-of-place version of [`torch.Tensor.index_add_()`](generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_
    "torch.Tensor.index_add_"). |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_add`](generated/torch.Tensor.index_add.html#torch.Tensor.index_add
    "torch.Tensor.index_add") | [`torch.Tensor.index_add_()`](generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_
    "torch.Tensor.index_add_")的非原地版本。 |'
- en: '| [`Tensor.index_copy_`](generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_
    "torch.Tensor.index_copy_") | Copies the elements of [`tensor`](generated/torch.tensor.html#torch.tensor
    "torch.tensor") into the `self` tensor by selecting the indices in the order given
    in `index`. |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_copy_`](generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_
    "torch.Tensor.index_copy_") | 通过按照`index`中给定的顺序选择的索引，将[`tensor`](generated/torch.tensor.html#torch.tensor
    "torch.tensor")的元素复制到`self`张量中。 |'
- en: '| [`Tensor.index_copy`](generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy
    "torch.Tensor.index_copy") | Out-of-place version of [`torch.Tensor.index_copy_()`](generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_
    "torch.Tensor.index_copy_"). |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_copy`](generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy
    "torch.Tensor.index_copy") | [`torch.Tensor.index_copy_()`](generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_
    "torch.Tensor.index_copy_")的非原地版本。 |'
- en: '| [`Tensor.index_fill_`](generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_
    "torch.Tensor.index_fill_") | Fills the elements of the `self` tensor with value
    `value` by selecting the indices in the order given in `index`. |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_fill_`](generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_
    "torch.Tensor.index_fill_") | 通过按照`index`中给定的顺序选择的索引，用值`value`填充`self`张量的元素。 |'
- en: '| [`Tensor.index_fill`](generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill
    "torch.Tensor.index_fill") | Out-of-place version of [`torch.Tensor.index_fill_()`](generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_
    "torch.Tensor.index_fill_"). |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_fill`](generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill
    "torch.Tensor.index_fill") | [`torch.Tensor.index_fill_()`](generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_
    "torch.Tensor.index_fill_")的非原地版本。 |'
- en: '| [`Tensor.index_put_`](generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_
    "torch.Tensor.index_put_") | Puts values from the tensor `values` into the tensor
    `self` using the indices specified in `indices` (which is a tuple of Tensors).
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_put_`](generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_
    "torch.Tensor.index_put_") | 使用`indices`中指定的索引（一个张量元组）将张量`values`中的值放入张量`self`中。
    |'
- en: '| [`Tensor.index_put`](generated/torch.Tensor.index_put.html#torch.Tensor.index_put
    "torch.Tensor.index_put") | Out-place version of [`index_put_()`](generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_
    "torch.Tensor.index_put_"). |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_put`](generated/torch.Tensor.index_put.html#torch.Tensor.index_put
    "torch.Tensor.index_put") | [`index_put_()`](generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_
    "torch.Tensor.index_put_")的非原地版本。 |'
- en: '| [`Tensor.index_reduce_`](generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_
    "torch.Tensor.index_reduce_") | Accumulate the elements of `source` into the `self`
    tensor by accumulating to the indices in the order given in `index` using the
    reduction given by the `reduce` argument. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_reduce_`](generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_
    "torch.Tensor.index_reduce_") | 通过使用`reduce`参数给定的减少方式，按照`index`中给定的顺序将`source`元素累加到`self`张量中。
    |'
- en: '| [`Tensor.index_reduce`](generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce
    "torch.Tensor.index_reduce") |  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_reduce`](generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce
    "torch.Tensor.index_reduce") |  |'
- en: '| [`Tensor.index_select`](generated/torch.Tensor.index_select.html#torch.Tensor.index_select
    "torch.Tensor.index_select") | See [`torch.index_select()`](generated/torch.index_select.html#torch.index_select
    "torch.index_select") |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.index_select`](generated/torch.Tensor.index_select.html#torch.Tensor.index_select
    "torch.Tensor.index_select") | 参见[`torch.index_select()`](generated/torch.index_select.html#torch.index_select
    "torch.index_select") |'
- en: '| [`Tensor.indices`](generated/torch.Tensor.indices.html#torch.Tensor.indices
    "torch.Tensor.indices") | Return the indices tensor of a [sparse COO tensor](sparse.html#sparse-coo-docs).
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.indices`](generated/torch.Tensor.indices.html#torch.Tensor.indices
    "torch.Tensor.indices") | 返回[稀疏COO张量](sparse.html#sparse-coo-docs)的索引张量。 |'
- en: '| [`Tensor.inner`](generated/torch.Tensor.inner.html#torch.Tensor.inner "torch.Tensor.inner")
    | See [`torch.inner()`](generated/torch.inner.html#torch.inner "torch.inner").
    |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.inner`](generated/torch.Tensor.inner.html#torch.Tensor.inner "torch.Tensor.inner")
    | 参见[`torch.inner()`](generated/torch.inner.html#torch.inner "torch.inner")。 |'
- en: '| [`Tensor.int`](generated/torch.Tensor.int.html#torch.Tensor.int "torch.Tensor.int")
    | `self.int()` is equivalent to `self.to(torch.int32)`. |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.int`](generated/torch.Tensor.int.html#torch.Tensor.int "torch.Tensor.int")
    | `self.int()`等同于`self.to(torch.int32)`。 |'
- en: '| [`Tensor.int_repr`](generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr
    "torch.Tensor.int_repr") | Given a quantized Tensor, `self.int_repr()` returns
    a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values
    of the given Tensor. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.int_repr`](generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr
    "torch.Tensor.int_repr") | 给定一个量化张量，`self.int_repr()`返回一个CPU张量，数据类型为uint8_t，存储给定张量的底层uint8_t值。
    |'
- en: '| [`Tensor.inverse`](generated/torch.Tensor.inverse.html#torch.Tensor.inverse
    "torch.Tensor.inverse") | See [`torch.inverse()`](generated/torch.inverse.html#torch.inverse
    "torch.inverse") |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.inverse`](generated/torch.Tensor.inverse.html#torch.Tensor.inverse
    "torch.Tensor.inverse") | 参见[`torch.inverse()`](generated/torch.inverse.html#torch.inverse
    "torch.inverse") |'
- en: '| [`Tensor.isclose`](generated/torch.Tensor.isclose.html#torch.Tensor.isclose
    "torch.Tensor.isclose") | See [`torch.isclose()`](generated/torch.isclose.html#torch.isclose
    "torch.isclose") |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.isclose`](generated/torch.Tensor.isclose.html#torch.Tensor.isclose
    "torch.Tensor.isclose") | 参见[`torch.isclose()`](generated/torch.isclose.html#torch.isclose
    "torch.isclose") |'
- en: '| [`Tensor.isfinite`](generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite
    "torch.Tensor.isfinite") | See [`torch.isfinite()`](generated/torch.isfinite.html#torch.isfinite
    "torch.isfinite") |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.isfinite`](generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite
    "torch.Tensor.isfinite") | 参见[`torch.isfinite()`](generated/torch.isfinite.html#torch.isfinite
    "torch.isfinite") |'
- en: '| [`Tensor.isinf`](generated/torch.Tensor.isinf.html#torch.Tensor.isinf "torch.Tensor.isinf")
    | See [`torch.isinf()`](generated/torch.isinf.html#torch.isinf "torch.isinf")
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.isinf`](generated/torch.Tensor.isinf.html#torch.Tensor.isinf "torch.Tensor.isinf")
    | 参见[`torch.isinf()`](generated/torch.isinf.html#torch.isinf "torch.isinf") |'
- en: '| [`Tensor.isposinf`](generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf
    "torch.Tensor.isposinf") | See [`torch.isposinf()`](generated/torch.isposinf.html#torch.isposinf
    "torch.isposinf") |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.isposinf`](generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf
    "torch.Tensor.isposinf") | 参见[`torch.isposinf()`](generated/torch.isposinf.html#torch.isposinf
    "torch.isposinf") |'
- en: '| [`Tensor.isneginf`](generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf
    "torch.Tensor.isneginf") | See [`torch.isneginf()`](generated/torch.isneginf.html#torch.isneginf
    "torch.isneginf") |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.isneginf`](generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf
    "torch.Tensor.isneginf") | 参见[`torch.isneginf()`](generated/torch.isneginf.html#torch.isneginf
    "torch.isneginf") |'
- en: '| [`Tensor.isnan`](generated/torch.Tensor.isnan.html#torch.Tensor.isnan "torch.Tensor.isnan")
    | See [`torch.isnan()`](generated/torch.isnan.html#torch.isnan "torch.isnan")
    |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.isnan`](generated/torch.Tensor.isnan.html#torch.Tensor.isnan "torch.Tensor.isnan")
    | 参见[`torch.isnan()`](generated/torch.isnan.html#torch.isnan "torch.isnan") |'
- en: '| [`Tensor.is_contiguous`](generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous
    "torch.Tensor.is_contiguous") | Returns True if `self` tensor is contiguous in
    memory in the order specified by memory format. |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_contiguous`](generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous
    "torch.Tensor.is_contiguous") | 如果`self`张量在内存中按照内存格式指定的顺序是连续的，则返回True。 |'
- en: '| [`Tensor.is_complex`](generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex
    "torch.Tensor.is_complex") | Returns True if the data type of `self` is a complex
    data type. |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_complex`](generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex
    "torch.Tensor.is_complex") | 如果`self`的数据类型是复数数据类型，则返回True。 |'
- en: '| [`Tensor.is_conj`](generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj
    "torch.Tensor.is_conj") | Returns True if the conjugate bit of `self` is set to
    true. |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_conj`](generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj
    "torch.Tensor.is_conj") | 如果`self`的共轭位设置为true，则返回True。 |'
- en: '| [`Tensor.is_floating_point`](generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point
    "torch.Tensor.is_floating_point") | Returns True if the data type of `self` is
    a floating point data type. |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_floating_point`](generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point
    "torch.Tensor.is_floating_point") | 如果`self`的数据类型是浮点数据类型，则返回True。 |'
- en: '| [`Tensor.is_inference`](generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference
    "torch.Tensor.is_inference") | See `torch.is_inference()` |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_inference`](generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference
    "torch.Tensor.is_inference") | 参见`torch.is_inference()` |'
- en: '| [`Tensor.is_leaf`](generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf
    "torch.Tensor.is_leaf") | All Tensors that have `requires_grad` which is `False`
    will be leaf Tensors by convention. |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_leaf`](generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf
    "torch.Tensor.is_leaf") | 所有`requires_grad`为`False`的张量按照惯例都将是叶张量。 |'
- en: '| [`Tensor.is_pinned`](generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned
    "torch.Tensor.is_pinned") | Returns true if this tensor resides in pinned memory.
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_pinned`](generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned
    "torch.Tensor.is_pinned") | 如果此张量驻留在固定内存中，则返回true。 |'
- en: '| [`Tensor.is_set_to`](generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to
    "torch.Tensor.is_set_to") | Returns True if both tensors are pointing to the exact
    same memory (same storage, offset, size and stride). |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_set_to`](generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to
    "torch.Tensor.is_set_to") | 如果两个张量指向完全相同的内存（相同的存储、偏移、大小和步幅），则返回True。 |'
- en: '| [`Tensor.is_shared`](generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared
    "torch.Tensor.is_shared") | Checks if tensor is in shared memory. |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_shared`](generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared
    "torch.Tensor.is_shared") | 检查张量是否在共享内存中。 |'
- en: '| [`Tensor.is_signed`](generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed
    "torch.Tensor.is_signed") | Returns True if the data type of `self` is a signed
    data type. |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_signed`](generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed
    "torch.Tensor.is_signed") | 如果`self`的数据类型是有符号数据类型，则返回True。 |'
- en: '| [`Tensor.is_sparse`](generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse
    "torch.Tensor.is_sparse") | Is `True` if the Tensor uses sparse COO storage layout,
    `False` otherwise. |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.is_sparse`](generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse
    "torch.Tensor.is_sparse") | 如果张量使用稀疏COO存储布局，则为`True`，否则为`False`。 |'
- en: '| [`Tensor.istft`](generated/torch.Tensor.istft.html#torch.Tensor.istft "torch.Tensor.istft")
    | See [`torch.istft()`](generated/torch.istft.html#torch.istft "torch.istft")
    |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.istft`](generated/torch.Tensor.istft.html#torch.Tensor.istft "torch.Tensor.istft")
    | 参见[`torch.istft()`](generated/torch.istft.html#torch.istft "torch.istft") |'
- en: '| [`Tensor.isreal`](generated/torch.Tensor.isreal.html#torch.Tensor.isreal
    "torch.Tensor.isreal") | See [`torch.isreal()`](generated/torch.isreal.html#torch.isreal
    "torch.isreal") |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.isreal`](generated/torch.Tensor.isreal.html#torch.Tensor.isreal
    "torch.Tensor.isreal") | 参见[`torch.isreal()`](generated/torch.isreal.html#torch.isreal
    "torch.isreal") |'
- en: '| [`Tensor.item`](generated/torch.Tensor.item.html#torch.Tensor.item "torch.Tensor.item")
    | Returns the value of this tensor as a standard Python number. |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.item`](generated/torch.Tensor.item.html#torch.Tensor.item "torch.Tensor.item")
    | 将此张量的值作为标准Python数字返回。 |'
- en: '| [`Tensor.kthvalue`](generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue
    "torch.Tensor.kthvalue") | See [`torch.kthvalue()`](generated/torch.kthvalue.html#torch.kthvalue
    "torch.kthvalue") |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.kthvalue`](generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue
    "torch.Tensor.kthvalue") | 参见[`torch.kthvalue()`](generated/torch.kthvalue.html#torch.kthvalue
    "torch.kthvalue") |'
- en: '| [`Tensor.lcm`](generated/torch.Tensor.lcm.html#torch.Tensor.lcm "torch.Tensor.lcm")
    | See [`torch.lcm()`](generated/torch.lcm.html#torch.lcm "torch.lcm") |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.lcm`](generated/torch.Tensor.lcm.html#torch.Tensor.lcm "torch.Tensor.lcm")
    | 查看 [`torch.lcm()`](generated/torch.lcm.html#torch.lcm "torch.lcm") |'
- en: '| [`Tensor.lcm_`](generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_ "torch.Tensor.lcm_")
    | In-place version of [`lcm()`](generated/torch.Tensor.lcm.html#torch.Tensor.lcm
    "torch.Tensor.lcm") |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.lcm_`](generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_ "torch.Tensor.lcm_")
    | [`lcm()`](generated/torch.Tensor.lcm.html#torch.Tensor.lcm "torch.Tensor.lcm")
    的原地版本。 |'
- en: '| [`Tensor.ldexp`](generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp "torch.Tensor.ldexp")
    | See [`torch.ldexp()`](generated/torch.ldexp.html#torch.ldexp "torch.ldexp")
    |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ldexp`](generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp "torch.Tensor.ldexp")
    | 查看 [`torch.ldexp()`](generated/torch.ldexp.html#torch.ldexp "torch.ldexp") |'
- en: '| [`Tensor.ldexp_`](generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_
    "torch.Tensor.ldexp_") | In-place version of [`ldexp()`](generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp
    "torch.Tensor.ldexp") |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ldexp_`](generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_
    "torch.Tensor.ldexp_") | [`ldexp()`](generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp
    "torch.Tensor.ldexp") 的原地版本。 |'
- en: '| [`Tensor.le`](generated/torch.Tensor.le.html#torch.Tensor.le "torch.Tensor.le")
    | See [`torch.le()`](generated/torch.le.html#torch.le "torch.le"). |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.le`](generated/torch.Tensor.le.html#torch.Tensor.le "torch.Tensor.le")
    | 查看 [`torch.le()`](generated/torch.le.html#torch.le "torch.le")。 |'
- en: '| [`Tensor.le_`](generated/torch.Tensor.le_.html#torch.Tensor.le_ "torch.Tensor.le_")
    | In-place version of [`le()`](generated/torch.Tensor.le.html#torch.Tensor.le
    "torch.Tensor.le"). |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.le_`](generated/torch.Tensor.le_.html#torch.Tensor.le_ "torch.Tensor.le_")
    | [`le()`](generated/torch.Tensor.le.html#torch.Tensor.le "torch.Tensor.le") 的原地版本。
    |'
- en: '| [`Tensor.less_equal`](generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal
    "torch.Tensor.less_equal") | See [`torch.less_equal()`](generated/torch.less_equal.html#torch.less_equal
    "torch.less_equal"). |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.less_equal`](generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal
    "torch.Tensor.less_equal") | 查看 [`torch.less_equal()`](generated/torch.less_equal.html#torch.less_equal
    "torch.less_equal")。 |'
- en: '| [`Tensor.less_equal_`](generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_
    "torch.Tensor.less_equal_") | In-place version of [`less_equal()`](generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal
    "torch.Tensor.less_equal"). |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.less_equal_`](generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_
    "torch.Tensor.less_equal_") | [`less_equal()`](generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal
    "torch.Tensor.less_equal") 的原地版本。 |'
- en: '| [`Tensor.lerp`](generated/torch.Tensor.lerp.html#torch.Tensor.lerp "torch.Tensor.lerp")
    | See [`torch.lerp()`](generated/torch.lerp.html#torch.lerp "torch.lerp") |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.lerp`](generated/torch.Tensor.lerp.html#torch.Tensor.lerp "torch.Tensor.lerp")
    | 查看 [`torch.lerp()`](generated/torch.lerp.html#torch.lerp "torch.lerp") |'
- en: '| [`Tensor.lerp_`](generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_ "torch.Tensor.lerp_")
    | In-place version of [`lerp()`](generated/torch.Tensor.lerp.html#torch.Tensor.lerp
    "torch.Tensor.lerp") |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.lerp_`](generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_ "torch.Tensor.lerp_")
    | [`lerp()`](generated/torch.Tensor.lerp.html#torch.Tensor.lerp "torch.Tensor.lerp")
    的原地版本。 |'
- en: '| [`Tensor.lgamma`](generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma
    "torch.Tensor.lgamma") | See [`torch.lgamma()`](generated/torch.lgamma.html#torch.lgamma
    "torch.lgamma") |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.lgamma`](generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma
    "torch.Tensor.lgamma") | 查看 [`torch.lgamma()`](generated/torch.lgamma.html#torch.lgamma
    "torch.lgamma") |'
- en: '| [`Tensor.lgamma_`](generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_
    "torch.Tensor.lgamma_") | In-place version of [`lgamma()`](generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma
    "torch.Tensor.lgamma") |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.lgamma_`](generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_
    "torch.Tensor.lgamma_") | [`lgamma()`](generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma
    "torch.Tensor.lgamma") 的原地版本。 |'
- en: '| [`Tensor.log`](generated/torch.Tensor.log.html#torch.Tensor.log "torch.Tensor.log")
    | See [`torch.log()`](generated/torch.log.html#torch.log "torch.log") |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.log`](generated/torch.Tensor.log.html#torch.Tensor.log "torch.Tensor.log")
    | 查看 [`torch.log()`](generated/torch.log.html#torch.log "torch.log") |'
- en: '| [`Tensor.log_`](generated/torch.Tensor.log_.html#torch.Tensor.log_ "torch.Tensor.log_")
    | In-place version of [`log()`](generated/torch.Tensor.log.html#torch.Tensor.log
    "torch.Tensor.log") |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.log_`](generated/torch.Tensor.log_.html#torch.Tensor.log_ "torch.Tensor.log_")
    | [`log()`](generated/torch.Tensor.log.html#torch.Tensor.log "torch.Tensor.log")
    的原地版本。 |'
- en: '| [`Tensor.logdet`](generated/torch.Tensor.logdet.html#torch.Tensor.logdet
    "torch.Tensor.logdet") | See [`torch.logdet()`](generated/torch.logdet.html#torch.logdet
    "torch.logdet") |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logdet`](generated/torch.Tensor.logdet.html#torch.Tensor.logdet
    "torch.Tensor.logdet") | 查看 [`torch.logdet()`](generated/torch.logdet.html#torch.logdet
    "torch.logdet") |'
- en: '| [`Tensor.log10`](generated/torch.Tensor.log10.html#torch.Tensor.log10 "torch.Tensor.log10")
    | See [`torch.log10()`](generated/torch.log10.html#torch.log10 "torch.log10")
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.log10`](generated/torch.Tensor.log10.html#torch.Tensor.log10 "torch.Tensor.log10")
    | 查看 [`torch.log10()`](generated/torch.log10.html#torch.log10 "torch.log10") |'
- en: '| [`Tensor.log10_`](generated/torch.Tensor.log10_.html#torch.Tensor.log10_
    "torch.Tensor.log10_") | In-place version of [`log10()`](generated/torch.Tensor.log10.html#torch.Tensor.log10
    "torch.Tensor.log10") |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.log10_`](generated/torch.Tensor.log10_.html#torch.Tensor.log10_
    "torch.Tensor.log10_") | [`log10()`](generated/torch.Tensor.log10.html#torch.Tensor.log10
    "torch.Tensor.log10") 的原地版本。 |'
- en: '| [`Tensor.log1p`](generated/torch.Tensor.log1p.html#torch.Tensor.log1p "torch.Tensor.log1p")
    | See [`torch.log1p()`](generated/torch.log1p.html#torch.log1p "torch.log1p")
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.log1p`](generated/torch.Tensor.log1p.html#torch.Tensor.log1p "torch.Tensor.log1p")
    | 查看 [`torch.log1p()`](generated/torch.log1p.html#torch.log1p "torch.log1p") |'
- en: '| [`Tensor.log1p_`](generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_
    "torch.Tensor.log1p_") | In-place version of [`log1p()`](generated/torch.Tensor.log1p.html#torch.Tensor.log1p
    "torch.Tensor.log1p") |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.log1p_`](generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_
    "torch.Tensor.log1p_") | [`log1p()`](generated/torch.Tensor.log1p.html#torch.Tensor.log1p
    "torch.Tensor.log1p") 的原地版本。 |'
- en: '| [`Tensor.log2`](generated/torch.Tensor.log2.html#torch.Tensor.log2 "torch.Tensor.log2")
    | See [`torch.log2()`](generated/torch.log2.html#torch.log2 "torch.log2") |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.log2`](generated/torch.Tensor.log2.html#torch.Tensor.log2 "torch.Tensor.log2")
    | 查看 [`torch.log2()`](generated/torch.log2.html#torch.log2 "torch.log2") |'
- en: '| [`Tensor.log2_`](generated/torch.Tensor.log2_.html#torch.Tensor.log2_ "torch.Tensor.log2_")
    | In-place version of [`log2()`](generated/torch.Tensor.log2.html#torch.Tensor.log2
    "torch.Tensor.log2") |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.log2_`](generated/torch.Tensor.log2_.html#torch.Tensor.log2_ "torch.Tensor.log2_")
    | [`log2()`](generated/torch.Tensor.log2.html#torch.Tensor.log2 "torch.Tensor.log2")
    的原地版本。 |'
- en: '| [`Tensor.log_normal_`](generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_
    "torch.Tensor.log_normal_") | Fills `self` tensor with numbers samples from the
    log-normal distribution parameterized by the given mean $\mu$μ and standard deviation
    $\sigma$σ. |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.log_normal_`](generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_
    "torch.Tensor.log_normal_") | 使用给定的均值 $\mu$ 和标准差 $\sigma$ 参数化的对数正态分布中的样本填充 `self`
    张量 |'
- en: '| [`Tensor.logaddexp`](generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp
    "torch.Tensor.logaddexp") | See [`torch.logaddexp()`](generated/torch.logaddexp.html#torch.logaddexp
    "torch.logaddexp") |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logaddexp`](generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp
    "torch.Tensor.logaddexp") | 参见 [`torch.logaddexp()`](generated/torch.logaddexp.html#torch.logaddexp
    "torch.logaddexp") |'
- en: '| [`Tensor.logaddexp2`](generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2
    "torch.Tensor.logaddexp2") | See [`torch.logaddexp2()`](generated/torch.logaddexp2.html#torch.logaddexp2
    "torch.logaddexp2") |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logaddexp2`](generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2
    "torch.Tensor.logaddexp2") | 参见 [`torch.logaddexp2()`](generated/torch.logaddexp2.html#torch.logaddexp2
    "torch.logaddexp2") |'
- en: '| [`Tensor.logsumexp`](generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp
    "torch.Tensor.logsumexp") | See [`torch.logsumexp()`](generated/torch.logsumexp.html#torch.logsumexp
    "torch.logsumexp") |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logsumexp`](generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp
    "torch.Tensor.logsumexp") | 参见 [`torch.logsumexp()`](generated/torch.logsumexp.html#torch.logsumexp
    "torch.logsumexp") |'
- en: '| [`Tensor.logical_and`](generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and
    "torch.Tensor.logical_and") | See [`torch.logical_and()`](generated/torch.logical_and.html#torch.logical_and
    "torch.logical_and") |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logical_and`](generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and
    "torch.Tensor.logical_and") | 参见 [`torch.logical_and()`](generated/torch.logical_and.html#torch.logical_and
    "torch.logical_and") |'
- en: '| [`Tensor.logical_and_`](generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_
    "torch.Tensor.logical_and_") | In-place version of [`logical_and()`](generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and
    "torch.Tensor.logical_and") |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logical_and_`](generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_
    "torch.Tensor.logical_and_") | [`logical_and()`](generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and
    "torch.Tensor.logical_and") 的原地版本 |'
- en: '| [`Tensor.logical_not`](generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not
    "torch.Tensor.logical_not") | See [`torch.logical_not()`](generated/torch.logical_not.html#torch.logical_not
    "torch.logical_not") |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logical_not`](generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not
    "torch.Tensor.logical_not") | 参见 [`torch.logical_not()`](generated/torch.logical_not.html#torch.logical_not
    "torch.logical_not") |'
- en: '| [`Tensor.logical_not_`](generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_
    "torch.Tensor.logical_not_") | In-place version of [`logical_not()`](generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not
    "torch.Tensor.logical_not") |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logical_not_`](generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_
    "torch.Tensor.logical_not_") | [`logical_not()`](generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not
    "torch.Tensor.logical_not") 的原地版本 |'
- en: '| [`Tensor.logical_or`](generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or
    "torch.Tensor.logical_or") | See [`torch.logical_or()`](generated/torch.logical_or.html#torch.logical_or
    "torch.logical_or") |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logical_or`](generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or
    "torch.Tensor.logical_or") | 参见 [`torch.logical_or()`](generated/torch.logical_or.html#torch.logical_or
    "torch.logical_or") |'
- en: '| [`Tensor.logical_or_`](generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_
    "torch.Tensor.logical_or_") | In-place version of [`logical_or()`](generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or
    "torch.Tensor.logical_or") |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logical_or_`](generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_
    "torch.Tensor.logical_or_") | [`logical_or()`](generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or
    "torch.Tensor.logical_or") 的原地版本 |'
- en: '| [`Tensor.logical_xor`](generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor
    "torch.Tensor.logical_xor") | See [`torch.logical_xor()`](generated/torch.logical_xor.html#torch.logical_xor
    "torch.logical_xor") |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logical_xor`](generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor
    "torch.Tensor.logical_xor") | 参见 [`torch.logical_xor()`](generated/torch.logical_xor.html#torch.logical_xor
    "torch.logical_xor") |'
- en: '| [`Tensor.logical_xor_`](generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_
    "torch.Tensor.logical_xor_") | In-place version of [`logical_xor()`](generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor
    "torch.Tensor.logical_xor") |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logical_xor_`](generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_
    "torch.Tensor.logical_xor_") | [`logical_xor()`](generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor
    "torch.Tensor.logical_xor") 的原地版本 |'
- en: '| [`Tensor.logit`](generated/torch.Tensor.logit.html#torch.Tensor.logit "torch.Tensor.logit")
    | See [`torch.logit()`](generated/torch.logit.html#torch.logit "torch.logit")
    |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logit`](generated/torch.Tensor.logit.html#torch.Tensor.logit "torch.Tensor.logit")
    | 参见 [`torch.logit()`](generated/torch.logit.html#torch.logit "torch.logit") |'
- en: '| [`Tensor.logit_`](generated/torch.Tensor.logit_.html#torch.Tensor.logit_
    "torch.Tensor.logit_") | In-place version of [`logit()`](generated/torch.Tensor.logit.html#torch.Tensor.logit
    "torch.Tensor.logit") |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.logit_`](generated/torch.Tensor.logit_.html#torch.Tensor.logit_
    "torch.Tensor.logit_") | [`logit()`](generated/torch.Tensor.logit.html#torch.Tensor.logit
    "torch.Tensor.logit") 的原地版本 |'
- en: '| [`Tensor.long`](generated/torch.Tensor.long.html#torch.Tensor.long "torch.Tensor.long")
    | `self.long()` is equivalent to `self.to(torch.int64)`. |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.long`](generated/torch.Tensor.long.html#torch.Tensor.long "torch.Tensor.long")
    | `self.long()` 等同于 `self.to(torch.int64)` |'
- en: '| [`Tensor.lt`](generated/torch.Tensor.lt.html#torch.Tensor.lt "torch.Tensor.lt")
    | See [`torch.lt()`](generated/torch.lt.html#torch.lt "torch.lt"). |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.lt`](generated/torch.Tensor.lt.html#torch.Tensor.lt "torch.Tensor.lt")
    | 参见 [`torch.lt()`](generated/torch.lt.html#torch.lt "torch.lt") |'
- en: '| [`Tensor.lt_`](generated/torch.Tensor.lt_.html#torch.Tensor.lt_ "torch.Tensor.lt_")
    | In-place version of [`lt()`](generated/torch.Tensor.lt.html#torch.Tensor.lt
    "torch.Tensor.lt"). |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.lt_`](generated/torch.Tensor.lt_.html#torch.Tensor.lt_ "torch.Tensor.lt_")
    | [`lt()`](generated/torch.Tensor.lt.html#torch.Tensor.lt "torch.Tensor.lt") 的原地版本
    |'
- en: '| [`Tensor.less`](generated/torch.Tensor.less.html#torch.Tensor.less "torch.Tensor.less")
    | lt(other) -> Tensor |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.less`](generated/torch.Tensor.less.html#torch.Tensor.less "torch.Tensor.less")
    | lt(other) -> Tensor |'
- en: '| [`Tensor.less_`](generated/torch.Tensor.less_.html#torch.Tensor.less_ "torch.Tensor.less_")
    | In-place version of [`less()`](generated/torch.Tensor.less.html#torch.Tensor.less
    "torch.Tensor.less"). |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.less_`](generated/torch.Tensor.less_.html#torch.Tensor.less_ "torch.Tensor.less_")
    | [`less()`](generated/torch.Tensor.less.html#torch.Tensor.less "torch.Tensor.less")
    的原地版本 |'
- en: '| [`Tensor.lu`](generated/torch.Tensor.lu.html#torch.Tensor.lu "torch.Tensor.lu")
    | See [`torch.lu()`](generated/torch.lu.html#torch.lu "torch.lu") |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.lu`](generated/torch.Tensor.lu.html#torch.Tensor.lu "torch.Tensor.lu")
    | 查看 [`torch.lu()`](generated/torch.lu.html#torch.lu "torch.lu") |'
- en: '| [`Tensor.lu_solve`](generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve
    "torch.Tensor.lu_solve") | See [`torch.lu_solve()`](generated/torch.lu_solve.html#torch.lu_solve
    "torch.lu_solve") |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.lu_solve`](generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve
    "torch.Tensor.lu_solve") | 查看 [`torch.lu_solve()`](generated/torch.lu_solve.html#torch.lu_solve
    "torch.lu_solve") |'
- en: '| [`Tensor.as_subclass`](generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass
    "torch.Tensor.as_subclass") | Makes a `cls` instance with the same data pointer
    as `self`. |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.as_subclass`](generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass
    "torch.Tensor.as_subclass") | 创建一个具有与`self`相同数据指针的`cls`实例。 |'
- en: '| [`Tensor.map_`](generated/torch.Tensor.map_.html#torch.Tensor.map_ "torch.Tensor.map_")
    | Applies `callable` for each element in `self` tensor and the given [`tensor`](generated/torch.tensor.html#torch.tensor
    "torch.tensor") and stores the results in `self` tensor. |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.map_`](generated/torch.Tensor.map_.html#torch.Tensor.map_ "torch.Tensor.map_")
    | 对`self`张量中的每个元素应用`callable`，并将结果存储在`self`张量中。 |'
- en: '| [`Tensor.masked_scatter_`](generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_
    "torch.Tensor.masked_scatter_") | Copies elements from `source` into `self` tensor
    at positions where the `mask` is True. |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.masked_scatter_`](generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_
    "torch.Tensor.masked_scatter_") | 将`source`中的元素复制到`self`张量中，其中`mask`为True。 |'
- en: '| [`Tensor.masked_scatter`](generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter
    "torch.Tensor.masked_scatter") | Out-of-place version of [`torch.Tensor.masked_scatter_()`](generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_
    "torch.Tensor.masked_scatter_") |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.masked_scatter`](generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter
    "torch.Tensor.masked_scatter") | [`torch.Tensor.masked_scatter_()`](generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_
    "torch.Tensor.masked_scatter_")的非就地版本 |'
- en: '| [`Tensor.masked_fill_`](generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_
    "torch.Tensor.masked_fill_") | Fills elements of `self` tensor with `value` where
    `mask` is True. |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.masked_fill_`](generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_
    "torch.Tensor.masked_fill_") | 在`mask`为True的位置，用`value`填充`self`张量的元素。 |'
- en: '| [`Tensor.masked_fill`](generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill
    "torch.Tensor.masked_fill") | Out-of-place version of [`torch.Tensor.masked_fill_()`](generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_
    "torch.Tensor.masked_fill_") |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.masked_fill`](generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill
    "torch.Tensor.masked_fill") | [`torch.Tensor.masked_fill_()`](generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_
    "torch.Tensor.masked_fill_")的非就地版本 |'
- en: '| [`Tensor.masked_select`](generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select
    "torch.Tensor.masked_select") | See [`torch.masked_select()`](generated/torch.masked_select.html#torch.masked_select
    "torch.masked_select") |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.masked_select`](generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select
    "torch.Tensor.masked_select") | 查看 [`torch.masked_select()`](generated/torch.masked_select.html#torch.masked_select
    "torch.masked_select") |'
- en: '| [`Tensor.matmul`](generated/torch.Tensor.matmul.html#torch.Tensor.matmul
    "torch.Tensor.matmul") | See [`torch.matmul()`](generated/torch.matmul.html#torch.matmul
    "torch.matmul") |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.matmul`](generated/torch.Tensor.matmul.html#torch.Tensor.matmul
    "torch.Tensor.matmul") | 查看 [`torch.matmul()`](generated/torch.matmul.html#torch.matmul
    "torch.matmul") |'
- en: '| [`Tensor.matrix_power`](generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power
    "torch.Tensor.matrix_power") |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.matrix_power`](generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power
    "torch.Tensor.matrix_power") |'
- en: Note
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '[`matrix_power()`](generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power
    "torch.Tensor.matrix_power") is deprecated, use [`torch.linalg.matrix_power()`](generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power
    "torch.linalg.matrix_power") instead.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '[`matrix_power()`](generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power
    "torch.Tensor.matrix_power")已弃用，请使用[`torch.linalg.matrix_power()`](generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power
    "torch.linalg.matrix_power")代替。'
- en: '|'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [`Tensor.matrix_exp`](generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp
    "torch.Tensor.matrix_exp") | See [`torch.matrix_exp()`](generated/torch.matrix_exp.html#torch.matrix_exp
    "torch.matrix_exp") |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.matrix_exp`](generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp
    "torch.Tensor.matrix_exp") | 查看 [`torch.matrix_exp()`](generated/torch.matrix_exp.html#torch.matrix_exp
    "torch.matrix_exp") |'
- en: '| [`Tensor.max`](generated/torch.Tensor.max.html#torch.Tensor.max "torch.Tensor.max")
    | See [`torch.max()`](generated/torch.max.html#torch.max "torch.max") |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.max`](generated/torch.Tensor.max.html#torch.Tensor.max "torch.Tensor.max")
    | 查看 [`torch.max()`](generated/torch.max.html#torch.max "torch.max") |'
- en: '| [`Tensor.maximum`](generated/torch.Tensor.maximum.html#torch.Tensor.maximum
    "torch.Tensor.maximum") | See [`torch.maximum()`](generated/torch.maximum.html#torch.maximum
    "torch.maximum") |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.maximum`](generated/torch.Tensor.maximum.html#torch.Tensor.maximum
    "torch.Tensor.maximum") | 查看 [`torch.maximum()`](generated/torch.maximum.html#torch.maximum
    "torch.maximum") |'
- en: '| [`Tensor.mean`](generated/torch.Tensor.mean.html#torch.Tensor.mean "torch.Tensor.mean")
    | See [`torch.mean()`](generated/torch.mean.html#torch.mean "torch.mean") |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.mean`](generated/torch.Tensor.mean.html#torch.Tensor.mean "torch.Tensor.mean")
    | 查看 [`torch.mean()`](generated/torch.mean.html#torch.mean "torch.mean") |'
- en: '| [`Tensor.nanmean`](generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean
    "torch.Tensor.nanmean") | See [`torch.nanmean()`](generated/torch.nanmean.html#torch.nanmean
    "torch.nanmean") |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nanmean`](generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean
    "torch.Tensor.nanmean") | 查看 [`torch.nanmean()`](generated/torch.nanmean.html#torch.nanmean
    "torch.nanmean") |'
- en: '| [`Tensor.median`](generated/torch.Tensor.median.html#torch.Tensor.median
    "torch.Tensor.median") | See [`torch.median()`](generated/torch.median.html#torch.median
    "torch.median") |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.median`](generated/torch.Tensor.median.html#torch.Tensor.median
    "torch.Tensor.median") | 查看 [`torch.median()`](generated/torch.median.html#torch.median
    "torch.median") |'
- en: '| [`Tensor.nanmedian`](generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian
    "torch.Tensor.nanmedian") | See [`torch.nanmedian()`](generated/torch.nanmedian.html#torch.nanmedian
    "torch.nanmedian") |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nanmedian`](generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian
    "torch.Tensor.nanmedian") | 查看 [`torch.nanmedian()`](generated/torch.nanmedian.html#torch.nanmedian
    "torch.nanmedian") |'
- en: '| [`Tensor.min`](generated/torch.Tensor.min.html#torch.Tensor.min "torch.Tensor.min")
    | See [`torch.min()`](generated/torch.min.html#torch.min "torch.min") |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.min`](generated/torch.Tensor.min.html#torch.Tensor.min "torch.Tensor.min")
    | 查看 [`torch.min()`](generated/torch.min.html#torch.min "torch.min") |'
- en: '| [`Tensor.minimum`](generated/torch.Tensor.minimum.html#torch.Tensor.minimum
    "torch.Tensor.minimum") | See [`torch.minimum()`](generated/torch.minimum.html#torch.minimum
    "torch.minimum") |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.minimum`](generated/torch.Tensor.minimum.html#torch.Tensor.minimum
    "torch.Tensor.minimum") | 查看 [`torch.minimum()`](generated/torch.minimum.html#torch.minimum
    "torch.minimum") |'
- en: '| [`Tensor.mm`](generated/torch.Tensor.mm.html#torch.Tensor.mm "torch.Tensor.mm")
    | See [`torch.mm()`](generated/torch.mm.html#torch.mm "torch.mm") |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.mm`](generated/torch.Tensor.mm.html#torch.Tensor.mm "torch.Tensor.mm")
    | 查看 [`torch.mm()`](generated/torch.mm.html#torch.mm "torch.mm") |'
- en: '| [`Tensor.smm`](generated/torch.Tensor.smm.html#torch.Tensor.smm "torch.Tensor.smm")
    | See [`torch.smm()`](generated/torch.smm.html#torch.smm "torch.smm") |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.smm`](generated/torch.Tensor.smm.html#torch.Tensor.smm "torch.Tensor.smm")
    | 查看 [`torch.smm()`](generated/torch.smm.html#torch.smm "torch.smm") |'
- en: '| [`Tensor.mode`](generated/torch.Tensor.mode.html#torch.Tensor.mode "torch.Tensor.mode")
    | See [`torch.mode()`](generated/torch.mode.html#torch.mode "torch.mode") |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.mode`](generated/torch.Tensor.mode.html#torch.Tensor.mode "torch.Tensor.mode")
    | 查看 [`torch.mode()`](generated/torch.mode.html#torch.mode "torch.mode") |'
- en: '| [`Tensor.movedim`](generated/torch.Tensor.movedim.html#torch.Tensor.movedim
    "torch.Tensor.movedim") | See [`torch.movedim()`](generated/torch.movedim.html#torch.movedim
    "torch.movedim") |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.movedim`](generated/torch.Tensor.movedim.html#torch.Tensor.movedim
    "torch.Tensor.movedim") | 查看 [`torch.movedim()`](generated/torch.movedim.html#torch.movedim
    "torch.movedim") |'
- en: '| [`Tensor.moveaxis`](generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis
    "torch.Tensor.moveaxis") | See [`torch.moveaxis()`](generated/torch.moveaxis.html#torch.moveaxis
    "torch.moveaxis") |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.moveaxis`](generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis
    "torch.Tensor.moveaxis") | 查看 [`torch.moveaxis()`](generated/torch.moveaxis.html#torch.moveaxis
    "torch.moveaxis") |'
- en: '| [`Tensor.msort`](generated/torch.Tensor.msort.html#torch.Tensor.msort "torch.Tensor.msort")
    | See [`torch.msort()`](generated/torch.msort.html#torch.msort "torch.msort")
    |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.msort`](generated/torch.Tensor.msort.html#torch.Tensor.msort "torch.Tensor.msort")
    | 查看 [`torch.msort()`](generated/torch.msort.html#torch.msort "torch.msort") |'
- en: '| [`Tensor.mul`](generated/torch.Tensor.mul.html#torch.Tensor.mul "torch.Tensor.mul")
    | See [`torch.mul()`](generated/torch.mul.html#torch.mul "torch.mul"). |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.mul`](generated/torch.Tensor.mul.html#torch.Tensor.mul "torch.Tensor.mul")
    | 查看 [`torch.mul()`](generated/torch.mul.html#torch.mul "torch.mul") |'
- en: '| [`Tensor.mul_`](generated/torch.Tensor.mul_.html#torch.Tensor.mul_ "torch.Tensor.mul_")
    | In-place version of [`mul()`](generated/torch.Tensor.mul.html#torch.Tensor.mul
    "torch.Tensor.mul"). |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.mul_`](generated/torch.Tensor.mul_.html#torch.Tensor.mul_ "torch.Tensor.mul_")
    | [`mul()`](generated/torch.Tensor.mul.html#torch.Tensor.mul "torch.Tensor.mul")
    的原地版本 |'
- en: '| [`Tensor.multiply`](generated/torch.Tensor.multiply.html#torch.Tensor.multiply
    "torch.Tensor.multiply") | See [`torch.multiply()`](generated/torch.multiply.html#torch.multiply
    "torch.multiply"). |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.multiply`](generated/torch.Tensor.multiply.html#torch.Tensor.multiply
    "torch.Tensor.multiply") | 查看 [`torch.multiply()`](generated/torch.multiply.html#torch.multiply
    "torch.multiply") |'
- en: '| [`Tensor.multiply_`](generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_
    "torch.Tensor.multiply_") | In-place version of [`multiply()`](generated/torch.Tensor.multiply.html#torch.Tensor.multiply
    "torch.Tensor.multiply"). |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.multiply_`](generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_
    "torch.Tensor.multiply_") | [`multiply()`](generated/torch.Tensor.multiply.html#torch.Tensor.multiply
    "torch.Tensor.multiply") 的原地版本 |'
- en: '| [`Tensor.multinomial`](generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial
    "torch.Tensor.multinomial") | See [`torch.multinomial()`](generated/torch.multinomial.html#torch.multinomial
    "torch.multinomial") |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.multinomial`](generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial
    "torch.Tensor.multinomial") | 查看 [`torch.multinomial()`](generated/torch.multinomial.html#torch.multinomial
    "torch.multinomial") |'
- en: '| [`Tensor.mv`](generated/torch.Tensor.mv.html#torch.Tensor.mv "torch.Tensor.mv")
    | See [`torch.mv()`](generated/torch.mv.html#torch.mv "torch.mv") |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.mv`](generated/torch.Tensor.mv.html#torch.Tensor.mv "torch.Tensor.mv")
    | 查看 [`torch.mv()`](generated/torch.mv.html#torch.mv "torch.mv") |'
- en: '| [`Tensor.mvlgamma`](generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma
    "torch.Tensor.mvlgamma") | See [`torch.mvlgamma()`](generated/torch.mvlgamma.html#torch.mvlgamma
    "torch.mvlgamma") |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.mvlgamma`](generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma
    "torch.Tensor.mvlgamma") | 查看 [`torch.mvlgamma()`](generated/torch.mvlgamma.html#torch.mvlgamma
    "torch.mvlgamma") |'
- en: '| [`Tensor.mvlgamma_`](generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_
    "torch.Tensor.mvlgamma_") | In-place version of [`mvlgamma()`](generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma
    "torch.Tensor.mvlgamma") |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.mvlgamma_`](generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_
    "torch.Tensor.mvlgamma_") | [`mvlgamma()`](generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma
    "torch.Tensor.mvlgamma") 的原地版本 |'
- en: '| [`Tensor.nansum`](generated/torch.Tensor.nansum.html#torch.Tensor.nansum
    "torch.Tensor.nansum") | See [`torch.nansum()`](generated/torch.nansum.html#torch.nansum
    "torch.nansum") |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nansum`](generated/torch.Tensor.nansum.html#torch.Tensor.nansum
    "torch.Tensor.nansum") | 查看 [`torch.nansum()`](generated/torch.nansum.html#torch.nansum
    "torch.nansum") |'
- en: '| [`Tensor.narrow`](generated/torch.Tensor.narrow.html#torch.Tensor.narrow
    "torch.Tensor.narrow") | See [`torch.narrow()`](generated/torch.narrow.html#torch.narrow
    "torch.narrow"). |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.narrow`](generated/torch.Tensor.narrow.html#torch.Tensor.narrow
    "torch.Tensor.narrow") | 查看 [`torch.narrow()`](generated/torch.narrow.html#torch.narrow
    "torch.narrow") |'
- en: '| [`Tensor.narrow_copy`](generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy
    "torch.Tensor.narrow_copy") | See [`torch.narrow_copy()`](generated/torch.narrow_copy.html#torch.narrow_copy
    "torch.narrow_copy"). |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.narrow_copy`](generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy
    "torch.Tensor.narrow_copy") | 查看 [`torch.narrow_copy()`](generated/torch.narrow_copy.html#torch.narrow_copy
    "torch.narrow_copy") |'
- en: '| [`Tensor.ndimension`](generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension
    "torch.Tensor.ndimension") | Alias for [`dim()`](generated/torch.Tensor.dim.html#torch.Tensor.dim
    "torch.Tensor.dim") |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ndimension`](generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension
    "torch.Tensor.ndimension") | [`dim()`](generated/torch.Tensor.dim.html#torch.Tensor.dim
    "torch.Tensor.dim") 的别名 |'
- en: '| [`Tensor.nan_to_num`](generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num
    "torch.Tensor.nan_to_num") | See [`torch.nan_to_num()`](generated/torch.nan_to_num.html#torch.nan_to_num
    "torch.nan_to_num"). |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nan_to_num`](generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num
    "torch.Tensor.nan_to_num") | 查看 [`torch.nan_to_num()`](generated/torch.nan_to_num.html#torch.nan_to_num
    "torch.nan_to_num") |'
- en: '| [`Tensor.nan_to_num_`](generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_
    "torch.Tensor.nan_to_num_") | In-place version of [`nan_to_num()`](generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num
    "torch.Tensor.nan_to_num"). |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nan_to_num_`](generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_
    "torch.Tensor.nan_to_num_") | [`nan_to_num()`](generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num
    "torch.Tensor.nan_to_num") 的原地版本。 |'
- en: '| [`Tensor.ne`](generated/torch.Tensor.ne.html#torch.Tensor.ne "torch.Tensor.ne")
    | See [`torch.ne()`](generated/torch.ne.html#torch.ne "torch.ne"). |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ne`](generated/torch.Tensor.ne.html#torch.Tensor.ne "torch.Tensor.ne")
    | 参见 [`torch.ne()`](generated/torch.ne.html#torch.ne "torch.ne") |'
- en: '| [`Tensor.ne_`](generated/torch.Tensor.ne_.html#torch.Tensor.ne_ "torch.Tensor.ne_")
    | In-place version of [`ne()`](generated/torch.Tensor.ne.html#torch.Tensor.ne
    "torch.Tensor.ne"). |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ne_`](generated/torch.Tensor.ne_.html#torch.Tensor.ne_ "torch.Tensor.ne_")
    | [`ne()`](generated/torch.Tensor.ne.html#torch.Tensor.ne "torch.Tensor.ne") 的原地版本。
    |'
- en: '| [`Tensor.not_equal`](generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal
    "torch.Tensor.not_equal") | See [`torch.not_equal()`](generated/torch.not_equal.html#torch.not_equal
    "torch.not_equal"). |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.not_equal`](generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal
    "torch.Tensor.not_equal") | 参见 [`torch.not_equal()`](generated/torch.not_equal.html#torch.not_equal
    "torch.not_equal") |'
- en: '| [`Tensor.not_equal_`](generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_
    "torch.Tensor.not_equal_") | In-place version of [`not_equal()`](generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal
    "torch.Tensor.not_equal"). |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.not_equal_`](generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_
    "torch.Tensor.not_equal_") | [`not_equal()`](generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal
    "torch.Tensor.not_equal") 的原地版本。 |'
- en: '| [`Tensor.neg`](generated/torch.Tensor.neg.html#torch.Tensor.neg "torch.Tensor.neg")
    | See [`torch.neg()`](generated/torch.neg.html#torch.neg "torch.neg") |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.neg`](generated/torch.Tensor.neg.html#torch.Tensor.neg "torch.Tensor.neg")
    | 参见 [`torch.neg()`](generated/torch.neg.html#torch.neg "torch.neg") |'
- en: '| [`Tensor.neg_`](generated/torch.Tensor.neg_.html#torch.Tensor.neg_ "torch.Tensor.neg_")
    | In-place version of [`neg()`](generated/torch.Tensor.neg.html#torch.Tensor.neg
    "torch.Tensor.neg") |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.neg_`](generated/torch.Tensor.neg_.html#torch.Tensor.neg_ "torch.Tensor.neg_")
    | [`neg()`](generated/torch.Tensor.neg.html#torch.Tensor.neg "torch.Tensor.neg")
    的原地版本。 |'
- en: '| [`Tensor.negative`](generated/torch.Tensor.negative.html#torch.Tensor.negative
    "torch.Tensor.negative") | See [`torch.negative()`](generated/torch.negative.html#torch.negative
    "torch.negative") |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.negative`](generated/torch.Tensor.negative.html#torch.Tensor.negative
    "torch.Tensor.negative") | 参见 [`torch.negative()`](generated/torch.negative.html#torch.negative
    "torch.negative") |'
- en: '| [`Tensor.negative_`](generated/torch.Tensor.negative_.html#torch.Tensor.negative_
    "torch.Tensor.negative_") | In-place version of [`negative()`](generated/torch.Tensor.negative.html#torch.Tensor.negative
    "torch.Tensor.negative") |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.negative_`](generated/torch.Tensor.negative_.html#torch.Tensor.negative_
    "torch.Tensor.negative_") | [`negative()`](generated/torch.Tensor.negative.html#torch.Tensor.negative
    "torch.Tensor.negative") 的原地版本。 |'
- en: '| [`Tensor.nelement`](generated/torch.Tensor.nelement.html#torch.Tensor.nelement
    "torch.Tensor.nelement") | Alias for [`numel()`](generated/torch.Tensor.numel.html#torch.Tensor.numel
    "torch.Tensor.numel") |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nelement`](generated/torch.Tensor.nelement.html#torch.Tensor.nelement
    "torch.Tensor.nelement") | [`numel()`](generated/torch.Tensor.numel.html#torch.Tensor.numel
    "torch.Tensor.numel") 的别名 |'
- en: '| [`Tensor.nextafter`](generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter
    "torch.Tensor.nextafter") | See [`torch.nextafter()`](generated/torch.nextafter.html#torch.nextafter
    "torch.nextafter") |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nextafter`](generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter
    "torch.Tensor.nextafter") | 参见 [`torch.nextafter()`](generated/torch.nextafter.html#torch.nextafter
    "torch.nextafter") |'
- en: '| [`Tensor.nextafter_`](generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_
    "torch.Tensor.nextafter_") | In-place version of [`nextafter()`](generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter
    "torch.Tensor.nextafter") |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nextafter_`](generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_
    "torch.Tensor.nextafter_") | [`nextafter()`](generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter
    "torch.Tensor.nextafter") 的原地版本。 |'
- en: '| [`Tensor.nonzero`](generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero
    "torch.Tensor.nonzero") | See [`torch.nonzero()`](generated/torch.nonzero.html#torch.nonzero
    "torch.nonzero") |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nonzero`](generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero
    "torch.Tensor.nonzero") | 参见 [`torch.nonzero()`](generated/torch.nonzero.html#torch.nonzero
    "torch.nonzero") |'
- en: '| [`Tensor.norm`](generated/torch.Tensor.norm.html#torch.Tensor.norm "torch.Tensor.norm")
    | See [`torch.norm()`](generated/torch.norm.html#torch.norm "torch.norm") |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.norm`](generated/torch.Tensor.norm.html#torch.Tensor.norm "torch.Tensor.norm")
    | 参见 [`torch.norm()`](generated/torch.norm.html#torch.norm "torch.norm") |'
- en: '| [`Tensor.normal_`](generated/torch.Tensor.normal_.html#torch.Tensor.normal_
    "torch.Tensor.normal_") | Fills `self` tensor with elements samples from the normal
    distribution parameterized by [`mean`](generated/torch.mean.html#torch.mean "torch.mean")
    and [`std`](generated/torch.std.html#torch.std "torch.std"). |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.normal_`](generated/torch.Tensor.normal_.html#torch.Tensor.normal_
    "torch.Tensor.normal_") | 使用由 [`mean`](generated/torch.mean.html#torch.mean "torch.mean")
    和 [`std`](generated/torch.std.html#torch.std "torch.std") 参数化的正态分布样本填充 `self`
    张量。 |'
- en: '| [`Tensor.numel`](generated/torch.Tensor.numel.html#torch.Tensor.numel "torch.Tensor.numel")
    | See [`torch.numel()`](generated/torch.numel.html#torch.numel "torch.numel")
    |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.numel`](generated/torch.Tensor.numel.html#torch.Tensor.numel "torch.Tensor.numel")
    | 参见 [`torch.numel()`](generated/torch.numel.html#torch.numel "torch.numel") |'
- en: '| [`Tensor.numpy`](generated/torch.Tensor.numpy.html#torch.Tensor.numpy "torch.Tensor.numpy")
    | Returns the tensor as a NumPy `ndarray`. |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.numpy`](generated/torch.Tensor.numpy.html#torch.Tensor.numpy "torch.Tensor.numpy")
    | 将张量返回为 NumPy `ndarray`。 |'
- en: '| [`Tensor.orgqr`](generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr "torch.Tensor.orgqr")
    | See [`torch.orgqr()`](generated/torch.orgqr.html#torch.orgqr "torch.orgqr")
    |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.orgqr`](generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr "torch.Tensor.orgqr")
    | 参见 [`torch.orgqr()`](generated/torch.orgqr.html#torch.orgqr "torch.orgqr") |'
- en: '| [`Tensor.ormqr`](generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr "torch.Tensor.ormqr")
    | See [`torch.ormqr()`](generated/torch.ormqr.html#torch.ormqr "torch.ormqr")
    |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ormqr`](generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr "torch.Tensor.ormqr")
    | 参见 [`torch.ormqr()`](generated/torch.ormqr.html#torch.ormqr "torch.ormqr") |'
- en: '| [`Tensor.outer`](generated/torch.Tensor.outer.html#torch.Tensor.outer "torch.Tensor.outer")
    | See [`torch.outer()`](generated/torch.outer.html#torch.outer "torch.outer").
    |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.outer`](generated/torch.Tensor.outer.html#torch.Tensor.outer "torch.Tensor.outer")
    | 参见 [`torch.outer()`](generated/torch.outer.html#torch.outer "torch.outer") |'
- en: '| [`Tensor.permute`](generated/torch.Tensor.permute.html#torch.Tensor.permute
    "torch.Tensor.permute") | See [`torch.permute()`](generated/torch.permute.html#torch.permute
    "torch.permute") |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.permute`](generated/torch.Tensor.permute.html#torch.Tensor.permute
    "torch.Tensor.permute") | 查看 [`torch.permute()`](generated/torch.permute.html#torch.permute
    "torch.permute") |'
- en: '| [`Tensor.pin_memory`](generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory
    "torch.Tensor.pin_memory") | Copies the tensor to pinned memory, if it''s not
    already pinned. |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.pin_memory`](generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory
    "torch.Tensor.pin_memory") | 如果尚未固定，将张量复制到固定内存中。 |'
- en: '| [`Tensor.pinverse`](generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse
    "torch.Tensor.pinverse") | See [`torch.pinverse()`](generated/torch.pinverse.html#torch.pinverse
    "torch.pinverse") |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.pinverse`](generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse
    "torch.Tensor.pinverse") | 查看 [`torch.pinverse()`](generated/torch.pinverse.html#torch.pinverse
    "torch.pinverse") |'
- en: '| [`Tensor.polygamma`](generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma
    "torch.Tensor.polygamma") | See [`torch.polygamma()`](generated/torch.polygamma.html#torch.polygamma
    "torch.polygamma") |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.polygamma`](generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma
    "torch.Tensor.polygamma") | 查看 [`torch.polygamma()`](generated/torch.polygamma.html#torch.polygamma
    "torch.polygamma") |'
- en: '| [`Tensor.polygamma_`](generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_
    "torch.Tensor.polygamma_") | In-place version of [`polygamma()`](generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma
    "torch.Tensor.polygamma") |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.polygamma_`](generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_
    "torch.Tensor.polygamma_") | [`polygamma()`](generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma
    "torch.Tensor.polygamma") 的原地版本 |'
- en: '| [`Tensor.positive`](generated/torch.Tensor.positive.html#torch.Tensor.positive
    "torch.Tensor.positive") | See [`torch.positive()`](generated/torch.positive.html#torch.positive
    "torch.positive") |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.positive`](generated/torch.Tensor.positive.html#torch.Tensor.positive
    "torch.Tensor.positive") | 查看 [`torch.positive()`](generated/torch.positive.html#torch.positive
    "torch.positive") |'
- en: '| [`Tensor.pow`](generated/torch.Tensor.pow.html#torch.Tensor.pow "torch.Tensor.pow")
    | See [`torch.pow()`](generated/torch.pow.html#torch.pow "torch.pow") |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.pow`](generated/torch.Tensor.pow.html#torch.Tensor.pow "torch.Tensor.pow")
    | 查看 [`torch.pow()`](generated/torch.pow.html#torch.pow "torch.pow") |'
- en: '| [`Tensor.pow_`](generated/torch.Tensor.pow_.html#torch.Tensor.pow_ "torch.Tensor.pow_")
    | In-place version of [`pow()`](generated/torch.Tensor.pow.html#torch.Tensor.pow
    "torch.Tensor.pow") |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.pow_`](generated/torch.Tensor.pow_.html#torch.Tensor.pow_ "torch.Tensor.pow_")
    | [`pow()`](generated/torch.Tensor.pow.html#torch.Tensor.pow "torch.Tensor.pow")
    的原地版本 |'
- en: '| [`Tensor.prod`](generated/torch.Tensor.prod.html#torch.Tensor.prod "torch.Tensor.prod")
    | See [`torch.prod()`](generated/torch.prod.html#torch.prod "torch.prod") |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.prod`](generated/torch.Tensor.prod.html#torch.Tensor.prod "torch.Tensor.prod")
    | 查看 [`torch.prod()`](generated/torch.prod.html#torch.prod "torch.prod") |'
- en: '| [`Tensor.put_`](generated/torch.Tensor.put_.html#torch.Tensor.put_ "torch.Tensor.put_")
    | Copies the elements from `source` into the positions specified by `index`. |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.put_`](generated/torch.Tensor.put_.html#torch.Tensor.put_ "torch.Tensor.put_")
    | 将 `source` 中的元素复制到由 `index` 指定的位置。 |'
- en: '| [`Tensor.qr`](generated/torch.Tensor.qr.html#torch.Tensor.qr "torch.Tensor.qr")
    | See [`torch.qr()`](generated/torch.qr.html#torch.qr "torch.qr") |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.qr`](generated/torch.Tensor.qr.html#torch.Tensor.qr "torch.Tensor.qr")
    | 查看 [`torch.qr()`](generated/torch.qr.html#torch.qr "torch.qr") |'
- en: '| [`Tensor.qscheme`](generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme
    "torch.Tensor.qscheme") | Returns the quantization scheme of a given QTensor.
    |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.qscheme`](generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme
    "torch.Tensor.qscheme") | 返回给定 QTensor 的量化方案。 |'
- en: '| [`Tensor.quantile`](generated/torch.Tensor.quantile.html#torch.Tensor.quantile
    "torch.Tensor.quantile") | See [`torch.quantile()`](generated/torch.quantile.html#torch.quantile
    "torch.quantile") |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.quantile`](generated/torch.Tensor.quantile.html#torch.Tensor.quantile
    "torch.Tensor.quantile") | 查看 [`torch.quantile()`](generated/torch.quantile.html#torch.quantile
    "torch.quantile") |'
- en: '| [`Tensor.nanquantile`](generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile
    "torch.Tensor.nanquantile") | See [`torch.nanquantile()`](generated/torch.nanquantile.html#torch.nanquantile
    "torch.nanquantile") |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.nanquantile`](generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile
    "torch.Tensor.nanquantile") | 查看 [`torch.nanquantile()`](generated/torch.nanquantile.html#torch.nanquantile
    "torch.nanquantile") |'
- en: '| [`Tensor.q_scale`](generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale
    "torch.Tensor.q_scale") | Given a Tensor quantized by linear(affine) quantization,
    returns the scale of the underlying quantizer(). |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.q_scale`](generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale
    "torch.Tensor.q_scale") | 给定通过线性（仿射）量化的张量，返回底层量化器的比例。 |'
- en: '| [`Tensor.q_zero_point`](generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point
    "torch.Tensor.q_zero_point") | Given a Tensor quantized by linear(affine) quantization,
    returns the zero_point of the underlying quantizer(). |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.q_zero_point`](generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point
    "torch.Tensor.q_zero_point") | 给定通过线性（仿射）量化的张量，返回底层量化器的零点。 |'
- en: '| [`Tensor.q_per_channel_scales`](generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales
    "torch.Tensor.q_per_channel_scales") | Given a Tensor quantized by linear (affine)
    per-channel quantization, returns a Tensor of scales of the underlying quantizer.
    |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.q_per_channel_scales`](generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales
    "torch.Tensor.q_per_channel_scales") | 给定通过线性（仿射）逐通道量化的张量，返回底层量化器的比例张量。 |'
- en: '| [`Tensor.q_per_channel_zero_points`](generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points
    "torch.Tensor.q_per_channel_zero_points") | Given a Tensor quantized by linear
    (affine) per-channel quantization, returns a tensor of zero_points of the underlying
    quantizer. |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.q_per_channel_zero_points`](generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points
    "torch.Tensor.q_per_channel_zero_points") | 给定通过线性（仿射）逐通道量化的张量，返回底层量化器的零点张量。 |'
- en: '| [`Tensor.q_per_channel_axis`](generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis
    "torch.Tensor.q_per_channel_axis") | Given a Tensor quantized by linear (affine)
    per-channel quantization, returns the index of dimension on which per-channel
    quantization is applied. |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.q_per_channel_axis`](generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis
    "torch.Tensor.q_per_channel_axis") | 给定通过线性（仿射）逐通道量化的张量，返回应用逐通道量化的维度索引。 |'
- en: '| [`Tensor.rad2deg`](generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg
    "torch.Tensor.rad2deg") | See [`torch.rad2deg()`](generated/torch.rad2deg.html#torch.rad2deg
    "torch.rad2deg") |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.rad2deg`](generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg
    "torch.Tensor.rad2deg") | 参见[`torch.rad2deg()`](generated/torch.rad2deg.html#torch.rad2deg
    "torch.rad2deg") |'
- en: '| [`Tensor.random_`](generated/torch.Tensor.random_.html#torch.Tensor.random_
    "torch.Tensor.random_") | Fills `self` tensor with numbers sampled from the discrete
    uniform distribution over `[from, to - 1]`. |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.random_`](generated/torch.Tensor.random_.html#torch.Tensor.random_
    "torch.Tensor.random_") | 用从离散均匀分布`[from, to - 1]`中抽样的数字填充`self`张量。 |'
- en: '| [`Tensor.ravel`](generated/torch.Tensor.ravel.html#torch.Tensor.ravel "torch.Tensor.ravel")
    | see [`torch.ravel()`](generated/torch.ravel.html#torch.ravel "torch.ravel")
    |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.ravel`](generated/torch.Tensor.ravel.html#torch.Tensor.ravel "torch.Tensor.ravel")
    | 参见[`torch.ravel()`](generated/torch.ravel.html#torch.ravel "torch.ravel") |'
- en: '| [`Tensor.reciprocal`](generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal
    "torch.Tensor.reciprocal") | See [`torch.reciprocal()`](generated/torch.reciprocal.html#torch.reciprocal
    "torch.reciprocal") |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.reciprocal`](generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal
    "torch.Tensor.reciprocal") | 参见[`torch.reciprocal()`](generated/torch.reciprocal.html#torch.reciprocal
    "torch.reciprocal") |'
- en: '| [`Tensor.reciprocal_`](generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_
    "torch.Tensor.reciprocal_") | In-place version of [`reciprocal()`](generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal
    "torch.Tensor.reciprocal") |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.reciprocal_`](generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_
    "torch.Tensor.reciprocal_") | [`reciprocal()`](generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal
    "torch.Tensor.reciprocal")的原位版本 |'
- en: '| [`Tensor.record_stream`](generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream
    "torch.Tensor.record_stream") | Marks the tensor as having been used by this stream.
    |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.record_stream`](generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream
    "torch.Tensor.record_stream") | 将张量标记为此流程已使用。 |'
- en: '| [`Tensor.register_hook`](generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook
    "torch.Tensor.register_hook") | Registers a backward hook. |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.register_hook`](generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook
    "torch.Tensor.register_hook") | 注册一个反向钩子。 |'
- en: '| [`Tensor.register_post_accumulate_grad_hook`](generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook
    "torch.Tensor.register_post_accumulate_grad_hook") | Registers a backward hook
    that runs after grad accumulation. |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.register_post_accumulate_grad_hook`](generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook
    "torch.Tensor.register_post_accumulate_grad_hook") | 注册一个在梯度累积后运行的反向钩子。 |'
- en: '| [`Tensor.remainder`](generated/torch.Tensor.remainder.html#torch.Tensor.remainder
    "torch.Tensor.remainder") | See [`torch.remainder()`](generated/torch.remainder.html#torch.remainder
    "torch.remainder") |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.remainder`](generated/torch.Tensor.remainder.html#torch.Tensor.remainder
    "torch.Tensor.remainder") | 参见[`torch.remainder()`](generated/torch.remainder.html#torch.remainder
    "torch.remainder") |'
- en: '| [`Tensor.remainder_`](generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_
    "torch.Tensor.remainder_") | In-place version of [`remainder()`](generated/torch.Tensor.remainder.html#torch.Tensor.remainder
    "torch.Tensor.remainder") |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.remainder_`](generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_
    "torch.Tensor.remainder_") | [`remainder()`](generated/torch.Tensor.remainder.html#torch.Tensor.remainder
    "torch.Tensor.remainder")的原位版本 |'
- en: '| [`Tensor.renorm`](generated/torch.Tensor.renorm.html#torch.Tensor.renorm
    "torch.Tensor.renorm") | See [`torch.renorm()`](generated/torch.renorm.html#torch.renorm
    "torch.renorm") |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.renorm`](generated/torch.Tensor.renorm.html#torch.Tensor.renorm
    "torch.Tensor.renorm") | 参见[`torch.renorm()`](generated/torch.renorm.html#torch.renorm
    "torch.renorm") |'
- en: '| [`Tensor.renorm_`](generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_
    "torch.Tensor.renorm_") | In-place version of [`renorm()`](generated/torch.Tensor.renorm.html#torch.Tensor.renorm
    "torch.Tensor.renorm") |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.renorm_`](generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_
    "torch.Tensor.renorm_") | [`renorm()`](generated/torch.Tensor.renorm.html#torch.Tensor.renorm
    "torch.Tensor.renorm")的原位版本 |'
- en: '| [`Tensor.repeat`](generated/torch.Tensor.repeat.html#torch.Tensor.repeat
    "torch.Tensor.repeat") | Repeats this tensor along the specified dimensions. |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.repeat`](generated/torch.Tensor.repeat.html#torch.Tensor.repeat
    "torch.Tensor.repeat") | 沿指定维度重复此张量。 |'
- en: '| [`Tensor.repeat_interleave`](generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave
    "torch.Tensor.repeat_interleave") | See [`torch.repeat_interleave()`](generated/torch.repeat_interleave.html#torch.repeat_interleave
    "torch.repeat_interleave"). |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.repeat_interleave`](generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave
    "torch.Tensor.repeat_interleave") | 参见[`torch.repeat_interleave()`](generated/torch.repeat_interleave.html#torch.repeat_interleave
    "torch.repeat_interleave")。 |'
- en: '| [`Tensor.requires_grad`](generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad
    "torch.Tensor.requires_grad") | Is `True` if gradients need to be computed for
    this Tensor, `False` otherwise. |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.requires_grad`](generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad
    "torch.Tensor.requires_grad") | 如果需要为此张量计算梯度，则为`True`，否则为`False`。 |'
- en: '| [`Tensor.requires_grad_`](generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_
    "torch.Tensor.requires_grad_") | Change if autograd should record operations on
    this tensor: sets this tensor''s `requires_grad` attribute in-place. |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.requires_grad_`](generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_
    "torch.Tensor.requires_grad_") | 更改是否应记录此张量上的操作的自动微分：就地设置此张量的`requires_grad`属性。
    |'
- en: '| [`Tensor.reshape`](generated/torch.Tensor.reshape.html#torch.Tensor.reshape
    "torch.Tensor.reshape") | Returns a tensor with the same data and number of elements
    as `self` but with the specified shape. |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.reshape`](generated/torch.Tensor.reshape.html#torch.Tensor.reshape
    "torch.Tensor.reshape") | 返回一个与`self`具有相同数据和元素数量但具有指定形状的张量。 |'
- en: '| [`Tensor.reshape_as`](generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as
    "torch.Tensor.reshape_as") | Returns this tensor as the same shape as `other`.
    |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.reshape_as`](generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as
    "torch.Tensor.reshape_as") | 将此张量返回为与`other`相同形状的张量。 |'
- en: '| [`Tensor.resize_`](generated/torch.Tensor.resize_.html#torch.Tensor.resize_
    "torch.Tensor.resize_") | Resizes `self` tensor to the specified size. |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.resize_`](generated/torch.Tensor.resize_.html#torch.Tensor.resize_
    "torch.Tensor.resize_") | 将`self`张量调整为指定大小。 |'
- en: '| [`Tensor.resize_as_`](generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_
    "torch.Tensor.resize_as_") | Resizes the `self` tensor to be the same size as
    the specified [`tensor`](generated/torch.tensor.html#torch.tensor "torch.tensor").
    |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.resize_as_`](generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_
    "torch.Tensor.resize_as_") | 将 `self` 张量调整为与指定的 [`tensor`](generated/torch.tensor.html#torch.tensor
    "torch.tensor") 相同大小 |'
- en: '| [`Tensor.retain_grad`](generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad
    "torch.Tensor.retain_grad") | Enables this Tensor to have their `grad` populated
    during `backward()`. |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.retain_grad`](generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad
    "torch.Tensor.retain_grad") | 在 `backward()` 过程中使此张量的 `grad` 被填充 |'
- en: '| [`Tensor.retains_grad`](generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad
    "torch.Tensor.retains_grad") | Is `True` if this Tensor is non-leaf and its `grad`
    is enabled to be populated during `backward()`, `False` otherwise. |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.retains_grad`](generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad
    "torch.Tensor.retains_grad") | 如果此张量是非叶子节点且其 `grad` 已启用填充，则为 `True`，否则为 `False`
    |'
- en: '| [`Tensor.roll`](generated/torch.Tensor.roll.html#torch.Tensor.roll "torch.Tensor.roll")
    | See [`torch.roll()`](generated/torch.roll.html#torch.roll "torch.roll") |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.roll`](generated/torch.Tensor.roll.html#torch.Tensor.roll "torch.Tensor.roll")
    | 查看 [`torch.roll()`](generated/torch.roll.html#torch.roll "torch.roll") |'
- en: '| [`Tensor.rot90`](generated/torch.Tensor.rot90.html#torch.Tensor.rot90 "torch.Tensor.rot90")
    | See [`torch.rot90()`](generated/torch.rot90.html#torch.rot90 "torch.rot90")
    |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.rot90`](generated/torch.Tensor.rot90.html#torch.Tensor.rot90 "torch.Tensor.rot90")
    | 查看 [`torch.rot90()`](generated/torch.rot90.html#torch.rot90 "torch.rot90") |'
- en: '| [`Tensor.round`](generated/torch.Tensor.round.html#torch.Tensor.round "torch.Tensor.round")
    | See [`torch.round()`](generated/torch.round.html#torch.round "torch.round")
    |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.round`](generated/torch.Tensor.round.html#torch.Tensor.round "torch.Tensor.round")
    | 查看 [`torch.round()`](generated/torch.round.html#torch.round "torch.round") |'
- en: '| [`Tensor.round_`](generated/torch.Tensor.round_.html#torch.Tensor.round_
    "torch.Tensor.round_") | In-place version of [`round()`](generated/torch.Tensor.round.html#torch.Tensor.round
    "torch.Tensor.round") |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.round_`](generated/torch.Tensor.round_.html#torch.Tensor.round_
    "torch.Tensor.round_") | [`round()`](generated/torch.Tensor.round.html#torch.Tensor.round
    "torch.Tensor.round") 的就地版本 |'
- en: '| [`Tensor.rsqrt`](generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt "torch.Tensor.rsqrt")
    | See [`torch.rsqrt()`](generated/torch.rsqrt.html#torch.rsqrt "torch.rsqrt")
    |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.rsqrt`](generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt "torch.Tensor.rsqrt")
    | 查看 [`torch.rsqrt()`](generated/torch.rsqrt.html#torch.rsqrt "torch.rsqrt") |'
- en: '| [`Tensor.rsqrt_`](generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_
    "torch.Tensor.rsqrt_") | In-place version of [`rsqrt()`](generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt
    "torch.Tensor.rsqrt") |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.rsqrt_`](generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_
    "torch.Tensor.rsqrt_") | [`rsqrt()`](generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt
    "torch.Tensor.rsqrt") 的就地版本 |'
- en: '| [`Tensor.scatter`](generated/torch.Tensor.scatter.html#torch.Tensor.scatter
    "torch.Tensor.scatter") | Out-of-place version of [`torch.Tensor.scatter_()`](generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_
    "torch.Tensor.scatter_") |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.scatter`](generated/torch.Tensor.scatter.html#torch.Tensor.scatter
    "torch.Tensor.scatter") | [`torch.Tensor.scatter_()`](generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_
    "torch.Tensor.scatter_") 的非就地版本 |'
- en: '| [`Tensor.scatter_`](generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_
    "torch.Tensor.scatter_") | Writes all values from the tensor `src` into `self`
    at the indices specified in the `index` tensor. |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.scatter_`](generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_
    "torch.Tensor.scatter_") | 将张量 `src` 中的所有值写入到指定在 `index` 张量中的 `self` 中 |'
- en: '| [`Tensor.scatter_add_`](generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_
    "torch.Tensor.scatter_add_") | Adds all values from the tensor `src` into `self`
    at the indices specified in the `index` tensor in a similar fashion as [`scatter_()`](generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_
    "torch.Tensor.scatter_"). |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.scatter_add_`](generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_
    "torch.Tensor.scatter_add_") | 将张量 `src` 中的所有值添加到 `index` 张量中指定的 `self` 中，类似于
    [`scatter_()`](generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_ "torch.Tensor.scatter_")
    的方式 |'
- en: '| [`Tensor.scatter_add`](generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add
    "torch.Tensor.scatter_add") | Out-of-place version of [`torch.Tensor.scatter_add_()`](generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_
    "torch.Tensor.scatter_add_") |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.scatter_add`](generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add
    "torch.Tensor.scatter_add") | [`torch.Tensor.scatter_add_()`](generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_
    "torch.Tensor.scatter_add_") 的非就地版本 |'
- en: '| [`Tensor.scatter_reduce_`](generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_
    "torch.Tensor.scatter_reduce_") | Reduces all values from the `src` tensor to
    the indices specified in the `index` tensor in the `self` tensor using the applied
    reduction defined via the `reduce` argument (`"sum"`, `"prod"`, `"mean"`, `"amax"`,
    `"amin"`). |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.scatter_reduce_`](generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_
    "torch.Tensor.scatter_reduce_") | 将 `src` 张量中的所有值按照应用的减少方式（`"sum"`、`"prod"`、`"mean"`、`"amax"`、`"amin"`）减少到
    `index` 张量中指定的索引中的 `self` 张量中 |'
- en: '| [`Tensor.scatter_reduce`](generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce
    "torch.Tensor.scatter_reduce") | Out-of-place version of [`torch.Tensor.scatter_reduce_()`](generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_
    "torch.Tensor.scatter_reduce_") |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.scatter_reduce`](generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce
    "torch.Tensor.scatter_reduce") | [`torch.Tensor.scatter_reduce_()`](generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_
    "torch.Tensor.scatter_reduce_") 的非就地版本 |'
- en: '| [`Tensor.select`](generated/torch.Tensor.select.html#torch.Tensor.select
    "torch.Tensor.select") | See [`torch.select()`](generated/torch.select.html#torch.select
    "torch.select") |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.select`](generated/torch.Tensor.select.html#torch.Tensor.select
    "torch.Tensor.select") | 查看 [`torch.select()`](generated/torch.select.html#torch.select
    "torch.select") |'
- en: '| [`Tensor.select_scatter`](generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter
    "torch.Tensor.select_scatter") | See [`torch.select_scatter()`](generated/torch.select_scatter.html#torch.select_scatter
    "torch.select_scatter") |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.select_scatter`](generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter
    "torch.Tensor.select_scatter") | 查看 [`torch.select_scatter()`](generated/torch.select_scatter.html#torch.select_scatter
    "torch.select_scatter") |'
- en: '| [`Tensor.set_`](generated/torch.Tensor.set_.html#torch.Tensor.set_ "torch.Tensor.set_")
    | Sets the underlying storage, size, and strides. |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.set_`](generated/torch.Tensor.set_.html#torch.Tensor.set_ "torch.Tensor.set_")
    | 设置底层存储、大小和步幅。 |'
- en: '| [`Tensor.share_memory_`](generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_
    "torch.Tensor.share_memory_") | Moves the underlying storage to shared memory.
    |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.share_memory_`](generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_
    "torch.Tensor.share_memory_") | 将底层存储移动到共享内存。 |'
- en: '| [`Tensor.short`](generated/torch.Tensor.short.html#torch.Tensor.short "torch.Tensor.short")
    | `self.short()` is equivalent to `self.to(torch.int16)`. |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.short`](generated/torch.Tensor.short.html#torch.Tensor.short "torch.Tensor.short")
    | `self.short()` 等同于 `self.to(torch.int16)`。 |'
- en: '| [`Tensor.sigmoid`](generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid
    "torch.Tensor.sigmoid") | See [`torch.sigmoid()`](generated/torch.sigmoid.html#torch.sigmoid
    "torch.sigmoid") |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sigmoid`](generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid
    "torch.Tensor.sigmoid") | 查看 [`torch.sigmoid()`](generated/torch.sigmoid.html#torch.sigmoid
    "torch.sigmoid") |'
- en: '| [`Tensor.sigmoid_`](generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_
    "torch.Tensor.sigmoid_") | In-place version of [`sigmoid()`](generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid
    "torch.Tensor.sigmoid") |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sigmoid_`](generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_
    "torch.Tensor.sigmoid_") | [`sigmoid()`](generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid
    "torch.Tensor.sigmoid") 的原地版本 |'
- en: '| [`Tensor.sign`](generated/torch.Tensor.sign.html#torch.Tensor.sign "torch.Tensor.sign")
    | See [`torch.sign()`](generated/torch.sign.html#torch.sign "torch.sign") |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sign`](generated/torch.Tensor.sign.html#torch.Tensor.sign "torch.Tensor.sign")
    | 查看 [`torch.sign()`](generated/torch.sign.html#torch.sign "torch.sign") |'
- en: '| [`Tensor.sign_`](generated/torch.Tensor.sign_.html#torch.Tensor.sign_ "torch.Tensor.sign_")
    | In-place version of [`sign()`](generated/torch.Tensor.sign.html#torch.Tensor.sign
    "torch.Tensor.sign") |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sign_`](generated/torch.Tensor.sign_.html#torch.Tensor.sign_ "torch.Tensor.sign_")
    | [`sign()`](generated/torch.Tensor.sign.html#torch.Tensor.sign "torch.Tensor.sign")
    的原地版本 |'
- en: '| [`Tensor.signbit`](generated/torch.Tensor.signbit.html#torch.Tensor.signbit
    "torch.Tensor.signbit") | See [`torch.signbit()`](generated/torch.signbit.html#torch.signbit
    "torch.signbit") |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.signbit`](generated/torch.Tensor.signbit.html#torch.Tensor.signbit
    "torch.Tensor.signbit") | 查看 [`torch.signbit()`](generated/torch.signbit.html#torch.signbit
    "torch.signbit") |'
- en: '| [`Tensor.sgn`](generated/torch.Tensor.sgn.html#torch.Tensor.sgn "torch.Tensor.sgn")
    | See [`torch.sgn()`](generated/torch.sgn.html#torch.sgn "torch.sgn") |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sgn`](generated/torch.Tensor.sgn.html#torch.Tensor.sgn "torch.Tensor.sgn")
    | 查看 [`torch.sgn()`](generated/torch.sgn.html#torch.sgn "torch.sgn") |'
- en: '| [`Tensor.sgn_`](generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_ "torch.Tensor.sgn_")
    | In-place version of [`sgn()`](generated/torch.Tensor.sgn.html#torch.Tensor.sgn
    "torch.Tensor.sgn") |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sgn_`](generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_ "torch.Tensor.sgn_")
    | [`sgn()`](generated/torch.Tensor.sgn.html#torch.Tensor.sgn "torch.Tensor.sgn")
    的原地版本 |'
- en: '| [`Tensor.sin`](generated/torch.Tensor.sin.html#torch.Tensor.sin "torch.Tensor.sin")
    | See [`torch.sin()`](generated/torch.sin.html#torch.sin "torch.sin") |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sin`](generated/torch.Tensor.sin.html#torch.Tensor.sin "torch.Tensor.sin")
    | 查看 [`torch.sin()`](generated/torch.sin.html#torch.sin "torch.sin") |'
- en: '| [`Tensor.sin_`](generated/torch.Tensor.sin_.html#torch.Tensor.sin_ "torch.Tensor.sin_")
    | In-place version of [`sin()`](generated/torch.Tensor.sin.html#torch.Tensor.sin
    "torch.Tensor.sin") |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sin_`](generated/torch.Tensor.sin_.html#torch.Tensor.sin_ "torch.Tensor.sin_")
    | [`sin()`](generated/torch.Tensor.sin.html#torch.Tensor.sin "torch.Tensor.sin")
    的原地版本 |'
- en: '| [`Tensor.sinc`](generated/torch.Tensor.sinc.html#torch.Tensor.sinc "torch.Tensor.sinc")
    | See [`torch.sinc()`](generated/torch.sinc.html#torch.sinc "torch.sinc") |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sinc`](generated/torch.Tensor.sinc.html#torch.Tensor.sinc "torch.Tensor.sinc")
    | 查看 [`torch.sinc()`](generated/torch.sinc.html#torch.sinc "torch.sinc") |'
- en: '| [`Tensor.sinc_`](generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_ "torch.Tensor.sinc_")
    | In-place version of [`sinc()`](generated/torch.Tensor.sinc.html#torch.Tensor.sinc
    "torch.Tensor.sinc") |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sinc_`](generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_ "torch.Tensor.sinc_")
    | [`sinc()`](generated/torch.Tensor.sinc.html#torch.Tensor.sinc "torch.Tensor.sinc")
    的原地版本 |'
- en: '| [`Tensor.sinh`](generated/torch.Tensor.sinh.html#torch.Tensor.sinh "torch.Tensor.sinh")
    | See [`torch.sinh()`](generated/torch.sinh.html#torch.sinh "torch.sinh") |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sinh`](generated/torch.Tensor.sinh.html#torch.Tensor.sinh "torch.Tensor.sinh")
    | 查看 [`torch.sinh()`](generated/torch.sinh.html#torch.sinh "torch.sinh") |'
- en: '| [`Tensor.sinh_`](generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_ "torch.Tensor.sinh_")
    | In-place version of [`sinh()`](generated/torch.Tensor.sinh.html#torch.Tensor.sinh
    "torch.Tensor.sinh") |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sinh_`](generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_ "torch.Tensor.sinh_")
    | [`sinh()`](generated/torch.Tensor.sinh.html#torch.Tensor.sinh "torch.Tensor.sinh")
    的原地版本 |'
- en: '| [`Tensor.asinh`](generated/torch.Tensor.asinh.html#torch.Tensor.asinh "torch.Tensor.asinh")
    | See [`torch.asinh()`](generated/torch.asinh.html#torch.asinh "torch.asinh")
    |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.asinh`](generated/torch.Tensor.asinh.html#torch.Tensor.asinh "torch.Tensor.asinh")
    | 查看 [`torch.asinh()`](generated/torch.asinh.html#torch.asinh "torch.asinh") |'
- en: '| [`Tensor.asinh_`](generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_
    "torch.Tensor.asinh_") | In-place version of [`asinh()`](generated/torch.Tensor.asinh.html#torch.Tensor.asinh
    "torch.Tensor.asinh") |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.asinh_`](generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_
    "torch.Tensor.asinh_") | [`asinh()`](generated/torch.Tensor.asinh.html#torch.Tensor.asinh
    "torch.Tensor.asinh") 的原地版本 |'
- en: '| [`Tensor.arcsinh`](generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh
    "torch.Tensor.arcsinh") | See [`torch.arcsinh()`](generated/torch.arcsinh.html#torch.arcsinh
    "torch.arcsinh") |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arcsinh`](generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh
    "torch.Tensor.arcsinh") | 查看 [`torch.arcsinh()`](generated/torch.arcsinh.html#torch.arcsinh
    "torch.arcsinh") |'
- en: '| [`Tensor.arcsinh_`](generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_
    "torch.Tensor.arcsinh_") | In-place version of [`arcsinh()`](generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh
    "torch.Tensor.arcsinh") |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arcsinh_`](generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_
    "torch.Tensor.arcsinh_") | [`arcsinh()`](generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh
    "torch.Tensor.arcsinh") 的原地版本 |'
- en: '| [`Tensor.shape`](generated/torch.Tensor.shape.html#torch.Tensor.shape "torch.Tensor.shape")
    | Returns the size of the `self` tensor. |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.shape`](generated/torch.Tensor.shape.html#torch.Tensor.shape "torch.Tensor.shape")
    | 返回 `self` 张量的大小。 |'
- en: '| [`Tensor.size`](generated/torch.Tensor.size.html#torch.Tensor.size "torch.Tensor.size")
    | Returns the size of the `self` tensor. |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.size`](generated/torch.Tensor.size.html#torch.Tensor.size "torch.Tensor.size")
    | 返回 `self` 张量的大小。 |'
- en: '| [`Tensor.slogdet`](generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet
    "torch.Tensor.slogdet") | See [`torch.slogdet()`](generated/torch.slogdet.html#torch.slogdet
    "torch.slogdet") |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.slogdet`](generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet
    "torch.Tensor.slogdet") | 查看 [`torch.slogdet()`](generated/torch.slogdet.html#torch.slogdet
    "torch.slogdet") |'
- en: '| [`Tensor.slice_scatter`](generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter
    "torch.Tensor.slice_scatter") | See [`torch.slice_scatter()`](generated/torch.slice_scatter.html#torch.slice_scatter
    "torch.slice_scatter") |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.slice_scatter`](generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter
    "torch.Tensor.slice_scatter") | 查看 [`torch.slice_scatter()`](generated/torch.slice_scatter.html#torch.slice_scatter
    "torch.slice_scatter") |'
- en: '| [`Tensor.softmax`](generated/torch.Tensor.softmax.html#torch.Tensor.softmax
    "torch.Tensor.softmax") | Alias for [`torch.nn.functional.softmax()`](generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax
    "torch.nn.functional.softmax"). |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.softmax`](generated/torch.Tensor.softmax.html#torch.Tensor.softmax
    "torch.Tensor.softmax") | [`torch.nn.functional.softmax()`](generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax
    "torch.nn.functional.softmax") 的别名。 |'
- en: '| [`Tensor.sort`](generated/torch.Tensor.sort.html#torch.Tensor.sort "torch.Tensor.sort")
    | See [`torch.sort()`](generated/torch.sort.html#torch.sort "torch.sort") |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sort`](generated/torch.Tensor.sort.html#torch.Tensor.sort "torch.Tensor.sort")
    | 查看 [`torch.sort()`](generated/torch.sort.html#torch.sort "torch.sort") |'
- en: '| [`Tensor.split`](generated/torch.Tensor.split.html#torch.Tensor.split "torch.Tensor.split")
    | See [`torch.split()`](generated/torch.split.html#torch.split "torch.split")
    |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.split`](generated/torch.Tensor.split.html#torch.Tensor.split "torch.Tensor.split")
    | 查看 [`torch.split()`](generated/torch.split.html#torch.split "torch.split") |'
- en: '| [`Tensor.sparse_mask`](generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask
    "torch.Tensor.sparse_mask") | Returns a new [sparse tensor](sparse.html#sparse-docs)
    with values from a strided tensor `self` filtered by the indices of the sparse
    tensor `mask`. |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sparse_mask`](generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask
    "torch.Tensor.sparse_mask") | 使用稀疏张量 `mask` 的索引过滤来自分块张量 `self` 的值，返回一个新的 [稀疏张量](sparse.html#sparse-docs)。
    |'
- en: '| [`Tensor.sparse_dim`](generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim
    "torch.Tensor.sparse_dim") | Return the number of sparse dimensions in a [sparse
    tensor](sparse.html#sparse-docs) `self`. |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sparse_dim`](generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim
    "torch.Tensor.sparse_dim") | 返回 [稀疏张量](sparse.html#sparse-docs) `self` 中稀疏维度的数量。
    |'
- en: '| [`Tensor.sqrt`](generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt "torch.Tensor.sqrt")
    | See [`torch.sqrt()`](generated/torch.sqrt.html#torch.sqrt "torch.sqrt") |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sqrt`](generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt "torch.Tensor.sqrt")
    | 查看 [`torch.sqrt()`](generated/torch.sqrt.html#torch.sqrt "torch.sqrt") |'
- en: '| [`Tensor.sqrt_`](generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_ "torch.Tensor.sqrt_")
    | In-place version of [`sqrt()`](generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt
    "torch.Tensor.sqrt") |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sqrt_`](generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_ "torch.Tensor.sqrt_")
    | [`sqrt()`](generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt "torch.Tensor.sqrt")
    的原地版本 |'
- en: '| [`Tensor.square`](generated/torch.Tensor.square.html#torch.Tensor.square
    "torch.Tensor.square") | See [`torch.square()`](generated/torch.square.html#torch.square
    "torch.square") |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.square`](generated/torch.Tensor.square.html#torch.Tensor.square
    "torch.Tensor.square") | 查看 [`torch.square()`](generated/torch.square.html#torch.square
    "torch.square") |'
- en: '| [`Tensor.square_`](generated/torch.Tensor.square_.html#torch.Tensor.square_
    "torch.Tensor.square_") | In-place version of [`square()`](generated/torch.Tensor.square.html#torch.Tensor.square
    "torch.Tensor.square") |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.square_`](generated/torch.Tensor.square_.html#torch.Tensor.square_
    "torch.Tensor.square_") | [`square()`](generated/torch.Tensor.square.html#torch.Tensor.square
    "torch.Tensor.square") 的原地版本 |'
- en: '| [`Tensor.squeeze`](generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze
    "torch.Tensor.squeeze") | See [`torch.squeeze()`](generated/torch.squeeze.html#torch.squeeze
    "torch.squeeze") |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.squeeze`](generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze
    "torch.Tensor.squeeze") | 查看 [`torch.squeeze()`](generated/torch.squeeze.html#torch.squeeze
    "torch.squeeze") |'
- en: '| [`Tensor.squeeze_`](generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_
    "torch.Tensor.squeeze_") | In-place version of [`squeeze()`](generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze
    "torch.Tensor.squeeze") |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.squeeze_`](generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_
    "torch.Tensor.squeeze_") | [`squeeze()`](generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze
    "torch.Tensor.squeeze") 的原地版本 |'
- en: '| [`Tensor.std`](generated/torch.Tensor.std.html#torch.Tensor.std "torch.Tensor.std")
    | See [`torch.std()`](generated/torch.std.html#torch.std "torch.std") |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.std`](generated/torch.Tensor.std.html#torch.Tensor.std "torch.Tensor.std")
    | 查看 [`torch.std()`](generated/torch.std.html#torch.std "torch.std") |'
- en: '| [`Tensor.stft`](generated/torch.Tensor.stft.html#torch.Tensor.stft "torch.Tensor.stft")
    | See [`torch.stft()`](generated/torch.stft.html#torch.stft "torch.stft") |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.stft`](generated/torch.Tensor.stft.html#torch.Tensor.stft "torch.Tensor.stft")
    | 查看 [`torch.stft()`](generated/torch.stft.html#torch.stft "torch.stft") |'
- en: '| [`Tensor.storage`](generated/torch.Tensor.storage.html#torch.Tensor.storage
    "torch.Tensor.storage") | Returns the underlying [`TypedStorage`](storage.html#torch.TypedStorage
    "torch.TypedStorage"). |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.storage`](generated/torch.Tensor.storage.html#torch.Tensor.storage
    "torch.Tensor.storage") | 返回底层的 [`TypedStorage`](storage.html#torch.TypedStorage
    "torch.TypedStorage")。 |'
- en: '| [`Tensor.untyped_storage`](generated/torch.Tensor.untyped_storage.html#torch.Tensor.untyped_storage
    "torch.Tensor.untyped_storage") | Returns the underlying [`UntypedStorage`](storage.html#torch.UntypedStorage
    "torch.UntypedStorage"). |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.untyped_storage`](generated/torch.Tensor.untyped_storage.html#torch.Tensor.untyped_storage
    "torch.Tensor.untyped_storage") | 返回底层的 [`UntypedStorage`](storage.html#torch.UntypedStorage
    "torch.UntypedStorage")。 |'
- en: '| [`Tensor.storage_offset`](generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset
    "torch.Tensor.storage_offset") | Returns `self` tensor''s offset in the underlying
    storage in terms of number of storage elements (not bytes). |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.storage_offset`](generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset
    "torch.Tensor.storage_offset") | 返回 `self` 张量在底层存储中的偏移量，以存储元素的数量表示（不是字节）。 |'
- en: '| [`Tensor.storage_type`](generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type
    "torch.Tensor.storage_type") | Returns the type of the underlying storage. |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.storage_type`](generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type
    "torch.Tensor.storage_type") | 返回底层存储的类型。 |'
- en: '| [`Tensor.stride`](generated/torch.Tensor.stride.html#torch.Tensor.stride
    "torch.Tensor.stride") | Returns the stride of `self` tensor. |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.stride`](generated/torch.Tensor.stride.html#torch.Tensor.stride
    "torch.Tensor.stride") | 返回 `self` 张量的步幅。 |'
- en: '| [`Tensor.sub`](generated/torch.Tensor.sub.html#torch.Tensor.sub "torch.Tensor.sub")
    | See [`torch.sub()`](generated/torch.sub.html#torch.sub "torch.sub"). |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sub`](generated/torch.Tensor.sub.html#torch.Tensor.sub "torch.Tensor.sub")
    | 查看 [`torch.sub()`](generated/torch.sub.html#torch.sub "torch.sub") |'
- en: '| [`Tensor.sub_`](generated/torch.Tensor.sub_.html#torch.Tensor.sub_ "torch.Tensor.sub_")
    | In-place version of [`sub()`](generated/torch.Tensor.sub.html#torch.Tensor.sub
    "torch.Tensor.sub") |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sub_`](generated/torch.Tensor.sub_.html#torch.Tensor.sub_ "torch.Tensor.sub_")
    | [`sub()`](generated/torch.Tensor.sub.html#torch.Tensor.sub "torch.Tensor.sub")
    的原地版本 |'
- en: '| [`Tensor.subtract`](generated/torch.Tensor.subtract.html#torch.Tensor.subtract
    "torch.Tensor.subtract") | See [`torch.subtract()`](generated/torch.subtract.html#torch.subtract
    "torch.subtract"). |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.subtract`](generated/torch.Tensor.subtract.html#torch.Tensor.subtract
    "torch.Tensor.subtract") | 查看 [`torch.subtract()`](generated/torch.subtract.html#torch.subtract
    "torch.subtract") |'
- en: '| [`Tensor.subtract_`](generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_
    "torch.Tensor.subtract_") | In-place version of [`subtract()`](generated/torch.Tensor.subtract.html#torch.Tensor.subtract
    "torch.Tensor.subtract"). |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.subtract_`](generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_
    "torch.Tensor.subtract_") | [`subtract()`](generated/torch.Tensor.subtract.html#torch.Tensor.subtract
    "torch.Tensor.subtract") 的原地版本 |'
- en: '| [`Tensor.sum`](generated/torch.Tensor.sum.html#torch.Tensor.sum "torch.Tensor.sum")
    | See [`torch.sum()`](generated/torch.sum.html#torch.sum "torch.sum") |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sum`](generated/torch.Tensor.sum.html#torch.Tensor.sum "torch.Tensor.sum")
    | 查看 [`torch.sum()`](generated/torch.sum.html#torch.sum "torch.sum") |'
- en: '| [`Tensor.sum_to_size`](generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size
    "torch.Tensor.sum_to_size") | Sum `this` tensor to `size`. |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.sum_to_size`](generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size
    "torch.Tensor.sum_to_size") | 将 `this` 张量求和到 `size` |'
- en: '| [`Tensor.svd`](generated/torch.Tensor.svd.html#torch.Tensor.svd "torch.Tensor.svd")
    | See [`torch.svd()`](generated/torch.svd.html#torch.svd "torch.svd") |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.svd`](generated/torch.Tensor.svd.html#torch.Tensor.svd "torch.Tensor.svd")
    | 查看 [`torch.svd()`](generated/torch.svd.html#torch.svd "torch.svd") |'
- en: '| [`Tensor.swapaxes`](generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes
    "torch.Tensor.swapaxes") | See [`torch.swapaxes()`](generated/torch.swapaxes.html#torch.swapaxes
    "torch.swapaxes") |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.swapaxes`](generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes
    "torch.Tensor.swapaxes") | 查看 [`torch.swapaxes()`](generated/torch.swapaxes.html#torch.swapaxes
    "torch.swapaxes") |'
- en: '| [`Tensor.swapdims`](generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims
    "torch.Tensor.swapdims") | See [`torch.swapdims()`](generated/torch.swapdims.html#torch.swapdims
    "torch.swapdims") |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.swapdims`](generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims
    "torch.Tensor.swapdims") | 查看 [`torch.swapdims()`](generated/torch.swapdims.html#torch.swapdims
    "torch.swapdims") |'
- en: '| [`Tensor.t`](generated/torch.Tensor.t.html#torch.Tensor.t "torch.Tensor.t")
    | See [`torch.t()`](generated/torch.t.html#torch.t "torch.t") |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.t`](generated/torch.Tensor.t.html#torch.Tensor.t "torch.Tensor.t")
    | 查看 [`torch.t()`](generated/torch.t.html#torch.t "torch.t") |'
- en: '| [`Tensor.t_`](generated/torch.Tensor.t_.html#torch.Tensor.t_ "torch.Tensor.t_")
    | In-place version of [`t()`](generated/torch.Tensor.t.html#torch.Tensor.t "torch.Tensor.t")
    |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.t_`](generated/torch.Tensor.t_.html#torch.Tensor.t_ "torch.Tensor.t_")
    | [`t()`](generated/torch.Tensor.t.html#torch.Tensor.t "torch.Tensor.t") 的原地版本
    |'
- en: '| [`Tensor.tensor_split`](generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split
    "torch.Tensor.tensor_split") | See [`torch.tensor_split()`](generated/torch.tensor_split.html#torch.tensor_split
    "torch.tensor_split") |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.tensor_split`](generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split
    "torch.Tensor.tensor_split") | 查看 [`torch.tensor_split()`](generated/torch.tensor_split.html#torch.tensor_split
    "torch.tensor_split") |'
- en: '| [`Tensor.tile`](generated/torch.Tensor.tile.html#torch.Tensor.tile "torch.Tensor.tile")
    | See [`torch.tile()`](generated/torch.tile.html#torch.tile "torch.tile") |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.tile`](generated/torch.Tensor.tile.html#torch.Tensor.tile "torch.Tensor.tile")
    | 查看 [`torch.tile()`](generated/torch.tile.html#torch.tile "torch.tile") |'
- en: '| [`Tensor.to`](generated/torch.Tensor.to.html#torch.Tensor.to "torch.Tensor.to")
    | Performs Tensor dtype and/or device conversion. |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.to`](generated/torch.Tensor.to.html#torch.Tensor.to "torch.Tensor.to")
    | 执行张量的数据类型和/或设备转换 |'
- en: '| [`Tensor.to_mkldnn`](generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn
    "torch.Tensor.to_mkldnn") | Returns a copy of the tensor in `torch.mkldnn` layout.
    |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.to_mkldnn`](generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn
    "torch.Tensor.to_mkldnn") | 返回在 `torch.mkldnn` 布局中的张量的副本 |'
- en: '| [`Tensor.take`](generated/torch.Tensor.take.html#torch.Tensor.take "torch.Tensor.take")
    | See [`torch.take()`](generated/torch.take.html#torch.take "torch.take") |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.take`](generated/torch.Tensor.take.html#torch.Tensor.take "torch.Tensor.take")
    | 查看 [`torch.take()`](generated/torch.take.html#torch.take "torch.take") |'
- en: '| [`Tensor.take_along_dim`](generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim
    "torch.Tensor.take_along_dim") | See [`torch.take_along_dim()`](generated/torch.take_along_dim.html#torch.take_along_dim
    "torch.take_along_dim") |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.take_along_dim`](generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim
    "torch.Tensor.take_along_dim") | 查看 [`torch.take_along_dim()`](generated/torch.take_along_dim.html#torch.take_along_dim
    "torch.take_along_dim") |'
- en: '| [`Tensor.tan`](generated/torch.Tensor.tan.html#torch.Tensor.tan "torch.Tensor.tan")
    | See [`torch.tan()`](generated/torch.tan.html#torch.tan "torch.tan") |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.tan`](generated/torch.Tensor.tan.html#torch.Tensor.tan "torch.Tensor.tan")
    | 查看 [`torch.tan()`](generated/torch.tan.html#torch.tan "torch.tan") |'
- en: '| [`Tensor.tan_`](generated/torch.Tensor.tan_.html#torch.Tensor.tan_ "torch.Tensor.tan_")
    | In-place version of [`tan()`](generated/torch.Tensor.tan.html#torch.Tensor.tan
    "torch.Tensor.tan") |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.tan_`](generated/torch.Tensor.tan_.html#torch.Tensor.tan_ "torch.Tensor.tan_")
    | [`tan()`](generated/torch.Tensor.tan.html#torch.Tensor.tan "torch.Tensor.tan")
    的原地版本 |'
- en: '| [`Tensor.tanh`](generated/torch.Tensor.tanh.html#torch.Tensor.tanh "torch.Tensor.tanh")
    | See [`torch.tanh()`](generated/torch.tanh.html#torch.tanh "torch.tanh") |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.tanh`](generated/torch.Tensor.tanh.html#torch.Tensor.tanh "torch.Tensor.tanh")
    | 查看 [`torch.tanh()`](generated/torch.tanh.html#torch.tanh "torch.tanh") |'
- en: '| [`Tensor.tanh_`](generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_ "torch.Tensor.tanh_")
    | In-place version of [`tanh()`](generated/torch.Tensor.tanh.html#torch.Tensor.tanh
    "torch.Tensor.tanh") |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.tanh_`](generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_ "torch.Tensor.tanh_")
    | [`tanh()`](generated/torch.Tensor.tanh.html#torch.Tensor.tanh "torch.Tensor.tanh")
    的原地版本 |'
- en: '| [`Tensor.atanh`](generated/torch.Tensor.atanh.html#torch.Tensor.atanh "torch.Tensor.atanh")
    | See [`torch.atanh()`](generated/torch.atanh.html#torch.atanh "torch.atanh")
    |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.atanh`](generated/torch.Tensor.atanh.html#torch.Tensor.atanh "torch.Tensor.atanh")
    | 查看 [`torch.atanh()`](generated/torch.atanh.html#torch.atanh "torch.atanh") |'
- en: '| [`Tensor.atanh_`](generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_
    "torch.Tensor.atanh_") | In-place version of [`atanh()`](generated/torch.Tensor.atanh.html#torch.Tensor.atanh
    "torch.Tensor.atanh") |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.atanh_`](generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_
    "torch.Tensor.atanh_") | [`atanh()`](generated/torch.Tensor.atanh.html#torch.Tensor.atanh
    "torch.Tensor.atanh") 的原地版本 |'
- en: '| [`Tensor.arctanh`](generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh
    "torch.Tensor.arctanh") | See [`torch.arctanh()`](generated/torch.arctanh.html#torch.arctanh
    "torch.arctanh") |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arctanh`](generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh
    "torch.Tensor.arctanh") | 参见 [`torch.arctanh()`](generated/torch.arctanh.html#torch.arctanh
    "torch.arctanh") |'
- en: '| [`Tensor.arctanh_`](generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_
    "torch.Tensor.arctanh_") | In-place version of [`arctanh()`](generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh
    "torch.Tensor.arctanh") |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.arctanh_`](generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_
    "torch.Tensor.arctanh_") | [`arctanh()`](generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh
    "torch.Tensor.arctanh") 的原地版本 |'
- en: '| [`Tensor.tolist`](generated/torch.Tensor.tolist.html#torch.Tensor.tolist
    "torch.Tensor.tolist") | Returns the tensor as a (nested) list. |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.tolist`](generated/torch.Tensor.tolist.html#torch.Tensor.tolist
    "torch.Tensor.tolist") | 将张量返回为（嵌套的）列表。 |'
- en: '| [`Tensor.topk`](generated/torch.Tensor.topk.html#torch.Tensor.topk "torch.Tensor.topk")
    | See [`torch.topk()`](generated/torch.topk.html#torch.topk "torch.topk") |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.topk`](generated/torch.Tensor.topk.html#torch.Tensor.topk "torch.Tensor.topk")
    | 参见 [`torch.topk()`](generated/torch.topk.html#torch.topk "torch.topk") |'
- en: '| [`Tensor.to_dense`](generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense
    "torch.Tensor.to_dense") | Creates a strided copy of `self` if `self` is not a
    strided tensor, otherwise returns `self`. |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.to_dense`](generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense
    "torch.Tensor.to_dense") | 如果 `self` 不是分块张量，则创建 `self` 的分块副本，否则返回 `self`。 |'
- en: '| [`Tensor.to_sparse`](generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse
    "torch.Tensor.to_sparse") | Returns a sparse copy of the tensor. |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.to_sparse`](generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse
    "torch.Tensor.to_sparse") | 返回张量的稀疏副本。 |'
- en: '| [`Tensor.to_sparse_csr`](generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr
    "torch.Tensor.to_sparse_csr") | Convert a tensor to compressed row storage format
    (CSR). |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.to_sparse_csr`](generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr
    "torch.Tensor.to_sparse_csr") | 将张量转换为压缩行存储格式（CSR）。 |'
- en: '| [`Tensor.to_sparse_csc`](generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc
    "torch.Tensor.to_sparse_csc") | Convert a tensor to compressed column storage
    (CSC) format. |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.to_sparse_csc`](generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc
    "torch.Tensor.to_sparse_csc") | 将张量转换为压缩列存储（CSC）格式。 |'
- en: '| [`Tensor.to_sparse_bsr`](generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr
    "torch.Tensor.to_sparse_bsr") | Convert a tensor to a block sparse row (BSR) storage
    format of given blocksize. |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.to_sparse_bsr`](generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr
    "torch.Tensor.to_sparse_bsr") | 将张量转换为给定块大小的块稀疏行（BSR）存储格式。 |'
- en: '| [`Tensor.to_sparse_bsc`](generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc
    "torch.Tensor.to_sparse_bsc") | Convert a tensor to a block sparse column (BSC)
    storage format of given blocksize. |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.to_sparse_bsc`](generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc
    "torch.Tensor.to_sparse_bsc") | 将张量转换为给定块大小的块稀疏列（BSC）存储格式。 |'
- en: '| [`Tensor.trace`](generated/torch.Tensor.trace.html#torch.Tensor.trace "torch.Tensor.trace")
    | See [`torch.trace()`](generated/torch.trace.html#torch.trace "torch.trace")
    |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.trace`](generated/torch.Tensor.trace.html#torch.Tensor.trace "torch.Tensor.trace")
    | 参见 [`torch.trace()`](generated/torch.trace.html#torch.trace "torch.trace") |'
- en: '| [`Tensor.transpose`](generated/torch.Tensor.transpose.html#torch.Tensor.transpose
    "torch.Tensor.transpose") | See [`torch.transpose()`](generated/torch.transpose.html#torch.transpose
    "torch.transpose") |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.transpose`](generated/torch.Tensor.transpose.html#torch.Tensor.transpose
    "torch.Tensor.transpose") | 参见 [`torch.transpose()`](generated/torch.transpose.html#torch.transpose
    "torch.transpose") |'
- en: '| [`Tensor.transpose_`](generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_
    "torch.Tensor.transpose_") | In-place version of [`transpose()`](generated/torch.Tensor.transpose.html#torch.Tensor.transpose
    "torch.Tensor.transpose") |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.transpose_`](generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_
    "torch.Tensor.transpose_") | [`transpose()`](generated/torch.Tensor.transpose.html#torch.Tensor.transpose
    "torch.Tensor.transpose") 的原地版本 |'
- en: '| [`Tensor.triangular_solve`](generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve
    "torch.Tensor.triangular_solve") | See [`torch.triangular_solve()`](generated/torch.triangular_solve.html#torch.triangular_solve
    "torch.triangular_solve") |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.triangular_solve`](generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve
    "torch.Tensor.triangular_solve") | 参见 [`torch.triangular_solve()`](generated/torch.triangular_solve.html#torch.triangular_solve
    "torch.triangular_solve") |'
- en: '| [`Tensor.tril`](generated/torch.Tensor.tril.html#torch.Tensor.tril "torch.Tensor.tril")
    | See [`torch.tril()`](generated/torch.tril.html#torch.tril "torch.tril") |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.tril`](generated/torch.Tensor.tril.html#torch.Tensor.tril "torch.Tensor.tril")
    | 参见 [`torch.tril()`](generated/torch.tril.html#torch.tril "torch.tril") |'
- en: '| [`Tensor.tril_`](generated/torch.Tensor.tril_.html#torch.Tensor.tril_ "torch.Tensor.tril_")
    | In-place version of [`tril()`](generated/torch.Tensor.tril.html#torch.Tensor.tril
    "torch.Tensor.tril") |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.tril_`](generated/torch.Tensor.tril_.html#torch.Tensor.tril_ "torch.Tensor.tril_")
    | [`tril()`](generated/torch.Tensor.tril.html#torch.Tensor.tril "torch.Tensor.tril")
    的原地版本 |'
- en: '| [`Tensor.triu`](generated/torch.Tensor.triu.html#torch.Tensor.triu "torch.Tensor.triu")
    | See [`torch.triu()`](generated/torch.triu.html#torch.triu "torch.triu") |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.triu`](generated/torch.Tensor.triu.html#torch.Tensor.triu "torch.Tensor.triu")
    | 参见 [`torch.triu()`](generated/torch.triu.html#torch.triu "torch.triu") |'
- en: '| [`Tensor.triu_`](generated/torch.Tensor.triu_.html#torch.Tensor.triu_ "torch.Tensor.triu_")
    | In-place version of [`triu()`](generated/torch.Tensor.triu.html#torch.Tensor.triu
    "torch.Tensor.triu") |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.triu_`](generated/torch.Tensor.triu_.html#torch.Tensor.triu_ "torch.Tensor.triu_")
    | [`triu()`](generated/torch.Tensor.triu.html#torch.Tensor.triu "torch.Tensor.triu")
    的原地版本 |'
- en: '| [`Tensor.true_divide`](generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide
    "torch.Tensor.true_divide") | See [`torch.true_divide()`](generated/torch.true_divide.html#torch.true_divide
    "torch.true_divide") |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.true_divide`](generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide
    "torch.Tensor.true_divide") | 参见 [`torch.true_divide()`](generated/torch.true_divide.html#torch.true_divide
    "torch.true_divide") |'
- en: '| [`Tensor.true_divide_`](generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_
    "torch.Tensor.true_divide_") | In-place version of [`true_divide_()`](generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_
    "torch.Tensor.true_divide_") |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.true_divide_`](generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_
    "torch.Tensor.true_divide_") | `true_divide_()` 的原位版本 |'
- en: '| [`Tensor.trunc`](generated/torch.Tensor.trunc.html#torch.Tensor.trunc "torch.Tensor.trunc")
    | See [`torch.trunc()`](generated/torch.trunc.html#torch.trunc "torch.trunc")
    |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.trunc`](generated/torch.Tensor.trunc.html#torch.Tensor.trunc "torch.Tensor.trunc")
    | 参见 `torch.trunc()` |'
- en: '| [`Tensor.trunc_`](generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_
    "torch.Tensor.trunc_") | In-place version of [`trunc()`](generated/torch.Tensor.trunc.html#torch.Tensor.trunc
    "torch.Tensor.trunc") |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.trunc_`](generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_
    "torch.Tensor.trunc_") | `trunc()` 的原位版本 |'
- en: '| [`Tensor.type`](generated/torch.Tensor.type.html#torch.Tensor.type "torch.Tensor.type")
    | Returns the type if dtype is not provided, else casts this object to the specified
    type. |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.type`](generated/torch.Tensor.type.html#torch.Tensor.type "torch.Tensor.type")
    | 如果未提供 dtype，则返回类型，否则将此对象转换为指定类型 |'
- en: '| [`Tensor.type_as`](generated/torch.Tensor.type_as.html#torch.Tensor.type_as
    "torch.Tensor.type_as") | Returns this tensor cast to the type of the given tensor.
    |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.type_as`](generated/torch.Tensor.type_as.html#torch.Tensor.type_as
    "torch.Tensor.type_as") | 返回此张量转换为给定张量类型的结果 |'
- en: '| [`Tensor.unbind`](generated/torch.Tensor.unbind.html#torch.Tensor.unbind
    "torch.Tensor.unbind") | See [`torch.unbind()`](generated/torch.unbind.html#torch.unbind
    "torch.unbind") |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.unbind`](generated/torch.Tensor.unbind.html#torch.Tensor.unbind
    "torch.Tensor.unbind") | 参见 `torch.unbind()` |'
- en: '| [`Tensor.unflatten`](generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten
    "torch.Tensor.unflatten") | See [`torch.unflatten()`](generated/torch.unflatten.html#torch.unflatten
    "torch.unflatten"). |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.unflatten`](generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten
    "torch.Tensor.unflatten") | 参见 `torch.unflatten()` |'
- en: '| [`Tensor.unfold`](generated/torch.Tensor.unfold.html#torch.Tensor.unfold
    "torch.Tensor.unfold") | Returns a view of the original tensor which contains
    all slices of size `size` from `self` tensor in the dimension `dimension`. |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.unfold`](generated/torch.Tensor.unfold.html#torch.Tensor.unfold
    "torch.Tensor.unfold") | 返回原始张量的视图，其中包含 `self` 张量在维度 `dimension` 中大小为 `size` 的所有切片
    |'
- en: '| [`Tensor.uniform_`](generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_
    "torch.Tensor.uniform_") | Fills `self` tensor with numbers sampled from the continuous
    uniform distribution: |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.uniform_`](generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_
    "torch.Tensor.uniform_") | 用从连续均匀分布中抽样的数字填充 `self` 张量 |'
- en: '| [`Tensor.unique`](generated/torch.Tensor.unique.html#torch.Tensor.unique
    "torch.Tensor.unique") | Returns the unique elements of the input tensor. |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.unique`](generated/torch.Tensor.unique.html#torch.Tensor.unique
    "torch.Tensor.unique") | 返回输入张量的唯一元素 |'
- en: '| [`Tensor.unique_consecutive`](generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive
    "torch.Tensor.unique_consecutive") | Eliminates all but the first element from
    every consecutive group of equivalent elements. |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.unique_consecutive`](generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive
    "torch.Tensor.unique_consecutive") | 消除每个连续等价元素组的除第一个元素之外的所有元素 |'
- en: '| [`Tensor.unsqueeze`](generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze
    "torch.Tensor.unsqueeze") | See [`torch.unsqueeze()`](generated/torch.unsqueeze.html#torch.unsqueeze
    "torch.unsqueeze") |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.unsqueeze`](generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze
    "torch.Tensor.unsqueeze") | 参见 `torch.unsqueeze()` |'
- en: '| [`Tensor.unsqueeze_`](generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_
    "torch.Tensor.unsqueeze_") | In-place version of [`unsqueeze()`](generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze
    "torch.Tensor.unsqueeze") |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.unsqueeze_`](generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_
    "torch.Tensor.unsqueeze_") | `unsqueeze()` 的原位版本 |'
- en: '| [`Tensor.values`](generated/torch.Tensor.values.html#torch.Tensor.values
    "torch.Tensor.values") | Return the values tensor of a [sparse COO tensor](sparse.html#sparse-coo-docs).
    |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.values`](generated/torch.Tensor.values.html#torch.Tensor.values
    "torch.Tensor.values") | 返回 [稀疏 COO 张量](sparse.html#sparse-coo-docs) 的值张量 |'
- en: '| [`Tensor.var`](generated/torch.Tensor.var.html#torch.Tensor.var "torch.Tensor.var")
    | See [`torch.var()`](generated/torch.var.html#torch.var "torch.var") |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.var`](generated/torch.Tensor.var.html#torch.Tensor.var "torch.Tensor.var")
    | 参见 `torch.var()` |'
- en: '| [`Tensor.vdot`](generated/torch.Tensor.vdot.html#torch.Tensor.vdot "torch.Tensor.vdot")
    | See [`torch.vdot()`](generated/torch.vdot.html#torch.vdot "torch.vdot") |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.vdot`](generated/torch.Tensor.vdot.html#torch.Tensor.vdot "torch.Tensor.vdot")
    | 参见 `torch.vdot()` |'
- en: '| [`Tensor.view`](generated/torch.Tensor.view.html#torch.Tensor.view "torch.Tensor.view")
    | Returns a new tensor with the same data as the `self` tensor but of a different
    `shape`. |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.view`](generated/torch.Tensor.view.html#torch.Tensor.view "torch.Tensor.view")
    | 返回一个与 `self` 张量具有相同数据但不同 `shape` 的新张量 |'
- en: '| [`Tensor.view_as`](generated/torch.Tensor.view_as.html#torch.Tensor.view_as
    "torch.Tensor.view_as") | View this tensor as the same size as `other`. |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.view_as`](generated/torch.Tensor.view_as.html#torch.Tensor.view_as
    "torch.Tensor.view_as") | 将此张量视为与 `other` 相同大小 |'
- en: '| [`Tensor.vsplit`](generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit
    "torch.Tensor.vsplit") | See [`torch.vsplit()`](generated/torch.vsplit.html#torch.vsplit
    "torch.vsplit") |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.vsplit`](generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit
    "torch.Tensor.vsplit") | 参见 `torch.vsplit()` |'
- en: '| [`Tensor.where`](generated/torch.Tensor.where.html#torch.Tensor.where "torch.Tensor.where")
    | `self.where(condition, y)` is equivalent to `torch.where(condition, self, y)`.
    |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.where`](generated/torch.Tensor.where.html#torch.Tensor.where "torch.Tensor.where")
    | `self.where(condition, y)` 等同于 `torch.where(condition, self, y)` |'
- en: '| [`Tensor.xlogy`](generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy "torch.Tensor.xlogy")
    | See [`torch.xlogy()`](generated/torch.xlogy.html#torch.xlogy "torch.xlogy")
    |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.xlogy`](generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy "torch.Tensor.xlogy")
    | 查看 [`torch.xlogy()`](generated/torch.xlogy.html#torch.xlogy "torch.xlogy") |'
- en: '| [`Tensor.xlogy_`](generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_
    "torch.Tensor.xlogy_") | In-place version of [`xlogy()`](generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy
    "torch.Tensor.xlogy") |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.xlogy_`](generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_
    "torch.Tensor.xlogy_") | [`xlogy()`](generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy
    "torch.Tensor.xlogy") 的原地版本 |'
- en: '| [`Tensor.zero_`](generated/torch.Tensor.zero_.html#torch.Tensor.zero_ "torch.Tensor.zero_")
    | Fills `self` tensor with zeros. |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| [`Tensor.zero_`](generated/torch.Tensor.zero_.html#torch.Tensor.zero_ "torch.Tensor.zero_")
    | 用零填充 `self` 张量。 |'
