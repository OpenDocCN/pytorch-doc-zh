- en: 'TorchMultimodal Tutorial: Finetuning FLAVA'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TorchMultimodal 教程：微调 FLAVA
- en: 原文：[https://pytorch.org/tutorials/beginner/flava_finetuning_tutorial.html](https://pytorch.org/tutorials/beginner/flava_finetuning_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pytorch.org/tutorials/beginner/flava_finetuning_tutorial.html](https://pytorch.org/tutorials/beginner/flava_finetuning_tutorial.html)
- en: Note
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Click [here](#sphx-glr-download-beginner-flava-finetuning-tutorial-py) to download
    the full example code
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 点击[这里](#sphx-glr-download-beginner-flava-finetuning-tutorial-py)下载完整示例代码
- en: Multimodal AI has recently become very popular owing to its ubiquitous nature,
    from use cases like image captioning and visual search to more recent applications
    like image generation from text. **TorchMultimodal is a library powered by Pytorch
    consisting of building blocks and end to end examples, aiming to enable and accelerate
    research in multimodality**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态人工智能最近变得非常流行，因为它的普遍性，从图像字幕和视觉搜索等用例到最近的应用，如根据文本生成图像。**TorchMultimodal 是一个由
    Pytorch 提供支持的库，包含构建模块和端到端示例，旨在促进和加速多模态研究**。
- en: In this tutorial, we will demonstrate how to use a **pretrained SoTA model called**
    [FLAVA](https://arxiv.org/pdf/2112.04482.pdf) **from TorchMultimodal library to
    finetune on a multimodal task i.e. visual question answering** (VQA). The model
    consists of two unimodal transformer based encoders for text and image and a multimodal
    encoder to combine the two embeddings. It is pretrained using contrastive, image
    text matching and text, image and multimodal masking losses.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将演示如何使用 TorchMultimodal 库中的 **预训练 SoTA 模型** [FLAVA](https://arxiv.org/pdf/2112.04482.pdf)
    **进行多模态任务微调，即视觉问答**（VQA）。该模型由两个基于 transformer 的文本和图像单模态编码器以及一个多模态编码器组成，用于组合这两个嵌入。它使用对比、图像文本匹配以及文本、图像和多模态掩码损失进行预训练。
- en: Installation
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: We will use TextVQA dataset and `bert tokenizer` from Hugging Face for this
    tutorial. So you need to install datasets and transformers in addition to TorchMultimodal.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本教程中使用 TextVQA 数据集和 Hugging Face 的 `bert tokenizer`。因此，除了 TorchMultimodal，您还需要安装
    datasets 和 transformers。
- en: Note
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'When running this tutorial in Google Colab, install the required packages by
    creating a new cell and running the following commands:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 中运行本教程时，请通过创建一个新单元格并运行以下命令来安装所需的包：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Steps
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤
- en: 'Download the Hugging Face dataset to a directory on your computer by running
    the following command:'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令将 Hugging Face 数据集下载到计算机上的一个目录中：
- en: '[PRE1]'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: If you are running this tutorial in Google Colab, run these commands in a new
    cell and prepend these commands with an exclamation mark (!)
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您在 Google Colab 中运行本教程，请在新单元格中运行这些命令，并在这些命令前加上感叹号（!）。
- en: For this tutorial, we treat VQA as a classification task where the inputs are
    images and question (text) and the output is an answer class. So we need to download
    the vocab file with answer classes and create the answer to label mapping.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本教程中，我们将将 VQA 视为一个分类任务，其中输入是图像和问题（文本），输出是一个答案类别。因此，我们需要下载包含答案类别的词汇文件，并创建答案到标签的映射。
- en: We also load the [textvqa dataset](https://arxiv.org/pdf/1904.08920.pdf) containing
    34602 training samples (images,questions and answers) from Hugging Face
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还从 Hugging Face 加载包含 34602 个训练样本（图像、问题和答案）的 [textvqa 数据集](https://arxiv.org/pdf/1904.08920.pdf)。
- en: We see there are 3997 answer classes including a class representing unknown
    answers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到有 3997 个答案类别，包括一个代表未知答案的类别。
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Lets display a sample entry from the dataset:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示数据集中的一个样本条目：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![flava finetuning tutorial](../Images/76fa83b90558dee8d4886152e84c9896.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![flava finetuning tutorial](../Images/76fa83b90558dee8d4886152e84c9896.png)'
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '3\. Next, we write the transform function to convert the image and text into
    Tensors consumable by our model - For images, we use the transforms from torchvision
    to convert to Tensor and resize to uniform sizes - For text, we tokenize (and
    pad) them using the `BertTokenizer` from Hugging Face - For answers (i.e. labels),
    we take the most frequently occurring answer as the label to train with:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 接下来，我们编写转换函数，将图像和文本转换为模型可消耗的张量 - 对于图像，我们使用 torchvision 中的转换将其转换为张量并调整为统一大小
    - 对于文本，我们使用 Hugging Face 的 `BertTokenizer` 对其进行标记化（和填充） - 对于答案（即标签），我们将最常出现的答案作为训练标签：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 4\. Finally, we import the `flava_model_for_classification` from `torchmultimodal`.
    It loads the pretrained FLAVA checkpoint by default and includes a classification
    head.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 最后，我们从 `torchmultimodal` 中导入 `flava_model_for_classification`。它默认加载预训练的
    FLAVA 检查点，并包含一个分类头。
- en: The model forward function passes the image through the visual encoder and the
    question through the text encoder. The image and question embeddings are then
    passed through the multimodal encoder. The final embedding corresponding to the
    CLS token is passed through a MLP head which finally gives the probability distribution
    over each possible answers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的前向函数将图像通过视觉编码器，问题通过文本编码器。然后将图像和问题嵌入传递给多模态编码器。对应于 CLS 令牌的最终嵌入通过 MLP 头传递，最终给出每个可能答案的概率分布。
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '5\. We put together the dataset and model in a toy training loop to demonstrate
    how to train the model for 3 iterations:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 我们将数据集和模型放在一个玩具训练循环中，以演示如何训练模型进行 3 次迭代：
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Conclusion
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: This tutorial introduced the basics around how to finetune on a multimodal task
    using FLAVA from TorchMultimodal. Please also check out other examples from the
    library like [MDETR](https://github.com/facebookresearch/multimodal/tree/main/torchmultimodal/models/mdetr)
    which is a multimodal model for object detection and [Omnivore](https://github.com/facebookresearch/multimodal/blob/main/torchmultimodal/models/omnivore.py)
    which is multitask model spanning image, video and 3d classification.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程介绍了如何使用 TorchMultimodal 中的 FLAVA 在多模态任务上进行微调的基础知识。请还查看库中的其他示例，如 [MDETR](https://github.com/facebookresearch/multimodal/tree/main/torchmultimodal/models/mdetr)，这是一个用于目标检测的多模态模型，以及
    [Omnivore](https://github.com/facebookresearch/multimodal/blob/main/torchmultimodal/models/omnivore.py)，这是一个跨图像、视频和
    3D 分类的多任务模型。
- en: '**Total running time of the script:** ( 2 minutes 31.510 seconds)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本的总运行时间：**（2 分 31.510 秒）'
- en: '[`Download Python source code: flava_finetuning_tutorial.py`](../_downloads/03d2b0c71eeabf3687d88081641d7a1c/flava_finetuning_tutorial.py)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[下载Python源代码：flava_finetuning_tutorial.py](../_downloads/03d2b0c71eeabf3687d88081641d7a1c/flava_finetuning_tutorial.py)'
- en: '[`Download Jupyter notebook: flava_finetuning_tutorial.ipynb`](../_downloads/3f1a1757bef27416aec84c890db7b50d/flava_finetuning_tutorial.ipynb)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[下载Jupyter笔记本：flava_finetuning_tutorial.ipynb](../_downloads/3f1a1757bef27416aec84c890db7b50d/flava_finetuning_tutorial.ipynb)'
- en: '[Gallery generated by Sphinx-Gallery](https://sphinx-gallery.github.io)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sphinx-Gallery生成的图库](https://sphinx-gallery.github.io)'
